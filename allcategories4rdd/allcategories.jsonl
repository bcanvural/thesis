{"id": 0, "category": "Education", "skillText": "A professional degree prepares someone for a particular profession by emphasizing skills and practical analysis over theory and research. Most but not all of the professions associated with professional degrees are professions that require licensing in order to practice in the field. Professions including audiology, architecture, dentistry, dietetics, many fields of engineering, K-12 public education, law (J.D. or LL.B.), medicine (M.D., D.O. or M.B.B.S.), chiropractic, podiatric medicine, nursing, medical laboratory science, music therapy, speech-language pathology, occupational therapy, physical therapy, optometry, counseling, psychology, pharmacy, radiography, social work, urban planning, and veterinary medicine all require a person to first obtain a professional degree in the relevant subject area(s) prior to professional licensure, certification or registration.[citation needed] Other fields, such as speech-language pathology, require the professional to earn a graduate degree as well as additional required licensing, registration and certification to obtain employment.\n\nContents\n\n    1 History\n        1.1 History of first professional degrees in Europe\n        1.2 History of first professional degrees in the United States\n        1.3 Global history of first professional degrees\n    2 First professional degrees by country\n        2.1 United States\n        2.2 Other countries\n    3 First professional degrees by field of study\n        3.1 Engineering\n        3.2 Forestry\n        3.3 Medicine\n    4 Higher professional degrees\n    5 See also\n    6 References\n\nHistory\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2014) (Learn how and when to remove this template message)\nHistory of first professional degrees in Europe\n\nIn Europe, the first academic degrees were law degrees, which were doctorates (see Juris Doctor). The first entry-level trade degree to be granted in the medical industry was the M.D. degree. This degree was granted by the ancient universities of Scotland upon completion of medical school until the mid-19th century. At that time, public bodies who regulated medical practice in the UK required practitioners in Scotland, as well as England, to uniformly hold the dual Bachelor of Medicine, Bachelor of Surgery degrees (variously abbreviated M.B.B.S., B.M.B.S., M.B.Ch.B., M.B.B.Chir., B.M.B.Ch., etc.).[citation needed]\nHistory of first professional degrees in the United States\n\nThe M.B. or Bachelor of Medicine was also the first type of medical degree to be granted in the United States and Canada. The first medical schools that granted the MB degree were Penn, Harvard, Toronto, Maryland, and Columbia. These first few North American medical schools that were established were (for the most part) proprietary schools founded by physicians and surgeons who had been trained in England and Scotland. North American medical schools switched to the tradition of the ancient universities of Scotland and began granting the M.D. title rather than the M.B. primarily in the 1800s. Columbia University in New York (which at the time was referred to as King's College of Medicine) was the first American university to grant the M.D. degree instead of the M.B. The medical degree could be obtained by night school study.[1] The M.D. was the first entry-level professional degree to be awarded as a purely trade school doctorate in the United States. This was nearly 60 years before the first European-style research doctorate, the Ph.D., was awarded in the U.S. in 1861.[2] It is important to note, however, that the assignment of a doctoral-level award to U.S. professional/vocational degrees is a historical result of study deemed more advanced than bachelor's/master's level work, but should not be equivalent to a doctorate (commonly Ph.D. and Sc.D.), according to the US Department of Education (2009). The latter is not profession-oriented or vocational, but a degree of academic scholarship/expertise requiring a dissertation defense for which there is no current equivalent. In the US, there are advanced professional degrees (requiring at least a first professional degree) such as Doctor of Medical/Dental Sciences (DMSc) offered by many universities including Harvard, SJD (Doctor of Juridical Science), and the PhD (such as Pharmaceutics, Oral Biology...)\n\nPhysical therapy programs in the US have transitioned their entry-level or \"first professional degree\" from the bachelor or master to a doctorate (Doctor of Physical Therapy) as well, just as they previously changed the standard entry-level degree from a bachelor's degree to a master's degree.[3]\nGlobal history of first professional degrees\n\nThere was a world-wide movement to structure vocational programs as \"graduate-entry\" (meaning requiring a previous degree).[citation needed][tone] In countries where professional degrees are undergraduate degrees, graduate-entry undergraduate programs have been established to allow students with a previous bachelor's[clarify] to enter the profession.[4] This movement towards the graduate-entry model reflects an emphasis that has been placed on teaching professional skills at an advanced, intensive level.[5] The switch to graduate-entry also allows for a greater diversity of applicants who are more mature and motivated to study at the professional level.[6]\nFirst professional degrees by country\nUnited States\n\nAccording to the U.S. Department of Education, \"A first-professional degree was an award that required completion of a program that met all of the following criteria: (1) completion of the academic requirements to begin practice in the profession; (2) at least 2 years of college work prior to entering the program; and (3) a total of at least 6 academic years of college work to complete the degree program, including prior required college work plus the length of the professional program itself.\n\nGenerally, first-professional degrees are considered graduate level degrees. First-professional degrees may be awarded in the following fields:[7]\n\n    Architecture (M.Arch or B.Arch, with the former being the most common)\n    Chiropractic (D.C., D.C.M.)\n    Dentistry (D.D.S., D.M.D.)\n    Divinity (B.Div., M.Div)\n    Law (LL.B., J.D.)\n    Medicine (M.B., M.D.)\n    Occupational Therapy (MA or MS)\n    Optometry (O.D.)\n    Osteopathic Medicine (D.O.)\n    Pharmacy (B.Pharm, Pharm.D.)\n    Physical Therapy (D.P.T)\n    Speech-Language Pathology (MA or MS)\n    Audiology (Au.D.)\n    Podiatry (D.P.M., D.P., Pod.D.)\n    Education (B.Ed., M.Ed.)\n    Veterinary Medicine (D.V.M. or V.M.D.)\"\n\nThe use of this[ambiguous] term was discontinued in IPEDS as of the 2010-11 data collection, when use of the new post-baccalaureate award categories became mandatory.[8]\nOther countries\n\nSome schools outside the U.S. offer professional doctorates (Pr.D.) for part-time students in a broad range of full-time careers. These programs typically require three to six years of structured study towards advanced professional practice. Coursework is followed by a professional project that contributes to the students organization, industry or profession.[citation needed].\n\nIn some other countries, such as the United Kingdom, the study of vocational subjects at the undergraduate level (and post-graduate qualifications outside the academic degree structure) also play a large role in professional training.[citation needed]\n\nMost countries outside the U.S. continue to only award doctorates as higher academic research degrees. For example, in the field of architecture, the professional first degree may be either the Bachelor of Architecture or the Master of Architecture while in the field of fine art, its professional first degree is the Master of Fine Arts. There is currently some debate in the architectural community to rename the degree a \"doctorate\".[9]\n\nMany of those who obtained their first professional degree outside of the United States (which may be a bachelor's degree) are considered to have an \"equivalent\" qualification to their doctoral counterpart for professional reasons. Equivalent does not equate to right to practice, as many are deemed not equivalent enough to grant a license to practice in the United States. Even in Canada, the medical degree of Doctor of Medicine is considered an undergraduate degree. For example, a British medical degree, the MBBS, is equivalent to the US-MD. An MBBS graduate if licensed to practice medicine in the United States is, in at least one state, allowed to use the \"M.D.\" and is referred to as \"doctor\" because it accurately describes their professional role.[10]\n\nIn addition, in the Netherlands, engineering students can earn bachelor's degrees (usually BSc.) and master's degrees (usually MSc.). Those wishing to continue their education within the engineering field can continue with academic research in their field (Doctor of Philosophy or Ph.D.) or a professionally applied approach (Professional Doctorate in Engineering or PDEng).[citation needed]\nFirst professional degrees by field of study\n\nSome first professional degrees (e.g. Juris Doctor, Doctor of Medicine, Doctor of Osteopathic Medicine, Doctor of Physical Therapy, Doctor of Chiropractic, Doctor of Podiatric Medicine, Doctor of Pharmacy, Doctor of Dental Surgery, Doctor of Optometry, Doctor of Veterinary Medicine, and Doctor of Audiology) have the term \"Doctor\" in the title. While such degrees are considered professional doctorates and are entitled to use the title of \"doctor,\" they are not equivalent to the Ph.D. in that Ph.D. students generally complete a program that includes the production of a dissertation that adds to the knowledge in the student's field, according to the U.S. Department of Education (2008). The minimum credit requirement for a doctoral level degree, such as a Ph.D., is 72 graduate level credits per the U.S. Department of Education (2008). However, this number is based on the fact that the majority of Phd students have received master's degrees before being accepted into the Phd program. Also, the actual credits may vary between institutions and degrees. More importantly, students taking PhD courses are assumed to have mastered the materials in the field taught at the undergraduate or Masters level. On the other hand, a juris doctorate (law) degree has a minimum of 88 units. Professional school students are not required to have had prior training in their field of study and classes are taught the undergraduate level. This is probably one of the reasons very few (if any) professional degree holders are considered equivalent to their PhD counterparts in academia. The typical Ph.D.-holder earns an advanced education, learning how to conduct research while an M.D.-holder earns a vocational skill (training), learning how to \"do\" as would other vocational graduates. (Note, however, that law professors commonly insist that they are teaching future lawyers \"how to think\"). As such, an M.D. (or similar professional degrees) is not considered a doctorate in academia. Even so, some lecturers, instructors, and professors in U.S. medical schools may just possess the M.D. or D.O. and board training in their field, without also having a Ph.D. or other graduate degree.\nEngineering\n\nIn engineering, Bachelor of Engineering and Bachelor of Applied Science degrees are commonly awarded in the U.K. and Canada, respectively, and the Bachelor of Science in an engineering field is awarded in the U.S. The advanced professional degree usually awarded is the Master of Engineering (M.Eng.), although some schools have the option of an engineer's degree. In several British commonwealth countries the Chartered Engineer (requires M.Eng. degree or City and Guilds Graduate Diploma). The Chartered Engineer qualifications represents the final stage of a fully qualified professional engineer including both academic, professional practice, and competency components. In South America, the professional title Ingeniero is the first level to qualify as a Professional Engineer. The terminal academic research degree is the Ph.D., Sc.D. or Eng.D.[citation needed]. The professional engineer (P.Eng./P.E.) designation in Canada and the U.S. is license to practice engineering in the public domain and should not be confused with academic qualifications.\nForestry\n\nIn the United States, a first professional degree in forestry may be awarded at either the bachelor's or master's level[citation needed].\nMedicine\n\nIn medicine, the distinction between first professional and advanced degrees depends on geography. Outside North America and Germany, the first professional degree in medicine is a Bachelor of Medicine and Surgery (B.M., Ch.B.), (M.B.B.S.), while an advanced professional degree can be a Master of Science (e.g. surgery), and the terminal academic research degree can be a Doctor of Medicine (non-US M.D.) or a Ph.D. in a medical science (e.g. anatomy). To be eligible to apply for an M.D. degree from a U.K. or Commonwealth University, one must hold either an M.B.B.S., M.B.Ch.B., B.M.B.S., B.Med., B.M., or US-MD degree and have at least five years of postgraduate experience. In the U.S., there are two types of physicians: Doctor of Medicine M.D. and Doctor of Osteopathic Medicine D.O. and both have unlimited full scope of practice for medicine.\nHigher professional degrees\nSome professional fields offer degrees beyond the first professional degree. For instance, in the U.S., in order to earn an LL.M., one must have received a J.D. Likewise, S.J.D. candidates must generally have an LL.M., although in rare circumstances S.J.D. candidates are admitted based on their first professional degree. Also, in the field of dentistry, M.S.D. (Masters of Science in Dentistry) applicants must hold a D.D.S./B.Dent./D.M.D./B.D.S. before admission to master's programs in dentistry, and a Ph.D. in Dental Science requires either a M.S.D. or D.D.S./B.Dent./D.M.D./B.D.S. Joint M.D./Ph.D. students in the U.S. must be accepted by both the school of medicine and the graduate school of the same institution.", "skillName": "First_professional_degree."}
{"id": 1, "category": "Education", "skillText": "The Bologna process for standardisation of European higher education specified an undergraduate degree of at least three years called the \"licence\" or bachelor's degree, followed by a two-year diploma called the master's degree, then a doctorate, meant to be obtained in at least three years. Because of these indicated schedules, the reform is sometimes (erroneously) referred to as \"3-5-8\". The system applies to the European Higher Education Area.\n\nContents\n\n    1 European Master's Market\n    2 Austria\n    3 Belgium\n    4 Denmark\n    5 Finland\n    6 France\n    7 Germany\n    8 Ireland\n    9 Italy\n    10 Lithuania\n    11 Netherlands\n    12 Norway\n    13 Poland\n    14 Portugal\n        14.1 Prior to the Bologna Process\n        14.2 After the Bologna Process\n    15 Sweden\n        15.1 Prior to the Bologna Process\n            15.1.1 Engineering\n        15.2 After the Bologna Process\n    16 United Kingdom\n        16.1 Undergraduate Master's Courses\n            16.1.1 With Dissertation\n            16.1.2 Without Dissertation\n        16.2 Postgraduate Master's degrees\n            16.2.1 Taught postgraduate Master's degrees\n            16.2.2 Research postgraduate Master's degrees\n        16.3 MAs in Oxford, Cambridge and Dublin\n        16.4 Scottish MA\n    17 Other approaches\n    18 References\n\nEuropean Master's Market\n\nThrough the Bologna initiatives and support of the European Union, Europe is unifying and standardising especially the structure of their masters' programmes, making them more and more accessible to foreign students.\n\nAn often cited advantage of the European universities is an advantageous cost/quality ratio. In Europe, especially continental Europe, universities are heavily subsidized by their national governments. In Germany, Scandinavia or Eastern Europe for instance, most masters programmes are totally free of charge. Recently, these governments are discussing the introduction of tuition fees. Sweden started charging tuition for non-EU students in 2010.\n\nIn the recent publication of the Times Higher Education Supplement, 36 of the top 100 universities in the world are located in Europe. There are large regional differences in the tuition fees in those top 100 universities:\n\nReputation of Universities.png International tuition fees.png\nAustria\nMain article: Diplom\n\nIn Austria, one obtains a bachelor's degree after 3 years of study and a master's degree after 2 more years of study. This is true for both the \"research-oriented university\" sector as well as the \"university of applied sciences\" sector which had been established in the 1990s.\n\nMedicine and dentistry pose an exception; these studies are not divided into bachelor's and master's degree, but take 6 years to complete and the degree obtained is called \"Dr. med.\" (However this is not an equivalent to other doctoral degrees, as one writes a \"diploma thesis\" and not a \"doctoral thesis\" or \"dissertation\".)\n\nIn addition to traditional master's degrees, Austrian universities also offer the Master of Advanced Studies which is a non-consecutive continuing education degree the degree. MAS programs tend to be interdisciplinary and tend to be focused toward meeting the needs of professionals rather than academics.\n\nBefore the Bologna process, the traditional Austrian equivalent to the master's degree was the Diplomstudium, leading to the title Diplom-Ingenieur (female title: Diplom-Ingenieurin)(Abbreviation: \"Dipl.-Ing.\" or \"DI\") in engineering or Magister (female: Magistra)(Abbreviation: \"Mag.\") in almost every discipline. The Diplomstudium took about 4–6 years of study.\nBelgium\n\nIn Belgium, possessing a master's degree means that one has completed a higher education (usually university or college) programme of 4 or 5 years. Before the Bologna process most university degrees required 4 years of studies (leading to a licence), but some programmes required 5 years of study. An example in the field of education in business/management was the 5-year programme of \"Handelsingenieur\" (Dutch) or \"Ingénieur de Gestion\" (French) (English: \"Commercial Engineer\") with an important amount of mathematics and sciences, and which corresponds to an M.Sc. in Management. This degree co-existed with a graduate degree in business economics (4 years) named \"Licentiaat in toegepaste economische wetenschappen\" (Dutch) or \"Licence en sciences économiques appliquées\" (French) (English: \"Licence in applied economics\").\nDenmark\n\nIn Denmark, a Master's degree is awarded. The MA and M.Sc. degrees and other master's degrees are distinguished. The MA and M.Sc. degrees are similar to a traditional Master's Programme, which are obtained by completing a higher education with a typical duration of five years on an accredited Danish university. Other master's degrees can be taken on an accredited Danish university, but these are made as adult (part-time) education such as the Master of IT (abbreviated M. IT) degree.\n\nA large number of subdivisions exist, usually designating the area of education (e.g. cand.theol., cand.arch. and cand.jur.), though some have more vague definitions (cand.mag., cand.scient., cand.polyt., and cand.scient.techn., each of which encompass broad, overlapping areas of science).\n\nThe Bologna process has widely prompted master's degree education to consist of either 120 ECTS or 180 ECTS credit cycles, where one academic year corresponds to 60 ECTS-credits that are equivalent to 1,500–1,800 hours of study. In most cases, these will take 2 to 3 years respectively to complete.\n\n    1st cycle: typically 180–240 ECTS credits, usually awarding a bachelor's degree. The European Higher Education Area did not introduce the Bachelor with Honours programme, which allows graduates with a \"BA hons.\" degree.\n    2nd cycle: typically 90–120 ECTS credits (a minimum of 60 on 2nd-cycle level). Usually awarding a master's degree.\n\nFinland\n\nIn Finland, the introduction of the Bologna Process has standardized most of the degrees into the European model. The master's degree takes 2–3 years (120 ECTS units) after the bachelor's degree. In English-speaking usage, the degree title is named after the particular faculty of study. In Finnish, the degree is called maisteri in most fields. When precision is needed, the term ylempi korkeakoulututkinto is used to denote all degrees of Master's level. Literally, this translates into English as higher diploma of higher education.\n\nMedicine-related fields of medicine, dentistry, and veterinary medicine pose an exception to Bologna system. In medical fields, the Licenciate (Finnish: lisensiaatti, Swedish: licensiat) is an equivalent degree, the completion of which takes five (dentistry) or six years (medicine and veterinary), while the Bachelor of Medicine's degree (Finnish: lääketieteen kandidaatti) is gained after second year of studies. In fields other than medicine, the Licentiate's degree is a post-graduate degree higher than Master's but lower than doctor's.\n\nIn Engineering, the higher degree is either diplomi-insinööri (Swedish: diplomingenjör, literally \"Engineer with diploma\") or arkkitehti (Swedish: arkitekt, English: Architect) although in international use MSc is used. In Pharmacy, the degree is proviisori (Swedish: provisor). All such degrees retaining their historical name are classified as master's degrees (ylempi korkeakoulututkinto) and in English usage, they are always translated as master's degrees. Some other master's degrees give the right to use the traditional title of the degree-holder. Most importantly, the degree of Master of Science in Economics and Business Administration gives the right to use the title of ekonomi, while the Masters of Science in Agriculture and Forestry may use the titles of metsänhoitaja (Forester) or agronomi (Agronomist) depending on their field of study.\nFrance\n\nIn France and many countries which follow the French model (like the Francophone regions in Switzerland, Belgium, Lebanon, Algeria, Morocco, Tunisia), higher education is divided in three tiers (cycles universitaires), leading to different degrees. The master's degree corresponds to the second tier. A master's degree is awarded to the holders of\n\n    master's diploma (diplôme de master). It is the more common master's degree. It is awarded mainly by universities in two principal qualifications\n        Master of Research (master recherche) the science-oriented degree, necessary step to proceed to doctoral studies. It can be a largely research degree, a taught one or a mix between the two.\n        vocational master (master professionnel) aimed at gaining working qualification. Usually a taught degree with internship required.\n    grandes écoles diploma. Not all grandes écoles diplomas programs are accredited by the State. Some grandes écoles deliver also master's diplomas, but their own diploma remains more prestigious.\n    Engineer's degree.\n    Architect's degree.\n    Some degrees from Schools of Fine Arts.\n\nFrance is also host to a number of private American-style universities like The American University of Paris \nor Schiller International University \n, that offer accredited American master's degrees in Europe. Admission into these Master's programs requires a completed American undergraduate degree or a similar French/European degree that can be acquired in four years of study.\nGermany\n\nDue to the EU-wide Bologna process, the traditional German academic degrees Diplom and Magister have mostly been replaced by the undergraduate Bachelor (3-4 year study programme) and postgraduate Master's degree (1-2 year study programme).\n\nIn Germany the Diplom (first degree after (usually) 4–6 years - from either a Universität (University), a Technische Hochschule or a Kunsthochschule with university status) and the Magister had traditionally been equivalated to the master's degree, the Magister being a degree after the study of two or three subjects (one main and one or two subsidiary subjects), as common in Humanities or Liberal Arts, whereas the Diplom is awarded after the study of one subject, commonly found in Natural Sciences, Social Sciences, Formal sciences and some Applied Sciences. The Fachhochschulen or Universities of Applied Sciences conferred the Diplom (FH), whose length of study is between the bachelor's and master's degree.\n\nUnder the harmonised system there is no legal academic difference between the bachelor's and master's degrees conferred by the Fachhochschulen and Universitäten.\n\nThe German Meister qualification for a master craftsman is neither a degree nor is it comparable to the academic master's degree. It, however, qualifies the holder to study at a University or Fachhochschule, whether the Meister holds the regular entry qualification (Abitur or Fachhochschulreife) or not.[1]\nIreland\n\nPostgraduate master's degrees in Ireland can either be taught degrees involving lectures, examination and a short dissertation, or research degrees. They usually are one of: MA (except Trinity College Dublin, where this is an undergraduate degree awarded 21 terms after matriculation such as in MAs awarded by the University of Oxford, University of Cambridge and University of Dublin) or MA, M.Sc., MBA, MAI, ME/MEng/MEngSc, MPhil, LLM, MLitt, MArch, MAgrSc, MSocSc, MCH, MAcc, MEconSc.\n\nWith respect to NUI post graduate qualifications, in general there is a simple distinction between MA and MPhil. An MA is a combination of taught (classroom) and research-based modules, whilst an MPhil is composed exclusively of research-based learning.[2]\n\nThe Magister in Arte Ingeniaria (MAI), literally meaning 'Master in the Art of Engineering', is awarded by the University of Dublin, Ireland, and is more usually referred to as Master of Engineering. While still available (via two routes), historically it was the engineering master's degree taken by the university's BAI graduates. Today the more common engineering master's degree in the University of Dublin is the M.Sc..\n\nA Master of Business Studies (MBS) refers to a qualification in the degree of master that can be obtained by students of recognized universities and colleges who complete the relevant approved programmes of study, pass the prescribed examinations, and fulfil all other prescribed conditions. An MBS can be studied in the following areas: Electronic Business, Finance, Human Resource Management, International Business, Management Information System, Management & Organisation Studies, Management Consultancy, Marketing, Project Management, Strategic Management & Planning and can be obtained from many universities in Ireland including University College Dublin.\n\nThe other universities in Ireland usually award a MEngSc, M.E., MEng or M.Sc. for their postgraduate master's degree in engineering.\nItaly\n\nThe old university system (Vecchio Ordinamento) consisted in a unique course, extended from four to five years or maximum of six (only Medicine), with a variable period (six-twelve months usually) for the thesis work. After the thesis discussion, students got the Master's Degree, simply called Laurea.\n\nThis system was reformed in 1999/2000 to comply to the Bologna process directives. The new university system (Nuovo Ordinamento) includes two levels of degrees: a three-year Bachelor's degree, called Laurea di Primo Livello or just Laurea (e.g. Laurea di Primo Livello in Ingegneria Elettronica is Bachelor of Science in Electronic Engineering) and a two-year course of specialization, leading to a master's degree called Laurea di Secondo Livello, Laurea Magistrale (e.g. Laurea Specialistica in Ingegneria Elettronica is Master of Science in Electronic Engineering). Both degrees include a thesis work with final discussion.\n\nA student can apply for the Ph.D. level course, called Dottorato di Ricerca, only after getting a Master's degree.\n\nMedicine and some other school (\"Facoltà\"), notably Law, have adopted the reformed system only partially, keeping the previous unique course. Medicine is therefore still a six-year course followed, possibly, by the specialization, requiring from three to six years more.\n\nHowever, these Facoltà also have other courses organized according to the new system (e.g., Tecniche di radiologia medica for Medicine, Consulente del lavoro for Law)\nLithuania\n[icon] \tThis section is empty. You can help by adding to it. (March 2013)\nNetherlands\n\nIn 2002, the Dutch degree system was changed to abide by international standards. This process was complicated by the fact that the Dutch higher education system has two separate branches, Hoger Beroeps Onderwijs (HBO, which indicates College or \"University of Professional Education\" level), and Wetenschappelijk Onderwijs (WO, which indicates University level). HBO level education focuses more on practical and professional education while WO is academic and scientific.\n\nBefore the Bachelor/Master system was introduced, HBO graduates received the title baccalaureus (with the corresponding pre-nominal abbreviation \"bc.\"), which was rarely used. On the other hand, the HBO graduates with an engineering degree used the degree ingenieur, with pre-nominal abbreviation \"ing.\", which was (and still is) used quite commonly. WO degrees consisted of several different titles, such as doctorandus (pre-nominal abbreviated to drs., corresponds to MA or MSc), ingenieur (ir. for WO level, corresponds to MSc) and meester in de rechten (mr., corresponds to LL.M.) These former titles are no longer granted (although they are still used, protected, and interchangeable with MA and MSc titles). The title of doctor (dr., corresponding to the PhD degree) is still awarded.\n\nPrior to the education reform, a single program leading to the doctorandus, ingenieur or meester degree was in effect, which comprised the same course load as the Bachelor and Master programs put together. Those who had already started the doctorandus, ingenieur or meester program could, upon completing it, opt for the old degree (before their name), or simply use the master's degree (behind their name) in accordance with the new standard. Since these graduates do not have a separate bachelor's degree (which is in fact – in retrospect – incorporated into the program), the master's degree is their first academic degree.\n\nIn the new system, completed college (HBO) degrees are equivalent to a bachelor's degree and are abbreviated to \"B\" with a subject suffix. Universities (WO) grant a bachelor's degree for the general portion of the curriculum. This degree is a \"Bachelor of Science\" or \"Bachelor of Arts\" with the appropriate suffix.\n\nBefore one is admitted to a Master's program, one must have obtained a bachelor's degree in the same field of study at the same level (although exceptions to this rule are possible, if the bachelor's degree has nearly been obtained). This means that someone with a HBO Bachelor's degree cannot start a WO Master program; still, many universities offer a so-called 'bridge year', in which HBO degree holders can attain the WO Bachelor and continue into the WO Master program.\n\nAll fully completed curricula in the Netherlands are equivalent to master's degrees with the addition of a \"of Science\" or \"of Arts\" to distinguish them from HBO Master's degrees, which are known simply as Master. WO Master's degrees focus on specialization in a sub-area of the general bachelor's degree subject and typically take 1 year except for research masters, engineering studies and medical school where the Master takes 2, 2 and 3 years, respectively.\n\nHBO Master's are usually started only after several years of work and are similarly focusses on specialization. The title is signified by the abbreviation M and therefore an MBA would indicate a HBO Master's degree in business administration, but use of the MBA title is protected and it can only be granted by accredited schools.\nNorway\n\nAs a result of the Bologna-process and the Quality reform, the degree system of Norwegian higher education consists of the two main levels Bachelor's degree and Master's degree. A Bachelor's degree at a Norwegian university/university college is equivalent to an undergraduate degree and takes three years (with the exception of the teaching courses, where a bachelor's degree lasts for four years). The master's degrees are either fully integrated five-year programmes (admission does not require undergraduate degree) leading up to a graduate degree, or two-year courses at graduate level which require an already completed undergraduate degree. Following the graduate level, education is given at the doctoral level, usually through a four-year research fellowship leading to a PhD.\n\nBefore the implementation of this system, various titles were given in accordance with the field of study and the length of the course. For instance, a three-year undergraduate degree in engineering would give the title \"høgskoleingeniør\" (Bachelor's degree), and a 4,5 to 5 year graduate degree in engineering would give the title \"sivilingeniør\" (Master's degree). That being said, these titles are still very common and are, although formally abolished, degrees granted earlier are still being used, also by academic personnel.\nPoland\n\nCurrently there are two models of higher education in Poland.\n\nIn the traditional model, a master's degree is awarded after completion of a university curriculum — a 5-year programme in science courses at a university or other similar institution, with a project in the final year called magisterium (it can be translated as a Master of Arts or a Master of Science thesis) that often requires carrying out research in a given field. An MA degree is called a magister (abbreviated mgr) except for medical education, where it is called a lekarz (this gives the holder the right to use the title of physician and surgeon), a lekarz weterynarii in the veterinary field and a dentysta in field of dentistry. Universities of technology usually give the title of magister inżynier (abbreviated mgr inż.) corresponding to an MSc Eng degree.\n\nMore and more institutions introduce another model, which as of 2005 is still less popular. In this model, following the Bologna process directives, higher education is split into a 3 to 4-year Bachelor programme ending with a title of licencjat (non-technical) or inżynier (technical fields), and a 2-year programme (uzupełniające studia magisterskie) giving the title of magister or magister inżynier. Nevertheless, even in these institutions, it is often possible to bridge the Bachelor education directly into the Master programme, without formally obtaining the licencjat degree, thus shortening the time needed for completing the education slightly.\n\nDepending on field and school, the timing may be slightly different.\nPortugal\nPrior to the Bologna Process\n\nPrior to the full implementation of the Bologna Process in July 2007 degrees in Portugal could be divided between Bacharelato (three years), Licenciatura (four years), Mestrado (Licenciatura + 2–3 years of postgraduate studies) and doutoramento (Mestrado + 4–6 years of postgraduate studies).\nAfter the Bologna Process\n\nWith the full implementation of the Bologna process in July 2007, a Licenciatura (3 years) with the criteria for the first cycle and a Mestrado ('Licenciatura' + 2 years) in line with the criteria for the second cycle. There are other posgraduated titles after some of these cycles.\nSweden\nPrior to the Bologna Process\n\nPrior to the full implementation of the Bologna Process in July 2007 degrees in Sweden could be divided between kandidat (three years), magister (four years), licentiat (magister + 2–3 years of postgraduate studies) and doktor (magister + 4–5 years of postgraduate studies).\nEngineering\n\nIn engineering disciplines M. Sc was called civilingenjör, a four and a half year academic program concluded with a thesis. There was no direct equivalent to a B.Sc, however, a three-year engineering degree with a more practical focus called högskoleingenjör was close.\nAfter the Bologna Process\n\nWith the full implementation of the Bologna process in July 2007, a kandidat (3 years) and a master (five years) was introduced in line with the criteria for the second cycle. The magister will still exist alongside the new master, but is expected to be largely neglected in favour of the new, internationally recognized degree. The M. Sc of engineering, civilingenjör, was expanded to five years and a new B. Sc was introduced to coexist with the unaltered högskoleingenjör\nUnited Kingdom\nUndergraduate Master's Courses\nWith Dissertation\n\nIn the UK, many universities now have four-year undergraduate programmes (or five-year in Scotland) mainly in the sciences or in engineering with a research project or Dissertation in the final year. The awards for these are named after the subject, so a course in mathematics would earn a Master in Mathematics degree, (abbreviated to MMath), or have a general title such as MSci (Master in Science at most universities but Master of Natural Sciences at Cambridge), MBiomed, MBiochem, MChem, MComp, MPharm, MEng, MESci, MMath, MPhys, MInf, MML, MDes, etc.\n\nIn content the first two years they are generally identical to those of the equivalent bachelor's degree while the third and fourth years are a combination of higher-level taught courses and a research project.\n\nAn example of an undergraduate master's degree in the professions in the United Kingdom is Pharmacy. In order to become a pharmacist, the undergraduate MPharm must be completed, followed by one year of pre-registration experience. A similar situation exists as regards Engineering.\n\nThe ancient universities of Scotland (St Andrews, Glasgow, Aberdeen, Edinburgh) and Dundee award a Master of Arts (MA) as an undergraduate degree after four years of study in Arts, Humanities or Social Sciences.\nWithout Dissertation\n\nThere exist undergraduate master's courses for which completion of a dissertation is not required, with attainment being measured either purely by examination, or through a combination of testing and shorter written work. One such course is the four-year Oxford MMath course, in which a dissertation is optional.[3]\n\nThe Master of Arts (MA) is awarded by the universities of Oxford, Cambridge and Trinity College, Dublin —without further examination— to those entitled to the degree of Bachelor of Arts.\nPostgraduate Master's degrees\n\nPostgraduate master's degrees in the United Kingdom can either be taught degrees involving lectures, examination and a short dissertation, or research degrees (though the latter have largely been replaced by MPhil and MRes programmes). Taught Master's programmes involve 1 or 2 years of full-time study. The programmes are often very intensive and demanding, and concentrate on one very specialised area of knowledge. Some universities also offer a Master's by Learning Contract scheme, where a candidate can specify his or her own learning objectives; these are submitted to supervising academics for approval, and are assessed by means of written reports, practical demonstrations and presentations.\nTaught postgraduate Master's degrees\n\n(MSc, MA, LL.M., MLitt, MSSc, MSt, MEnt etc.)\n\nThe most common types of postgraduate taught master's degrees are the Master of Arts (MA) awarded in Arts, Humanities, Theology and Social Sciences and the Master of Science (MSc) awarded in pure and applied Science. A number of taught programs in Social Sciences also receive the Master of Science (MSc) degree (e.g. MSc Development Studies at the London School of Economics and University of Bath).\n\nHowever, some universities - particularly those in Scotland - award the Master of Letters (MLitt) to students in the Arts, Humanities, Divinity and Social Sciences, often with the suffix (T) to indicate it is a taught degree, to avoid confusion with the MLitt. In the universities of Cambridge and Oxford on the other hand, the MPhil is a taught master's degree (normally also including a short research component), whereas the MLitt and the MSc degrees are offered as pure research degrees only. Some other universities, such as the University of Glasgow, previously used the designation MPhil for both taught and research master's degrees, but have recently changed the taught appellation to MLitt.\n\nIn Business Schools a special Masters of Business Administration MBA type of a degree is available to those who have business practice experience. For example, Salford Business School in Greater Manchester offers a degree which is only available to those who can show professional experience.\n\nIn Law the standard taught degree is the Master of Laws, but certain courses may lead to the award of MA or MLitt.\n\nUntil recently, both the undergraduate and postgraduate master's degrees were awarded without grade or class (like the class of an honours degree). Nowadays however, Master's degrees may be classified into a maximum of four categories (Distinction, Merit, Pass or Fail), while others can have a more simplified form of assessment by only distinguishing between a Pass or a Fail.\nResearch postgraduate Master's degrees\n\nThe Master of Philosophy (MPhil) is a research degree awarded for the completion of a thesis. It is a shorter version of the Ph.D. and some universities routinely enter potential PhD students into the MPhil programme and allow them to upgrade to the full PhD programme a year or two into the course. Advanced candidates for a taught postgraduate Master's sometimes undertake the MPhil as it is considered a more prestigious degree, but it may also mean that the student could not afford or could not complete the full PhD.\n\nThe Master of Research (MRes) degree is a more structured and organised version of the MPhil, usually designed to prepare a student for a career in research. For example, an MRes may combine individual research with periods of work placement in research establishments.\n\nThe Master of Letters (MLitt) degree is a two-year research degree at many universities, including Cambridge and the ancient Scottish universities, and is generally awarded when a student cannot or will not complete the final year(s) of their PhD and so writes their research up for the MLitt. Because MLitt is also used for a taught degree, the suffix (T) or (R) for taught or research is often added, so the more prestigious two-year research degree is called MLitt (R).\n\nLike the PhD, the MPhil and MRes degrees are generally awarded without class or grade as a pass (the standard grade) or can, rarely, be awarded with a distinction.\nMAs in Oxford, Cambridge and Dublin\n\nThe universities of Oxford, Cambridge and Dublin award master's degrees to BAs without further examination, where seven years after matriculation have passed, and (in some but not all cases) upon payment of a nominal fee. It is commonplace for recipients of the degree to have graduated several years previously and to have had little official contact with the university or academic life since then. The only real significance of these degrees is that they historically conferred voting rights in University elections, it was seen as the point at which one became eligible to teach at the University and certain other privileges e.g. the right to dine at the holder's college's high table. They still do confer some restricted and rarely used voting rights. The MAs awarded by Oxford and Cambridge are colloquially known as the Oxbridge MA, and that from Dublin as the Trinity MA, and would be usually distinguished respectively: MA (Oxon.), MA (Cantab.) and MA (Dubl.). \"Oxon.\" here is short for Oxoniensis, \"Cantab.\" for Cantabrigiensis, \"Dubl.\" for Dubliniensis, meaning \"of Oxford\", \"of Cambridge\", and \"of Dublin\" respectively. The Universities of Cambridge and Dublin also offer an MA to certain senior staff - both academic and non-academic - after a number of years' employment with the university.\n\nUntil the advent of the modern research university in the mid 19th century, several other British and American universities also gave such degrees \"in course\".\nScottish MA\nMain article: Master of Arts (Scotland)\n\nIn Scotland the first degree in Arts, Fine Art, Humanities and Social Sciences awarded by the ancient universities of Scotland is the Master of Arts. It should be noted the Science and Law faculties of Scottish universities award the BSc and LLB degrees respectively and the New Universities generally award the BA. The Scottish MA is roughly equivalent to an advanced BA from a University elsewhere in the United Kingdom, as it is an undergraduate degree. However, Scottish university courses are four years in length rather than the usual UK degrees, which last for only three years (but this is also true of Scottish BSc and LLB degrees), However, 3 year undergraduate degrees are available but do not include honours. Honours are conferred upon completion of the extra 4th year unlike the rest of the UK which confer honours based upon exam results. It is considered the norm in Scotland to undertake a 4-year course with honours and so when speaking to a Scot they will often refer to the 4-year course simply as a degree. Trinity College Dublin courses are also four years in length.\nOther approaches\n\nAs indicated above, even though higher education systems in Europe try to comply with the Bologna process, the process is not yet fully accomplished. Differences in methodology and curricula are still widely different in some cases. To mitigate this, several initiatives and approaches are currently tried, some of them with the support of the European Union institutions. Either in partnership or as private consortia, networks of universities in different countries are trying to work out shared curricula and adopt similar methodologies. In niche educational areas like translation and interpreting this has proved successful and the networks have become functional, i.e. European Master's in Translation and the European Master's in Conference Interpreting.[4][5] While these are not mainstream developments, it may be noted that in these networks of universities a similar master's degree certificate is offered for a given field, and the network/consortium collectively guarantees that these degrees have a high level of convergence.[6]", "skillName": "Master's_degree_in_Europe."}
{"id": 2, "category": "Education", "skillText": "A bachelor's degree (from Middle Latin baccalaureus) or baccalaureate (from Modern Latin baccalaureatus) is an undergraduate academic degree awarded by colleges and universities upon completion of a course of study lasting three to seven years (depending on institution and academic discipline). In some cases, it may also be the name of a second bachelor's degree, such as the Bachelor of Laws (LL.B.), Bachelor of Education (B.Ed.), Bachelor of Civil Law (B.C.L.), Bachelor of Music (B.Mus.), Bachelor of Philosophy (B.Phil.), or Bachelor of Sacred Theology (B.S.Th.) degrees, which in some countries are only offered after taking a first bachelor's degree. The second bachelor's degree is, thus, a type of graduate degree.\n\nThe term \"bachelor\" in the 12th century referred to a knight bachelor, who was too young or poor to gather vassals under his own banner. By the end of the 13th century, it was also used by junior members of guilds or universities. By folk etymology or wordplay, the word baccalaureus came to be associated with bacca lauri (\"laurel berry\") in reference to laurels being awarded for academic success or honours.[1]\n\nUnder the British system, and those influenced by it, undergraduate academic degrees are differentiated either as pass degrees (also known in some areas as ordinary degrees) or as honours degrees, the latter sometimes denoted by the appearance of \"(Hons)\" after the degree abbreviation.[2] An honours degree generally requires a higher academic standard than a pass degree, and in some universities a fourth year of study. In some countries, e.g., Australia and Canada, the honours degree should not be confused with the \"postgraduate\" bachelor's degree \"with honours\" or the baccalaureatus cum honore degree. It is a consecutive academic degree, which is the continuation of a completed (honours) bachelor's degree program in the same field and is usually obtained in order to join a doctoral programme; it requires a minimum of one year, but may also take longer.\n\nContents\n\n    1 Variations\n        1.1 Africa\n            1.1.1 Algeria\n            1.1.2 Botswana\n            1.1.3 Morocco\n            1.1.4 Nigeria\n            1.1.5 South Africa\n            1.1.6 Kenya\n        1.2 Asia\n            1.2.1 Bangladesh\n            1.2.2 China\n            1.2.3 Fiji\n            1.2.4 India\n            1.2.5 Indonesia\n            1.2.6 Jordan\n            1.2.7 Nepal\n            1.2.8 Malaysia\n            1.2.9 Pakistan\n            1.2.10 Philippines\n            1.2.11 Republic of Korea\n            1.2.12 Sri Lanka\n        1.3 Oceania\n            1.3.1 Australia\n            1.3.2 New Zealand\n        1.4 The Americas\n            1.4.1 Canada\n            1.4.2 United States\n            1.4.3 Mexico\n            1.4.4 Argentina\n            1.4.5 Brazil\n            1.4.6 Colombia\n            1.4.7 Guyana\n            1.4.8 Costa Rica, El Salvador and Venezuela\n        1.5 Europe\n            1.5.1 Austria\n            1.5.2 Belgium\n            1.5.3 Croatia\n            1.5.4 Czech Republic\n            1.5.5 Denmark\n            1.5.6 Faroe Islands\n            1.5.7 France\n            1.5.8 Germany\n            1.5.9 Italy\n            1.5.10 Former Yugoslav Republic of Macedonia\n            1.5.11 Netherlands\n            1.5.12 Poland\n            1.5.13 Portugal\n            1.5.14 Russia, Ukraine, and Armenia\n            1.5.15 Spain\n            1.5.16 Sweden\n            1.5.17 Switzerland\n            1.5.18 United Kingdom\n                1.5.18.1 England, Wales, and Northern Ireland\n                1.5.18.2 Scotland\n                1.5.18.3 UK medical schools\n            1.5.19 Turkey\n    2 Types\n        2.1 Agriculture\n        2.2 Architecture and design\n        2.3 Arts\n        2.4 Engineering\n        2.5 Business and management\n        2.6 Computer science and information systems\n        2.7 Health Care\n            2.7.1 Medicine\n            2.7.2 Dentistry\n            2.7.3 Midwifery\n            2.7.4 Physiotherapy\n            2.7.5 Optometry\n            2.7.6 Nursing\n            2.7.7 Veterinary science\n            2.7.8 Pharmacy\n            2.7.9 Public Health\n            2.7.10 Medical and Health Sciences\n            2.7.11 Kinesiology\n            2.7.12 Nutrition and Dietetics\n        2.8 Aviation\n        2.9 Divinity and theology\n        2.10 Fine arts\n        2.11 Film and television\n        2.12 Integrated studies\n        2.13 Journalism\n        2.14 Landscape architecture\n        2.15 Liberal arts\n        2.16 Library science\n        2.17 Music\n        2.18 Mortuary science\n        2.19 Philosophy\n        2.20 Psychology\n        2.21 Education\n        2.22 Science with education\n        2.23 Forestry\n        2.24 Science\n        2.25 Science in law\n        2.26 Social sciences\n        2.27 Social work\n        2.28 Technology\n        2.29 Law\n        2.30 Talmudic law\n        2.31 Tourism studies\n        2.32 Mathematics\n        2.33 Urban and regional planning\n        2.34 Public affairs and policy management\n        2.35 Innovation\n    3 See also\n    4 References\n    5 External links\n\nVariations\nAfrica\n\nIn most African countries, the university systems follow the model of their former colonizing power. For example, the Nigerian university system is similar to the British system, while the Ivorian system is akin to the French.\nAlgeria\n\nBachelor's degrees in Algerian universities are called \"الليسانس\" in Arabic or la license in French; the degree normally takes three years to complete and is a part of the LMD (license, master, doctorat) reform, students can enroll in a bachelor's degree program in different fields of study after having obtained their baccalauréat (the national secondary education test). The degree is typically identical to the program of France's universities, as specified in the LMD reform. Bachelor's degree programs cover most of the fields in Algerian universities, except some fields, such as Medicine and Pharmaceutical Science.\nBotswana\n\nBachelor's degrees at the University of Botswana normally take four years. The system draws on both British and American models. Degrees are classified as First Class, Second Class Division One (2:1), Second Class Division Two (2:2) and Third as in English degrees, but without being described as honours. The main degrees are named by British tradition (Arts, Science, Law, etc.), but in recent years there have been a numbers of degrees named after specific subjects, such as Bachelor of Library and Information.\nMorocco\n\nIn Morocco, a bachelor's degree is referred to as al-ʾijāzah (Arabic, French: license). It lasts three years that are further divided into two cycles. The first cycle comprises the first, or propedeutic, year. Students, after successfully completing their first two years, can pursue either theoretical specialization (études fondamentales) or professional specialization (études professionnelles). The second cycle is one year long, after completing which, students are conferred upon the Licence d'études fondamentales or the Licence professionnelle.[3] This academic degree system was introduced in September 2003.[4]\nNigeria\n\nUniversity admission is extremely competitive, with attendant advantages and disadvantages. Nonetheless, it takes four to five years to complete a bachelor's degree. In cases of poor performance, the time limit is double the standard amount of time. For example, one may not study for more than 10 years for a five-year course. Students are normally asked to leave if they must take longer. Nigerian universities offer B.Sc., B.Tech. (usually from Universities of Technology), B.Arch. (six years), and other specialized undergraduate degrees, such as B.Eng. Science undergraduate degrees may require six months or a semester dedicated to SIWES (Students Industrial Work Experience Scheme) but it is usually mandatory for all engineering degrees. A semester for project work/thesis is required, not excluding course work, during the bachelor thesis in the final year. The classifications of degrees: first-class, second-class (upper and lower), third-class (with honours; i.e., B.Sc. (Hons)) and a pass (no honours). First- and second-class graduates are immediately eligible for advanced postgraduate degrees (i.e., M.Sc. and Ph.D.), but other classes may be required for an additional postgraduate diploma before such eligibility.[5]\n\nFurthermore, all graduating students are obliged to do the National Youth Service Corps (NYSC) requirement, which usually takes one year, after which they are eligible to pursue higher degrees. The NYSC is a paramilitary service that involves students' being posted to different parts of the country to serve in various capacities. Principal objectives of the NYSC are to forge national cohesion, encourage students to apply their obtained knowledge to solving problems of rural Nigeria, and others. The NYSC was established by law after the Nigerian Civil War.[6]\n\nPolytechnical schools (polytechnics) in Nigeria are not considered universities. They are mandated to educate technicians of high calibre; they offer the OND (ordinary national diploma) and the HND (higher national diploma). The polytechnics focus very strongly on practical technical training. The B.Sc. and HND are compared in engineering circles but there are significant differences in training philosophies.\n\nHonours degrees in Nigeria are differentiated only on the basis of performance. Honours degrees include the first-class degree, second-class degrees (upper and lower) and the third-class degree, but not the pass. All university students must do an independent research project which applies the knowledge obtained during the previous years of study.\n\nThe project work must be submitted in the semester before graduation and usually takes a significant number of points. Further course work is not precluded during the project work, but the courses are fewer and are at an advanced level. Project work is orally defended before the faculty and before peers. In the sciences and engineering a demonstration of the project is usually required. The exceptions are theoretical work, for which a media project is required.\nSouth Africa\n\nIn South Africa, an honours degree is an additional postgraduate qualification in the same area as the undergraduate major, and requires at least one further year of study as well as a research report.\nKenya\n\nIn Kenya, university education is highly valued and supported by the government,[7] affluent individuals as well as corporate entities who demonstrate this by providing loans and scholarships to students who perform exceptionally well in their Kenya Certificate of Secondary Education (KCSE) examination. A bachelor's degree is awarded to students who successfully complete a three to seven-year course depending on the area of study. For most degree programs, a research project and an internship period after which a report is written by the student is a must before the student is allowed to graduate. In 2012, a number of select colleges were upgraded to university status in a bid to increase the intake of students into the degree program.[8]\nAsia\nBangladesh\n\nIn Bangladesh, universities and colleges award three- and four-year degrees (three-year degrees courses are called pass courses and four-year degree courses are called honours courses) in science and business (B.Sc., B.B.S., B.B.A., four-year and three months, etc.) and three- and four-year degrees in arts (B.A., B.S.S., etc.). Engineering universities provide four-year degree programs for bachelor's degree courses of study. Medical colleges have five-year degree programmes. In law education there is a two-year LL.B. degree after completing three years in a B.A. program for a total of five years of study. There is also a four-year LL.B. honours degree. All of these programs begin after achieving the Higher Secondary Certificate (HSC—in total 12 years of education).\nChina\n\nSince the undergraduate education system in China is modeled after its American counterpart, all the degrees are adapted from those of the United States excepting the release of the degree certificate. Once a student has fulfilled his/her course requirements, a graduate certificate will be given. In order to get the degree, a student must finish and pass the dissertation stage; only then will he or she be awarded a degree credentialed by the Ministry of Education of the People's Republic of China. Four years of education is the standard length, although some private small colleges not credentialed by the Ministry of Education do offer three-year programs. Normally, about 90% of graduates are able to obtain a degree; however, no degree is awarded with excellency or honor. It is also referred to as a \"Xueshi\" (学士).\nFiji\n\nThe colonial link and the establishment of the University of the South Pacific in 1968 allowed the education system to follow suit from the qualification system of the Commonwealth. University of the South Pacific is the only university in the Oceania region to be internationally recognized outside Australia and New Zealand with its bachelor's and other awards program. It is also the highest ranked in the university ranking in the island region and also ranked above some Australian universities like the University of Canberra, University of Sunshine Coast and New Zealand universities like Lincoln University and Waikato Institute of Technology.[9]\nIndia\n\nBachelor's degrees in engineering are four-year degree programmes while medical colleges are five-year degree programmes. Bachelor's degrees (BE, graduate in engineering, BArch, BTech, BSc) that also begin after secondary school year twelve (also called +2). The Bachelor of Technology (commonly abbreviated as BTech) is an undergraduate academic degree conferred after completion of a three or four-year programme of studies at an accredited university or accredited university-level institution. In India, the Bachelor of Technology degree is a professional engineering degree awarded after completion of four-years of extensive/vast engineering study and research.\n\nIn India, BTech. is otherwise called as BE. Some universities offer it as BTech and some as BE. However, the name of degree does not make any difference viz, as the curriculum of AICTE/UGC is standard all across. Mostly all autonomous government organisation confer a BTech degree and private institutes which are affiliated to regional universities confer BE degree. The Bachelor of Architecture (BArch) degree programme is of five years' duration while still people could pursue civil engineering which has a duration of four years its is under BTech as it known in India. The Bachelor of Science in Agriculture, BSc or is a four-year full-time degree. There are also some integrated programmes. The techno-legal degree like BTech with LLB is a six-year full-time degree course in Engineering and Law. In the general curriculum, there are three and four year programmes, with Honours track being in the four year category. A bachelor's degree (BA, BCom, BSc, BBA etc.) is awarded by the respective university to which the college is affiliated which is of three years. BCom is most commonly pursued degree in India. The duration of the course is three to four years and minimum eligibility is 10+2 from any stream. In India, Bachelor of Journalism is of three years. Journalism Courses in India are known by various names like BJ (Bachelor of Journalism), BCJ (Bachelor of Communication and Journalism), BMM (Bachelor of Mass Media), BJMC (Bachelor of Journalism and Mass Communication), BAJMC (Bachelor of Arts in Journalism and Mass Communication), BAMC (Bachelor of Arts in Mass Communication). Employability prospects vary by the reputation of the institute and course. A majority of BBA colleges in India offer this bachelor's degree programme in the form of a three-year course. However, there are four-year part-time courses as well. A student is eligible to study BBA in India only if s/he has passed the 10+2 level examination or higher secondary examination from a recognised board or council in the country. A BBA degree can be portrayed as the gateway to the global business sector. This authentic business management course includes subjects like the following:\n\n    Marketing\n    General Management\n    Finance\n    Human Resource Management (HRM)\n    Statistics\n    Supply Chain Management\n\nIntegrated Bachelor of Computer Application (BCA) can be pursued in India. Bachelor of Computer Applications is a three-year under-graduate degree course awarded in India in the field of Computer Applications. Some students use online or distance education programs to earn this degree.\n\nThe course aims at realising the following student objectives:\n\n    To demonstrate a sound knowledge in key areas of computer science or industrial computing.\n    To demonstrate a substantial understanding of concepts in key areas of computer science.\n    To carry out the required analysis and synthesis involved in computer systems, information systems and computer applications.\n    To demonstrate professional competence in developing software and in its design and implementation.\n    To develop sound practical skills to enable them to addressing problems which arise from computer systems and applications.\n\nAfter completion of this course, students may move on to higher studies, earning degrees such as:\n\n    Master of Computer Application (MCA)\n    Master in Business Administration (MBA)\n    Master of Science in Computer Science (MSc-CS)\n    Master of Science in Information Technology (MSc-IT)\n\nOther students move directly to industry, working as programmers, networking professionals, graphics designers, and related positions.\n\nSome of the institutes also provide the graduate diploma courses. A graduate diploma is basically the same thing as a graduate certificate. This terminology is more common in England, Australia, Canada, Scotland, Wales, etc., whereas \"certificate\" is more common in the US.\nIndonesia\n\nIn Indonesia, most of the current bachelor's degrees are domain-specific degrees. Therefore, there are probably more than 20 bachelor's degrees. For instance, S.Psi for Sarjana Psikologi (literally translated as \"Bachelor of Psychology/B.Psy., B.A.\"), S.T. for Sarjana Teknik (literally translated as \"Bachelor of Engineering\"), S.Si. for Sarjana Sains (literally translated as \"Bachelor of Science\"), S.Farm for Sarjana Farmasi (literally translated as \"Bachelor of Pharmacy\"), S.E for Sarjana Ekonomi (literally translated as \"Bachelor of Economy\"), S.Kom. for Sarjana Ilmu Komputer (literally translated as \"Bachelor of Computer Science\"), or S.Sos. for Sarjana Ilmu Sosial (literally translated as \"Bachelor of Social Sciences\"). In the past, the Indonesian academic system adopted the old European/western degrees, such as the Ir (inginieur) for an engineering degree and doctor's degree (doktorandus) for a degree in either social or natural sciences.\nJordan\n\nSince the undergraduate education system in Jordan is modeled after its American counterpart, all the degrees are adapted from those of the United States excepting the release of the degree certificate. Once a student has fulfilled his/her course requirements, a graduate certificate will be given. In order to get the degree, a student must finish and pass the dissertation stage; only then will he or she be awarded a degree credentialed by the Ministry of Higher Education of the Hashemite Kingdom of Jordan. Four years of education is the standard length.\nNepal\n\nIn Nepal, the bachelor's degree was initially a four-year program for courses like Bachelor of Business Studies (B.B.S.), Bachelor of Sciences (B.Sc)., Bachelor of Education (B.Ed.), Bachelor of Arts (B.A.) from Tribhuvan University, Pokhara University, Purbanchal University and other new regional university equivalent but now it is mostly a four-year program for new courses like Bachelor of Business Administration (B.B.A.), Bachelor of Business Information System (B.B.I.S.), Bachelor of Information Management(B.I.M.),Bachelor of Engineering (B.E.), Bachelor of Science in Computer Studies and Information Technology (B.Sc).C.S.I.T. Some bachelor's programs are still three years long, such as the Bachelor of Arts (B.A) and Bachelor of Education (B.Ed). It is completed after 10+2 level (High School). Bachelor of Business Administration (B.B.A), Bachelor of Information Management (B.I.M.), Bachelor of Business Information Systems (B.B.I.S.), Bachelor of Engineering, and Bachelor of Science in Computer Science and Information Technology (B.Sc.C.S.I.T.) are a few popular bachelor's degree programs. B.Sc. and B.B.Sc. have recently turned into four year programs from three year programs. In Nepal, Tribhuvan University as an oldest and biggest University based on number of student and academic department, Kathmandu University, Purbanchal University, Pokhara University, Nepal Sanskrit University and other new regional universities are operating currently. M.B.A. and B.B.A. from all universities are examined under the system of Percentage and G.P.A, and traditional university courses are accessed on division base like pass division, second division, first division and distinction. In Nepal, there is no top up, honours and exchange or related tie up degree courses authorised and practiced by Nepalese Government and other educational Institution but these day, Affiliation from foreign universities, online and distance mode is popular in modern working youth population. M.B.A., B.B.A., B. Pharm., B. Sc. Nursing, Bachelor of Nursing (B.N.), B. E. has a trending professional demand in Nepalese market.\nMalaysia\n\nInstitutes of higher learning in Malaysia provide three or four years of education leading to a B.Sc. Hons Degree. The standards of categorization is almost consistent among Malaysian Universities. Candidates who excel in their academic results will be awarded a First Class Bachelor Hons Degree (usually 3.67 CGPA and above), followed by Class Second Upper (usually between 3.00-3.66 CGPA), Class Second Lower (usually 2.50-2.99 CGPA), Class Three (usually 2.00-2.49 CGPA) and General Degree (Without Honours), for usually 1.99 and below CGPA candidates.\nPakistan\n\nIn Pakistan, commerce and science colleges provide four-year bachelor's degrees (B.A., B.Sc., B.B.A., B.Com., B.H.M.S., etc.). Generally these programs are of four years duration as elsewhere in the world and begin after completing higher secondary school education by receiving a HSSC certificate acknowledging one's twelve years of study by the respective board. After successful completion of these programs, a bachelor's degree is awarded by the respective university. Some colleges are affiliated with a university (mostly the state's central university) and teach a part-time degree equal to fourteen years of education such as a two-year B.A., B.Com. etc. A student may enroll in a two-year B.A., BCOM as well as a four-year B.A. as an external candidate (external candidate are enrolled for examination & study program on self basis or through private tuition providers). Main universities offering these two programs are University of Punjab and University of Karachi where more than 50,000 students appear in B.A. and B.Com. exam as external candidates. Whereas D.H.M.S. (Diploma in Homeopathic Medical System) is a four-year program and is offered after School Level (Science Group). National Council of Homeopathy (NCH) (Govt. of Pakistan) is the examination and Registration Body of Homeopathic Doctors after completion of 6–12 months House Job under UAH Act 1965 to practice as Homeopathic Doctor. Homeopathic Doctors are appointed in Government Hospitals up to BPS 19 (Gazetted Ranks). This is also equivalent to B.Sc. (Homeopathic) a fourteen-years education degree after that the students can continue M.Sc. in Physiology etc.\n\nEngineering and medical colleges provide four- and five-year degree programs respectively for bachelor's degrees (BE, B.Sc.Eng., B.Arch., and B.Tech. begin after a three-year Diploma of Associate Engineer, M.B.B.Sc.) that also begin after secondary school year 12. The Bachelor of Architecture (B.Arch.) degree program is of five to six years' duration.\n\nWhen a four-year Bachelor of Technology (B.Tech.) degree is earned after a three-year Diploma of Associate Engineer, it is called B.Tech. (Hons), and when a four-year B.Tech. degree is obtained after FSc (Two-year intermediate in Science), then it is called a four-year B.Tech. But generally B.Tech. Degree is obtained after three-year Diploma of Associate Engineer and therefore it is called B.Tech. Honors, because it is seventeen year of schooling as compared to other degrees like B.Sc. Engineering, or B.E., B.B.A., etc. which are sixteen year of schooling.\nPhilippines\n\nIn the Philippines, where the term \"course\" is commonly used to refer to a bachelor's degree major, course of study or program, several undergraduate categories exist—the two most common degrees awarded being Bachelor of Science (B.Sc.) and Bachelor of Arts (B.A. or A.B.). Specializations (\"majors\") in economics, business administration, social work, agriculture, nursing, accountancy, architecture and engineering are offered as B.S. degrees in most colleges and universities. The latter three specializations require five years of schooling, in contrast to the standard of four years. Other common degrees are Bachelor in Education (B.Ed.) and Bachelor of Laws (LL.B., a professional degree). Being patterned after the United States, all universities and colleges offer graduation with honors—cum laude, magna cum laude, and summa cum laude.\nRepublic of Korea\n\nUniversities, colleges, and institutions of higher learning provide the bachelor's degree, called 'haksa' (Korean: 학사). For example, a university student who majored in literature and graduates obtains a B.A., called 'munhaksa' (Korean: 문학사). Even if he or she does not go to an institution of higher learning, a person can get a bachelor's degree through the Bachelor's Degree Examination for Self-Education.\nSri Lanka\n\nRecognised institutes of higher learning only are authorised to award degrees in Sri Lanka. Three years full-time bachelor's degree without an area of specialization is known as a general degree. A degree with a specialization (in accounting, chemistry, plant biotechnology, zoology, physics, engineering, IT, law, etc.) is known as a special degree and requires four years of study and more entrance qualifications. A degree in medicine, an M.B.B.Sc., requires a minimum of six years.\nOceania\nAustralia\n\nIn Australia, a bachelor's degree is a three to five-year program. Entry to a number of professions, such as law practice and teaching, require a bachelor's degree (a 'professional' degree). Other degrees, such as Bachelor of Arts don't necessarily elicit entry into a profession, though many organisations require a bachelor's degree for employment.\n\nA one-year \"postgraduate\" (With) Honours degree can be achieved as a consecutive stand-alone Bachelor (with) Honours degree following a bachelor's degree in the same field. In some cases, it may be offered as an \"on-course\" degree program, which takes one year of research at the completion of an undergraduate four-year (Bachelor's) degree.[10] It is usually available only to students who achieve a distinction average in their undergraduate studies. Generally, the (With) Honours degree involves completion of higher-level courses and the submission of a research thesis. In this way, the Australian Honours degree differs from the English/Welsh Honours, which requires only the completion of a short so-called \"dissertation\" as part of the three-year bachelor's degree.\n\nSome bachelor's degrees (e.g. engineering and environmental science) include an integrated honours degree as part of a four-year program. Honours is generally for students who want to take up a research track for postgraduate studies, and increasingly for those who want an extra edge in the job market. Marking scales for Honours differ; generally, First Class Honours (80–100%) denotes an excellent standard of achievement; Second Class Division 1 (75–79%) a high standard; Second Class Division 2 (70–74%) a good standard; Third Class (65–69%) satisfactory standard; between 50-64% the degree is not awarded with honours, but may be conferred as a pass degree; a final mark below 50% is a fail of the course.\n\nThe Honours program allows students to pursue an independent research project in an area of interest under the supervision of an academic staff member. Students acquire skills which will enable them to work without close supervision in a research environment in industry or government, or to proceed to a higher degree by research (such as a Ph.D.).[11] First-class and second-class (first division) is generally the standard required for entry into a Ph.D. or very high research Master's program in Australia. In science, a second-class research honours or higher is generally a prerequisite for entrance to a Ph.D. program (a Master's is an uncommon route).\nNew Zealand\n\nIn New Zealand, only recognised institutions—usually universities and polytechnics—have degree-awarding powers.\n\nMost bachelor's degrees are three years full-time, but certain degrees, such as the Bachelor of Laws and the Bachelor of Engineering degrees, require four years of study. A Bachelor of Medicine degree requires a minimum of six years.\n\nWhere students opt to study two bachelor's degrees simultaneously—referred to as a \"conjoint degree\" or \"double degree\"—an extra year of study is added. The number of years of study required is determined based on the degree with the greatest number of years. For example, a B.Com. degree requires three years of full-time study, but a double B.Com.–LL.B. degree will require five years of full-time study because the LL.B. degree is four years long. Exceptional students may choose to complete a degree in a shorter amount of time by taking on extra courses, usually with the help of summer school. Students who complete a double degree program will have two separate bachelor's degrees at the end of their studies.\n\nConsistently high-performing students may also be invited to complete the 'honours' program. This usually requires an extra year of study with an extra honours dissertation. An honors award is credited with \"Hons.\" (e.g., Bachelor of Laws (Hons.)). Some degrees also offer a Post Graduate Diploma, which often consists of the same workload, but with added flexibility. PGDip does not usually require a dissertation. However, the student may complete one if desired. A diploma award is credited with 'PGDip' and the name of the degree (for example, 'PGDipArts' or 'PGDipScience'.\nThe Americas\n\nUsually the region presents bachelor's, Master's, doctoral, and postdoctoral degrees.\nCanada\n\nEducation in Canada is governed independently by each province and territory, and thus there are differences between provinces when it comes to the granting of degrees. Each province loosely follows the United States-based model but, e.g. in Québec, also the French-based, as well as the United Kingdom-Irish based and Commonwealth based model. Bachelor's degrees may take either three or four years to complete and are awarded by colleges and universities. In many universities and colleges bachelor´s degrees are differentiated either as bachelor´s or as honours bachelor´s degrees. The term \"Honours\" is an academic distinction, which indicates that students must achieve their bachelor's degree with a sufficiently high overall grade point average; in addition, some programs may require more education than non-honours programs. The honours degrees are sometimes designated with the abbreviation in brackets of '(Hon(s))'. It should not be confused with the consecutive bachelor's degree \"with Honours\", from Latin \"Baccalaureatus Cum Honore\", abbr. e.g. B.A. hon. de jure without brackets and with dot. It is a \"postgraduate\" degree which is considered to be the equivalent of corresponding maîtrise degrees under the French influenced system.\n\nGoing back in history, a three-year bachelor´s degree (also known e.g. in Québec as grade de bachelier) was also called a pass degree or general degree. A student who first achieves a general bachelor's degree with a sufficiently high overall average may be admitted to a \"postgraduate\" Baccalaureatus Cum Honore degree in the same field; it requires a minimum of one year but may take longer; however, it typically does not exceed two years. Students are required to undertake a long high quality research empirical thesis (Honours Seminar Thesis) combined with a selection of courses from the relevant field of studies. The consecutive degree is essential if a student's ultimate goal is to study towards a two- or three-year very high research master's degree qualification. A student holding a Baccalaureatus Cum Honore degree also may choose to complete a Doctor of Philosophy (Ph.D.) program without the requirement to first complete a master's degree. Over the years, in some universities certain Baccalaureatus Cum Honore programs have been changed to corresponding master's degrees.\n\nIn the province of Quebec, students have to go through a minimum of two years of college before entering, for example, a three-year Bachelor of Science (B.Sc.) or a four-year Bachelor of Engineering (B.Eng.) program. As a consequence, there is no de jure \"honors degree\" (although some universities market some of their programs as being de facto honors degrees in their English-language materials[citation needed]), but there are some specializations called \"concentrations\" in French, which are mostly taken as optional courses.\n\nIn the province of Ontario, the vast majority of bachelor's degrees offered by Ontario universities are academic in nature. On the other hand, Ontario provincial legislation requires bachelor's degrees offered by Ontario colleges to be applied and vocationally-focused[12]\nUnited States\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (February 2015) (Learn how and when to remove this template message)\n\nBachelor's degrees in the United States are typically designed to be completed in four years of full-time study, although some programs (such as engineering or architecture)[13] usually take five, and some universities and colleges allow ambitious students (usually with the help of summer school, who are taking many classes each semester, and/or who have existing credit from high school Advanced Placement or International Baccalaureate course exams) to complete them in as little as three years. Some US colleges and universities have a separate academic track known as an \"honors\" or \"scholars\" program, generally offered to the top percentile of students (based on GPA), that offers more challenging courses or more individually directed seminars or research projects in lieu of the standard core curriculum. Those students are awarded the same bachelor's degree as students completing the standard curriculum but with the notation in cursu honorum on the transcript and the diploma. Usually, the above Latin honors are separate from the notation for this honors course, but a student in the honors course generally must maintain grades worthy of at least the cum laude notation anyway.[14] Hence, a graduate might receive a diploma Artium Baccalaureatum rite or Artium Baccalaureatum summa cum laude in the regular course or Artium Baccalaureatum summa cum laude in cursu honorum in the honors course.\n\nIf the student has completed the requirements for an honors degree only in a particular discipline (e.g., English language and literature), the degree is designated accordingly (e.g., B.A. with Honors in English). In this case, the degree candidate will complete the normal curriculum for all subjects except the selected discipline (\"English,\" in the preceding example). The requirements in either case usually require completion of particular honors seminars, independent research at a level higher than usually required (often with greater personal supervision by faculty than usual), and a written honors thesis in the major subject.\n\nMany universities and colleges in the United States award bachelor's degrees with Latin honors, usually (in ascending order) cum laude (\"with honor/praise\"), magna cum laude (\"with great honor/praise\"), summa cum laude (\"with highest honor/praise\"), and the occasionally seen maxima cum laude (\"with maximal honor/praise\"). Requirements for such notations of honors generally include minimum grade point averages (GPA), with the highest average required for the summa distinction (or maxima, when that distinction is present). In the case of some schools, such as Bates College, Colby College, Middlebury College, Guilford College, Franklin College Switzerland, and larger universities like the University of Virginia, Princeton University, North Carolina State University, University of Massachusetts Amherst, a senior thesis for degrees in the humanities or laboratory research for natural science (and sometimes social science) degrees is also required. Five notable exceptions are Reed College, Massachusetts Institute of Technology, The Evergreen State College, Sarah Lawrence College, and Bennington College, which do not have deans' lists, Latin honors recognitions, or undergraduate honors programs or subjects.\nMexico\n\nBachelor's degrees may take an average of five years (from four to five years) to complete depending on the course load and the program and they are awarded by colleges and universities. Medicine is from 6 to 7 years. Each college has its own curriculum and requirements with an emphasis of their choice, governed independently by each state of the republic. After finishing all the subjects the student require a final work, which means the completion of particular honors seminars, research and development or a written thesis in a particular field. Mexico's regulations established as an obligation in order to receive their license and title the fulfillment of a \"Social Service\" to the nation (usually for those who finished their studies in a public institution) as a remuneration to society in the form of social actions, the benefits, as students, were received during training. This requirement takes about six months to one year depending on the type of degree. Bachelor's degree should not be falsely related with its Spanish cognate \"Bachiller\", which designate a prerequisite for matriculate in a career or bachelor studies. The official name for a bachelor's degree in Mexico is \"Licenciado\" and such studies are referred as \"Licenciatura\".\n\nBachelor's degrees should not be confused with Engineering Degrees, where an Ingeniería is prefixed to the name and requieres additional courses for certification as an Engineer.\nArgentina\nSee also: List of universities in Argentina\n\nIn Argentina, a bachelor's degree takes from 3 to 6 years to complete. To enter the University of Buenos Aires, most students also must pass a one-year course called CBC (Ciclo Básico Común, English Common Basic Cycle). Master's and doctoral degrees are additional specializations.\nBrazil\nSee also: Universities and higher education in Brazil\n\nIn Brazil, a bachelor's degree takes from three years to six years to complete depending on the course load and the program. A bachelor's degree is the title sought by Brazilians in order to be a professional in a certain area of human knowledge. Master's and doctoral degrees are additional degrees for those seeking an academic career or a specific understanding of a field.\n\nEven without a formal adhesion to the Bologna system, a Brazilian \"bachelor's\" would correspond to a European \"first cycle.\" A Brazilian \"bachelor's\" takes three to six years[15] for completion, as well as usually a written monograph or concluding project, in the same way that a European bachelor's can be finished in three to four years, after which time Europeans may embark on a one- to two-year 2nd cycle program usually called a \"Master's\", according to the Bologna Process.\n\nDepending on programs and personal choices, Europeans can achieve a master's degree in as little as four years (a three-year bachelor's and a one-year Master's) and as long as six years (a four-year bachelor's, a two-year Master's) of higher education. In Brazil it would be possible to have a specialization \"lato-sensu\" degree—which differs from a Brazilian \"stricto-sensu\" master's degree—in as little as three years (two years for a \"tecnólogo\"[16] degree and an additional year for a specialization) or as long as eight years (six years for professional degrees, plus two years for a master's \"stricto-sensu\" degree—typical in medicine or engineering).\nColombia\n\nIn Colombia, secondary school has two milestones, in 9th and 11th grades. After completing the first 4 years of secondary school (6th, 7th, 8th and 9th grades), a student is considered to have completed the basic secondary school while after completed the last two years (10th and 11th grades) is considered to have completed \"bachillerato\" or high school diploma.\n\nThis degree can be only academic (the most common) or:\n\n    military, given by military specialised schools and gives the opportunity for male students not to go to obligatory military service.\n    commercial, which grants students focussed skills on accountancy.\n    technical, which grants students focussed skills on technical abilities such in electricity, mechanics and related matters.\n    Academic, which grants students focussed skills on elementary education.\n\nAfter graduating from high-school, hopeful students must present a nationwide exam that determines their eligibility to apply for their desired program, depending on the score the student achieves on the exam. In Colombia, the system of academic degrees is similar to the US model. After completing their \"bachillerato\" (high school), students can take one of three options. The first one is called a \"Profesional\" (professional career), which is similar to a bachelor's degree requiring from four to six years of study according to the chosen program, However, strictly-career-related subjects are taken from the very beginning unlike US where focused career-related subjects usually are part of the curriculum from the third year. The other option is called a \"Técnico\" (technician); this degree consists of only two and a half years of study and prepares the student for technical or mechanical labors. Finally, the third option is called a \"Tecnólogo\" (equivalent to an associate degree), and consist of 3 years of study. A technical school gives to the student, after a program of two years, an under graduate degree in areas like software development, networks and IT, accountancy, nursing and other areas of health services, mechanics, electricity and technic-like areas.\n\nUniversities offer graduate degrees in ICFES endorsed programs like medicine, engineering, laws, accountancy, business management and other professional areas. A typical undergraduate program usually takes 10 or 11 semesters and some (i.e. medicine) require an additional period of service or practice to apply for the degree. A student who has obtained an undergraduate degree can opt to continue studying a career after completing their undergraduate degree by continuing onto Master's and Doctorate degrees. They can also choose to a specialization in certain fields of study by doing an extra year.\n\nICFES is the national authority for the education quality. A complete list of under graduate and graduate programs approved by ICFES can be found here: http://snies.mineducacion.gov.co/consultasnies/programa/buscar.jsp?control=0.09832581685767972 \nGuyana\n\nIn Guyana, the universities offer Bachelor programs in different streams like Bachelor of Atrs (B.A), Bachelor of Science in Nursing, Design and Arts, Liberal Arts, Psychology, Doctor of Medicine (MD) and other health science programs. These programs are delivered by University of Guyana, Texila American University, Green Heart Medical University, Lesley university and many more offers these bachelor programs.\nCosta Rica, El Salvador and Venezuela\nSee also: Education in Costa Rica, Education in El Salvador, and Education in Venezuela\n\nIn these countries, there are two titles that should not be confused:\n\n    High school students who pass their bachillerato exams obtain a certificate of Bachiller en Educación Secundaria (\"bachelor's degree in secondary education\"), which is needed in order to enter a university and is usually requested by companies in their profiles.\n    University students obtain a licenciatura degree in their respective fields after completing four years of education, five in Venezuela, (and meeting other requisites unique to each institution), enabling them to work as professionals in their chosen areas; for example, a Bachiller en Enseñanza Secundaria (\"bachelor's degree in secondary teaching\") enables a person to work as a high school teacher. Currently, the trend is for universities not to offer a bachelor's degree and to offer instead a licentiate's or \"Ingeniero\" degree after five years of education.\n\nEurope\n\nBachelor's degrees exist in almost every country in Europe. However, these degrees were only recently introduced in some Continental European countries, where bachelor's degrees were unknown before the Bologna process. Undergraduate programs in Europe overall lead to the following most widely accepted degrees:\n\n    Bachelor of Science degree (B.Sc.), 35%–40% of undergraduate programs;\n    Bachelor of Arts degree (B.A.), 30%–35% of undergraduate programs;\n    Bachelor of Laws degree (LL.B.), 1% of total programs, however widely accepted in the law discipline.\n\nThe rest of the programmes typically lead to Bachelor of Engineering degree (B.Eng.), Bachelor go Business Administration degree (B.B.A.), or other variants. Also, associate degrees are rising in popularity on the undergraduate level in Europe.\n\nOn a per-country, per-discipline and sometimes even per-institute basis, the duration of an undergraduate degree program is typically three or four years, but can range anywhere from three to six years. This is an important factor in the student's decision-making process.\nAustria\n\nThe historical situation in Austria is very similar to the situation in Germany. The traditional first degrees are also the Magister and the Diplom. A new piece of educational legislation in 2002 reintroduced the bachelor's degree (awarded after three or four years) and master's degree (another one or two years) in Austria.\nBelgium\n\nIn accordance with the agreements made in the Bologna process, the system of higher education in Belgium was reformed. A three-year bachelor's training was introduced to replace the former two- or three-year degree, which was called \"graduaat\" (in Dutch)/\"graduat\" (in French) or \"kandidatuur\" (in Dutch)/\"candidature\" (in French), the latter being part of a college or university education.\nCroatia\n\nMost universities and colleges in Croatia today offer a three-year bachelor program, which can be followed up typically with a two-year master's (graduate) program.\n\n    Upon completion of undergraduate professional studies, students are awarded the professional title of Professional Bachelor, abbreviated bacc. (baccalaureus or stručni prvostupnik in Croatian) with a reference to a specialisation.\n    Undergraduate university studies normally last for three to four years and upon completion, students are awarded an academic title of Bachelor, abbreviated univ. bacc. (baccalaureus or sveučilišni prvostupnik in Croatian).[17]\n\nAcademies that specialize in the arts, e. g. the Academy of Fine Arts in Zagreb, have four-year bachelor's programs followed by a one-year master's.\nCzech Republic\n\nHistorically, the baccalareus was the undergraduate degree awarded to students who graduated from the course of trivium (grammar, dialectic and rhetoric) at a faculty of liberal arts (either at the Charles University or at the University of Olomouc). It was a necessary prerequisite to continue either with the faculty of liberal arts (quadrivium leading to a master's degree and further to a doctoral degree) or to study at one of the other three historical faculties—law, medicine or theology.\n\nA bachelor's degree, abbreviated Bc.A., in the field of fine arts, and Bc. (Bakalář in Czech) in other fields is awarded for accredited undergraduate programs at universities and colleges.\n\nThe vast majority of undergraduate programmes offered in the Czech Republic have a standard duration of three years.\n\nIn the Czech tertiary education system, most universities and colleges today offer a three-year bachelor program, which can be followed up typically with a two-year master's (graduate) program. Some specializations, such as doctors of medicine and veterinary doctors, hold exceptions from the general system in that the only option is a six-year master's program with no bachelor stage (graduate with title doctor). This is due mainly to the difficulty of meaningfully splitting up the education for these specialisations.\nDenmark\n\nThe bachelor's degree was re-introduced at universities in Denmark in 1993, after the original degree (baccalaureus) was abandoned in 1775. The bachelor's degree is awarded after three or four years of study at a university and follows a scheme quite similar to the British one. Two bachelor's degrees are given at the university level today:\n\n    Bachelor of Science (B.Sc.), awarded to students with main focus on scientific, medical, or technical areas;\n    Bachelor of Arts (B.A.), awarded to students whose main focus is on humanistic, theological, or jurisprudence areas.\n\nHowever, both in the business and the academic world in Denmark, the bachelor's degree is still considered to be \"the first half\" of a master's (candidatus). It is often not considered a degree in its own right .[citation needed], despite the politicians' best attempts to make it more accepted.\n\nThe bachelor's degree has also been used since the late 1990s in a number of areas like nursing and teaching. Usually referred to as a \"Professional Bachelor\" (Danish: professionsbachelor), these degrees usually require 3 to 4½ years of combined theoretical and practical study at a so-called \"(professional) university college\" (Danish: professionshøjskole). These professional bachelor's degrees do grant access to some university Master's program. These professional bachelor's degrees are considered to be a full education.\nFaroe Islands\n\nBachelor's degrees in the Faroe Islands are much the same as in Denmark.\nFrance\nSee also: Licentiate § France\n\nThe traditional bachelor's degree is the equivalent of the French Maîtrise four-year degree. Since the new European system of 2004 LMD Bologna process was founded, it has become standard to recognize a bachelor's degree over three years with a licence degree, a master's degree over five years, and a doctorate over eight years.\n\nSome private institutions are however literally naming their degrees Bachelor's, Master's and Executive, such as the Bordeaux MBA/Collège International de Bordeaux. Not all of them are yet accredited by the French State, but offer similar course subjects, structures and methods to those found in Anglo-Saxon institutions.\nGermany\nMain article: Diplom\n\nBachelor's degrees, called \"Bakkalaureus\", originally existed in Germany but were abolished up until 1820 as part of educational reforms at this time. The Abitur degree—the final degree received in school after a specialized 'college phase' of two years—replaced it, and universities only awarded graduate degrees.\n\nThe Magister degree, a graduate degree, was awarded after five years of study. In 1899, a second graduate degree, the Diplom, was introduced when the Technische Hochschulen received university status. Since the introduction of the universities of applied sciences, a shortened version of the latter, referred to as Diplom (FH) and designed to take three to four years, was introduced between 1969 and 1972.\n\nHowever, to comply with the Bologna process, in 1998 a new educational law reintroduced the bachelor's degree (first degree after three years of study) in Germany. Today, these degrees can be called either \"Bakkalaureus\" or \"Bachelor\" (in accordance with federal law), but the English term is more common. The traditional degrees were abolished in 2010.\n\nThe traditional degrees have been re-mapped to the new European Credit Transfer and Accumulation System (ECTS) point system to make them comparable to the new bachelor's degree. Traditional and Bologna process degrees are ranked as follows in Germany:\n\n    Bachelor o.d. (ordinary degree): New, 180, 210, or 240 ECTS points required;\n    Diplom FH: Traditional, 240 ECTS;\n    Diplom Uni or TH: Traditional, 300 ECTS;\n    Master: New, also 300 ECTS (including bachelor).\n\nItaly\n\nThe old four-, five-, or six-year laurea system was discontinued in the early 2000s as per the Bologna process, with some exceptions such as law school or medical school. The bachelor's degree, called \"Laurea\", takes three years to complete (note that Italian students graduate from high school at age 19) and grants access to graduate degrees (known as \"Laurea Magistrale\"). In order to graduate, students must earn 180 credits (ECTS) and write a thesis for which students have to elaborate on an argument under the supervision of a professor (generally from three to eight ECTS). Graduation marks go from 66 to 110. According to each faculty internal ruling, a lode may be awarded to candidates with a 110/110 mark for recognition of the excellence of the final project.\nFormer Yugoslav Republic of Macedonia\n\nIn 2003, the German-style education system was changed to conform to the ECTS because of the Bologna process. The existing academic degree granted with a diploma was transformed into a baccalaureus (bachelor's degree). The universities usually award a bachelor's degree after three years (following which, a master's degree will be two years long) or four years (following which, a master's degree will be one year long).\nNetherlands\n\nIn the Netherlands, the Bachelor of Arts and Master of Arts degrees were introduced in 2002. Until that time, a single program that led to the doctorandus degree was in effect, which comprised the same course load as the bachelor's and Master's programs put together. (The doctorandus title was in use for almost all fields of study; other titles were used for legal studies (meester) and engineering (ingenieur).) Those who had already started the doctorandus program could, upon completing it, opt for the doctorandus degree (before their name, abbreviated to 'drs.'), or simply use the master's degree (behind their name) in accordance with the new standard. Since these graduates do not have a separate bachelor's degree (which is in fact—in retrospect—incorporated into the program), the master's degree is their first academic degree.\n\nIn 2003/2004, the Dutch degree system was changed because of the Bologna process. Former degrees included:\n\n    baccalaureus (bc. for bachelor, corresponding to a B.A.Sc. or B.A.A. degree, it may be formally rendered as \"B\", followed by the specialization field, instead of \"bc.\")\n    doctorandus (prefix abbreviated to drs.; it corresponds to M.A. or M.Sc., but it may be formally rendered as M instead of drs.),[18]\n\ningenieur\n\n    ing. for graduates of the four-year courses offered by Dutch higher vocational colleges (HBO, that is; hoger beroepsonderwijs) see: university of applied science. It is similar to a B.A.Sc., B.Eng., B.B.E., B.A.S. or B.I.C.T. (B.I.T.), and it may be formally rendered as B followed by the specialization field, instead of ing.\n    ir. for those having graduated from technical university after a minimum of five years, corresponding to a M.Sc., but it may be formally rendered as M, instead of ir.),\n    meester in de rechten (mr.; it corresponds to LL.M., but it may be formally rendered as M instead of mr.) and\n    doctor (dr.; it corresponds to Ph.D., but it may formally be rendered as D instead of dr.)[19] are still granted along with their international equivalents.[20]\n\nWhile the titles ing., bc., ir., mr., drs. and dr. are used before one's own name, the degrees B, M or D are mentioned after one's name. It is still allowed to use the traditional titles.\n\nWhether a bachelor's degree is granted by a hogeschool or university is highly relevant since these parallel systems of higher education have traditionally served somewhat different purposes, with the vocational colleges mainly concentrating on skills and practical training. A B.A. or B.Sc. from a university grants 'immediate' entry into a master's program. Moreover, this is usually considered a formality to allow students to switch to foreign universities master's programs. Meanwhile, those having completed a HBO from a vocational college, which represented the highest possible level of vocational education available, can only continue to a \"master's\" on completion of a challenging year of additional study, which in itself can serve as a type of selection process, with the prospective M.Sc. students being required to cover a great deal of ground in a single year.\n\nRecently, HBO (vocational) master's degrees have been introduced in the Netherlands. Graduates thereof may use neither the extension \"of Arts\" (M.A.) nor \"of Science\" (M.Sc.). They may use an M followed by the field of specialization (e.g., M.Des).\n\nThis year of study to \"convert\" from the vocational to academic (WO-wetenschappelijk onderwijs, literally \"scientific education\") is also known as a \"bridge\" or \"premasters\" year. Note that despite the use of the terminology \"university of applied science\" the higher vocational colleges are not considered to be \"universities\" within the Netherlands.\n\nImportant aspects of Dutch bachelor's degree courses (and others) relative to some of those offered abroad include:\n\n    Duration. While in many countries courses are completed in a given time under normal circumstances, degree courses offered at some (though by no means all) Dutch institutions, including the most prestigious, can only be completed in three years by the best performing students.\n    Academic year. The Dutch academic year has a formal duration of 42 weeks. In practice students are often expected and required to spend a great deal of the \"free\" time revising for examinations. This is not always true elsewhere, as in many countries a very long summer break is taken and/or examinations are before the winter break rather than after.\n    Learning curve. Some education systems, notably the British one, involve a gentle introduction during the first year. This is generally not the case in the Netherlands, with the difficulty level in the first year serving as a type of \"self-selection\" with less committed and less able students routinely finding it difficult to keep up.\n\nIn February, 2011, the Dutch State Secretary of Education decided to adhere to the recommendations written in a report by the Veerman Commission. In the near future, the distinction between academic and higher vocational degrees will disappear.\nPoland\n\nIn Poland, the licentiate degree corresponds to the bachelor's degree in Anglophone countries. In Polish, it is called licencjat. To obtain the licencjat degree, one must complete three years of study. There is also a similar degree called engineer (Inżynier) which differs from the licencjat in that it is awarded by technical universities and the program usually lasts for 3.5 years. After that, the student can continue education for 2 or 1.5 years, respectively, to obtain the Polish magisterium degree, which corresponds to a master's degree.\nPortugal\nA licenciatura (equivalent to a bachelor) degree diploma from Portugal\n\nPresently, the Portuguese equivalent of a bachelor's degree is the licenciatura, awarded after three years of study (four in some few cases) at an accredited university or polytechnical institution. It is an undergraduate first study cycle program which is required to advance into further studies such as master's degree programs.\n\nBefore the Bologna process (2006/2007), the bacharelato (bachelor's degree) existed in the Portuguese higher education system. It required three years of study, being roughly equivalent to the present licenciatura. At that time, the licenciatura referred to a licentiate's degree (equivalent to the present master's degree), which required usually five years of study. A licenciatura could also be obtained by performing two years of study after obtaining a bacharelato.\n\nToday, the former and current licenciatura degrees are referred in Portugal, respectively, as pre-Bologna and post-Bologna licenciaturas.\nRussia, Ukraine, and Armenia\n\nThe specialist's degree (Russian: специалист), (Ukrainian: спецiалiст) was the first academic distinction in the Soviet Union, awarded to students upon completion of five-year studies at the university level. The degree can be compared both to the bachelor's and master's degree. In the early 1990s, Bakalavr (Бакалавр, \"bachelor\") degrees were introduced in all the countries of the Commonwealth of Independent States except Turkmenistan. After the bakalavr degree (usually four years), one can earn a master's degree (another one or two years) while preserving the old five-year specialist scheme.\nSpain\n\nIn Spain, due to the ongoing transition to a model compliant with the Bologna agreement, exact equivalents to the typical Anglo-Saxon bachelor's degree and master's degree are being implemented progressively. Currently, there is an undergraduate bachelor's degree called \"Título de Grado\" or simply \"Grado\" (its duration generally being four years), a postgraduate master's degree called \"Título de Máster\" or \"Máster\" (between one and two years), and a doctor's degree called \"Título de Doctor\" or \"Doctorado\". The \"Título de Grado\" is now the prerequisite to access to a Master study. The \"Título de Máster\" is now the prerequisite to access to doctoral studies, and its duration and the kind of institutions that can teach these programs are regulated in the framework of the European Higher Education Area.\n\nUp until 2009/2010, the system was split into three categories of degrees. There were the so-called first-cycle degrees: \"Diplomado\" or \"Ingeniero Técnico\", with nominal durations varying between three and four years; there were also second-cycle degrees: \"Licenciado\" or \"Ingeniero\" with nominal durations varying between four and six years; and finally the third-cycle degrees: \"Doctor.\" The official first-cycle degrees are comparable in terms of duration, scope, and educational outcomes to an Anglo-Saxon bachelor's degree. Meanwhile, the second-cycle degrees are comparable in terms of duration, scope, and educational outcomes to an Anglo-Saxon bachelor's + Master's degrees combination if compared with the Anglo-Saxon system. In this traditional system the access to doctoral studies was granted only to the holders of \"Licenciado\", \"Ingeniero\" or \"Arquitecto\" (second-cycle) degrees, and the \"Master\" or \"Magister\" titles were unregulated (so, there coexisted so-called \"Master\" programs with different durations, from some months to two years, backed by universities or centers without any official recognition) and only the reputation of the program/institution could back them.\nSweden\n\nThe Swedish equivalent of a bachelor's degree is called kandidatexamen. It is earned after three years of studies, of which at least a year and a half in the major subject. A thesis of at least 15 ECTS credits must be included in the degree. Previously, there was a Bachelor of Law degree (juris kandidat) which required 4.5 years of study, but this degree now has a new name, juristexamen (\"law degree\").\nSwitzerland\n\nLike Austria and Germany, Switzerland did not have a tradition of bachelor's and master's degrees. In 2003, after the application of the Bologna process, bachelor's and graduate master's degrees replaced the old degrees. As of 1 December 2005 the Rectors' Conference of the Swiss Universities granted holders of a lizentiat or diploma the right to use the corresponding master title. As of 2006, certificates of equivalence are issued by the university that issued the original degree. Currently three to four years of study are required to be awarded a bachelor's degree. A master's degree will require another two to three years of coursework and a thesis.\nUnited Kingdom\n\nThe Universities of Oxford and Cambridge are perhaps alone in the United Kingdom today in awarding the B.A. for all undergraduate degrees. However, on a global scale, many universities over the last hundred years have expanded the range of bachelor's degrees enormously, especially in countries such as Australia, New Zealand, Pakistan, India, and South Africa. This represents a move towards specialization in tertiary education, in which college or university in these countries is intended to be a training for a specific career, and therefore akin to vocational education. It is a departure from the liberal arts approach common in the United States, in which the graduate is versed in a wide variety of subjects in addition to an academic major with the intent they be well prepared to pursue any number of careers or a progression of careers.\n\nIn England, most first degrees not leading to professions (such as law, engineering, medicine) are now assumed to be honours degrees, although ordinary degrees are still awarded to those who do not meet the required pass mark for a third-class honours degree.\n\nA full list of British degree abbreviations is also available.\nEngland, Wales, and Northern Ireland\n\nDegrees awarded carry designations related to the broad subject areas such as B.A., B.Sc., and B.Eng. The majority of bachelor's degrees are now honours degrees. Until the mid-20th century, some candidates (but not, for example, those at Oxford or Cambridge) would take an ordinary degree and then be selected to go on for a final year for the honours degree. A first degree course is usually three years but it might be reduced to two either by direct second-year entry (for people who have done foundation degrees or changed subject areas or done something similar) or by doing compressed courses (which are being piloted by several newer universities).[21] For funding reasons (funding for undergraduate programs is automatic, while funding for postgraduate programs is not), it is becoming increasingly common to skip the bachelor's stage entirely and to go straight to the Master's level on a four-year (five-year if with industrial experience) course (which often shares the first two years with the equivalent bachelor's course).\n\nHonours degrees are of a superior academic standard. An honours degree is always awarded in one of four classes depending upon the marks gained in the final assessments and examinations. The top students are awarded a first-class degree, the next best, an upper second-class degree (usually referred to as a 2:1), the next a lower second-class degree (usually referred to as a 2:2), and those with the lowest marks gain a third-class degree. An ordinary or unclassified degree (which does not give the graduate the right to add (Hons)) may be awarded if a student has completed the full honours degree course but has not obtained the total required passes sufficient to merit a third-class honours degree. Alternatively, a student may be denied honours if he/she has had to retake courses.\nMain article: British undergraduate degree classification\n\nOrdinary degrees are unclassified degrees awarded to all students who have completed the course and obtained sufficient marks to pass the final assessments and examinations. Although ordinary degree courses are often considered to be easier than honours degree courses, this is not always the case, and much depends on the university attended and the subject being studied. Some modern universities offer the opportunity for ordinary degree students to transfer to an honours degree course in the same subject if an acceptable standard is reached after the first or second year of study.\n\nThe graduateship (post-nominal GCGI) and associateship (post-nominal ACGI, Imperial College) awarded by the City & Guilds of London Institute are mapped to a British honours degree.\nScotland\n\nAt Scottish universities, undergraduate degrees are differentiated as either Designated Degrees or Honours Degrees.\n\nAn Honours degree, B.A. (Hons), (awarded as an M.A. (Hons) by some universities) for arts and social sciences, or B.Sc. (Hons) for sciences, is awarded for students who have completed four years at university—two years at sub-honours level, studying a variety of different subjects, and two years at honours level studying one subject in depth. Depending on the University and course, the third and fourth years can be divided into junior honours and senior honours while in other cases the final year is simply called the honours year.\n\nEach year, students will be given a number of subjects to study, with each subject given a number of credits which can be awarded upon passing that subject. A certain number of credits are required to gain entry into the next year and the amount of credits awarded each year is directly related to the subjects passed that year. As such, it is generally required to pass all subjects in a year to gain entry into the next.\n\nIt is typical that the grade of the degree will be assessed from the combined grades of both years at honours level, meaning the grades achieved in each subject in these years will account to the final class of degree awarded. Generally the percentage contributed toward the degree class in the final honours year will be much higher than that of the first honours year. Also, sometime in the honour years, a course usually includes a large project or number of projects to be undertaken, such as a Dissertation—although not always the case. A dissertation contributes a large percentage to that years grade as the workload is often viewed as a number of subjects combined and is therefore graded in the same manner.\n\nHonours degrees are subdivided into classes depending on the overall grade achieved. These are from highest to lowest; first class, upper second class (2:1), lower second class (2:2), and third class.\n\nA designated degree (B.A., M.A. or B.Sc.) is awarded to students who have completed three years at university studying a variety of related subjects. The first two years, sometimes three, of both a designated degree and an honours degree are identical, but candidates for the designated degree study in less depth in their final year and often over a wider variety of subjects. Candidates for the designated degree do not usually complete a dissertation. A Scottish \"designated degree\" is different from an English \"pass degree\", even though both are denoted \"B.Sc.\" (Bachelor of Science) and are often referred to as ordinary degrees. In keeping with the Scottish \"broad education\" philosophy, ordinary degrees (and more rarely honours ones) may mix different disciplines such as sciences and humanities taught in different faculties and in some cases even different universities.\nUK medical schools\n\nDifferent universities and/or degrees however may have different processes. For example, the University of St Andrews' Bute Medical School traditionally awards medical students a B.Sc. (Hons) after a three-year degree course—one year studying \"Foundations of Medicine\" involving basic medicine training, followed by a two-year honours course and dissertation.[22] This is not consistent with the usual four years required to attain such a degree.\n\nMedical students at many other UK institutions have the further privilege of obtaining intercalated degrees (see medical school in the United Kingdom); a process which allows participants to obtain an intercalated B.Sc. in a given field after only one year of study. Such programs are available at most universities and are in place to offer students studying medicine a wider perspective on fields that are often only briefly covered in a medicine course.\n\nStudents are not automatically eligible to participate. This process in many ways reflects the US system of first obtaining a degree before studying postgraduate medicine.\n\nBelow, see discussion of the norm for obtaining a degree in medicine and of medical school in the United Kingdom.\nTurkey\n\nBachelor's degrees exist in almost every city in Turkey. Mostly preferred universities of Turkey are Middle East Technical University, Boğaziçi University, Hacettepe University, Izmir University of Economics[23] Ankara University, Istanbul Technical University, Istanbul University, Yildiz Technical University, Bilkent University, Koç University, by B.A. students. They all grants Bachelor of Arts or Bachelor of Science degrees upon completion of eight-semester programs offered by its faculties and the School of Foreign Languages. Also double-major is available in those universities. Some universities offer the opportunity for ordinary degree students to transfer to an honours degree course in the same subject if an acceptable standard is reached after the first or second year of study. It is called in Turkish \"Lisans Mezunu.\"\n\nWhile some of the public and private universities are offering 30% English in their programs, there are also many universities which offer 100% English language in the programs such as Middle East Technical University[24] or Izmir University of Economics.[25]\nTypes\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (May 2013) (Learn how and when to remove this template message)\n\tIt has been suggested that this section be split into an article. (Discuss) (June 2014)\n\nMany other specialized bachelor's degrees are offered as well. Some are in very specialized areas, like the five-year B.I.Des. or B.Sc.I.Des. degree in industrial design.[26] Others are offered only at a limited number of universities, such as the Walsh School of Foreign Service at Georgetown University's Bachelor of Science in Foreign Service (B.Sc.F.S.). The University of Delaware offers a Bachelor of Applied Arts and Science (B.A.A.Sc.) degree, a degree which often indicates an interdisciplinary course of study for many majors within its School of Arts and Science.[27] Stanford University's Bachelor of Arts and Science degree is for students who are receiving one degree but who have completed two arts and sciences majors, one of which would ordinarily lead to the B.A. and one of which would ordinarily lead to the B.Sc.\n\nAt many institutions one can only complete a two-degree program if the bachelor's degrees to be earned are of different types (e.g., one could earn a B.A. in philosophy and a B.Sc.C.Eng. in chemical engineering simultaneously, but a person studying philosophy and English would receive only a single B.A. with the two majors). Rules on this vary considerably, however.\nAgriculture\n\nThe Bachelor of Science in Agriculture [B.Sc. (Ag) or B.Sc. (Hons.) Agriculture] offers a broad training in the sciences. The focus of this four-year applied degree is on the development of analytical, quantitative, computing and communication skills. Students learn how to apply the knowledge and principles of science to the understanding and management of the production, processing and marketing of agricultural products, and to the management and conservation of our natural resources. All students undertake rural field trips and approved professional experience within agricultural or horticultural enterprises, natural resource management, agribusiness industries, or commercial or government organisations active in the field.\nArchitecture and design\n\nThe Bachelor of Architecture (B.Arch.) degree is a professional degree awarded to students who complete the five-year course of study in the field at some universities. Many universities offer a B.Sc. or B.A. (majoring in Architecture) after the first three or four years, and then a post-graduate diploma, B.Arch. or M.Arch. for the following two to four years.\n\nThe Bachelor of Design (B.Des., or S.Des. in Indonesia) is awarded to those who complete the four- or four-and-a-half-year course of study in design, usually majoring in a specific field of design, such as interior design or graphic design.\nArts\nMain article: Bachelor of Arts\n\nThe Bachelor of Arts degrees (B.A., A.B.; also known as Artium Baccalaureus) along with the Bachelor of Science degrees are the most common undergraduate degrees given. Originally, in the universities of Oxford, Cambridge and Dublin, all undergraduate degrees were in the faculty of arts, hence the name of the degree. The Bachelor of Applied Arts and Sciences (B.A.A.Sc.) is an undergraduate degree that bridges academic and work-life experiences.\nEngineering\n\n    The Bachelor of Engineering (Baccalaureus in Arte Ingeniaria) degree or Bachelor of Applied Science[28] degree is a professional degree awarded to students who have completed the three- or four-year course of study in engineering. Common abbreviations include B.Eng., B.Sc.Eng., B.A.Sc., B.Tech., AMIE, and GradIETE. The B.A.Ing. (baccalaureus in arte ingeniaria) is awarded by the University of Dublin (Trinity College Dublin); some South African universities refer to their engineering degrees as B.Ing. (Baccalaureus Ingeniaria).\n    There are more specific variants for many subfields, such as the B.Sc.E.Eng. degree (Bachelor of Science in Electrical Engineering). The B.Sc.Eng. and B.S.Eng (Bachelor of Software Engineering) are awarded by the University of Waterloo, McMaster University, and University of Victoria, respectively. In India the Bachelor of Engineering, Bachelor of Technology, and AMIE (Association of Membership of Institution of Engineers) are professional degrees awarded in specific engineering disciplines such as computers, electrical, electronics, mechanical, communication, civil, plastics, chemical, etc. Specialization is referred to in brackets (e.g., \"BE (computers)\").\n    The Bachelor of Science in Engineering Technology degree (B.Sc.Eng.Tech.) is a professional degree awarded to students who have completed a four-year course of study in engineering technology. There are variants including general engineering technology, mechanical engineering technology, electrical engineering technology, and civil engineering technology. Some of these variants even have optional areas of concentration within them. For instance mechanical engineering technology could include mechanical systems design, manufacturing systems, marine engineering technology, and others. Engineering technology degrees usually lead to licensing as engineering technologists requiring further studies for licensing as professional engineers.\n\nBusiness and management\n\n    See: Business education#Undergraduate education.\n\nComputer science and information systems\n\nThere are various undergraduate degrees in information technology incorporating programming, database design, software engineering, networks and information systems. These programs prepare graduates for further postgraduate research degrees or for employment in a variety of roles in the information technology industry. The program focus may be on the technical or theoretical aspects of the subject matter, depending on which course is taken.\n\n    Theoretically oriented degrees focus on computer science and are correspondingly titled. These include the Bachelor of Computing (B.Comp.) and Bachelor of Computer Science (B.Comp.Sc.) degrees. Computer science is also offered as a major within most Bachelor of Science programs.\n    The practically oriented degrees cover many disciplines from within the IT industry including software engineering, information systems, and data communications. Examples here include the Bachelor of Science in Information Technology (B.Sc.I.T.), the Bachelor of Computer Applications (B.C.A.), the Bachelor of Information Technology, and the Bachelor of Applied Science (Information Technology) (B.A.Sc.I.T.)) degrees. Many of the disciplines taught as part of these degrees are covered under other degrees, such as engineering, as well.\n    Degrees combining IT with business study are also offered at many universities. Specialized programs in information systems—such as the bachelor of business information systems (BBIS) program—are often positioned as professionally oriented degrees. More general degrees here would include business degrees, such as the B.B.A. or B.Com., with information systems as a concentration.\n\nHealth Care\nMedicine\n\nIn countries following British tradition, (the University of Malta is an exception) medical students pursue an undergraduate medical education and receive bachelor's degrees in medicine and surgery (M.B.B.Chir., M.B.B.S., B.M.B.S., B.M., M.B.Ch.B., etc.). This was historically taken at the universities of Oxford, Cambridge, and Dublin after the initial B.A. degree, and in Oxford, Cambridge, and Dublin the B.A. is still awarded for the initial three years of medical study, with the B.M.B.Ch., M.B.B.Chi., or M.B.B.Ch.B.A.O., respectively, being awarded for the subsequent clinical stage of training. Some British universities give a bachelor's degree in science, or medical science, mid-way through the medical course, and most allow students to intercalate a year of more specialized study for an intercalated Bachelor of Science (B.Sc.), Bachelor of Medical Science (B.Med.Sc.), or Bachelor of Medical Biology (B.Med.Biol.) degree with honors. Although notionally M.B. and B.Sc. are two degrees, they must be taken together, and by convention entitle the bearer to use the title of doctor. In some Irish universities, a third degree, Bachelor of Arts in Obstetrics (B.A.O.), is added. However, this third degree is an anachronism from the 19th century and is not registerable with the Irish Medical Council. The nonuniversity (licentiate) qualifications allowing registration as a medical practitioner in the UK, a registration that has not been granted by the United Examining Board since 1999, also conferred the courtesy title of \"doctor.\"\nDentistry\n\nDentistry is offered both as an undergraduate and a postgraduate course. The doctorate of dental surgery (DDS) is the usual undergraduate program. Postgraduate courses such as the Bachelor of Dentistry (B.Dent.)—awarded exclusively by the University of Sydney in Australia—requires a previous bachelor's degree.\nMidwifery\n\nThe Bachelor of Midwifery degree is a professional degree awarded to students who have complete a three- to five-year (depending on the country) course of study in midwifery. Common abbreviations include B.Sc.Mid, B.Mid, B.H.Sc.Mid.\nPhysiotherapy\n\nPhysiotherapy is offered both as an undergraduate and a graduate course of study. Studies leading to the Bachelor of Physiotherapy (B.P.T.) degree usually constitute the undergraduate program. In the graduate program, courses leading to a degree such as the Master of Physiotherapy degree are offered.\n\nIn the Canadian province of Quebec, French universities offer both undergraduate and graduate courses leading to the obtention of a Bachelor of Science degree with a major in physiotherapy and a Master of Science degree specialized in physiotherapy. McGill University, the Université de Montréal, and the Université de Sherbrooke are among the post-secondary institutions that offer such programs.\nOptometry\n\nOptometry is a four-year or five-year course. Although students graduate with a B.Sc. after three years of study, passing a further supervised preregistration year is required to become a fully qualified optometrist. The National Institute of Ophthalmic Sciences is among the post-secondary institutions that offer such programs. It is the academic arm of The Tun Hussein Onn National Eye Hospital and the only eye hospital based institution in Malaysia.\nNursing\n\nThe Bachelor of Nursing degree is a three- to five-year undergraduate degree that prepares students for a career in nursing. Often the degree is required to gain \"registered nurse\", or equivalent, status—subject to completion of exams in the area of residence. Sometimes, though, the degree is offered only to nurses who are already registered. Alternate titles include Bachelor of Science in Nursing and Bachelor of Nursing Science, with abbreviations B.Sc.N., B.N.Sc.\nVeterinary science\n\nThe Bachelor of Veterinary Science program is generally a five-year course of study that is required for becoming a veterinarian. It is also known as the Bachelor of Veterinary Medicine and Surgery at some universities (B.V.M.S.). In the United States a degree in veterinary medicine is completed after a bachelor's has been earned (usually in four years). The recipient is called \"doctor,\" as is a medical doctor who treats human beings, and the training lasts as long as the training of a medical doctor, does, usually four years. Thus it takes eight years, usually, after high school, to become a veterinarian. Admission to veterinary training programs is considered at least as competitive as for medical school, in fact, it is generally found to be more competitive. No bachelor's degree of veterinary science is given in the United States, only the Doctor of Veterinary Medicine (D.V.M.) degree.\nPharmacy\n\nThe Bachelor of Pharmacy (B.Pharm.) degree is a common undergraduate degree for the practice of pharmacy. In the United States, Canada, and France, however, all colleges of pharmacy have now phased out the degree in favor of the Pharm.D., or doctor of pharmacy, degree or the Ph.D., doctor of philosophy, degree in pharmacy. Some universities, such as the University of Mississippi, award a Bachelor of Science in pharmaceutical sciences (B.Sc.P.Sc.) degree as a part of the seven-year Pharm.D. program after the completion of the first four years. However, the B.ScP.Sc. degree does not qualify the recipient for the practice of pharmacy, for which it is required that students earn a Pharm.D. degree.\nPublic Health\n\nPublic health is usually studied at the master's degree level. The Bachelor of Science in Public Health (B.Sc.P.H.) degree is a four-year undergraduate degree that prepares students for careers in the public, private, or nonprofit sectors in areas such as public health, environmental health, health administration, epidemiology, or health policy and planning.\nMedical and Health Sciences\n\n    The Bachelor of Health Science (B.H.Sc.) is a specialized degree awarded to students whose studies have focused on health care or the health sciences. Specific areas of study can include nursing, radiography, health care management, and other allied health fields. The degree is typically awarded following four to five years of collegiate study.\n    The title B.Med.Sc., B.Bio.Med.Sc., B.Med.Sc. or B.V.Med.Sc. is granted to students who have qualified in the field of biomedical science and medical science or veterinary medical science respectively. Universities that offer this course include the University of Western Ontario in Canada, University of Birmingham in the UK and the University of New South Wales, the University of Canberra, the University of Queensland, the University of Sydney, Flinders University, Griffith University, Monash University, Australian National University and the University of Melbourne in Australia.\n    The degree of B.Med.Sc. can be awarded for students completing an intercalated degree whilst studying medicine as an intermediate award. The degree of B.Med.Sc. may also be awarded to an individual who, having followed the prescribed course of study for the degrees of M.B.Ch.B., does not complete the undergraduate clinical training. In brief, this is normally awarded after the candidate has completed successfully the first three years of an undergraduate medical degree at certain UK (and Commonwealth) medical institutions.\n    The Bachelor of Science in human biology degree is awarded by several universities around the world and focuses on biomedical research, health care, biotech business, pharmaceutical sciences, or a combination thereof.\n\nKinesiology\n\nThe Bachelor of Kinesiology degree (B.K. or B.Sc.K.)is a specialized degree in the field of human movement and kinetics. Some schools still offer it under the aegis of a School of Physical Education (B.P.Ed. or B.H.P.Ed.), although \"kinesiology\" or \"human kinetics\" is currently the more popularly accepted term for the discipline.\nNutrition and Dietetics\n\nBachelor of Science in Nutrition and Dietetics (B.S.N.D.), Bachelor of Food Science and Nutrition (B.F.S.N.) Specific areas of study include clinical nutrition, food technology, hospitality and services management, research, community worker, health care management, educator, sports science, agricultural sciences, private practice and other allied health fields. The degree is awarded following four to six years of collegiate study in America (average five years), from three to four in Europe and Australia. In America (especially Latin America) Nutrition per se is separated from Dietetics, where the latter is equivalent to a technical degree.\nAviation\n\nThe Bachelor of Aviation (B.Av.) is awarded to students who complete a four-year course of study in the field.\nDivinity and theology\n\nThe Bachelor of Divinity, Bachelor of Theology, Bachelor of Religious Studies, Bachelor of Biblical Studies, and Bachelor of Religious Education (B.D., B.Th., B.R.S., B.B.S., and B.R.E.) degrees are awarded on completion of a program of study of divinity or related disciplines, such as theology, religious studies, or religious education.\n\nTraditionally the B.D. was in fact a graduate degree rather than a first degree, and typically emphasised academic theology, biblical languages etc. This has become a less common arrangement, but should be noted since, for example, a B.D. takes precedence over a Ph.D. in Cambridge University's order of seniority.\n\nWhile the theological bachelor's degree is generally conferred upon completion of a four-year program, it is also conferred in some specialized three-year programs. From there, the next level of advancement is generally the Master of Divinity (M.Div.), Master of Theology (Th.M.), Master of Religious Studies, or Master of Religious Education (M.R.E.) degree. In the United States the \"main line\" Protestant clergy typically take a four-year bachelor's degree in whatever field they choose, then earn the M.Div. (Master of Divinity) degree in an additional three years as part of preparation for ordination.\nFine arts\n\nThe Bachelor of Fine Arts (B.F.A.) degree is a specialized degree awarded for courses of study in the fine and/or performing arts, frequently by an arts school or conservatory, although it is equally available at a significant number of traditional colleges and universities. In contrast to the B.A. or B.S., which are generally considered to be academic degrees, the B.F.A. is usually referred to as a professional degree, whose recipients have generally received four years of study and training in their major field as compared to the two years of study in the major field usually found in most traditional non-Commonwealth Bachelor of Arts or Bachelor of Science programs.\n\nIn the United States, the Bachelor of Fine Arts degree differs from a Bachelor of Arts degree in that the majority of the program consists of a practical studio component, as contrasted with lecture and discussion classes. A typical B.F.A. program in the United States consists of two-thirds study in the arts, with one-third in more general liberal arts studies. For a B.A. in Art, the ratio might be reversed.\nFilm and television\n\nThe Bachelor of Film and Television (B.F.T.V.) degree is an undergraduate degree for the study of film and/or television production including areas of cinematography, directing, scriptwriting, sound, animation, and typography.\nIntegrated studies\n\nThe Bachelor of Integrated Studies (B.I.S.) is an interdisciplinary bachelor's degree offered by several universities in the United States and Canada. It allows students to design a customized and specific course of study to best suit their educational and professional objectives. Generally, this degree is sponsored by two or more departments within the university. Schools which confer the B.I.S. degree include the University of Manitoba, Pittsburg State University, University of South Carolina Upstate, Weber State University, Ferris State University, Arizona State University, University of Minnesota, Miami University (Ohio), the University of Virginia, the University of New Brunswick, and Tallinn University of Technology amongst others.\nJournalism\n\nThe Bachelor of Journalism (B.A.J. or B.Sc.J.) degree is a professional degree awarded to students who have studied journalism at a four-year accredited university. Not all universities, however, grant this degree. In the United States, schools tend to offer the B.A. or B.S. with a major in journalism instead. The world's oldest school of journalism at the University of Missouri offers a B.J. degree, not to be confused with the bachelor's degree in jurisprudence at Oxford University. In South Africa, Rhodes University has the oldest school of journalism in Africa and allows students to take a fourth-year specialisation to raise their B.A. to B.A.J. status, equivalent to a B.A. (Hons).\nLandscape architecture\n\nThe Bachelor of Landscape Architecture (B.L.Arch.) degree is awarded to students who complete the five- (in some countries four-) year course of study in the field.\nLiberal arts\n\nThe Bachelor of Liberal Arts, Bachelor of General Studies, Bachelor of Liberal Studies, Bachelor of Science in general studies, or Bachelor of Applied Studies (B.L.A., B.G.S., B.L.S., B.Sc.G.S., B.A.S.) degree is sometimes awarded to students who major in the liberal arts, in general, or in interdisciplinary studies. The Bachelor of Professional Studies is awarded to students who major in professional career studies.\nLibrary science\n\nThe Bachelor of Library Science or Bachelor of Library and Information Science (B.L.Sc., B.L.I.Sc.) degree is sometimes awarded to students who major in library science, although Master's of library science degrees are more common.\nMusic\n\nThe Bachelor of Music (B.Mus.) degree is a professional or academic undergraduate degree in music at most conservatories in the US and the UK. It is also commonly awarded at schools of music in large private or public universities. Areas of study typically include music performance, music education, music therapy, music composition, academic fields (music history/musicology, music theory, ethnomusicology), and may include jazz, commercial music, recording technology, sacred music/music ministry, or music business. Small liberal arts colleges and universities without schools of music often award only B.A. in music, with different sets of requirements. (see also: BFA)\nMortuary science\n\nThe Bachelor of Mortuary Science (B.M.S.) is a professional undergraduate degree, awarded by the Cincinnati College of Mortuary Science of Cincinnati, Ohio and Southern Illinois University Carbondale. It was introduced in 1986 and it is awarded to students that complete 120 semester hours of course work and receive passing scores on the National Board Exam administered by The International Conference of Funeral Service Examining Boards.[29]\nPhilosophy\n\nThe Bachelor of Philosophy (B.Phil. or Ph.B.) degree is either an undergraduate or graduate degree. Generally, it entails independent research or a thesis/capstone project.\nPsychology\n\nThe Bachelor of Arts or Science in Psychology (B.A.Psy., B.Sc.Psy) degree is a degree awarded to students who have completed a course of study in the field of psychology. Courses typically last five years, but may last as long as six. In Nepal there are three- and four-year courses available for higher-level students. See Psychologist#Licensing and regulation, Training and licensing of clinical psychologists.\nEducation\n\nThe Bachelor of Science in Education degree (B.Sc.Ed.) is a four-year undergraduate professional degree offered by many American colleges and universities for those preparing to be licensed as teachers. Variants include the B.Ed., B.A.Ed, B.A.T. (Bachelor of Arts for Teaching), and B.S.T. degrees. Preparation for the M.S. in education, this degree is most often received by those interested in early childhood, elementary level, and special education, or by those planning to be school administrators. Secondary level teachers often major in their subject area instead (i.e., history, chemistry, or mathematics), with a minor in education. Some states require elementary teachers to choose a subject major as well, and minor in education.\n\nIn Canada, the bachelor of education is a two-year professional degree in which students will specialise in either elementary or secondary education, and that is taken after the completion of a three or four year bachelor's degree with a major in a teachable subject, such as English, French, Mathematics, Biology, Chemistry, or a social science. Some universities also offer concurrent, five year programs with student completing both a bachelor's degree in arts or science as well as their B.Ed. The possession of a B.Ed. and a second bachelor's degree is required to teach in most public anglophone and francophone schools in Canada. The B.Ed. prepares teachers for completion of either M.A. (master's of arts) programs in education, M.Ed. (masters of education) programs, or post graduate certificates in education.\nScience with education\n\nThe Bachelor of Science and/with Education degree (B.Sc.Ed.) is a degree awarded to students who complete the four- to five-year course of study in the field of science (major and minor in General Biology, Chemistry, Physics, and Mathematics) and education. Although notionally B.Sc. and B.Ed. are two degrees, they must be taken together. The graduates will work as science (physics, chemistry, biology) teachers in high schools, as lecturers in pre university colleges and matriculation centers and can progress to postgraduate programs (M.Sc. and Ph.D.) in various areas in science or education.\nForestry\n\nThe Bachelor of Science in Forestry (B.Sc.F.) is a degree awarded to students who complete the four-year course of study in the field of forestry.\nScience\nMain article: Bachelor of Science\n\nThe Bachelor of Science degrees (B.Sc., Sc.B.) along with the Bachelor of Arts degrees are the most common undergraduate degrees given. The Bachelor of Applied Arts and Sciences (B.A.A.Sc.) is an undergraduate degree that bridges academic and work-life experiences.\nScience in law\n\nThe Bachelor of Science in Law degree (B.Sc.L.) is a special-purpose degree that allows someone who has had some prior studies but has not achieved a bachelor's degree to resume his or her education and take up the study of law with the goal of eventually receiving the juris doctor degree.\nSocial sciences\n\nThe Bachelor of Social Science (B.S.Sc.) is a three- or four-year undergraduate British degree that enables students to specialize in the area of social science. Compared to the Bachelor of Arts, which allows students to study a vast range of disciplines, the Bachelor of Social Science enables students to develop more central and specialized knowledge of the social sciences. Many universities place the Bachelor of Social Science between the Bachelor of Arts and Bachelor of Science undergraduate degrees.\nSocial work\n\nThe Bachelor of Social Work (B.S.W.) degree is a four-year undergraduate degree. Usually the first two years consist of liberal arts courses and the last two years focus on social work classes in human development, policy/law, research, and practice. Programs accredited by the Council on Social Work Education require B.S.W. students to complete a minimum of 400 field education or internship hours. Accredited B.S.W. programs often allow students who are interested in obtaining a Master of Social Work degree to complete the degree in a shorter amount of time or waive courses. In Latin America this is a four to five year degree that can replace liberal arts subjects into health sciences, resulting in social work as a type of community psychology and socioeconomic studies, focused in hospitals, prisons or pedagogy, among others.\nTechnology\n\nThe Bachelor of Technology degree (B.Tech) is a three- or four-year undergraduate degree. Generally, the program is comparable to a Bachelor of Science degree program, which is additionally supplemented by either occupational placements (supervised practical or internships) or practice-based classroom courses.\nLaw\n\nThe Bachelor of Laws (LL.B.) is the principal academic degree in law in most common law countries other than the United States, and anglophone Canada, where it has been superseded by the juris doctor (J.D.) degree.\nTalmudic law\n\nThe Bachelor of Talmudic Law degree (B.T.L.) is the degree awarded in most Yeshivas around the United States.\nTourism studies\n\nThe Bachelor of Tourism Studies (B.T.S.) degree is awarded to those who complete the four- or five-year course of study in tourism, laws regarding tourism, planning and development, marketing, economics, sociology, anthropology, arts and world history (dependent on the country in which one takes the course), ticketing, hospitality, computer applications, and much more. The course would have an interdisciplinary approach with a vast range of units so the tourismologist professional would be able to identify necessary actions toward a sustainable touristic environment focus on local community uniqueness, values and traditions. As tourism is a growing industry, in India there is a lot of opportunity for those who complete this course of study. It is available in select universities of India.\nMathematics\n\nThe Bachelor of Mathematics or Bachelor of Mathematical Sciences degree (B.Math. and B.Math.Sc.) is given at the conclusion of a four-year honors program or a three-year general program. Several universities, mostly in Canada and Australia, award such degrees. The usual degree for mathematics in all other countries is the B.Sc.\nUrban and regional planning\n\nThe Bachelor of Urban and Regional Planning or Bachelor of Urban Planning or just Bachelor of Planning degree (B.U.R.P., B.U.P., or B.Plan) is a degree offered at some institutions as a four or five-year[30] professional undergraduate degree in urban planning. Programs vary in their focus on studio work and may or may not involve professional practice.\nPublic affairs and policy management\n\nThe Bachelor of Public Affairs and Policy Management degree (B.P.A.P.Mgt.) is a specialized four-year honors degree dedicated to the study of public policy within an interdisciplinary framework. The degree was created as a direct response to the changing nature of civic society and the growing need for university graduates who can work effectively in the new policy environment.\nInnovation\nThe Bachelor of Innovation is a four-year degree in a range of different fields.[31][32] The major fields, in engineering business, arts, science or education, are similar to their associated B.A. or B.Sc. degrees. The general education elements are restructured to provide a common core of innovation, entrepreneurship and team skills.[33] The degree was created as a direct response to the increasing pace of innovation in today's society and the need for graduates that understanding effective teaming, as well as the innovation process.", "skillName": "Bachelor_degree."}
{"id": 3, "category": "Education", "skillText": "Master of Applied Science (abbreviations include MASc, MAppSc, MApplSc, M.A.Sc. and MAS) is an academic degree. It is conferred far more widely in the Commonwealth of Nations than in the US.\n\n\"MASc\" degrees are generally conferred in Commonwealth of Nations for engineering-related studies more academic than those required for the Master of Engineering (MEng). MASc degrees require coursework and a thesis, with the thesis being the major component, whereas a Master of Engineering may require only coursework and a project, with the coursework being the major component.[1]\n\n\"MAppSc\" and \"MApplSc\" degrees are conferred in Australia and New Zealand for a wider variety of professional studies, to include practitioner fields outside of engineering.\n\n\"MASc\" is conferred in at least thirteen North American universities: Arizona State University, Dalhousie University,[2] Delta State University, University of Delaware, University of Nebraska, Carleton university, University of Denver, University of Guelph, Memorial University of Newfoundland, Concordia University,University of Toronto,[3] McMaster University,[4] and York University,[5] in the more expansive manner of Australia and New Zealand.\n\n\"MESc\" is conferred at The University of Western Ontario,[6] although the degree is equivalent to the MASc.", "skillName": "Master_of_Applied_Science."}
{"id": 4, "category": "Education", "skillText": "The Chemistry Quality Eurolabels or European Quality Labels in Chemistry (Labels européens de Qualité en Chimie) is a marketing scheme for chemistry degrees at institutions located within the 45 countries involved in the Bologna process. Labels are awarded to qualifying institutions under the names are Eurobachelor and Euromaster, as well as the proposed Eurodoctorate. Label Committee not only prepares for the ECTN Administrative Council proposals to award the Eurolabels but also judge the quality of chemical education programmes at HEIs. ECTN and its Label Committee closely collaborates with EuCheMS and American Chemical Society.\n\nIt is a framework which is supported by EuCheMS, and the labels are awarded by ECTN. The project is supported by the European Commission (EC) through its SOCRATES programme. The purpose of the framework is to \"promote recognition of first, second cycle degrees, and third cycle degrees not only within the 45 countries involved in the Bologna process\".\n\nContents\n\n    1 History\n    2 Label committees\n        2.1 2016 - 2018\n        2.2 2015 - 2016\n        2.3 2014 - 2015\n        2.4 2013 - 2014\n        2.5 2008 - 2013\n        2.6 2006 - 2008\n        2.7 2004 - 2006\n    3 Eurobachelor\n    4 Euromaster\n    5 Eurodoctorate\n    6 Awarded labels\n    7 See also\n    8 References\n    9 External links\n\nHistory\n\nEuropean Union promoted the Bologna process and the creation of a single European higher education area, both of which require mobility of graduates across Europe.\n\nECTN (European Chemistry Thematic Network) worked in the EU project \"Tuning Educational Structures in Europe\" and developed Eurobachelor, a framework for a first cycle qualification (first degree) in chemistry. EuCheMS approved Eurobachelor in October 2003.[1]\n\nIn June 2004 the Bologna process seminar \"Chemistry Studies in the European Higher Education Area\" approved Eurobachelor.[1]\nLabel committees\n\nThe label committee members are as follows:[2]\n2016 - 2018\n\n    Reiner Salzer (Chair), University of Technology, Dresden, Germany\n    Martino Di Serio (Vice-Chair), University Federico II, Naples, Italy\n    Jiri Barek (Secretary), Charles University, Prague, Czech Republic\n\nand a number of members\n2015 - 2016\n\n    Reiner Salzer (Chair), University of Technology, Dresden, Germany\n    Martino Di Serio (Vice-Chair), University Federico II, Naples, Italy\n    Ray Wallace (Secretary), University of Nottingham Trent, Nottingham, UK\n    a number of members\n\n2014 - 2015\n\n    Reiner Salzer (Chair), University of Technology, Dresden, Germany\n    Pavel Drasar (Past-Chair), University of Chemistry and Technology, Prague, Czech Republic\n    Ray Wallace (Secretary), University of Nottingham Trent, Nottingham, UK\n    a number of members\n\n2013 - 2014\n\n    Reiner Salzer (Chair), University of Technology, Dresden, Germany\n    Pavel Drasar (Past-Chair), University of Chemistry and Technology, Prague, Czech Republic\n    Evangelia Varella (Secretary), University of Thessaloniki, Thessaloniki, Greece\n    a number of members\n\n2008 - 2013\n\n    Pavel Drasar (Chair), University of Chemistry and Technology, Prague, Czech Republic\n    Reiner Salzer (Vice-Chair), Technical University, Dresden, Germany\n    Richard Whewell (Secretary 2008), Strathclyde University, Glasgow, Scotland, United Kingdom\n    Evangelia Varella (Secretary 2008-2013), University of Thessaloniki, Thessaloniki, Greece\n    a number of members\n\n2006 - 2008\n\n    Raffaella Pagani (Chair)\n    Pavel Drasar (Vice-chair), University of Chemistry and Technology, Prague, Czech Republic\n    Terry Mitchell (Secretary)\n    a number of members\n\n2004 - 2006\n\n    Terry Mitchell (Chair)\n    Raffaella Pagani (Vice Chair)\n    David Barr (Secretary), Royal Society of Chemistry, United Kingdom\n    a number of members\n\nEurobachelor\n\nEurobachelor is a registered trademark and an initiative adopted by the EuCheMS General Assembly in 2003. It is associated with the Chemistry Quality Eurolabels. As of 8 April 2013, 60 Eurobachelor quality labels have been awarded. The label is intended for first cycle qualifications (bachelor's degrees).\n\nEurobachelor is based on 180 ECTS (European credits), which is comparable to the three-year British degrees, but it does not include the British concepts of honours degrees and ordinary degrees[3]\nEuromaster\n\nEuromaster is a registered trademark and an initiative adopted by the EuCheMS General Assembly in 2005. It is associated with the Chemistry Quality Eurolabels. As of 8 April 2013, 36 Euromaster quality labels have been awarded. The label is intended for master's degrees.\n\nEuromaster, introduced after Eurobachelor, is intended for second cycle qualifications (postgraduate degrees)[4]\nEurodoctorate\n\nEurodoctorate is associated with the Chemistry Quality Eurolabels. As of 8 April 2013, 1 Eurodoctorate quality label was awarded. The label is intended for third cycle qualifications (i.e. doctoral degrees).[5]\n\nThe Tuning Chemistry Subject Area Group (Tuning SAG) discussed with a working party of ECTN (European Chemistry Thematic Network Association) in a meeting held in February 2006 in Helsinki, Finland, taking into account the declarations of the Bergen Communiqué 2005. The EHEA Overarching Framework, which was approved by the Ministers of Education of European Union member states in Bergen uses the Dublin descriptors and Tuning SAG decided to use the Dublin descriptors to form a new set of descriptors, the Budapest descriptors for third cycle qualifications.[5]\n\nThe Chemistry Eurodoctorate Framework version 1 was published in November 2006.\nAwarded labels\n\nAs of 8 April 2013, 60 Eurobachelor, 36 Euromaster, and 1 Eurodoctorate labels have been awarded to 52 institutions and 3 consortia from 20 countries.[6]\n\nThe countries that have been awarded labels include:\n\n    Austria\n    Belgium\n    Czech Republic\n    Estonia\n    Finland\n    France\n    Germany\n    Greece\n    Hungary\n    Ireland\n    Italy\n    Kazakhstan\n    Morocco\n    Poland\n    Portugal\n    Slovakia\n    Slovenia\n    Spain\n    The Netherlands\n    The United Kingdom\n\nSee also\n\n    Bologna process\n    European higher education area\n    European Chemistry Thematic Network Association\n    EChemTest\n    Tuning Educational Structures in Europe (European Union project)\n    EuCheMS (European Association for Chemical and Molecular Sciences)\n    Tuning Chemistry Subject Area Group\n    Dublin descriptor\n    European Quality Labels", "skillName": "Chemistry_Quality_Eurolabels."}
{"id": 5, "category": "Education", "skillText": "A terminal degree is a university degree that is either highest on the academic track or highest on the professional track in a given field of study. The phrase \"terminal degree\" is used heavily in the United States, but is used less often outside of North America. The term is not generally used in the United Kingdom or Canada, for example, and its exact meaning varies somewhat between those areas and disciplines in which the term is used.\n\nAn earned academic (or research) doctorate[1] such as a Doctor of Philosophy (PhD) is considered the terminal degree in most academic fields of study in the United States. However, professional doctorates may be considered terminal degrees within the professional degree track, even though they are prerequisites for research degrees. In addition, in some countries there are degrees which are more advanced than the PhD, such as the higher doctorates in the United Kingdom and Russia, and the habilitation degree awarded in Germany.\n\nAlso, not all terminal degrees are doctorates. For example, in professional practice fields there are often terminal master-level degrees such as MEng (Master Engineering), MLArch and MArch standing for Master Landscape Architect and Master Architect or even bachelor-level degrees such as BArch which stands for Bachelor of Architecture or BEng for Engineers, MB (Bachelor of Medicine - UK). Most non-doctoral degrees are not terminal in academic terms, with the exception of the Master of Fine Arts (MFA). The MFA is an academically recognized terminal degree and is given to practitioners in the fine arts and performing arts.\n\nContents\n\n    1 Research degrees\n        1.1 Typical first professional and advanced professional degrees, professional/clinical doctorates and research doctorates\n    2 Professional degrees\n        2.1 Typical first professional degree\n        2.2 Advanced professional degrees\n    3 See also\n    4 References\n\nResearch degrees\nMain article: Doctor of Philosophy\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2008) (Learn how and when to remove this template message)\nGlobe icon.\n\tThe examples and perspective in this section may not represent a worldwide view of the subject. Please improve this article and discuss the issue on the talk page. (December 2010) (Learn how and when to remove this template message)\n\nIn academic fields, the typical terminal degree is that of Doctor of Philosophy, although others also exist. The first phase of the Ph.D. consists of coursework in the student's field of study and requires one to three years to complete. This is often followed by a preliminary or comprehensive examination and/or a series of cumulative examinations, in which the emphasis is on breadth rather than depth of knowledge. Finally, another two to four years is usually required for the composition of a substantial and original contribution to human knowledge embodied in a written dissertation that in the social sciences and humanities is typically 250 to 450 pages in length. Dissertations generally consist of (i) a comprehensive literature review, (ii) an outline of methodology, and (iii) several chapters of scientific, social, historical, philosophical, or literary analysis. Typically, upon completion, the candidate undergoes an oral examination, sometimes public, by his or her supervisory committee with expertise in the given discipline.\nTypical first professional and advanced professional degrees, professional/clinical doctorates and research doctorates\n\n    Doctor of Education (EdD)\n    Educational Specialist (EdS)\n    Technology:\n        Doctor of Engineering (Dr.-Ing./DEng/Dr. Eng./EngD)\n        Doctor of Information Technology (DIT)\n        Master of Library and Information Science (MLIS, MLS, MSLS) (Given in the US, by an ALA accredited school or program.)\n        Doctor of Computer Science (DSc.Comp, DCS, D.C.Sc.), D.C.Sc.)\n    Priesthood:\n        Doctor of Ministry (DMin)\n        Doctor of Theology (ThD or DTh)\n    Design:\n        Master of Art and Design (MAD) [2]\n        Master of Architecture (M.Arch)\n        Doctor of Architecture (D.Arch)\n        Doctor of Design (DDes)\n        Master of Design (MDes) [3]\n        Master of Landscape Architecture (MLArch and/or MLA)\n        Master of City Planning (MPLAN, MCRP, MUP, MCP, MCD or MURP)\n        Master of Fine Arts (MFA)\n        Master of Graphic Design (MGraph) [4]\n    Arts:\n        Doctor of Arts (DA)\n        Doctor of Music (DM)\n        Doctor of Musical Arts (DMA) (Usually awarded to performance majors in the musical arts)\n        Doctor of Modern Languages (DML)\n        Doctor of Philosophy (PhD or DPhil)\n        Doctor of Professional Studies (DPS)\n    Management:\n        Doctor of Business Administration (DBA)\n        Doctor of Management (DMgt or DM)\n        Master of Project Management (MPM)\n        Doctor of Public Administration (DPA)\n        Doctor of Economic Development(DED) [5]\n    Healthcare:\n        Doctor of Behavioral Health (DBH)\n        Doctor of Healthcare Administration (DHA)\n        Doctor of Health Science (DHSc)\n        Doctor of Medical Physics (DMP)\n        Doctor of Nursing Practice (DNP)\n        Doctor of Occupational Therapy (DOT or OTD)\n        Doctor of Physical Therapy (DPT)\n        Doctor of Podiatric Medicine (DPM)\n        Doctor of Psychology (PsyD)\n        Doctor of Public Health (DrPH, DPH)\n        Doctor of Science (DSc)\n        Doctor of Social Science (DSocSci)\n    Law:\n        Doctor of Canon Law (JCD)\n        Doctor of Juridical Science (JSD/SJD) (in the U.S.)\n\nProfessional degrees\nMain article: First Professional degree\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2008) (Learn how and when to remove this template message)\n\nA professional degree is a degree that is required, often by law as in the case of medical and other clinical professions, that must be earned first before the holder of the degree can practice in the profession. A speech-language pathologist, for example, must hold a master's degree in communicative disorders: speech-language pathology in order to practice, but an actor does not need a degree to act, even though there are degrees for acting available. In some fields, especially those linked to a profession (such as medicine or law), a distinction is to be drawn between a first professional degree, an advanced professional degree, and a terminal academic degree. A first professional degree is generally required by law or custom to practice the profession without limitation. An advanced professional degree provides further training in a specialized area of the profession. A first professional degree is an academic degree designed to prepare the holder for a particular career or profession, fields in which scholarly research and academic activity are not the profession, but rather the practice of the profession. In many cases such as law and medicine, the first professional degree is also the terminal degree, usually because no further advanced degree is required for practice in that field, even though more advanced academic degrees may exist.\nTypical first professional degree\n\n    Business:\n        Master of Accountancy (MAcc, MAcy, MSAcy)\n        Business Administration (MBA)[6]\n    Design:\n        Architect (B.Arch, M.Arch, D.Arch)\n        Architectural Engineering (B.A.E, M.A.E)\n        Landscape Architect (BLArch-First Professional Degree)\n        Urban Planning (MPLAN, MCRP, MUP, MCP, MCD or MURP)\n    Bachelor of Education or a Bachelor's usable for teaching (BEd), or BA, BME, BSE,BSocSc, BSc\n    Engineer/ Engineering (BE, BEng, MEng, BSE, BScEng, BASc)[7]\n    Healthcare:\n        Advanced Practice Nurse (DNP, DNAP, DNS, DNSc)\n        Audiologist (MS, or AuD)\n        Master of Health Administration (MHA)\n        Mental Health Counselor (MA, MSc, PhD)\n        Dentist (DMD, BDent, DDS, BDS, BDSc, BChD, CD, Cand.Odont., Dr.Med.Dent. etc.)\n        Midwife (BMid, BScMid)\n        Nurse (BSN, BSc)\n        Occupational Therapy (OTD, DrOT, or MSOT, MA, MOT)\n        Optometrist (OD, B.Optom)\n        Pharmacist (BPharm, BScPhm, PharmB, MPharm, PharmD)\n        Physical Therapist (DPT, DPhysio, MPT, BSPT)\n        Physician or Surgeon (M.D., D.O., MBBS, MDCM, MBChB, BMed, Dr.med, Dr.MuD, Cand.med, etc.).\n        Physician Assistant (MPAS, MPS, MS)\n        Podiatrist (DPM, DP, BPod, PodB, or PodD)\n        Psychologist (PhD, PsyD, ClinPsyD or EdS)\n        Radiation Therapist (BSc, BRad)\n        Radiographer (BSc, BMRSc or BRad)\n        Scientist (BSc, BS)\n        Social worker (BSW, BA, BSc)\n        Speech-Language Pathology (MA, MS)\n        Veterinarian (DVM, VMD, BVS, BVSc, BVMS, etc.)\n    Lawyer (J.D., LL.B.)\n    Minister (MDiv)\n\nAdvanced professional degrees\n\n    Education (MEd, MAT, MT, EdS)[8]\n    Engineering (MEng, MASc, MMSc, PD[9])\n    Health care:\n        Advanced Practice Registered Nurse (APRN: CRNA, NP, CNM, CNS) (DNP, DNAP, DNS, DNSc)\n        Biotechnology (ALM)\n        Dental Science (DDSc, Dr.Odont) (advanced degree in countries that award a bachelor's degree in dental surgery as first professional degree, usually awarded for outstanding research to a particular field of Dentistry)\n        Dentistry (MDS, MSD, MDSc, or DClinDent) (these are usually granted at the culmination of a specialty training program in dentistry in those programs that also require research and a thesis to be completed)\n        Medicine (MD) (advanced degree in countries that award a bachelor's degree in medicine or surgery as first professional degree, usually awarded for outstanding research to a particular field of Medicine)** Midwifery (MMid, MScMid)\n        Surgery (MS, MSurg, MCh, ChM, or MChir) (Usually granted after completion of surgery training program in conjunction with a research thesis)\n        Psychology (PsyD)\n        Social Science (DSocSci)\n        Social Work (MSW, DSW, ProfD or PhD)\n    Lawyer (LLM, LLD, PhD, JSD)\n    Ministry (DMin)", "skillName": "Terminal_degree."}
{"id": 6, "category": "Education", "skillText": "A Doctor of Philosophy (PhD or DPhil) is a type of doctorate degree awarded by universities in many countries. The degree, named after its historic roots as a Doctorate licensed to one allowed to profess a Philosophia, was formally awarded by European universities since the Medieval era circa 1150-1200.[1] PhDs are awarded for a wide range of programs in the sciences (e.g., biology, physics, mathematics, etc.), engineering, and humanities (e.g., history, English literature, musicology, etc.). The PhD is a terminal degree in many fields. The completion of a PhD is a requirement for employment as a university professor, researcher, or scientist in many fields. A clear distinction is made between an \"earned doctorate\", which is awarded for completion of a course of study and thesis or dissertation and an \"honorary doctorate\", which is an honorary title granted by a university to a successful or notable person.\n\nThe PhD degree varies considerably according to the country, institution, and time period, from entry-level research degrees to higher doctorates. A person who attains a doctorate of philosophy is automatically awarded the academic title of doctor. During the studies that lead to the degree, the student is called doctoral student or PhD student, but also \"doctoral candidate\" or \"PhD candidate\" once the student has completed all of the coursework and comprehensive examinations and is working on his or her thesis or dissertation.\n\nA PhD candidate must submit a project, thesis or dissertation often consisting of a body of original academic research, which is in principle worthy of publication in a peer-reviewed journal.[2] In many countries a candidate must defend this work before a panel of expert examiners appointed by the university. Universities award other types of doctorates besides the PhD, such as the Doctor of Musical Arts (DMA), a degree for music performers and the Doctor of Education (Ed.D.), a degree for professional educators.\n\nIn the context of academic degrees, the term \"philosophy\" does not refer solely to the field or academic discipline of philosophy, but is used in a broader sense in accordance with its original Greek meaning, which is \"love of wisdom\". In most of Europe, all fields (history, philosophy, social sciences, mathematics and natural philosophy/natural sciences)[3] other than theology, law, and medicine (the so-called professional, vocational, or technical curriculum) were traditionally known as philosophy, and in Germany and elsewhere in Europe the basic faculty of liberal arts was known as the \"faculty of philosophy\".\n\nContents\n\n    1 Terminology\n    2 History\n        2.1 Educational reforms in Germany\n        2.2 The doctorate in the United Kingdom\n        2.3 The doctorate in the United States\n    3 Requirements\n        3.1 PhD confirmation\n    4 Value and criticism\n        4.1 National variations\n    5 Degrees around the globe\n        5.1 Argentina\n            5.1.1 Admission\n            5.1.2 Funding\n            5.1.3 Requirements for completion\n        5.2 Australia\n            5.2.1 Admission\n            5.2.2 Scholarships\n            5.2.3 Fees\n            5.2.4 Requirements for completion\n        5.3 Canada\n            5.3.1 Admission\n            5.3.2 Funding\n            5.3.3 Requirements for completion\n        5.4 Colombia\n            5.4.1 Admission\n            5.4.2 Funding\n            5.4.3 Requirements for completion\n        5.5 Finland\n        5.6 France\n            5.6.1 History\n            5.6.2 Admission\n            5.6.3 Funding\n        5.7 India\n            5.7.1 Admission\n        5.8 Germany\n            5.8.1 Admission\n            5.8.2 Structure\n            5.8.3 Duration\n        5.9 USSR, Russian Federation and former Soviet Republics\n        5.10 Italy\n            5.10.1 History\n            5.10.2 Admission\n        5.11 Nepal\n        5.12 Poland\n        5.13 Scandinavia\n        5.14 Spain\n        5.15 United Kingdom\n            5.15.1 Admission\n            5.15.2 Funding\n            5.15.3 Completion\n            5.15.4 Other doctorates\n        5.16 United States\n    6 Models of supervision\n    7 International PhD equivalent degrees\n    8 See also\n    9 Notes and references\n    10 Bibliography\n    11 External links\n\nTerminology\n\nThe degree is also abbreviated Ph.D. or DPhil.[4] It is also called a Doctorate of Philosophy, from the Latin Doctor Philosophiae (for DPhil) or Philosophiae Doctor (for PhD or Ph.D.).\nHistory\n\nIn the universities of Medieval Europe, study was organized in four faculties: the basic faculty of arts, and the three higher faculties of theology, medicine, and law (canon law and civil law). All of these faculties awarded intermediate degrees (bachelor of arts, of theology, of laws, of medicine) and final degrees. Initially, the titles of master and doctor were used interchangeably for the final degrees—being that the title Doctor merely was a formality bestowed on a Teacher/Master of the art—but by the late Middle Ages the terms Master of Arts and Doctor of Theology/Divinity, Doctor of Law, and Doctor of Medicine had become standard in most places (though in the German and Italian universities the term Doctor was used for all faculties). The doctorates in the higher faculties were quite different from the current Ph.D. degree in that they were awarded for advanced scholarship, not original research. No dissertation or original work was required, only lengthy residency requirements and examinations. Besides these degrees, there was the licentiate. Originally this was a license to teach, awarded shortly before the award of the master or doctor degree by the diocese in which the university was located, but later it evolved into an academic degree in its own right, in particular in the continental universities.\n\nIn theory the full course of studies might lead in succession to the degrees of, e.g., Bachelor of Arts, Licentiate of Arts, Master of Arts, Bachelor of Medicine, Licentiate of Medicine, Doctor of Medicine. There were many exceptions to this, however. Most students left the university before becoming masters of arts, whereas regulars (members of monastic orders) could skip the arts faculty entirely.[5][6][7]\nEducational reforms in Germany\n\nThis situation changed in the early 19th century through the educational reforms in Germany, most strongly embodied in the model of the University of Berlin, founded and controlled by the Prussian government in 1810. The arts faculty, which in Germany was labelled the faculty of philosophy, started demanding contributions to research, attested by a dissertation, for the award of their final degree, which was labelled Doctor of Philosophy (abbreviated as Ph.D.)—originally this was just the German equivalent of the Master of Arts degree. Whereas in the Middle Ages the arts faculty had a set curriculum, based upon the trivium and the quadrivium, by the 19th century it had come to house all the courses of study in subjects now commonly referred to as sciences and humanities.[8] Professors across the humanities and Sciences focused on their advanced research.[9] Practically all the funding came from the central government, and could be cut off if the professor was politically unacceptable.[relevant? – discuss][10]\n\nThese reforms proved extremely successful, and fairly quickly the German universities started attracting foreign students, notably from the United States. The American students would go to Germany to obtain a Ph.D. after having studied for a bachelor's degrees at an American college. So influential was this practice that it was imported to the United States, where in 1861 Yale University started granting the Ph.D. degree to younger students who, after having obtained the bachelor's degree, had completed a prescribed course of graduate study and successfully defended a thesis or dissertation containing original research in science or in the humanities.[11] This research degree of doctor of philosophy was the first to be given in North America.[12] Even though in Germany the name of the doctorate was adapted accordingly after the philosophy faculty started being split up − e.g. Dr. rer. nat. for doctorates in the faculty of natural sciences − in most of the English-speaking world the name of Doctor of Philosophy was retained for research doctorates in all disciplines.\nThe doctorate in the United Kingdom\n\nThe Ph.D. degree spread to Canada in 1901, and then to Great Britain in 1917.[13] In particular in the English universities the introduction of the research doctorate largely happened to compete with Germany for American students, but the initiative was first halted by internal criticism. In first instance, in particular at the University of London (from about 1860 onwards), the degrees of Doctor of Science (DSc) and Doctor of Literature (DLit) were introduced, which could be awarded upon presentation of a thesis containing original work.[citation needed] This involved no research training however, and did not have the desired effect of attracting foreign research students. Finally in 1917 the current degree of Ph.D. was introduced, along the lines of the American and German model, and quickly became popular with both British and foreign students.[14] The slightly older degrees of Doctor of Science and Doctor of Literature/Letters still exist in British universities; together with the much older degrees of Doctor of Divinity (DD), Doctor of Music (DMus), Doctor of Civil Law (DCL) and Doctor of Medicine (MD) they form the higher doctorates, but apart from honorary degrees they are only infrequently awarded.\n\nIt should be noted that in the English (but not the Scottish) universities the Faculty of Arts had become dominant by the early 19th century. Indeed, the higher faculties had largely atrophied, since medical training had shifted to teaching hospitals,[15] the legal training for the common law system was provided by the Inns of Court (with some minor exceptions, see Doctors' Commons), and few students undertook formal study in theology. This is contrast with the situation in the continental European universities at the time, where the preparatory role of the Faculty of Philosophy or Arts was to a great extent taken over by secondary education, as is testified by the ongoing use to this day of the degree of Baccalaureat in France as the qualification obtained after secondary studies. The reforms at the Humboldt University transformed the Faculty of Philosophy or Arts (and its more recent successors such as the Faculty of Sciences) from a lower faculty into one on par with the Faculties of Law and Medicine. A similar evolution happened in many other continental European universities, and at least until reforms in the early 21st century many European countries (e.g. Belgium, Spain and the Scandinavian countries) had in all faculties triple degree structures of bachelor (or candidate) − licentiate − doctor as opposed to bachelor − master − doctor; the meaning of the different degrees varied a lot from country to country however. To this day this is also still the case for the pontifical degrees in theology and canon law: for instance, in Sacred theology the degrees are Bachelor of Sacred Theology (STB), Licentiate of Sacred Theology (STL), and Doctor of Sacred Theology (STD), and in Canon law: Bachelor of Canon Law (JCB), Licentiate of Canon Law (JCL), and Doctor of Canon Law (JCD).\nThe doctorate in the United States\n\nUntil the mid-19th century, advanced degrees were not a criterion for professorships at most colleges. That began to change as the more ambitious scholars at major schools went to Germany for 1 to 3 years to obtain a Ph.D. in the sciences or humanities.[16][17] Graduate schools slowly emerged in the United States. In 1861, Yale awarded the first three Ph.D.s in North America to Eugene Schuyler, Arthur Williams Wright, and James Morris Whiton.[18] In the next two decades, NYU, the University of Pennsylvania, Harvard, and Princeton also began granting the degree. Major shifts toward graduate education were foretold by the opening of Clark University in 1887, which only offered graduate programs, and the Johns Hopkins University which focused on its Ph.D. program. By the 1890s, Harvard, Columbia, Michigan and Wisconsin were building major graduate programs, whose alumni were hired by new research universities. By 1900, 300 Ph.D.'s were awarded annually, most of them by six universities. It was no longer necessary to study in Germany.[19][20]\n\nIn Germany, the national government funded the universities and the research programs of the leading professors. It was impossible for professors who were not approved by Berlin to train graduate students. In the United States, by contrast, private universities and state universities alike were independent of the federal government. Independence was high, but funding was low. The breakthrough came from private foundations, which began regularly supporting research in science and history; large corporations sometimes supported engineering programs. The postdoctoral fellowship was established by the Rockefeller Foundation in 1919. Meanwhile, the leading universities, in cooperation with the learned societies, set up a network of scholarly journals. \"Publish or perish\" became the formula for faculty advancement in the research universities. After World War II, state universities across the country expanded greatly in undergraduate enrollment, and eagerly added research programs leading to masters or doctorate degrees. Their graduate faculties had to have a suitable record of publication and research grants. Late in the 20th century, \"publish or perish\" became increasingly important in colleges and smaller universities.[21]\nRequirements\n\nDetailed requirements for the award of a Ph.D. degree vary throughout the world and even from school to school. It is usually required for the student to hold an Honours degree or a Master's Degree with high academic standing, in order to be considered for a Ph.D. program.[citation needed] In the US, Canada, India and Denmark, for example, many universities require coursework in addition to research for Ph.D. degrees. In other countries (such as the UK) there is generally no such condition, though this varies by university and field.[22] Some individual universities or departments specify additional requirements for students not already in possession of a bachelor's degree or equivalent or higher. In order to submit a successful Ph.D. admission application, copies of academic transcripts, letters of recommendation, a research proposal and a personal statement are often required. Most universities also invite for a special interview before admission.\n\nA candidate must submit a project or thesis or dissertation often consisting of a body of original academic research, which is in principle worthy of publication in a peer-reviewed context.[2] In many countries a candidate must defend this work before a panel of expert examiners appointed by the university; in other countries, the dissertation is examined by a panel of expert examiners who stipulate whether the dissertation is in principle passable and any issues that need to be addressed before the dissertation can be passed.\n\nSome universities in the non-English-speaking world have begun adopting similar standards to those of the anglophone Ph.D. degree for their research doctorates (see the Bologna process).[23]\n\nA Ph.D. student or candidate is conventionally required to study on campus under close supervision. With the popularity of distance education and e-learning technologies, some universities now accept students enrolled into a distance education part-time mode.\n\nIn a \"sandwich Ph.D.\" program, Ph.D. candidates do not spend their entire study period at the same university. Instead, the Ph.D. candidates spend the first and last periods of the program at their home universities, and in between conduct research at another institution or field research.[24] Occasionally a \"sandwich Ph.D.\" will be awarded by two universities.[25]\nPhD confirmation\n\nA PhD confirmation is a preliminary presentation or lecture that a PhD candidate presents to faculty and possibly other interested members. The lecture follows after a suitable topic has been identified, and can include such matters as the aim of the research, methodology, first results, planned (or finished) publications, etc.\n\nThe confirmation lecture can be seen as a trial run for the final public defense, though faculty members at this stage can still largely influence the direction of the research. At the end of the lecture, the PhD candidate can be seen as \"confirmed\" - faculty members give their approval and trust that the study is well directed and will with high probability result in the candidate being successful.\nValue and criticism\n\nPh.D. students are often motivated to pursue the Ph.D. by scientific and humanistic curiosity, the desire to contribute to the academic community, service to others, or personal development. A career in academia generally requires a Ph.D., though in some countries, it is possible to reach relatively high positions without a doctorate. In North America, professors are increasingly being required to have a Ph.D., because the percentage of faculty with a Ph.D. is used as a university ratings measure.\n\nThe motivation may also include increased salary, but in many cases this is not the result. Research by Casey suggests that, over all subjects, Ph.D.s provide an earnings premium of 26%, but notes that master's degrees provide a premium of 23% already. While this is a small return to the individual (or even an overall deficit when tuition and lost earnings during training are accounted for), he claims there are significant benefits to society for the extra research training.[26] However, some research suggests that overqualified workers are often less satisfied and less productive at their jobs.[27] These difficulties are increasingly being felt by graduates of professional degrees, such as law school, looking to find employment. Ph.D. students often have to take on debt to undertake their degree.\n\nA Ph.D. is also required in some positions outside academia, such as research jobs in major international agencies. In some cases, the Executive Directors of some types of foundations may be expected to hold a Ph.D. In the article \"The Peril of Credential Creep in Foreign Policy\", it is stated that \"[m]ore and more foreign policy professionals are required to hold master's or Ph.D. degrees to compete in their field.\"[28] The article states that \"If having a master's degree at the minimum is de rigueur in Washington's foreign policy world, it is no wonder many are starting to feel that the Ph.D. is a necessary escalation, another case of costly signaling to potential employers.\"[28] An article on the Australian public service states that \"credentialism in the public service is seeing a dramatic increase in the number of graduate positions going to Ph.D.s and masters degrees [are] becoming the base entry level qualification.\"[29]\n\nThe Economist published an article citing various criticisms against the state of Ph.D.s.[27] Richard B. Freeman explains that, based on pre-2000 data, at most only 20% of life science Ph.D. students end up getting jobs specifically in research. In Canada, where the overflow of Ph.D. degree holders is not as severe, 80% of postdoctoral research fellows earn less than or equal to the average construction worker (roughly $38,000 a year) during their postdoctoral research tenure.[27] Only in the fastest developing countries (e.g. China or Brazil) is there a shortage of Ph.D.s.\n\nHigher education systems often offer little incentive to move students through Ph.D. programs quickly, and may even provide incentive to slow them down. To counter this, the United States introduced the Doctor of Arts degree in 1970 with seed money from the Carnegie Foundation for the Advancement of Teaching. The aim of the Doctor of Arts degree was to shorten the time needed to complete the degree by focusing on pedagogy over research, although the Doctor of Arts still contains a significant research component. Germany is one of the few nations engaging these issues, and it has been doing so by reconceptualising Ph.D. programs to be training for careers, outside academia, but still at high-level positions. This development can be seen in the extensive number of Ph.D. holders, typically from the fields of law, engineering and economics, at the very top corporate and administrative positions. To a lesser extent, the UK research councils have tackled the issue by introducing, since 1992, the EngD.\n\nMark C. Taylor opines that total reform of Ph.D. programs in almost every field is necessary in the U.S., and that pressure to make the necessary changes will need to come from many sources (students, administrators, public and private sectors, etc.). These issues and others are discussed in an April 2011 issue of the journal Nature.[30][31][32][33]\nNational variations\n\nIn German-speaking nations; most Eastern European nations; successor states of the former Soviet Union; most parts of Africa, Asia, and many Spanish-speaking countries, the corresponding degree to a Doctor of Philosophy is simply called \"Doctor\" (Doktor), and the subject area is distinguished by a Latin suffix (e.g., \"Dr. med.\" for Doctor medicinae, Doctor of Medicine; \"Dr. rer. nat.\" for Doctor rerum naturalium, Doctor of the Natural Sciences; \"Dr. phil.\" for Doctor philosophiae, Doctor of Philosophy; \"Dr. iur.\" for Doctor iuris, Doctor of Laws).\nDegrees around the globe\nMain article: List of doctoral degrees awarded by country\nSee also: Doctor (title) § Worldwide usage, and Doctorate § Practice by country\n\nThe UNESCO, in its International Standard Classification of Education (ISCED), states that \"Programmes to be classified at ISCED level 8 are referred to in many ways around the world such as Ph.D., D.Phil., D.Litt., D.Sc., LL.D., Doctorate or similar terms. However, it is important to note that programmes with a similar name to \"doctor\" should only be included in ISCED level 8 if they satisfy the criteria described in Paragraph 263. For international comparability purposes, the term \"doctoral or equivalent\" is used to label ISCED level 8\".[34]\nArgentina\nSee also: Education in Argentina\nAdmission\n\nIn Argentina, the admission to a Ph.D. program at public Argentine University requires the full completion of a Master's degree or a Licentiate's degree. Non-Argentine Master's titles are generally accepted into a Ph.D. program when the degree comes from a recognized university.\nFunding\n\nWhile a significant portion of postgraduate students finance their tuition and living costs with teaching or research work at private and state-run institutions, international institutions, such as the Fulbright Program and the Organization of American States (OAS), have been known to grant full scholarships for tuition with apportions for housing.[35]\nRequirements for completion\n\nUpon completion of at least two years' research and course work as a graduate student, a candidate must demonstrate truthful and original contributions to his or her specific field of knowledge within a frame of academic excellence.[36] The doctoral candidate's work should be presented in a dissertation or thesis prepared under the supervision of a tutor or director, and reviewed by a Doctoral Committee. This Committee should be composed of examiners that are external to the program, and at least one of them should also be external to the institution. The academic degree of Doctor, respective to the correspondent field of science that the candidate has contributed with original and rigorous research, is received after a successful defense of the candidate's dissertation.[37]\nAustralia\nSee also: Education in Australia and Australian Qualifications Framework\nAdmission\n\nAdmission to a Ph.D. program in Australia requires applicants to demonstrate capacity to undertake research in the proposed field of study. The standard requirement is a bachelor's degree with either first-class or upper second-class honours. Research master's degrees and coursework master's degrees with a 25% research component are usually considered equivalent. It is also possible for research master's degree students to 'upgrade' to Ph.D. candidature after demonstrating sufficient progress.\nScholarships\n\nPh.D. students are sometimes offered a scholarship to study for their Ph.D. degree. The most common of these are the government-funded Australian Postgraduate Award (APA), which provides a living stipend to students of approximately A$25,800 a year (tax free). APAs are paid for a duration of 3 years, while a 6-month extension is usually possible upon citing delays out of the control of the student.[38] Some universities also fund a similar scholarship that matches the APA amount. Due to a continual increase in living costs, many Ph.D. students are forced to live under the poverty line.[39] In addition to the more common APA and university scholarships, Australian students have other sources of scholarship funding.\nFees\n\nAustralian citizens, permanent residents and New Zealand citizens are not charged course fees for their Ph.D. or research master's degree, with exception to the student services and amenities fee (SSAF) which is set by each university and typically involves the largest amount allowed by the Australian government. All fees are paid for by the Australian government, except for the SSAF, under the Research Training Scheme.[40] International students and coursework master's degree students must pay course fees, unless they receive a scholarship to cover them.\nRequirements for completion\n\nCompletion requirements vary. Most Australian Ph.D. programs do not have a required coursework component. The credit points attached to the degree are all in the product of the research, which is usually an 80,000 word thesis that makes a significant new contribution to the field. The Ph.D. thesis is sent to external examiners who are experts in the field of research and who have not been involved in the work. Examiners are nominated by the candidate's university and their identities are often not revealed to the candidate until the examination is complete. A formal oral defence is generally not part of the examination of the thesis, largely because of the distances that would need to be traveled by the overseas examiners. Scholar Bruno Starrs has examined the growing popularity of the PhD by Published Papers \nin Australia, with particular reference to the discipline of film and TV studies and the proliferation of honorary doctorates.\nCanada\nAdmission\n\nAdmission to a doctoral programme at a Canadian university usually requires completion of a Master's degree in a related field, with sufficiently high grades and proven research ability. In some cases, a student may progress directly from an Honours Bachelor's degree to a Ph.D. program; other programs allow a student to fast-track to a doctoral program after one year of outstanding work in a Master's program (without having to complete the Master's).\n\nAn application package typically includes a research proposal, letters of reference, transcripts, and in some cases, a writing sample or Graduate Record Examination scores. A common criterion for prospective Ph.D. students is the comprehensive or qualifying examination, a process that often commences in the second year of a graduate program. Generally, successful completion of the qualifying exam permits continuance in the graduate program. Formats for this examination include oral examination by the student's faculty committee (or a separate qualifying committee), or written tests designed to demonstrate the student's knowledge in a specialized area (see below) or both.\n\nAt English-speaking universities, a student may also be required to demonstrate English language abilities, usually by achieving an acceptable score on a standard examination (for example the Test of English as a Foreign Language). Depending on the field, the student may also be required to demonstrate ability in one or more additional languages. A prospective student applying to French-speaking universities may also have to demonstrate some English language ability.\nFunding\n\nWhile some students work outside the university (or at student jobs within the university), in some programs students are advised (or must agree) not to devote more than ten hours per week to activities (e.g., employment) outside of their studies, particularly if they have been given funding. For large and prestigious scholarships, such as those from NSERC and FQRNT, this is an absolute requirement.\n\nAt some Canadian universities, most Ph.D. students receive an award equivalent to part or all of the tuition amount for the first four years (this is sometimes called a tuition deferral or tuition waiver). Other sources of funding include teaching assistantships and research assistantships; experience as a teaching assistant is encouraged but not requisite in many programs. Some programs may require all Ph.D. candidates to teach, which may be done under the supervision of their supervisor or regular faculty. Besides these sources of funding, there are also various competitive scholarships, bursaries, and awards available, such as those offered by the federal government via NSERC, CIHR, or SSHRC.\nRequirements for completion\n\nIn general, the first two years of study are devoted to completion of coursework and the comprehensive examinations. At this stage, the student is known as a \"Ph.D. student\" or \"doctoral student\". It is usually expected that the student will have completed most of his or her required coursework by the end of this stage. Furthermore, it is usually required that by the end of eighteen to thirty-six months after the first registration, the student will have successfully completed the comprehensive exams.\n\nUpon successful completion of the comprehensive exams, the student becomes known as a \"Ph.D. candidate\". From this stage on, the bulk of the student's time will be devoted to his or her own research, culminating in the completion of a Ph.D. thesis or dissertation. The final requirement is an oral defense of the thesis, which is open to the public in some, but not all, universities. At most Canadian universities, the time needed to complete a Ph.D. degree typically ranges from four to six years.[citation needed] It is, however, not uncommon for students to be unable to complete all the requirements within six years, particularly given that funding packages often support students for only two to four years; many departments will allow program extensions at the discretion of the thesis supervisor and/or department chair. Alternate arrangements exist whereby a student is allowed to let their registration in the program lapse at the end of six years and re-register once the thesis is completed in draft form. The general rule is that graduate students are obligated to pay tuition until the initial thesis submission has been received by the thesis office. In other words, if a Ph.D. student defers or delays the initial submission of their thesis they remain obligated to pay fees until such time that the thesis has been received in good standing.\nColombia\nAdmission\n\nIn Colombia, the Ph.D. course admission may require a master's degree (Magíster) in some universities, specially public universities. However, it could also be applied for a direct doctorate in specific cases, according to the jury's recommendations on the thesis proposal.\nFunding\n\nMost of postgraduate students in Colombia must finance their tuition fees by means of teaching assistant seats or research works. Some institutions such as Colciencias, Colfuturo and Icetex grant scholarships or provide awards in the form of forgivable loans [41]\nRequirements for completion\n\nAfter two or two and a half years it is expected the research work of the doctoral candidate to be submitted in the form of oral qualification, where suggestions and corrections about the research hypothesis and methodology, as well as on the course of the research work are performed. The Ph.D. degree is only received after a successful defense of the candidate's thesis is performed (four or five years after the enrollment), and most of the times also requiring the most important results having been published in at least one peer-reviewed high impact international journal.\nFinland\n\nIn Finland, the degree of filosofian tohtori (abbreviated FT) is awarded by traditional universities, such as University of Helsinki. A Master's degree is required, and the doctorate combines approximately 4–5 years of research (amounting to 3-5 scientific articles, some of which must be first-author) and 60 ECTS points of studies.[42] Other universities such as Aalto University award degrees such as tekniikan tohtori (TkT, engineering), taiteen tohtori (TaT, art), etc., which are translated in English to Doctor of Science (D.Sc.), and they are formally equivalent. The licentiate (filosofian lisensiaatti or FL) requires only 2–3 years of research, and is sometimes done before a FT.\nFrance\nHistory\nSee also: Doctorate § France\n\nBefore 1984 three research doctorates existed in France: the State doctorate (doctorat d'État, the old doctorate introduced in 1808), the third cycle doctorate (doctorat de troisième cycle, created in 1954 and shorter than the State doctorate) and the diploma of doctor-engineer (diplôme de docteur-ingénieur created in 1923), for technical research. After 1984, only one type of doctoral degree remained, called \"doctorate\" (Doctorat). The latter is equivalent to the Ph.D.\nAdmission\n\nStudents pursuing the Ph.D. degree must first complete a master's degree program, which takes two years after graduation with a bachelor's degree (five years in total). The candidate must find funding and a formal doctoral advisor (Directeur de thèse) with an habilitation throughout the doctoral program.\n\nThe Masters program is divided into two branches: \"master professionnel\", which orientates the students towards the working world, and Master of Research (Master-recherche), which is oriented towards research. The Ph.D. admission is granted by a graduate school (in French, \"école doctorale\"). A Ph.D. Student has to follow some courses offered by the graduate school while continuing his/her research at laboratory. His/her research may be carried out in a laboratory, at a university, or in a company. In the last case, the company hires the student as an engineer and the student is supervised by both the company's tutor and a labs' professor. The validation of the Ph.D. degree requires generally 3 to 4 years after the master's degree.\nFunding\n\nThe financing of Ph.D. studies comes mainly from funds for research of the French Ministry of Higher Education and Research. The most common procedure is a short-term employment contract called doctoral contract: the institution of higher education is the employer and the Ph.D. candidate the employee. However, the student can apply for funds from a company who can host him/her at its premises (as in the case where Ph.D. students do their research in a company). As another encountered situation, the company and the institute can sign together a funding agreement so that the student still has a public doctoral contract, but is daily located in the company (for example, it is particularly the case of (French) Scientific Cooperation Foundation). Many other resources come from some regional/city projects, some associations, etc.\nIndia\nAdmission\n\nIn India, generally a master's degree is required to gain admission to a doctoral program. Direct admission to a Ph.D. programme after bachelors is also offered by the IITs, the IIITs, the NITs and the Academy of Scientific and Innovative Research. In some subjects, doing a Masters in Philosophy (M.Phil.) is a prerequisite to starting a Ph.D. For funding/fellowship, it is required to qualify for the National Eligibility Test for Lectureship and Junior Research fellowship (NET for LS and JRF) [43] conducted by the federal research organisation Council of Scientific and Industrial Research (CSIR) and University Grants Commission (UGC).\n\nIn the last few years, there have been many changes in the rules relating to a Ph.D. in India .[citation needed] According to the new rules described by UGC, universities must have to conduct entrance exams in general ability and the selected subject. After clearing these tests, the shortlisted candidates need to appear for an interview by the available supervisor/guide. After successful completion of the course work, the students are required to give presentations of the research proposal (plan of work or synopsis) at the beginning, submit progress reports, give a pre-submission presentation and finally defend the thesis in an open defence viva-voce.\nGermany\nSee also: Education in Germany\nAdmission\n\nIn Germany, admission to a doctoral program is generally on the basis of having an advanced degree (i.e., a master's degree, diplom, magister, or staatsexamen), mostly in a related field and having above-average grades. A candidate must also find a tenured professor from a university to serve as the formal advisor and supervisor (Betreuer) of the dissertation throughout the doctoral program called Promotion. This supervisor is informally referred to as Doktorvater or Doktormutter, which literally translate as \"doctor's father\" or \"doctor's mother\", respectively. Due to the different college systems in Germany, only professors from universities (Univ.-Prof.) can serve as a doctor's supervisor and therefore, Universities of Applied Sciences (Fachhochschulen) are not entitled to award a doctorate.[44]\nStructure\n\nDepending on the university, doctoral students (Doktoranden) can be required to attend formal classes or lectures, some of them also including exams or other scientific assignments, in order to get one or more certificates of qualification (Qualifikationsnachweise). Depending on the doctoral regulations (Promotionsordnung) of the university and sometimes on the status of the doctoral student, such certificates may not be required. Usually, former students, research assistants or lecturers from the same university, may be spared from attending extra classes. Instead, under the tutelage of a single professor or advisory committee, they are expected to conduct independent research. In addition to doctoral studies, many doctoral candidates work as teaching assistants, research assistants, or lecturers.\n\nMany universities have established research-intensive Graduiertenkollegs (\"graduate colleges\"), which are graduate schools that provide funding for doctoral studies.\nDuration\n\nThe usual duration of a doctoral program largely depends on the subject and area of research; but, often three to five years of full-time research work are required.\n\nIn 2014, the median age of new Ph.D. graduates was 30.4 years of age.[45]\nUSSR, Russian Federation and former Soviet Republics\nGlobe icon\n\tThe examples and perspective in this article or section might have an extensive bias or disproportional coverage towards one or more specific regions. Please improve this article or discuss the issue on the talk page. (June 2013) (Learn how and when to remove this template message)\n\nThe degree of Candidate of Sciences (Russian: кандидат наук, Kandidat Nauk) was the first advanced research qualification in the former USSR (it was introduced there in 1934) and some Eastern Bloc countries (Czechoslovakia, Hungary) and is still awarded in some post-Soviet states (Russian Federation, Ukraine, Belarus and others). According to \"Guidelines for the recognition of Russian qualifications in the other countries\" \n, in countries with a two-tier system of doctoral degrees (like Russian Federation, some post-Soviet states, Germany, Poland, Austria and Switzerland), should be considered for recognition at the level of the first doctoral degree, and in countries with only one doctoral degree, the degree of Kandidat Nauk should be considered for recognition as equivalent to this Ph.D. degree.\n\nAs most education systems only have one advanced research qualification granting doctoral degrees or equivalent qualifications (ISCED 2011,[46] par.270), the degree of Candidate of Sciences (Kandidat Nauk) of the former USSR counties is usually considered at the same level as the doctorate or Ph.D. degrees of those countries.[47][48]\n\nAccording to the Joint Statement by the Permanent Conference of the Ministers for Education and Cultural Affairs of the Länder of the Federal Republic of Germany (Kultusministerkonferenz, KMK), German Rectors' Conference (HRK) and the Ministry of General and Professional Education of the Russian Federation, the degree of Kandidat Nauk is recognised in Germany at the level of the German degree of Doktor and the degree of Doktor Nauk at the level of German Habilitation.[49][50] The Russian degree of Kandidat Nauk is also officially recognised by the Government of the French Republic as equivalent to French doctorate.[51][52]\n\nIn Ukraine, the Supreme Certifying Commission (official English self-denomination, also known as Higher Attestation Commission or \"VAK\", Ukrainian: Вища атестаційна комісія України), before it was merged into the Ministry of Education and Science, Youth and Sport of the Ukraine, would issue official international diploma supplements to holders of Ukrainian degrees of Kandydat Nauk (Candidate of Sciences, Ukrainian: кандидат наук)[53] stating that the degree was \"comparable to the academic degree of Doctor of Philosophy, Ph.D.\".[54][55][56] In several former Eastern Bloc countries (Czech Republic, Slovakia, Hungary), in which the Candidate of Sciences degrees used to be modeled after the Soviet ones, those degrees have been replaced with Ph.D. or equivalent doctoral degrees, with the recognition of the essential equivalency between the old and the new degrees.[57]\n\nAccording to the International Standard Classification of Education (ISCED) 2011 \n, for purposes of international educational statistics, Kandidat Nauk (Candidate of Sciences) belongs to ISCED level 8, or \"doctoral or equivalent\", together with Ph.D., D.Phil., D.Litt., D.Sc., LL.D., Doctorate or similar. It is mentioned in the Russian version of ISCED 2011 (par.262) on the UNESCO website as an equivalent to Ph.D. belonging to this level.[46] In the same way as Ph.D. degrees awarded in many English-speaking countries, Kandidat Nauk (Candidate of Sciences) allows its holders to reach the level of the Docent.[56] The second doctorate[47] (or post-doctoral degree)[55][58] in some post-Soviet states called Doctor of Sciences (Russian: доктор наук, Doktor Nauk) is given as an example of second advanced research qualifications or higher doctorates in ISCED 2011[46] (par.270) and is similar to Habilitation in Germany, Poland and several other countries.[47][58] It constitutes a higher qualification compared to Ph.D. as against the European Qualifications Framework (EQF) \nor Dublin Descriptors \n.[58]\n\nAbout 88% of Russian students studying at state universities study at the expense of budget funds.[59] The average stipend in Russia (as of August 2011) is $430 a year ($35/month).[60] The average tuition fee in graduate school is $2,000 per year.[61]\nItaly\nHistory\n\nThe Dottorato di ricerca (research doctorate), abbreviated to \"Dott. Ric.\" or \"Ph.D.\", is an academic title awarded at the end of a course of not less than three years, admission to which is based on entrance examinations and academic rankings in the Bachelor of Arts (\"Laurea Triennale\") and Master of Arts (\"Laurea Magistrale\" or \"Laurea Specialistica\"). While the standard Ph.D. follows the Bologna process, the M.D.-Ph.D. programme may be completed in two years.\n\nThe first institution in Italy to create a doctoral program (Ph.D.) was Scuola Normale Superiore di Pisa in 1927 under the historic name \"Diploma di Perfezionamento\".[62][63] Further, the research doctorates or Ph.D. (Dottorato di ricerca) in Italy were introduced by law and Presidential Decree in 1980,[64][65] referring to the reform of academic teaching, training and experimentation in organisation and teaching methods.[66][67]\n\nHence, the Superior Graduate Schools in Italy [68] (Scuola Superiore Universitaria),[69] also called Schools of Excellence (Scuole di Eccellenza)[68][70] such as Scuola Normale Superiore di Pisa and Sant'Anna School of Advanced Studies still keep their reputed historical \"Diploma di Perfezionamento\" Ph.D. title by law[63][71] and MIUR Decree.[72][73]\nAdmission\n\nDoctorate courses are open, without age or citizenship limits, to all those who already hold a \"laurea magistrale\" (master degree) or similar academic title awarded abroad which has been recognised as equivalent to an Italian degree by the Committee responsible for the entrance examinations.\n\nThe number of places on offer each year and details of the entrance examinations are set out in the examination announcement.\nNepal\n\nIn Federal Democratic Republic of Nepal, generally a master's degree is required to gain admission to a doctoral program. Direct admission to a Ph.D. programme after bachelors is also offered by the NITs, the NITs and the ACSIR. In some subjects, doing a Masters in Philosophy (M.Phil.) is a prerequisite to starting a Ph.D. For funding/fellowship, it is required to qualify for the Nepal Eligibility Test for Lectureship and Junior Research fellowship (NET for LS and JRF) [43] conducted by the federal research organisation Council of Scientific and Industrial Research (Nepal) (CSIR) and University Grants Commission (UGC). In the last few years, there have been many changes in the rules relating to a Ph.D. in Nepal.[citation needed] According to the new rules, most universities conduct entrance exams in general ability and the selected subject. Apart from Tribhuvan University, many other Universities and private colleges such as South Western State College facilitate scholars to pursue PhD study.\nPoland\n\nA doctoral degree (Pol. doktor), abbreviated to Ph.D. (Pol. dr) is an advanced academic degree awarded by universities in most fields [74][75][76][77][78] as well as by the Polish Academy of Sciences,[79] regulated by the Polish parliament acts[80] and the government orders, in particular by the Ministry of Science and Higher Education of the Republic of Poland. Commonly, students with a master's degree or equivalent are accepted to a doctoral entrance exam. The title of Ph.D. is awarded to a scientist who 1) completed a minimum of 3 years of Ph.D. studies (Pol. studia doktoranckie; not required to obtain Ph.D.), 2) finished his/her theoretical and/or laboratory's scientific work, 3) passed all Ph.D. examinations, 4) submitted his/her dissertation, a document presenting the author's research and findings,[81] 5) successfully defended his/her doctoral thesis. Typically, upon completion, the candidate undergoes an oral examination, always public, by his/her supervisory committee with expertise in the given discipline.\nScandinavia\n\nThe doctorate was introduced in Sweden in 1477 and in Denmark-Norway in 1479 and awarded in theology, law and medicine, while the magister's degree was the highest degree at the Faculty of Philosophy, equivalent to the doctorate.\n\nScandinavian countries were among the early adopters of a degree known as a doctorate of philosophy, based upon the German model. Denmark and Norway both introduced the Dr. Phil(os). degree in 1824, replacing the Magister's degree as the highest degree, while Uppsala University of Sweden renamed its Magister's degree Filosofie Doktor (fil. dr) in 1863. These degrees, however, became comparable to the German Habilitation rather than the doctorate, as Scandinavian countries did not have a separate Habilitation.[82] The degrees were uncommon and not a prerequisite for employment as a professor; rather, they were seen as distinctions similar to the British (higher) doctorates (D.Litt., D.Sc.). Denmark introduced an American-style Ph.D. in 1989; it formally replaced the Licentiate's degree, and is considered a lower degree than the dr. phil. degree; officially, the ph.d. is not considered a doctorate, but unofficially, it is referred to as \"the smaller doctorate\", as opposed to the dr. phil., \"the grand doctorate\". Holders of a ph.d. degree are not entitled to style themselves as \"Dr.\"[83] Currently Denmark distinctions between the dr. phil. as the proper doctorate and a higher degree than the ph.d., whereas in Norway, the historically analogous dr. philos. degree is officially regarded as equivalent to the new ph.d.\n\nIn Sweden, the doctorate of philosophy was introduced at Uppsala University's Faculty of Philosophy in 1863. In Sweden, the Latin term is officially translated into Swedish filosofie doktor and commonly abbreviated fil. dr or FD. The degree represents the traditional Faculty of Philosophy and encompasses subjects from biology, physics and chemistry, to languages, history and social sciences, being the highest degree in these disciplines. Sweden currently has two research-level degrees, the Licentiate's degree, which is comparable to the Danish degree formerly known as the Licentiate's degree and now as the ph.d., and the higher doctorate of philosophy, Filosofie Doktor. Some universities in Sweden also use the term teknologie doktor for doctorates awarded by institutes of technology (for doctorates in engineering or natural science related subjects such as materials science, molecular biology, computer science etc.). The Swedish term fil. dr is often also used as a translation of corresponding degrees from e.g. Denmark and Norway.\nSpain\n\nDoctoral degrees are regulated by Real Decreto (Royal Decree in Spanish) R.D. 1393/2007,[84] and will be regulated R.D. 99/2011,[85] starting with the 2014/2015 academic year. They are granted by a university on behalf of the King, and its diploma has the force of a public document. The Ministry of Science keeps a National Registry of Theses called TESEO.[86]\n\nAll doctoral programs are of a research nature. A minimum of three years of study are required, in one only stage:\n\n    A 3-year (or longer) period of research. Extensions may be requested for maximum 5 years. The student must write his thesis presenting a new discovery or original contribution to science. If approved by her or his \"thesis director (or directors)\", the study will be presented to a panel of 5 distinguished scholars. Any doctor attending the public presentations is allowed to challenge the candidate with questions on his research. If approved, he will receive the doctorate. Four marks can be granted: Unsatisfactory, Pass, Satisfactory, and Excellent. \"Cum laude\" (with all honours, in Latin) denomination can be added to the Excellent ones if all five members of the tribunal agree.[87]\n\nA doctoral degree is required to apply to a long-term teaching position at a university.\n\nThe social standing of doctors in Spain is evidenced by the fact that only Ph.D. holders, Grandees and Dukes can take seat and cover their heads in the presence of the King.[88] All Doctor Degree holders are reciprocally recognized as equivalent in Germany and Spain (\"Bonn Agreement of November 14, 1994\").[89]\nUnited Kingdom\nSee also: Doctorates in the United Kingdom\nAdmission\n\nUniversities admit applicants to Ph.D. programmes on a case-by-case basis; depending on the university, admission is typically conditional on the prospective student having successfully completed an undergraduate degree with at least upper second-class honours, or a postgraduate master's degree, but requirements can vary.\n\nIn the case of the University of Oxford, for example, \"The one essential condition of being accepted ... is evidence of previous academic excellence, and of future potential.\"[90] The University of Oxford (and the University of Sussex) also abbreviates their Doctor of Philosophy degree as D.Phil. but in other respects is equivalent to a PhD. Commonly, students are first accepted on to an MPhil programme and may transfer to Ph.D. regulations upon satisfactory progress and is referred to as APG (Advanced Postgraduate) status. This is typically done after one or two years, and the research work done may count towards the PhD degree. If a student fails to make satisfactory progress, he or she may be offered the opportunity to write up and submit for an MPhil degree as is the case at the King's College London and University of Manchester. In contrast, in other universities such as the University of Cambridge and University College London the MPhil is offered as a taught or stand-alone research degree.\n\nIn addition, Ph.D. students from countries outside the EU/EFTA area are required to comply with the Academic Technology Approval Scheme (ATAS), which involves undergoing a security clearance process with the Foreign Office for certain courses in medicine, mathematics, engineering and material sciences.[91][92] This requirement was introduced in 2007 due to concerns about terrorism and weapons proliferation.[92]\nFunding\n\nIn the United Kingdom, funding for Ph.D. students is sometimes provided by government-funded Research Councils or the European Social Fund, usually in the form of a tax-free bursary which consists of tuition fees together with a stipend of around £13,000 per year for three years (higher in London),[93] whether or not the degree continues for longer. Scientific studentships are usually paid at a higher rate, for example, in London, Cancer Research UK, the ICR and the Wellcome Trust stipend rates start at around £19,000 and progress annually to around £23,000 a year; an amount that is tax and national insurance free. Research Council funding is sometimes 'earmarked' for a particular department or research group, who then allocate it to a chosen student, although in doing so they are generally expected to abide by the usual minimum entry requirements (typically a first degree with upper second class honours, although successful completion of a postgraduate master's degree is usually counted as raising the class of the first degree by one division for these purposes). However, the availability of funding in many disciplines (especially humanities, social studies, and pure science[citation needed] subjects) means that in practice only those with the best research proposals, references and backgrounds are likely to be awarded a studentship. The ESRC (Economic and Social Science Research Council) explicitly state that a 2.1 minimum (or 2.2 plus additional master's degree) is required—no additional marks are given for students with a first class honours or a distinction at masters level. Since 2002, there has been a move by research councils to fund interdisciplinary doctoral training centres which concentrate resources on fewer higher quality centres.\n\nMany students who are not in receipt of external funding may choose to undertake the degree part-time, thus reducing the tuition fees, as well as creating free time in which to earn money for subsistence. Students may also take part in tutoring, work as research assistants, or (occasionally) deliver lectures, at a rate of typically £25–30 per hour, either to supplement existing low income or as a sole means of funding.[94]\nCompletion\n\nThere is usually a preliminary assessment to remain in the programme and the thesis is submitted at the end of a three- to four-year programme. These periods are usually extended pro rata for part-time students. With special dispensation, the final date for the thesis can be extended for up to four additional years, for a total of seven, but this is rare.[95] For full-time Ph.D.s, a 4-year time limit has now been fixed and students must apply for an extension to submit a thesis past this point. Since the early 1990s, British funding councils have adopted a policy of penalising departments where large proportions of students fail to submit their theses in four years after achieving Ph.D.-student status (or pro rata equivalent) by reducing the number of funded places in subsequent years.[96]\n\nThere has recently been an increase in the number of Integrated Ph.D. programs available, such as at the University of Southampton. These courses include a Master of Research (MRes) in the first year, which consists of a taught component as well as laboratory rotation projects. The Ph.D. must then be completed within the next 3 years. As this includes the MRes all deadlines and timeframes are brought forward to encourage completion of both MRes and Ph.D. within 4 years from commencement. These programmes are designed to provide students with a greater range of skills than a standard Ph.D.; and for the university they are a means of gaining an extra years' fees from public sources.\nOther doctorates\n\nIn the United Kingdom Ph.D. degrees are distinct from other doctorates, most notably the higher doctorates such as D.Litt. (Doctor of Letters) or D.Sc. (Doctor of Science), which may be granted on the recommendation of a committee of examiners on the basis of a substantial portfolio of submitted (and usually published) research. However, some UK universities still maintain the option of submitting a thesis for the award of a higher doctorate.\n\nRecent years have seen the introduction of professional doctorates (D.Prof or ProfD), which are the same level as Ph.D.'s but more specific in their field.[97] These tend not to be solely academic, but combine academic research, a taught component and a professional qualification. These are most notably in the fields of engineering (Eng.D.), education (Ed.D.), educational psychology (D.Ed.Psych), occupational psychology (D.Occ Psych.) clinical psychology (D.Clin.Psych.), social work (DSW), nursing (DNP), public administration (DPA), business administration (DBA), and music (DMA). These typically have a more formal taught component consisting of smaller research projects, as well as a 40,000–60,000 word thesis component, which together are officially considered equivalent to a Ph.D. degree.\nUnited States\nMain article: Graduate science education in the United States\nFurther information: Doctorate § United States\n\nIn the United States, the Ph.D. degree is the highest academic degree awarded by universities in most fields of study. U.S. students typically undergo a series of three phases in the course of their work toward the Ph.D. degree. The first phase consists of coursework in the student's field of study and requires one to three years to complete. This often is followed by a preliminary, a comprehensive examination, or a series of cumulative examinations where the emphasis is on breadth rather than depth of knowledge. The student is often later required to pass oral and written examinations in the field of specialization within the discipline, and here, depth is emphasized. Some Ph.D. programs require the candidate to successfully complete requirements in pedagogy (taking courses on higher level teaching and teaching undergraduate courses) or applied science (e.g., clinical practice and predoctoral clinical internship in Ph.D. programs in clinical, counseling, or school psychology).[citation needed]\n\nAnother two to eight years are usually required for the composition of a substantial and original contribution to human knowledge in the form of a written dissertation, which in the social sciences and humanities typically ranges from 50 to 450 pages. In many cases, depending on the discipline, a dissertation consists of a comprehensive literature review, an outline of methodology, and several chapters of scientific, social, historical, philosophical, or literary analysis. Typically, upon completion, the candidate undergoes an oral examination, sometimes public, by his or her supervisory committee with expertise in the given discipline.\n\nThere are 282 universities in the United States that award the Ph.D. degree, and those universities vary widely in their criteria for admission, as well as the rigor of their academic programs.[98] Typically, Ph.D. programs require applicants to have a bachelor's degree in a relevant field (and, in many cases in the humanities, a master's degree), reasonably high grades, several letters of recommendation, relevant academic coursework, a cogent statement of interest in the field of study, and satisfactory performance on a graduate-level exam specified by the respective program (e.g., GRE, GMAT).[99][100] The number of Ph.D. diplomas awarded by US universities has risen nearly every year since 1957, according to data compiled by the US National Science Foundation. In 1957, US universities awarded 8,611 Ph.D. diplomas; 20,403 in 1967; 31,716 in 1977; 32,365 in 1987; 42,538 in 1997; and 48,133 in 2007.[101]\n\nDepending on the specific field of study, completion of a Ph.D. program usually takes four to eight years of study after the Bachelor's Degree; those students who begin a Ph.D. program with a master's degree may complete their Ph.D. degree a year or two sooner.[102] As Ph.D. programs typically lack the formal structure of undergraduate education, there are significant individual differences in the time taken to complete the degree. Overall, 57% of students who begin a Ph.D. program in the US will complete their degree within ten years, approximately 30% will drop out or be dismissed, and the remaining 13% of students will continue on past ten years.[103]\n\nPh.D. students at U.S. universities typically receive a tuition waiver and some form of annual stipend.[citation needed] Many U.S. Ph.D. students work as teaching assistants or research assistants. Graduate schools increasingly[citation needed] encourage their students to seek outside funding; many are supported by fellowships they obtain for themselves or by their advisers' research grants from government agencies such as the National Science Foundation and the National Institutes of Health. Many Ivy League and other well-endowed universities provide funding for the entire duration of the degree program (if it is short) or for most of it.[citation needed]\nModels of supervision\n\nAt some universities, there may be training for those wishing to supervise Ph.D. studies. There is now a lot of literature published for academics who wish to do this, such as Delamont, Atkinson and Parry (1997). Indeed, Dinham and Scott (2001) have argued that the worldwide growth in research students has been matched by increase in a number of what they term \"how-to\" texts for both students and supervisors, citing examples such as Pugh and Phillips (1987). These authors report empirical data on the benefits that a Ph.D. candidate may gain if he or she publishes work, and note that Ph.D. students are more likely to do this with adequate encouragement from their supervisors.\n\nWisker (2005) has noticed how research into this field has distinguished between two models of supervision: The technical-rationality model of supervision, emphasising technique; The negotiated order model, being less mechanistic and emphasising fluid and dynamic change in the Ph.D. process. These two models were first distinguished by Acker, Hill and Black (1994; cited in Wisker, 2005). Considerable literature exists on the expectations that supervisors may have of their students (Phillips & Pugh, 1987) and the expectations that students may have of their supervisors (Phillips & Pugh, 1987; Wilkinson, 2005) in the course of Ph.D. supervision. Similar expectations are implied by the Quality Assurance Agency's Code for Supervision (Quality Assurance Agency, 1999; cited in Wilkinson, 2005).\nInternational PhD equivalent degrees\n\n    Afghanistan: ډاکټر\n    Albania: Doktorature (Dr.)\n    Algeria: Doctorat, دكتوراه\n    Argentina: Doctorado (Dr.)\n    Armenia: գիտությունների թեկնածու, դոցենտ\n    Austria: Doktor (Dr., plural: DDr.)\n    Azerbaijan: Doktorantura (Dr.)\n    Bangladesh: Doctorate\n    Belarus: кандидат наук\n    Belgium (Dutch-speaking): Doctor\n    Belgium (French-speaking): Doctorat\n    Bosnia and Herzegovina: Doktor\n    Brazil: Doutorado\n    Bulgaria: Доктор\n    Burma: ပါရဂူဘြဲ႕\n    China: 博士 (Bo-shi)\n    Chile: Doctorado\n    Colombia: Doctorado\n    Costa Rica: Ph.D. or Doctorado (Dr.)\n    Croatia: Doktor\n    Czech Republic: CSc. and DrSc. was used till 1998, since 1998 Ph.D. written as Ph.D. is used\n    Denmark: Licentiate, Magister, Ph.D. (the doctorates are higher degrees)\n    Dominican Republic: Doctorado\n    Ecuador: Doctorado\n    El Salvador: Doctorado\n    Egypt: Doctorat, دكتوراه\n    Estonia: Doktor\n    Ethiopia: ዶክተር, Doctor (Ph.D., Dr.)\n    Finland: Filosofian tohtori and any degree of tohtori\n    France: Doctorat\n    Germany: Doktor\n    Greece: Διδακτορικό\n    Hong Kong: 博士 (Doctor)\n    India: Doctorate\n    Indonesia: Doktor\n    Iran: دکترا (Doctora)\n    Iraq: دكتوراه (Duktorah)\n    Ireland: an Doctúireacht\n    Israel: דוקטורט (\"doctorat\")\n    Italy: Dottorato di ricerca\n    Japan: 博士 (hakase)\n    Jordan: دكتوراه (Doctorah)\n    Korea: 박사 (baksa)\n    Kuwait: دكتوراه (Dektoraah)\n    Latin America: Doctorado/Doctorate\n    Latvia: Zinātņu doktors\n    Lebanon: دكتوراه (doktorah)\n    Lithuania: Daktaras\n    Macau: 博士 (Doutoramento)\n    Macedonia: Докторат\n    Malaysia: Doktor Falsafah\n    Mexico: Doctorado\n    Mongolia: Эрдэмтэн\n    Morocco: Doctorat\n    Netherlands: Doctor\n    Nigeria: Doctor of Philosophy (Ph.D.)\n    Norway: Magister, Licentiate, doctorates (traditionally considered higher degrees), Ph.D.\n    Pakistan: Doctor\n    Paraguay: Ph.D. or Doctorado (Dr.)\n    Peru: Doctorado\n    Philippines: Doktor\n    Poland: Doktor\n    Portugal: pt:Doutoramento\n    Romania: Doctorat\n    Russia: ru: кандидат наук (PhD junior grade), ru: доктор наук (PhD senior grade)\n    Saudi Arabia دكتوراه\n    Singapore: Doctor\n    Serbia: Доктор\n    Slovakia: CSc. was used during communism and some years in 90s, now Ph.D. written as Ph.D. is used; DrSc. is a higher degree.\n    Slovenia: Doktor\n    Spain: Doctorado\n    Sweden: Filosofie doktor (fil.dr., FD)\n    Switzerland: Doctorat (Dr)\n    Syria: دكتوراه (doktorah)\n    Taiwan： 博士\n    Thailand: ดุษฎีบัณฑิต\n    Tunisia: دكتوراه (doktorah)\n    Turkey: Doktora\n    United Arab Emirates: ar:دكتوراه (doktorah)\n    United Kingdom: Doctor of Philosophy (PhD, doctor, the abbreviation DPhil is used only by the University of Oxford and the University of Sussex)\n    United States: Doctor of Philosophy (PhD)\n    Ukraine: uk: кандидат наук (CSc.)\n    Uruguay: Doctorado\n    Uzbekistan: Fan nomzodi (CSc.)\n    Venezuela: Doctorado\n    Vietnam: Tiến sĩ", "skillName": "Doctor_of_Philosophy."}
{"id": 7, "category": "Education", "skillText": "A Master of Science degree (Latin: Magister Scientiae; abbreviated M.S., MS, M.Sc., MSc, M.Sci., MSci, S.M., Sc.M., ScM., or Sci.M.) is a type of master's degree awarded by universities in many countries. The degree is usually contrasted with the Master of Arts degree. The Master of Science degree is typically granted for studies in sciences, engineering, and medicine, and is usually for programs that are more focused on scientific and mathematical subjects; however, different universities have different conventions and may also offer the degree for fields typically considered within the humanities and social sciences. While it ultimately depends upon the specific program, individuals who pursue a Master of Science degree typically require a thesis.\n\nContents\n\n    1 Algeria\n    2 Argentina, Brazil, Mexico, Colombia, Perú and Uruguay\n    3 Australia\n    4 Bangladesh\n    5 Canada\n        5.1 Quebec\n    6 Chile\n    7 Czech Republic, Slovakia\n    8 Egypt\n    9 Finland\n    10 Germany\n    11 Southeastern Europe\n    12 Guyana\n    13 Iran\n    14 Ireland\n    15 Israel\n    16 India\n    17 Italy\n    18 Nepal\n    19 Netherlands\n    20 Norway\n    21 Pakistan\n    22 Poland\n    23 Russia\n    24 Spain\n    25 Sweden\n    26 Syria\n    27 United Kingdom\n    28 United States\n    29 See also\n    30 References\n\nAlgeria\n\nAlgeria follows the Bologna process.\nArgentina, Brazil, Mexico, Colombia, Perú and Uruguay\n\nIn Argentina, Brazil, Ecuador, Mexico, Colombia, Perú and Uruguay, the Master of Science or Magister is a postgraduate degree of two to four years of duration.[1] The admission to a Master's program (Spanish: Maestría; Portuguese: Mestrado) requires the full completion of a four to five years long undergraduate degree, bachelor's degree or a Licentiate's degree of the same length. Defense of a research thesis is required. All master's degrees qualify for a doctorate program.\nAustralia\n\nAustralian universities commonly have coursework or research based Master of Science courses for graduate students. They typically run for 1–2 years full-time, with varying amounts of research involved. Research based master's degrees are usually accompanied by an extensive thesis before completion.\nBangladesh\n\nIn Bangladesh, all universities, including Dhaka University, University of Chittagong, Jahangirnagar University and Rajshahi University, have Master of Science courses as postgraduate degrees. After passing Bachelor of Science, any student becomes eligible to study in this discipline.\nCanada\n\nIn Canada, Master of Science (MSc) degrees may be entirely course-based, entirely research-based or (more typically) a mixture. Master's programs typically take one to three years to complete and the completion of a scientific thesis is often required. Admission to a master's program is contingent upon holding a four-year university bachelor's degree. Some universities require a master's degree in order to progress to a doctoral program (PhD).\nQuebec\n\nIn the province of Quebec, the Master of Science follows the same principles as in the rest of Canada. There is one exception, however, regarding admission to a master's program. Since Québécois students complete two to three years of college before entering university, they have the opportunity to complete a bachelor's degree in three years instead of four. Some undergraduate degrees such as the Bachelor of Education and the Bachelor of Engineering requires four years of studies. Following the obtention of their bachelor's degree, students can be admitted into a graduate program to eventually obtain a master's degree.\nChile\n\nCommonly the Chilean universities have used \"Magíster\" for a master degree.\nCzech Republic, Slovakia\n\nThe Czech Republic and Slovakia are using two master's degree systems. Both award a title of Mgr. or Ing. to be used before the name. The older system requires a 5-year program. The new system takes only 2 years, but requires a previously completed 3-year bachelor program (a Bc. title). It is required to write a thesis (in both master and bachelor program) and also to pass final exams. It is mostly the case that the final exams cover the main study areas of the whole study program, i.e. a student is required to prove his/her knowledge in many subjects he attended during the 2 resp. 3 years.\nEgypt\n\nThe Master of Science (M.Sc.) is an academic degree for a post-graduate candidates or researchers, it usually takes from 2 to 3 years after passing The Bachelor of Science (B.Sc.) degree. Master programs is awarded in many sciences in the Egyptian Universities. A completion of the degree requires finishing a pre-master studies followed by a scientific thesis or research. All M.Sc. degree holders are allowable to take a step forward in the academic track to get the PhD degree.\nFinland\n\nThe Master of Science (M.Sc.) academic degree usually follows the Bachelor of Science (B.Sc.) studies which typically last from four to five years. For the completion of both the bachelor and the master studies the student must accumulate a total of 300 ECTS credits, thus most Masters programs are two-year programs with 120 credits. The completion of a scientific thesis is required.\nGermany\n\nThe Master of Science (M.Sc.) academic degree replaces the once common Diplom or Magister programs that typically lasted four to five years. It is awarded in science related studies with a high percentage of mathematics. For the completion the student must accumulate 300 ECTS Credits, thus most Masters programs are two-year programs with 120 credits. The completion of a scientific thesis is required.\nSoutheastern Europe\n\nIn Slavic countries in European southeast (particularly former Yugoslavian republics), education system was largely based on German university system (largely due to presence and influence of Austria-Hungary Empire to the area). Prior to the implementation of Bologna process, academic university studies comprised to 4-5 year long graduate Diplom program, which could have been followed by 2-4 year long Magister program, and then later with 2-4 year long Doctoral studies.\n\nAfter the Bologna process implementation, again based on German implementation, Diplom titles and programs were replaced by the M.Sc. and M.A. programs (depending on field of study). The studies are structured such that Master program lasts long enough for the student to accumulate total of 300 ECTS Credits, so length would depend on amount of credits acquired during the Bachelor studies. Pre-Bologna Magister programs were abandoned - after earning a M.Sc/M.A. degree, and satisfying other academic requirements a student could proceed to earn a Doctor of Philosophy degree directly.\nGuyana\n\nIn Guyana, all universities, including University of Guyana, Texila American University, American International School of Medicine have Master of Science courses as postgraduate degrees. Students who have completed undergraduate Bachelor of Science degree are eligible to study in this discipline\nIran\n\nIn Iran, similar to Canada, Master of Science (MSc) or in Iranian form Karshenasi-arshad degrees may be entirely course-based, entirely research-based or sometimes a mixture. Master's programs typically take two to three years to complete and the completion of a scientific thesis is often required.\nIreland\n\nIn Ireland, Master of Science (MSc) may be course-based with a research component, or entirely research based. The program is most commonly a one-year program and a thesis is required for both course based and research based degrees.\nIsrael\n\nIn Israel, Master of Science (MSc) may be entirely course-based, or include research. The program is most commonly a two-year program and a thesis is required only for research based degrees.\nIndia\n\nIn India, all science universities offer MSc programs. However a few technological Universities do offer MS degrees alongside the conventional MTech or ME degrees. Generally speaking, in India, post graduate technological courses lead to MTech while engineering courses lead to ME. For example, a Masters in automotive engineering would normally be an ME, while a Masters in physics would be an MS or an MSc. This is in contradiction with universities in UK and the USA which usually offers MS or MSc degrees for all science, technology and engineering programs. A few top universities offer undergraduate programmes leading to a master's degrees, these are known as Integrated M.Sc./ M.S. / B.S. + M.S. degrees.\nItaly\n\nSimilar to Germany. The degree Master of Science is awarded in the Italian form, Laurea Magistrale (formerly Laurea specialistica; before the introduction of the Laurea the corresponding degree was Laurea quinquennale, or Vecchio Ordinamento).\nNepal\n\nIn Nepal, all the universities, Tribhuvan University, Pokhara University, Kathmandu University, offer master's degree in technological areas. Tribhuvan University offers MSc degree for all the science and engineering courses, Pokhara University offers ME for engineering and MSc for science. Kathmandu University offers MTech,MS by Research,ME for science and engineering.\nNetherlands\n\nSimilar to Germany. A graduate who is awarded the title Master of Science may still use the previously awarded Dutch title ingenieur (abbreviated as ir.) (for graduates who followed a technical or agricultural program), meester (abbreviated as mr.) (for graduates who followed an LLM law program) or doctorandus (abbreviated as drs.)(in all other cases).\nNorway\n\nFor engineering, the Master of Science academic degree has been recently introduced, and has replaced the previous award forms \"Sivilingeniør\" (engineer, a.k.a. engineering master) and \"Hovedfag\" (academic master). Both were awarded after 5 years university-level studies and required the completion of a scientific thesis.\n\n\"Siv.ing\", is a protected title exclusively awarded to engineering students who completed a five-year education at The Norwegian University of Science and Technology (Norwegian: Norges teknisk-naturvitenskapelige universitet, NTNU) or other universities. Historically there was no bachelor's degree involved, and today's program is a five years master's degree education. The \"Siv.ing\" title is in the process of being phased out, replaced by (for now, complemented by) the \"M.Sc.\" title. By and large, \"Siv.ing\" is a title tightly being held on to for the sake of tradition. In academia, the new program offers separate three-year bachelor and two-year master programs. It is awarded in the natural sciences, mathematics and computer science fields. The completion of a scientific thesis is required. All master's degrees are designed to certify a level of education and qualify for a doctorate program.\n\nMaster of Science in Business is the English title for those taking a higher business degree, \"Siviløkonom\" in Norwegian. In addition there is for example the 'Master of Business Administration' (MBA), a practically oriented master's degree in business, but with less mathematics and econometrics, due to its less specific entry requirements and smaller focus on research.\nPakistan\n\nPakistan inherited its conventions pertaining to higher education from United Kingdom after independence in 1947. Master of Science degree is typically abbreviated as M.Sc. (as in United Kingdom) and which is awarded after 16 years of education. Recently, in pursuance to some of the reforms by the Higher Education Commission (the regulatory body of higher education in Pakistan), the traditional 2-year Bachelor of Science (B.Sc.) degree has been replaced by the 4-year Bachelor of Science degree, which is abbreviated as B.S. to enable the Pakistani degrees with the rest of the world. Subsequently, students who pass 4-year B.S. degree that is awarded after 16 years of education are then eligible to apply for M.S. degree, which is considered with Master of Philosophy (M.Phil.) degree.\nPoland\n\nThe Polish equivalent of Master of Science is \"magister\" (abbreviated \"mgr\", written pre-nominally much like \"Dr\"). Starting in 2001, the MSc programs typically lasting 5 years began to be replaced as below:\n\n    3-year associates programs (termed \"licencjat\". No abbreviated pre-nominal or title.)\n    3.5-year engineer programs (termed \"inżynier\", utilizing the pre-nominal abbreviation \"inż.\")\n    2-year master programs open to both \"licencjat\" and \"inż.\" graduates.\n    1.5-year master programs open only to \"inż.\" graduates.\n\nThe degree is awarded predominantly in the natural sciences, mathematics, computer science, economics, as well as in the arts and other disciplines. Those who graduate from an engineering program prior to being awarded a master's degree are allowed to use the \"mgr inż.\" pre-nominal (\"master engineer\"). This is most common in engineering and agricultural fields of study. Defense of a research thesis is required. All master's degrees in Poland qualify for a doctorate program.\nRussia\n\nThe title of \"master\" was introduced by Alexander I at 24 January 1803. The Master had an intermediate position between the candidate and doctor according to the decree \"About colleges structure\". The master's degree was abolished since 1917 to 1934. Russia follows the Bologna process for higher education in Europe since 2011.\nSpain\n\nThe Master of Science (MSc) degree is an officially recognized program by the Spanish Ministry of Educations. It usually involves 1 or 2 years of full-time study. It is targeted at pre-experience candidates who have recently finished their undergraduate studies. A MSc degree can be awarded in every field of study. Some universities require a MSc degree in order to progress to a PhD. MSci, MPhil and DEA are equivalent in Spain.\nSweden\n\nThe Master of Science academic degree has, like in Germany, recently been introduced in Sweden. Students studying Master of Science in Engineering programs are rewarded both the English Master of Science Degree, but also the Swedish equivalent \"Teknologie masterexamen\" and \"Civilingenjör\".\nSyria\n\nThe Master of Science is a degree that can be studied only in public universities. The program is usually 2 years, but it can be extended to 3 or 4 years, the student is required to pass a specific bachelor's degree to attend a specific master of science degree program, the master of science is mostly a research master (except for some types of programs held with cooperation of foreign universities), The student should attend some courses in the first year of the master then he/she should prepare a research thesis. Publishing two research papers is recommended and will increase the final evaluation grade.\nUnited Kingdom\n\nThe MSc is typically a postgraduate degree, involving lectures, examination, and a project. The masters programs usually involve a minimum of 1 year of full-time study which can sometimes be extended to 2 years of full-time study (or the equivalent period part-time). Some universities also offer research MSc programs, where a longer project or set of projects is undertaken full-time.\n\nUntil recently, both the undergraduate and postgraduate master's degrees were awarded without grade or class (like the class of an honours degree). Nowadays, however, taught master's degrees are normally classified into the categories of Pass and Distinction, with some universities also using an intermediate Merit category.[2] The MSc by research and thesis, a purely research degree with no formal taught component, remains ungraded.\n\nThe more recent MSci degree, now offered by UK institutions, is an abbreviation for Master in Science. It is equally reputed and acknowledged by employers within the UK and internationally. According to the UK National Qualifications Framework, an MSci and other such master's degrees (MPhys, MChem etc.) are termed integrated masters and are classed as level 7 qualifications—the same level as a Master of Science or Master of Arts degree.[3]\n\nThis education pattern in United Kingdom is also followed in Hong Kong SAR and in some Commonwealth Nations.\nUnited States\n\nThe Master of Science (Magister Scientiæ) degree is the primary type in most subjects and may be entirely course-based, entirely research-based, or, (more typically), a combination of the two. The combination often involves writing and defending a thesis or completing a research project which represents the culmination of the material learned.\n\nAdmission to a master's program is normally contingent upon holding a bachelor's degree, and progressing to a doctoral program may require a master's degree. In some fields or graduate programs, work on a doctorate can begin immediately after the bachelor's degree. Some programs provide for a joint bachelor's and master's degree after about five years. Some universities use the Latin degree names, and due to the flexibility of word order in Latin, Artium Magister (AM) or Scientiæ Magister (SM) may be used in some institutions.", "skillName": "Master_of_Science."}
{"id": 8, "category": "Education", "skillText": "A master's degree (from Latin magister) is an academic degree awarded by universities upon completion of a course of study demonstrating a mastery or high-order overview of a specific field of study or area of professional practice.[1] Within the area studied, graduates are posited to possess advanced knowledge of a specialized body of theoretical and applied topics; high order skills in analysis, critical evaluation, or professional application; and the ability to solve complex problems and think rigorously and independently. The master's degree may qualify the holder to teach at a college or university in certain disciplines.\n\nContents\n\n    1 Titles\n        1.1 Types\n    2 Structure\n        2.1 Duration\n        2.2 Admission\n    3 Comparable European degrees\n    4 South America\n        4.1 Brazil\n    5 Asia\n        5.1 Hong Kong\n        5.2 Pakistan\n        5.3 India\n        5.4 Israel\n        5.5 Nepal\n        5.6 Taiwan\n    6 See also\n    7 References\n\nTitles\n\nThe two most common titles of master's degrees are the Master of Arts (M.A.) and Master of Science (M.S., M.Sc., M.Si., or M.C.A.) degrees; these may be course-based, research-based, or a mixture of the two. Some universities use the Latin degree names; because of the flexibility of syntax in Latin, the Master of Arts and Master of Science degrees may be known as Magister artium or Artium magister and Magister scientiæ or Scientiæ magister, respectively. Harvard University, University of Chicago, and MIT, for example, use A.M. and S.M. for their master's degrees. More commonly, Master of Science often is abbreviated MS or M.S. in the United States, and MSc or M.Sc. in Commonwealth nations and Europe.\n\nOther master's degrees are more specifically named (\"tagged degrees\"), including, for example, the Master of Business Administration (M.B.A.), the Master of Health Administration (M.H.A.), the Master of IT in Business (M.I.T.B.), the Master of Business Engineering (M.B.E.), Master in European Business (M.E.B.), Master of Counselling (M.C.), Master of Divinity (M.Div.), Master of Library Science (M.L.S.), Master of Public Administration (M.P.A.), Master of Social Work (M.S.W.), Master of Public Policy (M.P.P.), Master of Laws (LL.M.), Master of Music (M.M. or M.Mus.), Master of Information (M.I.), Master of Fine Arts (M.F.A.), Master of Public Health (M.P.H.).\n\nSome are further general, for example the Master of Philosophy (M.Phil.), Master of Arts in Liberal Studies (M.A.L.S., M.L.A./A.L.M., and M.L.S.), and the Master of Studies (Master of Advanced Study/Master of Advanced Studies).\n\nSee List of master's degrees.\nTypes\n\n    Post-graduate master's degree (M.A., M.S., M.Ed., M.E.B., M.Des., M.N.C.M., M.S.N., M.S.W., M.P.A., M.P.C., M.P.P., M.P.H., M.C., M.C.A., M.Couns., M.L.A., M.L.I.S., M.Div., A.L.M.,N.T.P M.M., M.B.A., M.Tech., M.I.T.B., M.B.E., M.Com., M.M.C., M.I.B., M.I., P.S.M. and other subject specific master's degrees) is designed for anyone who holds a bachelor's degree.\n    Post-graduate research master's degree (M.Res., M.A.Res., M.S.Res., M.St., M.Phil., M.Litt.) - the Master by Research is designed for those who hold a bachelor's degree with a significant research component and/or have several publications. It is designed for those wishing to pursue higher research.\n    Executive master's degree (E.M.B.A., E.M.S.) is a master's degree designed specially for executive professionals. Admission, graduation requirements, and structure of executive master's degrees differ from that of the regular full-time program.[citation needed]\n    Integrated master's degree (M.Pharm., M.Pharmacol., M.Eng., M.Math., M.Phys., M.Psych., M.Sci., M.Chem., M.Biol., M.Geol., etc.) is an undergraduate degree combined with an extra master's year. The first three years of study are often the same as a bachelor's degree, followed by an additional year of study at a master's degree level. The degree is usually only conferred at the end of study as a full master's - an intermediate bachelor's degree is not awarded.[citation needed] Integrated master's are most common in scientific disciplines.\n\nStructure\nFurther information: Master's degree in North America, Master's degree in Europe, and Master's degree non-Euroamerican\n\nThere are a range of pathways to the degree, with entry based on evidence of a capacity to undertake higher degree studies in the proposed field. A dissertation may or may not be required, depending on the program. In general, the structure and duration of a program of study leading to a master's degree will differ by country and by university.\nDuration\n\nIn some systems, such as those of the United States and Japan, a master's degree is a strictly postgraduate academic degree. Particularly in the U.S., in some fields/programs, work on a doctorate begins immediately after the bachelor's degree, but the master's may be earned along the way as a 'master's degree \"en route\"', following successful completion of coursework and certain examinations. Master's programs are thus one to six years in duration, with two to three years being a common length of time to complete. Some universities offer evening options so that students can work during the day and earn a master's degree in the evenings.[2]\n\nUnder the Angloamerican systems many master's degrees are differentiated either as 'master (thesis)' or as 'master (non-thesis)' programs. Regardless of a de jure minimum period of a master's degree program in the same discipline, the required de facto duration to complete the program may vary highly by university. One of the main reasons of this is the fact that the required level of courses or research complexity and quality of a thesis also can vary greatly, e.g. in \"very high research activity\" elite universities students who are admitted to a \"very high research\" master (thesis), have to fulfill course and thesis level requirements at a regular PhD level, however.\n\nBy contrast, in some cases, such as the integrated master's degree in the UK, the degree is combined with a Bachelor of Science, as a four-year degree. Unlike a traditional M.Sc., the fourth year finishes at the same time as undergraduate degrees in the early summer, whereas traditional M.Sc. students typically spend the summer vacation completing a dissertation and finish in September. Examples include M.Math. (see also Part III of the Mathematical Tripos at Cambridge), M.Eng. and M.Sci. (not to be confused with an M.Sc.).\n\nIn the recently standardized European System of higher education (Bologna process), a master's degree programme normally carries 90–120 ECTS credits, with a minimum requirement of at least 60 ECTS credits at master level (one- or two-year full-time postgraduate program) undertaken after at least three years of undergraduate studies. It provides higher qualification for employment or prepares for doctoral studies. As one ECTS credit is equivalent to 25 hours of study this means that a master's degree programme should include 2250 hours of study. Current UK M.Sc./M.A. programmes tend to include 1800 hours of study (or 180 UK credits), although many claim to be equivalent to an ECTS accredited master's degree.\nAdmission\n\nIn countries in which a master's degree is a postgraduate degree, admission to a master's program normally requires holding a bachelor's degree, and in the United Kingdom, Canada and much of the Commonwealth, an \"honours\" bachelor degree.[citation needed] In both cases, relevant work experience may qualify a candidate. In some cases the student's bachelor's degree must be in the same subject as the intended master's degree (e.g. a Master of Economics will typically require a bachelor's degree with a major in economics), or in a closely allied, \"cognate\", discipline (e.g. Applied Mathematics degrees may accept graduates in physics, mathematics or computer science); in others, the subject of the bachelor's degree is unimportant (e.g. M.B.A.) although, often in these cases, undergraduate coursework in specific subjects may be required (e.g. some M.S.F. degrees require credits in calculus for admission, but none in finance or economics; see also under Business education#Postgraduate education). Most competitive programs also have a grade point average (GPA) that the student must have achieved in their undergraduate degree.\nComparable European degrees\n\nIn some European countries, a magister is a first degree and may be considered equivalent to a modern (standardized) master's degree (e.g., the German, Austrian and Polish university Diplom/Magister, or the similar five-year Diploma awarded in several subjects in Greek, Spanish, Portuguese, and other universities and polytechnics).\n\n    In Denmark the title candidatus or candidata (female) abbreviated cand. is used as a master's equivalent. Upon completion of for instance, an engineeral master's degree, a person becomes cand.polyt. (polytechnical). Similar abbreviations, inspired by Latin, apply for a large number of educations, such as sociology (cand.scient.soc), economics (cand.merc., cand.polit. or cand.oecon), law (cand.jur), humanities (cand.mag) etc. A cand. title requires the obtainment of a master's degree. Holders of a cand. degree are also entitled to use M.Sc. or M.A. titles, depending on the field of study. In Finland and Sweden, the title of kand. equates to a bachelor's degree.\n    In France, the equivalent of master's degrees is the combination of two individual years : the master 1 (M1) and master 2 (M2), following the Bologna Process. Depending on the goal of the student (a doctorate or a professional career) the master 2 can also be called a \"Master Recherche\" (research master) and a \"Master Professionnel\" (professional master), each with different requirements. To obtain a national diploma for the master 2 requires a minimum of one year of study after the master 1.\n    A French \"diplôme d'Ingénieur\" is also the equivalent of a master's degree, provided the diploma is recognised by the Commission des titres d'ingénieur.\n    In Italy the master's degree is equivalent to the two-year Laurea magistrale, which can be earned after a Laurea (a three-year undergraduate degree, equivalent to a bachelor's degree). In particular fields, namely law, pharmacy and medicine, this distinction is not made. University courses are therefore single and last five to six years, after which the master's degree is awarded (in this case referred to as Laurea magistrale a ciclo unico). The old Laurea degree (Vecchio Ordinamento, Old Regulations), which was the only awarded in Italy before the Bologna process, is equivalent[3] to the current Laurea Magistrale.\n    In the Netherlands the titles ingenieur (ir.), meester (mr.) and doctorandus (drs.) may be rendered, if obtained in the Netherlands from a university, after the application of the Bologna process, as: MSc instead of ir., LL.M. instead of mr. and MA or MSc instead of drs.[4] This is because a single program that led to these degree was in effect before 2002, which comprised the same course load as the bachelor and master programs put together. Those who had already started the program could, upon completing it, bear the appropriate title (MSc, LL.M. or MA), but alternatively still use the old-style title (ir., mr. or drs.), corresponding to their field of study. Since these graduates do not have a separate bachelor's degree (which is in fact – in retrospect – incorporated into the program), the master's degree is their first academic degree. Bearers of foreign master's degree are able to use the titles ir., mr. and drs. only after obtaining a permission to bear such titles from the Dienst Uitvoering Onderwijs. Those who received their mr., ir. or drs. title after the application of the Bologna process have the option of signing as A. Jansen, M.A. or A. Jansen, M.Sc., depending on the field in which the degree was obtained, since the ir., mr. and drs. titles are similar to a master's degree, and the shortcut MA or M.Sc. may officially be used in order to render such title as an international title.[5][6][7][8]\n    In Switzerland, the old Licence or Diplom (4 to 5 years in duration) is considered equivalent to the master's degree.[9]\n    In Slovenia and Croatia, during the pre-Bologna process education, all Academic degrees were awarded after a minimum of four years of university studies and a successful defence of a written thesis are considered equivalent to the master's degree.[citation needed]\n    In Baltic countries there is a two-year education program that offers a chance to gain a master's degree in interdisciplinary issues. The system offers an education in different areas, such as humanities, environmental and social issues, whilst paying specific consideration to the Baltic Sea area. It is a joint-degree program, which is part of a team effort with four universities. There is the University of Tartu in Estonia, the University of Turku in Finland, Vytautas Magnus University in Lithuania and the University of Latvia. The educational programs are very good; allowing students to be mobile within the system, for example one semester may be taken in a confederate school without paying additional membership or tuition fees. Subsequently after passing the qualifications provided, people may procure teaching qualifications and continue their scholastic research around doctoral studies, or carry on studying within their career in the private or public sector. Graduates of the program, within the Baltic Sea area are also given the chance to continue onwards with their studies within the postgraduate system if they have studied the social sciences or humanities field.\n    In Greece, the metaptychiako which literally translates as post-degree (...programme or title), lasts normally from one to, more often, two years, and can be studied after a, at least, four-years undergraduate ptychio, which means degree.\n    In Russia master (магистр) degree can be obtained after a 2-year master course (магистратура) which is available after a 4-year bachelor or a 5-year specialist course. A graduate may choose a master course completely different from his/her previous one. During these 2 years master students attend specialized lectures in chosen profile, choose a faculty advisor and prepare their master thesis which is eventually defended before certifying commission consisting mostly of professors.\n\nSouth America\nBrazil\n\nIn Brazil, after a regular graduation (after acquiring a bachelor's degree), students have the option to continue their academic career through a master's course (a.k.a. stricto sensu post-graduation) or specialization (a.k.a. lato sensu post-graduation) degrees.\n\nAt the master's degree (\"mestrado\", in Portuguese, also referred as \"pós-graduação stricto sensu\") there are 2–3 years of full-time graduate-level studies. Usually focused on academic research, the master's degree (on any specific knowledge area) requires the development of a thesis, presented (and defended) to a board of Ph.D. after the period of research. Differently, the \"specialization\" degree (also referred as \"pós-graduação lato-sensu\"), also comprehends a 1–2 years studies, but do not require a new thesis to be purposed and defended, being usually attended by professionals looking for a complimentary formation on a different knowledge area than their original graduation.\n\nIn addition, a great part of Brazilian universities offers a M.B.A. (Master of Business Administration) degree. Those, nevertheless, are not the equivalent of US M.B.A. degree though, as it does not formally certifies the student/professional with a master's degree (stricto-sensu) but a post-graduation degree instead. A regular post-graduation course has to comply with a minimum of 360 class-hours, while a M.B.A. degree has to comply with a minimum of 400 class-hours. Master's degree (stricto sensu) does not requires minimum class-hours, but it's practically impossible to finish it before 1.5 year due the workload and research required; an average time for the degree is 2.5 years[citation needed].\n\nSpecialization (lato sensu) and M.B.A. degrees can be also offered as distance education courses, while the master's degree (stricto-sensu) requires physical attendance.\n\nOften serves as additional qualification for those seeking a differential on the job market, or for those who want to pursue a Ph.D. It corresponds to the European (Bologna Process) 2nd Cycle or the North American master's.\nAsia\nHong Kong\n\nM.Arch., M.L.A., M.U.D., M.A., M.Sc., M.Soc.Sc., M.S.W., M.Eng., LL.M. Hong Kong requires one or two years of full-time coursework to achieve a master's degree.\n\nFor part-time study, two or three years of study are normally required to achieve a postgraduate degree.\n\nM.Phil. As in the United Kingdom, M.Phil. or Master of Philosophy is a research degree awarded for the completion of a thesis, and is a shorter version of the Ph.D.\nPakistan\n\nIn Pakistani education system, there are two different master's degree programmes[citation needed]:\n\n    2 years master's programmes: these are mostly Master of Arts (M.A.) leading to M.Phil.;\n    4 years master's programmes: these are mostly Master of Science (M.S.) leading to Ph.D.\n\nBoth M.A. and M.S. are offered in all major subjects.\nIndia\n\nIn the Indian system, a master's degree is a postgraduate degree following a Bachelor's degree and preceding a Doctorate, usually requiring two years to complete. The available degrees include:\n\n    Master of Arts (M.A.);\n    Master of Business Administration (M.B.A.);\n    Master of Computer Applications (M.C.A.);\n    Master of Computer Management (M.C.M.);\n    Master of Design (M.Des.);\n    Master of Engineering (M.Eng.);\n    Master of Philosophy (M.Phil.);\n    Master of Science (M.Sc.);\n    Master of Science in Information Technology (M.Sc.I.T.);\n    Master of Technology (M.Tech.);\n    Master of Statistics (M.Stat.);\n    Master of Laws (LL.M.);\n    Master of Commerce (M.Com.).\n    Master in Business Studies (M.B.S)\n\nIsrael\n\n    M.A., M.Sc., M.B.A.: postgraduate studies in Israel require the completion of a bachelor's degree and is dependent upon this title's grades. There exists also a direct track to a doctorate degree for graduate students, which lasts four to five years. Taking this route, the students must prepare a preliminary research paper during their first year, they then have to pass an exam after which they are automatically awarded a master's degree.\n    M.Eng.: It is given by the Technion – Israel Institute of Technology. Comparing to the M.Sc., it is a non-thesis track.[10]\n\nNepal\n\tThis section may require cleanup to meet Wikipedia's quality standards. The specific problem is: needs copyediting for grammar and clarity (January 2016) (Learn how and when to remove this template message)\n\nIn Nepal, after bachelor's degree about to at least three or four years with full-time study in college and university with an entrance test for those people who want to study further can study in master and further Ph.D. and Doctorate degree. All Doctoral and Ph.D. or third cycle degree are based on research and experience oriented and result based. Master of Engineering (M.Eng.), Master of Education (M.Ed.), Master of Arts (M.A.) and all law and medicine related courses are studied after completion of successful bachelor towards doctoral degree. M.B.B.S. is only a medical degree with six and half years of study resulting medical doctor and need to finish its study o 4 years of period joining after master degree with minimum education with 15 or 16 years of university bachelor's degree education. The most professional and internationalised program in Nepal are:\n\n    Master of Business Administration (M.B.A.);\n    Master of Computer Applications (M.C.A.);\n    Master of Engineering (M.Eng.);\n    Master of Science (M.Sc.);\n    Master of Science in Information Technology (M.Sc.I.T.);\n    Master of Business Studies ( M.B.S.);\n    Master of Education (M.Ed);\n    Master of Arts (M.A.);\n    Master of Agriculture (M.Sc. Ag.);\n    Master of Laws (LL.M.);\n    Master of Management (M.M.).\n\nTaiwan\n\nIn Taiwan, bachelor's degrees are about four years (with honors) and there is an entrance examination required for people who want to study in master and Ph.D. degrees. The courses offered for master and PhD normally are research-based.\n\nThe most foreign-friendly programs in Taipei, Taiwan are at:\n\n    National Taiwan University College of Management – Global M.B.A. (M.B.A. in Finance, Accounting, Management, International Business and Information Management);\n    National ChengChi University – I.M.B.A.\n\nPrograms are entirely in English and tuition is less than would be paid in North America, with as little as US$5000 for an M.B.A.[citation needed]\n\nAs an incentive to increase the number of foreign students, the government of Taiwan and universities have made extra efforts to provide a range of quality scholarships available.[citation needed] These are university-specific scholarships ranging from tuition waivers, up to NT$20,000 per month. The government offers the Taiwan Scholarship ranging from NT$20,000–30,000 per month for two years. (US$18,000–24,000 for a two-year program)", "skillName": "Master_degree."}
{"id": 9, "category": "Education", "skillText": "The European Joint Master degree in Economics provides a rigorous education in fundamental quantitative tools by combining economic theory with related quantitative disciplines such as Econometrics, Finance, Actuarial Science, Probability, Statistics, Mathematical Modeling, Computation and Simulation, Experimental Design, and Political Science, managed by consortia of higher education institutions from the European Union.[1]\n\nThe first European Joint Master degree in Economics was the Erasmus Mundus QEM - Models and Methods of Quantitative Economics.[2] It was approved by the European Union in 2006 as international graduate degree program designed by a Consortium of European Universities. The number of European universities or institutions making up a consortia varies from the degree awarding program. The nature of the degree differs from other master's degree in Economics or finance. Compared to the Master of Finance or Master of Economics, this European degree is joint and prepares graduate for a wide range of careers which utilize their competency in economics, including economic theory, macroeconomics and financial forecasting, financial engineering and risk management, quantitative asset management, computational economics, quantitative trading, and applied and theoretical research. Unlike programs which are substantially quantitative, this degree provide a merge of both theory and empirics useful in practice.\n\nClosely related degrees include the \"Master of Quantitative Finance and Economics\" and \"Master of Finance and Economics\".\n\nOften, the degree prepares graduates for both research orientation for further studies and in the job market for positions in government organizations, private companies or financial institutions. This joint degree is officially recognized as a master degree in each university and each country with the same rights and duties with the national degree.\n\nThe degree is gaining in recognition as graduate placements have increased over the past few years.\n\nContents\n\n    1 Structure\n    2 List of European Joint Master degree Programs in Economics\n    3 List of Participating Institutions\n    4 References\n    5 External links\n\nStructure\n\nEuropean Joint Master degree in Economics is usually two years in duration, and typically include a thesis, an internship or research component. The programs require a bachelor's degree or an equivalent qualification at the same level, i.e. 180 ECTS credits in the European Credit Transfer and Accumulation System prior to admission. A typical requirement is exposure to economics, mathematics, applied mathematics, quantitative economics or finance. Like many master programs in Economics or finance, a review of statistics, probability theory, linear algebra and calculus are provided as a preliminary course. The curriculum is distributed between theory, applications, and simulations, with the emphasis on each differing by university in the consortium. The degree programs are usually funded by the Erasmus Mundus Scholarship.\nList of European Joint Master degree Programs in Economics\n\nThe list of programs awarding this joint degree are as follows:\n\n    EMLE - European Master in Law and Economics[3]\n    EMIN - Erasmus Mundus Joint Master in Economics and Management of Network Industries[4]\n    EPOG - Master's Course - Economic Policies in the age of Globalisation : knowledge, finance and development[5]\n    MEGEI - MA Economics of Globalisation and European Integration[6]\n    QEM - Erasmus Mundus Models and Methods of Quantitative Economics[7]\n\nList of Participating Institutions\n\nHere are some institutions among consortium of universities offering the European Joint Master degree in Economics:\n\n    Université Paris 1 Panthéon-Sorbonne, Paris, France\n    Universidad Autónoma de Barcelona Barcelona, Spain\n    Universität Bielefeld, Bielefeld, Germany\n    Università Ca' Foscari Venezia, Venice, Italy[8]\n    Comillas Pontifical University, Madrid, Spain\n    Technische Universiteit Delft, The Netherlands\n    University of Paris-Sud, France\n    Florence School of Regulation of the European University Institute (Italy)[9]\n    Catholic University of Leuven, Belgium\n    Università di Bologna, Italy\n    Université d'Aix-Marseille, France[10]", "skillName": "European_Joint_Master_degree_in_Economics."}
{"id": 10, "category": "Education", "skillText": "An engineer's degree is an advanced academic degree in engineering that is conferred in Europe, some countries of Latin America, and a few institutions in the United States.\n\nIn Europe, the engineer degree is ranked at the same academic level as a bachelor's degree or master's degree, and is often known literally as an \"engineer diploma\" (abbreviated Dipl.-Ing. or DI). In some countries of Latin America and the United States, the engineer's degree can be studied after the completion of a master's degree and is usually considered higher than the master's degree but below the doctorate in engineering (abbreviated Dr.-Ing.) in Europe. In other countries of Latin America, there is no proper engineer's degree, but the title of Engineer is used for 5 year bachelor's graduates.\n\nContents\n\n    1 Canada\n    2 Europe, prior to the Bologna Process\n        2.1 Germany and Austria\n        2.2 Turkey\n        2.3 Finland\n        2.4 Poland, Czech Republic and Slovakia\n        2.5 Belgium\n        2.6 Portugal\n        2.7 Greece\n        2.8 The Netherlands\n        2.9 United Kingdom\n        2.10 Russia\n        2.11 Belarus and Ukraine\n        2.12 France, Algeria, Morocco and Tunisia\n        2.13 Italy\n        2.14 Romania\n        2.15 Spain\n        2.16 Croatia, Bosnia-Herzegovina, Macedonia, Montenegro, Serbia and Ex-Yugoslavia\n    3 Europe, according to the Bologna Process\n    4 United States\n    5 Latin America\n        5.1 Mexico\n        5.2 Chile\n    6 Pakistan\n    7 Bangladesh\n    8 India\n    9 See also\n    10 References\n    11 External links\n\nCanada\n\nThrough the Canadian Engineering Accreditation Board (CEAB), Engineers Canada accredits Canadian undergraduate engineering programs that meet the standards of the profession. Graduates of those programs are deemed by the profession to have the required academic qualifications to be licensed as professional engineers in Canada.[1]\n\nIn Canada, A CEAB-accredited degree is the minimum academic requirement for registration as a professional engineer anywhere in Canada, and the standard against which all other engineering academic qualifications are measured.[2]\n\nA graduate of a non-CEAB-accredited program must demonstrate that his or her education is at least equivalent to that of a graduate of a CEAB-accredited program.[2]\nEurope, prior to the Bologna Process\n\nIn most countries of continental Europe, universities specializing in technical fields have traditionally awarded their students an engineer's degree lasting 5 years. This degree was typically the first university-awarded degree after finishing secondary education, and completing it granted qualifications to further pursue a doctorate.\n\nFollowing German custom in higher education, the engineer's degree is called Diplom. In addition to Germany itself, this system was common in states like Austria, Belarus, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Czech Republic, Finland, Greece, Hungary, Montenegro, the Netherlands, Norway, Poland, Portugal, Romania, Russia, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, R. Macedonia and Ukraine.\nGermany and Austria\n\nIn German, the traditional engineer's degree is called Diplom-Ingenieur (Dipl.-Ing., in Austria also DI) . This degree is generally equivalent to a Master's degree, which is not to be confused with the old Magister degree. Most programs that used to lead to a Dipl.-Ing. degree lead to master's degrees today, as the Diplom-Ingenieur as an academical title is phased out because of the Bologna process. However, some universities continue to hand out so-called equivalence certificates that certify the equivalence of a Dipl.-Ing. with the newly introduced M.Sc. Degrees.\n\nThe German \"Universities of Applied Sciences\" (Fachhochschule) awarded the traditional engineering degree Diplom-Ingenieur (FH) (Dipl.-Ing. (FH)). This degree also required to write a Diplom thesis. This is also being modified by the Bologna process, as bachelor's and master's degrees from a University of Applied Sciences are equal to the degrees from a traditional university.[3] Universities of Applied Sciences are phased out and they are changed to universities with own faculties and research institutes. These universities are strongly focused on fields like computer science, engineering or business schools. Subjects like Law or Human Medicine etc. which requires a Staatsexamen (state exam) can only be studied at traditional universities. Since 2009, most Universities in Germany offer bachelor's degree programmes (B.Sc., B.Eng. M.Sc., M.Eng, M.C.A. and others) and Master programmes that lead to the academic degree Master of Science, Master of Engineering, Master of Business Administration and others.\n\nIn Austria there also exists the Ingenieur (respectively Ing.) . This is not an academic degree as it is given to graduates of a so-called HTL (Hoehere Technische Lehranstalt) which is a High School with a very technical background. Graduates of these schools are allowed to start their studies at universities.\nTurkey\n\nIn Turkey typical length of study for bachelor's degree at any engineering program is 4 years. Owner of a valided degree which given by faculty titled with engineering at Turkish universities, can be called Mühendis, meaning \"geometrist, one who knows geometry and calculation\". The \"Mühendis\" title is limited by law to people with a valid engineering degree, and the use of the title by others. (even persons with much more work experience) is illegal. (Law No: 3458 Law about Engineers and Architects 28.06.1938 Publish no:3945) [4] If a person get master's degree at same title with previous engineering degree can called as \"Yüksek Mühendis\" (meaning Higher Engineer)\nFinland\n\nIn Finnish, the engineer's degree is called diplomi-insinööri and abbreviated dipl.ins., DI or M.Sc.(Tech.). It is possible to obtain after 5 years of studying, or even faster, but the average is around six years. Under the Bologna process, this is split into two parts, the first being one where the students can get the intermediate tekniikan kandidaatti (B.Sc.(Tech.)) degree.\n\nThe degree of insinööri (AMK) is a bachelor's degree from a Finnish University of Applied Sciences (ammattikorkeakoulu), similar to a German Fachhochschule, but it is not interchangeable with the academic tekniikan kandidaatti.\n\nDue to the Bologna process, a new master's degree called insinööri (ylempi AMK) or \"engineer (higher amk)\" has been introduced. It carries a requirement of two years of work experience after the degree of insinööri (AMK) and then some studies.\nPoland, Czech Republic and Slovakia\n\nIn the western Slavic-speaking countries, the engineer's degree is called inżynier (Polish), inžinier (Slovak) or inženýr (Czech), and the abbreviation is inż. in Poland, and Ing. in the Czech Republic and Slovakia, which may be written before the person's name.\n\nIn the Czech Republic and Slovakia, the degree of Ing. is given for complete university studies in technical (like engineering), economical or agricultural fields. In one of these cases it can be equivalent to a Master of Science in Engineering.\n\nIn Poland, the degree of inżynier is available after 3, 3.5 or 4 years of studies (like the licencjat in non-engineering science) after a final thesis is completed. A magister inżynier (abbreviated mgr inż.) refers to a Master of Science in Engineering, after completing 5 years of study and a written thesis. Originally there were \"inżynier\" studies that lasted for 4 years, and after that degree one could obtain the \"magister\" title in 2 years of studies - the total of 6 years resulted in two degrees, \"magister\" and \"inżynier\". In the early 1960s a new track of studies was developed to speed up education and the \"magister inżynier\" 5-year track was created. Whichever way one obtained the education the \"magister inżynier\" (mgr inż. before the name) was the equivalent degree with \"inżynier\" designating the professional level and \"magister\" designating the academic level. After the Bologna process the first level is \"inżynier,\" obtainable after nominally 3 years of studies (although some are longer) with the same professional privileges as before, and \"masters\" after 1 or 2 years gives the same academic and professional designation as before. But the ultimate shortening of the period of studies resulted in some professional groups (e.g. architects) demanding that \"magister inżynier\" be made a basis for professional rights.\nBelgium\n\nIn Belgium, there are three types of engineering degrees :\n\n    \"Burgerlijk Ingenieur\" /\"Ingénieur civil\" or \"Master of Science in Engineering\" (abbrev. \"ir.\") - 5 years study (3 BSc. + 2 MSc.)\n    \"Bio-ingenieur\"/ \"Bioingénieur\" or \"Master of Science in Bioscience Engineering\" (abbrev. \"ir.\") - 5 years study (3 BSc. + 2 MSc.)\n    \"Industrieel Ingenieur\" or \"Master of Science in Engineering Technology\" (abbrev. \"Ing.\") - 5 years study (3 BSc. + 2 Msc.)\n\nThere used to a difference between the Flemisch (Flanders) and French region (Walloon Region) regarding the length of the study, 4 years vs. 5 years, but over the past years all universities have changed to a5 years program (3 BSc. + 2 MSc.)\n\nNames are traditionally prefixed with the ir. and/ or ing. titles, although this practice is not as widespread as in The Netherlands. Use of these titles are regulated and protected by law. Belgium is particularly noteworthy as having a system under which demands made on students of engineering are particularly severe.[citation needed]\nPortugal\n\nIn Portugal, Engineer (Portuguese: engenheiro, feminine: engenheira, abrev.: eng.) is a professional degree reserved for the effective members of the Portuguese Institution of the Engineers (OE, Ordem dos Engenheiros). There is not an academic degree of engenheiro, despite the title being used informally to refer to anyone who has a degree in engineering, even not member of the OE.\n\nUntil the Bologna Process, having a licenciado (5 years) degree in engineering was a previous condition to be able to be admitted to the OE. Presently, admission to the OE requires a post-Bologna licenciado (3 years) degree or a mestre (5 years) master in engineering. OE members holders of a post-Bologna licenciado degree are now classified as E1 grade engineers, while holders of a pre-Bologna licenciado or a mestre are classified as E2 grade engineers.\nGreece\n\nIn Greece, the title of \"Engineer\" is awarded by two parallel and complementary parts of public higher education.\n\nThe title \"Diplomate Engineer\" is awarded after completion of the five-year engineering study course (300 ECTS, ISCED 5A) at a University or a Technical University. Although some institutions issue certificates of equivalence to a master's degree, these are not officially recognized by the Greek legislation.[5]\n\nThe title of \"Certified Engineer\" is awarded after completion of the four-year course of engineering studies, equivalent to Honours bachelor's degree (240 ECTS, ISCED 5A),at a Technological Educational Institute - T.E.I.\nThe Netherlands\n\nIn the Netherlands, somebody holding an engineer's degree is an ingenieur. There are two types of engineers with different abbreviations:\n\n    Ir. is obtained by university graduates (Wetenschappelijk onderwijs or WO). It is the highest engineer qualification. - 5 years study (3 B.Sc. + 2 M.Sc.)\n    Ing. is obtained by graduates from polytechnics (hoger beroeps onderwijs or HBO). - 3 or 4 years study depending on previous education (3 B.Eng.)\n\nNames are traditionally prefixed with the ir. and/ or ing. titles. Use of these titles are regulated and protected by Dutch law.[6] Under the Bologna agreement the titles are increasingly interchanged with the English-language degrees (B.Sc, BBE, M.Sc, etc.).\n\nCompletion of Dutch engineering qualifications have historically demanded a lot of effort. On average the 5-year course takes 7 years to finish.[7][8]\nUnited Kingdom\n\nThe highest engineer qualification in the UK and Ireland is the Chartered Engineer (C.Eng) which is a minimum of 8 years but usually a 12-year education, training and professional practice process. Chartered Engineer is a terminal qualification. The academic qualification which is minimum for gaining chartership in engineering is the Master of Engineering (MEng) or BEng plus MSc in addition to 4–6 years peer reviewed professional practice. In England, Northern Ireland and Wales this is a four-year course or a 'sandwich' five-year course (with one year spent working in industry). In Scotland, it is a five-year course. The Bachelor of Engineering (BEng) is usually a three-year course (four in Scotland), or can also include a year in industry. Many universities offer the BEng, and may then allow a transfer onto the MEng.\n\nThe City and Guilds of London Institute have established a recognized route to higher engineering qualifications with the Institution of Engineering and Technology (IET), the Institution of Mechanical Engineers (IMechE) and The Institution of Civil Engineers (ICE). CGLI has align the Graduate Diploma (level 6) and Postgraduate Diploma (level 7) with the registration requirements for Incorporated Engineer and Chartered Engineer status. The Graduate Diploma is set at the same level as the final year of a British BEng and its Postgraduate Diploma is set at the same level as the final year of a British MEng.\n\nEngineers who have been awarded a BEng(Ordinary) or BEng(Honours) and have appropriate training and experience in the work place are able to apply to become an Incorporated Engineer (IEng), if the qualification has been accepted for this classification, by the engineering council. If an engineer has studied beyond the BEng for an MSc or has an MEng, they may apply to become a Chartered Engineer (CEng), once they have completed the required amount of post graduate work-based competency training and experience. Competency and training requirements are met over a period of 4–8 years in practice for a total of 8–12 years education, training and professional responsibility. Formal structured post graduate training schemes such as the monitored professional development programme administered by IMechE enables the Engineer in training to satisfy the requirements for Chartered Engineer faster.[5]\n\nChartered Engineer and Incorporated Engineer titles awarded by the Engineering Council UK, are similar but not equivalent to North American Professional Engineer (PEng / PE) and Professional Technologist (PTech) designations, but with often a far greater geographical recognition. The UK and North American system of regulating engineering professional qualifications is very different and not equivalent. In the USA and Canada engineering is a regulated profession in terms of practice and enforced by law and licensing. It is more than the regulation of engineering titles - like the UK. This means that many UK chartered Engineers who are grandfathered to the title or are non degrees holders (HNC/HND) will be unable to be recognized formally as professional engineers in Canada and the USA. There is strict interpretation of qualifications and education because there are legal implications. There is no back door entry or experiential exemption from the formal academic requirements. This is simply because of the legal legislation on practice. This engineering governance legal framework does not exist in the UK.\n\nMIET-Member of the Institution of Engineering and Technology is recognised as regulated engineering profession by virtue of the Statutory Instruments n.2007/2781-The European Communities (Recognition of Professional Qualifications) Regulation 2007-Directive European Union 2005/36/EC. http://ec.europa.eu/internal_market/qualifications/regprof/index.cfm?fuseaction=profession.regProfs&profId=6361&mode=desc&cId=0&quid=3 \n\nThis meaning retains the 19th century idea that \"real\" engineers were military personnel, while \"other\" engineers were civilians.\nRussia\n\nAs of year 2012 in Russia, following degrees correspond to the \"engineer's degree\" in the sense of this article:\n\n    Инженер - \"engineer\", which was formerly awarded after 4, 5, 6 years of study, may also contain clarification on the nature of engineering field, such as \"mining engineer\" or \"systems engineer\".\n    Инженер по специальности - \"engineer at specialty\" (where specialty's name is mentioned), currently awarded after 5 or 6 years of study, may also contain clarification on the nature of engineering field, such as \"engineer-ecologist at specialty \"Rational usage of natural resources and protection of the environment\"\n    Инженер-исследователь по специальности - \"engineer-researcher at specialty\" (where specialty's name is mentioned), was formerly awarded after 7 years of study\n    Бакалавр по направлению - \"bachelor at area\" (where area's name is mentioned), currently awarded after 4 years of study, and \"area\" is an engineering area, corresponding to one or more of former \"specialties\"\n    Магистр по направлению - \"magister (master) at area\" (where area's name is mentioned), currently awarded after 2 years of study to those already having any higher-education degree, and \"area\" is an engineering area, corresponding to one or more of former \"specialties\"\n\nAnything but \"bachelor\" is considered \"second-level\" higher education and gives access to post-graduate education for \"candidate of sciences\" degree, \"bachelor\" is considered \"first-level\" higher education degree and gives access to study for master's (magister) degree.\n\nPost-graduate scientific degrees in engineering areas include кандидат технических наук - \"candidate of technical sciences\" and доктор технических наук - \"doctor of technical sciences\". Sometimes in English translations \"...of technical sciences\" is exchanged for \"...of engineering\".\nBelarus and Ukraine\n\nIn Belarus and Ukraine, the degree is спеціаліст інженер (specialist inzhener), a first degree after five years of study.\nFrance, Algeria, Morocco and Tunisia\n\nIn France, Algeria, Morocco and Tunisia the degree is Diplôme d'Ingénieur, while the title is Ingénieur diplômé (ID) but is never used before the holder's name. The degree can be obtained after five years of engineering studies after the Baccalauréat. The degree deliverance is regulated by the Commission des titres d'ingénieur (CTI) independent organism. The CTI mandates the French engineering schools (mainly Grandes Écoles) to deliver the Diplôme d'Ingénieur. This degree is officially considered as a European Master's Degree since the Bologna Process.\n\nAfter the French/francophone baccalaureat (high school diploma), about 10% of students are accepted in \"Classes Préparatoires aux Grandes Écoles\" (intensive preparatory classes), where mathematics, physics, technology or biology are taught. National examinations, with restricted acceptances in the engineering schools, are realized after the first two years. Some schools also include a two-year program shaped with continuous assessments and do not require to pass a national examination.\nItaly\n\nIn Italy until 2001 there were two degrees: a three-year diploma in ingegneria (BEng level, title abbrev. \"dipl. ing.\") and a five-year laurea in ingegneria (MEng level, title abbrev. \"ing.\"). Since 2001 reform, the bachelor's level is called laurea (abbrev. \"L\") and master's degree level is called laurea specialistica or laurea magistrale (abbrev. \"LS\"). Accordingly, today after three years of engineering studies, the degree called laurea in ingegneria (BEng level) and the title of dottore in Ingegneria (abbrev. \"dott.\") can be obtained. After two additional years of engineering studies, the degree called laurea magistrale in ingegneria (MEng level) and the title of dottore magistrale in Ingegneria (abbrev. \"dott.\") can be obtained. After a \"state exam\" you become \"Ingegnere\" (abbrv. Ing)[9]\nRomania\n\nRomania followed the French system. One needs a baccalaureate diploma to be accepted at the university. The engineering degree was called Diploma de inginer, and the graduate was called a Inginer diplomat. These five years of study are equivalent to a Bologna Master(M.Sc/M.Eng/MCA). The five-year course concludes with a comprehensive set of specialising exams (examen de diploma). Marks 9 and 10 are considered exceptional. Some universities had called Diploma de Sub-inginer which was a 3-year course equivalent with a college degree. Following the Bologna process, the graduates obtain the Inginer licentiat degree, after following a four-year program. In this case the Inginer Master degree is obtained after an additional one- or two-year graduate program.\nSpain\n\nThe situation in Spain is very similar to France but without the grandes écoles and Germany-U.k. in relation to the Technical Engineering Degrees. Long cycle Engineer's degrees (Ingenieros) traditionally used to be (at least nominally) six-year programs but the tendency since the mid-1990s has been to reduce them to five years. The last step to get the degree is the Proyecto de Fin de Carrera (Degree Project), which involves a combination of application development and some research work. Students submit a dissertation that they have to defend. The Spanish official name for the degree is Ingeniero (Engineer), or other degree called Ingeniero Técnico (Technical Engineer), which is a three-four years degree (involving also a Final Degree Project) and is equivalent to a Bachelor of Engineering, the Technical Engineer in Spain has full competencies and legal authority in their field. A distinctive characteristic of Spanish engineering degrees is that the average duration of studies up to graduation is about 40% above the nominal duration and that the drop-out rate is considerable due to stricter examination standards.[10]\nCroatia, Bosnia-Herzegovina, Macedonia, Montenegro, Serbia and Ex-Yugoslavia\n\nIn Croatia, the old system included the engineer's degrees diplomirani inženjer (abbr. dipl.ing.) which was awarded by university faculties, and a lower ranked engineer's degree inženjer (abbr. ing.) which was awarded by polytechnics, in a similar vein to the situation in the Netherlands. The old dipl.ing. degree could later be upgraded to a magistar (abbr. mr., Magister degree) and then a doktor (abbr. dr., Doctorate). The situation was the same in other countries previously part of Yugoslavia. In Serbian, the abbreviation is dipl.inž.[11] Serbian titles of magistar (abbr. mr, Magister degree) and doktor (abbr. dr, Doctorate) in abbreviated versions are used without full stop as a punctuation mark at the end.\nEurope, according to the Bologna Process\nMain article: Bologna process\n\nFollowing the introduction of the Bologna process, universities divide higher-education studies in three cycles, corresponding to a three-year bachelor's degree, a two-year master's degree, and a doctoral degree.\n\nAccordingly, engineering studies which lasted five years are now divided in two parts: first, the bachelor's degree (baccalaureus, three years), and the second optional part (two years), after which either the traditional engineer's degree or a master's degree (MEng or MSc) is awarded. In this new scheme, the graduate must complete the master's degree before attempting to pursue doctoral education.\n\nCountries have varied in the implementation of the Bologna process. Most traditional universities continue to have a primary academic degree program, for example, a five-year Civilingenjör in Sweden, that is distinct from the 3+2 scheme that awards the bachelor's and master's degrees but a student who has done both at a Swedish technical university will in most cases also fulfill the requirements for the civilingenjör degree.\n\nIn France, engineering is taught in écoles d’ingénieurs, which are part of the French grandes écoles system. Since the Bologna process, the Diplôme d’Ingénieur is officially considered to be at the level of a European master's degree.\nUnited States\n\nIn the United States, the degree of engineer or engineer's degree is the least commonly obtained advanced degree in engineering. It is usually preceded by a master's degree and is not a prerequisite to a doctoral degree, sometimes serving as a terminal degree but just as often a mid-PhD degree where preliminary work in the candidates respective field is recognized. The availability of degrees and the specific requirements differ considerably between institutions and between specialties within an institution. In the past, it was not uncommon for a would-be engineer to earn an engineer's degree as their first and only college degree. But since World War II this has fallen out of favor, and it becomes continually more difficult to find a school that offers this option. Regulation and licensure in engineering in the U.S. usually requires an Accreditation Board for Engineering and Technology (ABET) engineering program accreditation, which is granted only to bachelor's degrees and, rarely, master's degrees.\n\nFor graduate students in engineering, the two-year master's degree is most commonly followed by a traditional research doctorate (Ph.D.). However, the engineer's degree provides an alternative to the doctorate for professional engineers rather than academicians. Some graduate programs, such as those offered at Stanford, Caltech and the Naval Postgraduate School, require a thesis for the engineer's degree but the research requirements are generally less than those of Ph.D. candidates and more comparable to those of master of science students. For this reason, some consider an engineer's degree to be on a level between a master's degree and a doctorate.\n\nA degree with some form of the word engineer or engineering in the title is not necessarily an engineer's degree. Particularly, a \"Master of Engineering\" (M.Eng.) or \"Engineering Doctorate\" (Eng. D) degree is not an Engineer's degree, nor is any other bachelor's, master's, or doctoral degree. Rather, the engineer's degree is in a category of its own. For example, a student with a B.S. and M.S. in electrical engineering might next earn the degree Electrical Engineer. The person would then have a B.S. in E.E., a M.S. in E.E., and an E.E. degree. The former two are degrees in engineering, and only the latter degree is actually an engineer's degree.\nLatin America\n\nIn Latin America a degree or title of \"Ingeniero\" is awarded after completing a 5 years of college, that may be translated as \"Engineer\", however, its international academic equivalence depends on each country's educational system, and can be compared to a 6-year post-master's degree. Its award may imply obtaining a state licence to legally practice in the field, or a professional certification outside academic environments.\nMexico\nSee also: Licenciatura\n\nIn Mexico, the educational system follows closely that of the United States. University education is structured in three cycles, as Licenciatura (bachelor's degree), Maestría (master's degree), and Doctorado (doctorate).\n\nThe Ingeniero (Engineer) degree is awarded depending on the nature of the studies he wish to pursuit. In order to be called an Engineer, he must have passed certain courses related to engineering and awarded a professional credential (cédula profesional) that certifies him as an engineer. The graduate has different abilities and capabilities related to the nature of the title Engineer. In this case, having a professional credential (cédula profesional) in engineering allows you to partake in various engineering-only activities, and certifies the ability of the engineer to exercise his profession.[12]\n\nFor engineering, completing all taught courses of a Licenciatura does not automatically awards the graduate the title and licence of Ingeniero, for this, it depends on the name of the career and having the prefix Ingeniería attached to the name of the degree. It is the case that many people finish all courses in an engineering program, they obtain the diploma and title of Engineer, and the professional credential, after getting their professional credential (cédula profesional).\n\nDegrees in engineering require 4 to 5 years to complete. After this, the graduate may enroll in a postgraduate program, such as a master's degree, or in exceptional cases, directly into a doctoral degree.\nChile\n\nIn Chile, the educational system does not strictly follows that of the United States. University education is structured in four cycles, as Licenciatura (bachelor's degree), Título Profesional (Professional Title), Magíster (master's degree) and Doctorado (doctorate or Ph.D).\n\nIn this case, the engineer state is a general denomination for a person who has a professional title of engineering (Título Profesional en Ingeniería). The title is obtained after 6 years of study, in this way including a bachelor's degree in science, which is obtained after four years of study.\n\nAs in other countries, students only need to obtain the bachelor's degree to enter a master's degree or doctoral program, which commonly last 2 and 3 years respectively. Since a professional title considers six years of study, after obtaining this title the graduate can enroll directly into a doctoral degree. In this manner, the international academic equivalence of a Professional Title in Chile corresponds to a 6-year post-master's degree.\nPakistan\n\nPakistan Engineering Council (PEC) accredits the undergraduate engineering degrees and regulates the engineering profession in Pakistan. The qualification of B-Tech(Honours) Degree is also equivalent to Bachelor of Engineering or Bachelor of Science in Engineering according to decision & solemn statement of HEC (Higher Education Commission) Pakistan. In Pakistan: Bachelor of Technology (Honors) degree is a four-year undergraduate university level degree, including Engineering and Applied Science courses, supervised industrial Practical training and projects. The prerequisite for this program is a three-year Diploma of Associate Engineering in specific fields e.g. Chemical Engineering, Electronics Engineering, Electrical Engineering, Mechanical Engineering and Civil Engineering etc. or two-year Fsc pre engineering. B.Tech Honors four years degree awarded by public and private sector Universities under the supervision of Higher Education Commission (Govt. of Pakistan). Public & private universities Such as IIUI,IUB, MUET, NED, UET, PRESTON, INDUS offer admission in B-Tech degree Program.B.Tech program was formally launched in 1973 in Pakistan and then Ministry of Education directed to give status of B.Tech (Hons) degree at par with B.Sc Engineering/B.E degree,& professionals called B-Tech Engineers according to the letter No. 15-29/73-Tech. According to the letter no PEC/4-P/QEC, PEC stated that B.Tech degree will be considered equivalent to B.Sc/BE there is no difference in the program objectives and learning outcomes of B.Tech or BE. Pakistan represented by PEC is a provisional signatory to Washington Accord.[13]\n\nEngineering degrees at undergraduate level are referred to as: BS Engineering / B.Sc Engineering (Bachelor of Science in Engineering). B.E. (Bachelor of Engineering) - 4 years duration.\n\nLikewise, a postgraduate degree in engineering is referred to as: MS Engineering (Master of Science in Engineering) - 2 years duration.\n\nA doctorate in engineering is simply referred to as: Ph.D. Engineering - 4 to 5 years duration.\nBangladesh\n\nIn Bangladesh, the engineering degree is referred to as B.E. (Bachelor of Engineering) and B.Sc Engineering (Bachelor of Science in Engineering) at the undergraduate level. Likewise, a master's degree is referred to as Master of Science in Engineering (M.Eng), Master of Science in Engineering (M.Sc), Master of Computer Application (MCA). At the undergraduate level, the duration of the course is 4 years whereas a master's degree has a duration usually between 1.5 and 2 years. Graduated Engineers who are members of Institution of Engineers, Bangladesh are legally and formally allowed to use 'Engr.' before their names (e.g. Engr. John Smith).\nIndia\n\nThe engineering courses in India are regulated and set up under the aegis of the University Grants Commission (UGC) and the All India Council for Technical Education (AICTE).\n\nIn India, engineering qualifications include the following:\n\n    Diploma in engineering (3 years, after class 10).\n    Bachelor's degrees (4 years, after class 12): BE (Bachelor of Engineering) and BTech (Bachelor of Technology). Both are equivalent. Admission is usually through an engineering entrance examination.\n    Master's degrees (2 or more years, after BE, BTech, MCA or equivalent): ME (Master of Engineering), MTech (Master of Technology), MSc (Eng) (Master of Science in Engineering), MS (Master of Science), MCA (Master in Computer Application). Admission usually requires qualification in the Graduate Aptitude Test in Engineering, or some work experience in engineering. The MTech and ME degrees are of 2 years duration, and usually have coursework in the first year and a project in the second year. The MSc (Eng) and MS degrees are usually research - oriented, and may require 2 or more years to complete.\n    Doctoral degrees (3 to 5 years): PhD (Doctor of Philosophy). Admission requirements vary across universities, but usually include a master's degree in engineering or science, or a bachelor's degree in engineering with an exceptionally good academic record.", "skillName": "Engineer_degree."}
{"id": 11, "category": "Education", "skillText": "The Education Specialist, also referred to as Educational Specialist or Specialist in Education (Ed.S.), is an advanced degree in the U.S. that is designed for individuals who wish to develop advanced knowledge and theory beyond the master's degree level, but may not wish to pursue a degree at the doctoral level. For those wishing to pursue a career at the community college level, Arkansas State University in Jonesboro, Arkansas offers the S.C.C.T., or Specialist in Community College. Students choosing this degree have the option to choose either an administrative track degree or one that will enhance a career as an instructor or professor in any of several subject matter fields in which the individual has already gained a master's degree.\n\nAdvanced programs beyond the master's degree are designed to provide the necessary background and professional expertise for students planning to go into university teaching, supervisory or leadership roles in post secondary schools, curriculum planning, consultant work, or similar positions. Generally, the Ed.S. or S.C.C.T. is considered a sixth year course of study although it may take more than an additional year beyond the master's program to complete. This degree may be completed en route to a Ph.D., Ed.D., or as a terminal (end) degree, which was its original intent.\n\nSince the course work in an Education Specialist degree is at an advanced graduate level many schools will transfer the credits earned directly into a doctoral degree (Ed.D., Doctor of Education).\n\nContents\n\n    1 About\n    2 Framework\n    3 Academic dress\n    4 See also\n    5 References\n\nAbout\n\nIn the K-12 arena, individuals who earn an Ed.S. degree seek to increase their skills for advanced licensure requirements (such as principalship), earn the credits needed for re-certification or other professional objectives. Others may pursue an Ed.S. degree in order to meet state or professional requirements for career advancement.\n\nIn the higher education arena, individuals who earn an Ed.S. seek to increase their knowledge for preparation in academic or administrative leadership role.\n\nMajor areas available with this degree include educational technology, adult education, adult learning, special education, higher education administration, school counseling, school psychology, educational leadership, ESL, educational administration, curriculum and instruction, superintendent, career and adult technical education, community college administration, and community college teaching.[1]\nFramework\n\nThe Ed.S. degree is an advanced professional degree program that is considered by some accrediting bodies as the completion of the sixth year of collegiate study, (between the master's and doctorate). Historically, it has also been considered a terminal degree. Programs typically require from 30 to 45 semester hours beyond a master's degree. In some instances, an oral defense of a scholarly thesis or field study may be required, similar to a dissertation at the culmination of the degree. Some post-secondary faculty union contracts in the U.S. recognize the Ed.S. as equivalent to a doctorate on their salary scales. Some Ed.S. degree holders were on their path to earn the Ph.D. or Ed.D. but may have stopped short of completion due to some unforeseen contingencies. Some Ed.S. programs function as a bridge between a master's degree and a doctorate via articulation agreements.\n\nAn Ed.S. program typically requires about 60-70 semester hours beyond a bachelor's degree, or about 30 hours beyond a master's (making it approximately the same workload as a second Master's in terms of credits, but often the coursework is at the doctoral level). However, according to the U.S. Department of Education's International Affairs Office's leaflet, entitled, \"Structure of the U.S. Education System: Intermediate Graduate Qualifications,\" (Feb 2008), the Ed.S., as a degree, is equivalent to the Doctor of Ministry (D.Min.), the Doctor of Psychology (Psy.D./D.Psy.), and the Licentiate in Sacred Theology (S.T.L.).[2]\n\nThe Specialist in School Psychology (SSP) degree is similar to the Ed.S. in School Psychology. It is typically granted when the program is located in a department of psychology rather than education.\n\nSome universities may use an abbreviation other than Ed.S. to indicate completion of this degree. At Arkansas State University, for example, students may earn an S.C.C.T. (Specialist in Community College Teaching).\nAcademic dress\n\nAccording to The American Council on Education “six-year specialist degrees (Ed.S., etc.) and other degrees that are intermediate between the master's and the doctor's degree may have hoods specially designed (1) intermediate in length between the master's and doctor's hood, (2) with a four-inch velvet border (also intermediate between the widths of the borders of master's and doctor's hoods), and (3) with color distributed in the usual fashion and according to the usual rules. Cap tassels should be uniformly black.”[3] The other such degrees in the United States are the Licentiate degrees granted by pontifical universities and the professional engineer diploma.[2]", "skillName": "Educational_specialist."}
{"id": 12, "category": "Education", "skillText": "A graduate school (sometimes shortened as grad school) is a school that awards advanced academic degrees (i.e. master's and doctoral degrees) with the general requirement that students must have earned a previous undergraduate (bachelor's) degree[1][2] with a high grade point average. A distinction is typically made between graduate schools (where courses of study vary in the degree to which they provide training for a particular profession) and professional schools, which offer specialized advanced degrees in professional fields such as medicine, nursing, business, engineering, or law. The distinction between graduate schools and professional schools is not absolute, as various professional schools offer graduate degrees (e.g., some nursing schools offer a master's degree in nursing). Also, some graduate degrees train students for a specific profession (e.g. an MSc or a PhD in epidemiology trains a person to be an epidemiologist).\n\nMany universities award graduate degrees; a graduate school is not necessarily a separate institution. While the term \"graduate school\" is typical in the United States and often used elsewhere (e.g. Canada), \"postgraduate education\" is also used in some English-speaking countries (Australia, Canada, Ireland, India, Bangladesh, New Zealand, Pakistan and the UK) to refer to the spectrum of education beyond a bachelor's degree. Those attending graduate schools are called \"graduate students\" (in both American and British English), or often in British English as \"postgraduate students\" and, colloquially, \"postgraduates\" and \"postgrads\". Degrees awarded to graduate students include master's degrees, doctoral degrees, and other postgraduate qualifications such as graduate certificates and professional degrees.\n\nProducing original research is a significant component of graduate studies in the humanities (e.g., English literature, history, philosophy), sciences (e.g., biology, chemistry, zoology) and social sciences (e.g., sociology). This research typically leads to the writing and defense of a thesis or dissertation. In graduate programs that are oriented towards professional training (e.g., MPA, MBA, MHA), the degrees may consist solely of coursework, without an original research or thesis component. The term \"graduate school\" is primarily North American. Additionally, in North America, the term does not usually refer to medical school (whose students are called \"medical students\"), and only occasionally refers to law school or business school; these are often collectively termed professional schools. Graduate students in the humanities, sciences and social sciences often receive funding from the school (e.g., fellowships or scholarships) and/or a teaching assistant position or other job; in the profession-oriented grad programs, students are less likely to get funding, and the fees are typically much higher.\n\nAlthough graduate school programs are distinct from undergraduate degree programs, graduate instruction (in the US, Australia and other countries) is often offered by some of the same senior academic staff and departments who teach undergraduate courses. Unlike in undergraduate programs, however, it is less common for graduate students to take coursework outside their specific field of study at graduate or graduate entry level. At the Ph.D. level, though, it is quite common to take courses from a wider range of study, for which some fixed portion of coursework, sometimes known as a residency, is typically required to be taken from outside the department and college of the degree-seeking candidate, to broaden the research abilities of the student. Some institutions[which?] designate separate graduate versus undergraduate staff and denote other divisions.[not verified in body]\n\nContents\n\n    1 Australia\n    2 Brazil\n    3 Canada\n        3.1 Admission\n        3.2 Funding\n        3.3 Requirements for completion\n    4 France\n    5 Germany\n    6 United Kingdom\n    7 United States\n        7.1 Admission\n        7.2 Non-degree seeking\n        7.3 Requirements for completion\n        7.4 Funding\n        7.5 Graduate employee unions\n    8 See also\n    9 Notes\n    10 References\n    11 Further reading\n    12 External links\n\nAustralia\nFor more details on graduate schools in Australia, see Postgraduate education § Australia.\n[icon] \tThis section is empty. You can help by adding to it. (January 2015)\nBrazil\nFurther information: Universities and higher education in Brazil\n\nGraduate degrees in Brazil are called \"postgraduate\" degrees, and can be taken only after an undergraduate education has been concluded\".\n\n    Lato sensu graduate degrees: degrees that represent a specialization in a certain area, and take from 1 to 2 years to complete. Sometimes it can be used to describe a specialization level between a master's degree and a MBA. In that sense, the main difference is that the Lato Sensu courses tend to go deeper into the scientific aspects of the study field, while MBA programs tend to be more focused on the practical and professional aspects, being used more frequently to Business, Management and Administration areas. However, since there are no norms to regulate this, both names are used indiscriminately most of the time.\n    Stricto sensu graduate degrees: degrees for those who wish to pursue an academic career.\n        Masters: 2 years for completion. Usually serves as additional qualification for those seeking a differential on the job market (and maybe later a PhD), or for those who want to pursue a PhD. Most doctoral programs in Brazil require a master's degree (stricto sensu), meaning that a Lato Sensu Degree is usually insufficient to start a doctoral program.\n        Doctors / PhD: 3–4 years for completion. Usually used as a stepping stone for academic life.\n\nCanada\n\nIn Canada, the Schools and Faculties of Graduate Studies are represented by the Canadian Association of Graduate Studies (CAGS) or Association canadienne pour les études supérieures (ACES). The Association brings together 58 Canadian universities with graduate programs, two national graduate student associations, and the three federal research-granting agencies and organizations having an interest in graduate studies.[3] Its mandate is to promote, advance, and foster excellence in graduate education and university research in Canada. In addition to an annual conference, the association prepares briefs on issues related to graduate studies including supervision, funding, and professional development.\nAdmission\n\nAdmission to a master's program generally requires a bachelor's degree in a related field, with sufficiently high grades usually ranging from B+ and higher (note that different schools have different letter grade conventions, and this requirement may be significantly higher in some faculties), and recommendations from professors. Some schools require samples of the student's writing as well as a research proposal. At English-speaking universities, applicants from countries where English is not the primary language are required to submit scores from the Test of English as a Foreign Language (TOEFL).\n\nAdmission to a doctoral program typically requires a master's degree in a related field, sufficiently high grades, recommendations, samples of writing, a research proposal, and typically an interview with a prospective supervisor. Requirements are often set higher than those for a master's program. In exceptional cases, a student holding an honours BA with sufficiently high grades and proven writing and research abilities may be admitted directly to a Ph.D. program without the requirement to first complete a master's. Many Canadian graduate programs allow students who start in a master's to \"reclassify\" into a Ph.D. program after satisfactory performance in the first year, bypassing the master's degree.\n\nStudents must usually declare their research goal or submit a research proposal upon entering graduate school; in the case of master's degrees, there will be some flexibility (that is, one is not held to one's research proposal, although major changes, for example from premodern to modern history, are discouraged). In the case of Ph.D.s, the research direction is usually known as it will typically follow the direction of the master's research.\n\nMaster's degrees can be completed in one year but normally take at least two; they typically may not exceed five years. Doctoral degrees require a minimum of two years but frequently take much longer, although not usually exceeding six years.\nFunding\n\nGraduate students may take out student loans, but instead they often work as teaching or research assistants. Students often agree, as a condition of acceptance to a programme, not to devote more than twelve hours per week to work or outside interests. Various universities in Canada have different policies in terms of how much funding is available. This funding may also be different within a university in each of the disciplines. Many universities offer Evening MBA programs where students are able to work full-time while obtaining an MBA through evening classes, which allows them to pay their way through school.[4]\n\nFor Masters students, funding is generally available to first-year students whose transcripts reflect exceptionally high grades; this funding can also be obtained in the second year of studies. Funding for Ph.D. students comes from a variety of sources, and many universities waive tuition fees for doctoral candidates (This may also occur for masters students of some universities). Funding is available in the form of scholarships, bursaries and other awards, both private and public.\nRequirements for completion\n\nBoth master's and doctoral programs may be done by coursework or research or a combination of the two, depending on the subject and faculty. Most faculties require both, with the emphasis on research, and with coursework being directly related to the field of research.\n\nMaster's candidates undertaking research are typically required to complete a thesis comprising some original research and ranging from seventy to two-hundred pages. Some fields may require candidates to study at least one foreign language if they have not already earned sufficient foreign-language credits. Some faculties require candidates to defend their thesis, but many do not. Those that do not often have a requirement of taking two additional courses, minimum, in lieu of preparing a thesis.\n\nPh.D. candidates undertaking research must typically complete a thesis, or dissertation, consisting of original research representing a significant contribution to their field, and ranging from two-hundred to five-hundred pages. Most Ph.D. candidates will be required to sit comprehensive examinations—examinations testing general knowledge in their field of specialization—in their second or third year as a prerequisite to continuing their studies, and must defend their thesis as a final requirement. Some faculties require candidates to earn sufficient credits in a third or fourth foreign language; for example, most candidates in modern Japanese topics must demonstrate ability in English, Japanese, and Mandarin, while candidates in pre-modern Japanese topics must demonstrate ability in English, Japanese, Classical Chinese, and Classical Japanese language.\n\nAt English-speaking Canadian universities, both master's and Ph.D. theses may be presented in English or in the language of the subject (German for German literature, for example), but if this is the case, an extensive abstract must be also presented in English. In exceptional circumstances, a thesis may be presented in French.\n\nFrench-speaking universities have varying sets of rules; some will accept students with little knowledge of French if they can communicate with their supervisors (usually in English).\nFrance\n\nThe écoles doctorales (\"Doctoral schools\") are educational structures similar in focus to graduate schools, but restricted at PhD level. These schools have the responsibilities of providing students with a structured doctoral training in a disciplinary field. The field of the school is related to the strength of the university : while some have two or three schools (typically \"Arts and Humanities\" and \"Natural and Technological Sciences\"), others have more specialized schools (History, Aeronautics, etc.).\n\nAdmission to a doctoral program requires a master's degree, both research-oriented and disciplinary focused. High marks are required (typically a très bien honour, equating a cum laude), but the acceptance is linked to a decision of the School Academical Board.\n\nA large share of the funding offered to junior researchers is channeled through the école doctorale, mainly in the shape of three-years \"Doctoral Fellowships\" (contrats doctoraux). These fellowships are awarded after submitting a biographical information, undergraduate and graduate transcripts where applicable, letters of recommendation, and research proposal, then an oral examination by an Academical Committee.\nGermany\nSee also: List of graduate schools, German Universities Excellence Initiative\n\nThe establishment of Graduiertenkollegs in Germany started in the early 1990s funded by the Deutsche Forschungsgemeinschaft (DFG), the German Research Foundation. Unlike the American model of graduate schools, a Graduiertenkolleg consists only of a PhD programme. In contrast with the traditional German apprentice model of pursuing a PhD, a Graduiertenkolleg aims to provide young researchers with a structured doctoral training within an excellent research environment. A Graduiertenkolleg typically consists of 20-30 doctoral students, about half of whom are supported by stipends from the DFG or another sponsor. The research programme is usually narrowly defined around a specific topic and has an interdisciplinary aspect. The programme is set up for a specific period of time (up to nine years if funded by the DFG). The official English translation of the term Graduiertenkolleg is Research Training Group.\n\nIn 2006, a different type of graduate school, termed Graduiertenschule, was established by the DFG as part of the German Universities Excellence Initiative. They are thematically much broader than the focused Graduiertenkollegs and consist often of 100-200 doctoral students.\nUnited Kingdom\nMain article: Postgraduate education § United Kingdom\n\nThe term \"graduate school\" is used more widely by North American universities than by those in the UK. However, numerous universities in the UK have formally launched graduate schools, including the University of Birmingham, Durham University, Keele University, the University of Nottingham, Bournemouth University, Queen's University Belfast and the University of London, which includes graduate schools at King's College London, Royal Holloway and University College London. They often coordinate the supervision and training of candidates for doctorates.\nUnited States\nAdmission\n\nWhile most graduate programs will have a similar list of general admission requirements, the importance placed on each type of requirement can vary drastically between graduate schools, departments within schools, and even programs within departments. The best way to determine how a graduate program will weigh admission materials is to ask the person in charge of graduate admissions at the particular program being applied to. Admission to graduate school requires a bachelor's degree. High grades in one's field of study are important—grades outside the field less so. The Graduate Record Examination standardized test is required by almost all graduate schools, while other additional standardised tests (such as the Graduate Management Admission Test (GMAT) and Graduate Record Examination (GRE) Subject Tests) scores may be required by some institutions or programs.[5][6] In addition, good letters of recommendation from undergraduate instructors are often essential,[7] as strong recommendation letters from mentors or supervisors of undergraduate research experience provide evidence that the applicant can perform research and can handle the rigors of a graduate school education.\n\nWithin the sciences and some social sciences, previous research experience may be important.[5][8] By contrast, within most humanities disciplines, an example of academic writing normally suffices. Many universities require a personal statement (sometimes called Statement of purpose or Letter of Intent), which may include indications of the intended area(s) of research;[6] how detailed this statement is or whether it is possible to change one's focus of research depends strongly on the discipline and department to which the student is applying.\n\nIn some disciplines or universities, graduate applicants may find it best to have at least one recommendation from their research work outside of the college where they earned their bachelor's degree;[citation needed] however, as with previous research experience, this may not be very important in most humanities disciplines.\n\nSome schools set minimum GPAs and test scores below which they will not accept any applicants;[9] this reduces the time spent reviewing applications. On the other hand, many other institutions often explicitly state that they do not use any sort of cut-offs in terms of GPA or the GRE scores. Instead, they claim to consider many factors, including past research achievements, the compatibility between the applicant's research interest and that of the faculty, the statement of purpose and the letters of reference, as stated above. Some programs also require professors to act as sponsors. Finally, applicants from non-English speaking countries often must take the Test of English as a Foreign Language (TOEFL).[10]\n\nAt most institutions, decisions regarding admission are not made by the institution itself but the department to which the student is applying. Some departments may require interviews before making a decision to accept an applicant.[6] Most universities adhere to the Council of Graduate Schools' Resolution Regarding Graduate Scholars, Fellows, Trainees, and Assistants, which gives applicants until April 15 to accept or reject offers that contain financial support.[11]\nNon-degree seeking\n\nIn addition to traditional \"degree-seeking\" applications for admission, many schools allow students to apply as \"non degree-seeking\".[12] Admission to the Non Degree category is usually restricted primarily to those who may benefit professionally from additional study at the graduate level. For example, current primary, middle grades and secondary education teachers wishing to gain re-certification credit most commonly apply as Non Degree-Seeking students.\nRequirements for completion\n\nGraduate students often declare their intended degree (master's or doctorate) in their applications. In some cases, master's programs allow successful students to continue toward the doctorate degree. Additionally, doctoral students who have advanced to candidacy but not filed a dissertation (\"ABD,\" for \"all but dissertation\") often receive master's degrees and an additional master's called a Master of Philosophy (MPhil), or a Candidate of Philosophy (C.Phil.) degree. The master's component of a doctorate program often requires one or two years.\n\nMany graduate programs require students to pass one or several examinations in order to demonstrate their competence as scholars.[5] In some departments, a comprehensive examination is often required in the first year of doctoral study, and is designed to test a student's background undergraduate-level knowledge. Examinations of this type are more common in the sciences and some social sciences but relatively unknown in most humanities disciplines.\n\nMost graduate students perform teaching duties, often serving as graders and tutors. In some departments, they can be promoted to lecturer status, a position that comes with more responsibility.\n\nDoctoral students generally spend roughly their first two to three years taking coursework and begin research by their second year if not before. Many master's and all specialist students will perform research culminating in a paper, presentation, and defense of their research. This is called the master's thesis (or, for Educational Specialist students, the specialist paper). However, many US master's degree programs do not require a master's thesis, focusing instead primarily on course work or on \"practicals\" or \"workshops.\" Some students complete a final culminating project or \"capstone\" rather than a thesis. Such \"real-world\" experience may typically require a candidate work on a project alone or in a team as a consultant, or consultants, for an outside entity approved or selected by the academic institution and under faculty supervision.\n\nIn the second and third years of study, doctoral programs often require students to pass more examinations.[5] Programs often require a Qualifying Examination (\"Quals\"), a PhD Candidacy Examination (\"Candidacy\"), or a General Examination (\"Generals\"), designed to ensure students have a grasp of a broad sample of their discipline, and/or one or several Special Field Examinations (\"Specials\"), which test students in their narrower selected areas of specialty within the discipline. If these examinations are held orally, they may be known colloquially as \"orals.\" For some social science and many humanities disciplines, where graduate students may or may not have studied the discipline at the undergraduate level, these exams will be the first set and be based either on graduate coursework or specific preparatory reading (sometimes up to a year's work in reading).\n\nIn all cases, comprehensive exams are normally both stressful and time consuming and must be passed to be allowed to proceed on to the dissertation. Passing such examinations allows the student to stay, begin doctoral research, and rise to the status of a doctoral candidate, while failing usually results in the student leaving the program or re-taking the test after some time has passed (usually a semester or a year). Some schools have an intermediate category, passing at the master's level, which allows the student to leave with a master's without having completed a master's dissertation.\n\nFor the next several years the doctoral candidate primarily performs his or her research. Usually this lasts three to eight years, though a few finish more quickly, and some take substantially longer. In total, the typical doctoral degree takes between four and eight years from entering the program to completion, though this time varies depending upon the department, dissertation topic, and many other factors. For example, astronomy degrees take five to six years on average, but observational astronomy degrees take six to seven due to limiting factors of weather, while theoretical astronomy degrees take five.\n\nThough there is substantial variation among universities, departments, and individuals, humanities and social science doctorates on average take somewhat longer to complete than natural science doctorates. These differences are due to the differing nature of research between the humanities and some social sciences and the natural sciences and to the differing expectations of the discipline in coursework, languages, and length of dissertation. However, time required to complete a doctorate also varies according to the candidate's abilities and choice of research. Some students may also choose to remain in a program if they fail to win an academic position, particularly in disciplines with a tight job market; by remaining a student, they can retain access to libraries and university facilities, while also retaining an academic affiliation, which can be essential for conferences and job-searches.\n\nTraditionally, doctoral programs were only intended to last three to four years and, in some disciplines (primarily the natural sciences), with a helpful advisor and a light teaching load, it is possible for the degree to be completed in that amount of time. However, increasingly many disciplines, including most humanities, set their requirements for coursework, languages, and the expected extent of dissertation research by the assumption that students will take five years minimum or six to seven years on average; competition for jobs within these fields also raises expectations on the length and quality of dissertations considerably.\n\nCompetition for jobs within certain fields, such as the life sciences, is so great that almost all students now enter a second training period after graduate school called a postdoctoral fellowship. In total most life scientists will invest 12–14 years in low-paid training positions and only 14% will obtain tenure track jobs (Miller McCune, the real science gap). The average age at which life scientists obtain their first R01 grant to conduct independent research is now 42.\n\nIn some disciplines, doctoral programs can average seven to ten years. Archaeology, which requires long periods of research, tends towards the longer end of this spectrum. The increase in length of degree is a matter of great concern for both students and universities, though there is much disagreement on potential solutions to this problem.\nFunding\n\nIn general, there is less funding available to students admitted to master's degrees than for students admitted to Ph.D. or other doctoral degrees. Many departments, especially those in which students have research or teaching responsibilities, offer Ph.D. students tuition waivers and a stipend that pays for most expenses. At some elite universities, there may be a minimum stipend established for all Ph.D. students, as well as a tuition waiver. The terms of these stipends vary greatly, and may consist of a scholarship or fellowship, followed by teaching responsibilities. At many elite universities, these stipends have been increasing, in response both to student pressure and especially to competition among the elite universities for graduate students.\n\nIn some fields, research positions are more coveted than teaching positions because student researchers are typically paid to work on the dissertation they are required to complete anyway, while teaching is generally considered a distraction from one's work. Research positions are more typical of science disciplines; they are relatively uncommon in humanities disciplines, and where they exist, rarely allow the student to work on their own research. Science PhD students can apply for individual NRSA fellowships from the NIH or fellowships from private foundations. US universities often also offer competitive support from NIH-funded training programs. One example is the Biotechnology Training Program – University of Virginia. Departments often have funds for limited discretionary funding to supplement minor expenses such as research trips and travel to conferences.\n\nA few students can attain funding through dissertation improvement grants funded by the National Science Foundation (NSF), or through similar programs in other agencies. Many students are also funded as lab researchers by faculty who have been funded by private foundations or by the NSF, National Institutes of Health (NIH), or federal \"mission agencies\" such as the Department of Defense or the Environmental Protection Agency. The natural sciences are typically well funded, so that most students can attain either outside or institutional funding, but in the humanities, not all do. Some humanities students borrow money during their coursework, then take full-time jobs while completing their dissertations. Students in the social sciences are less well funded than are students in the natural and physical sciences, but often have more funding opportunities than students in the humanities, particularly as science funders begin to see the value of social science research.\n\nFunding differs greatly by departments and universities; some universities give five years of full funding to all Ph.D. students, though often with a teaching requirement attached; other universities do not. However, because of the teaching requirements, which can be in the research years of the Ph.D., even the best funded universities often do not have funding for humanities or social science students who need to do research elsewhere, whether in the United States or overseas.[citation needed] Such students may find funding through outside funders such as private foundations, such as the German Marshall Fund or the Social Science Research Council (SSRC).\n\nForeign students are typically funded the same way as domestic (US) students, although federally subsidized student and parent loans and work-study assistance are generally limited to U.S. citizens and nationals, permanent residents, and approved refugees.[13] Moreover, some funding sources (such as many NSF fellowships) may only be awarded to domestic students. International students often have unique financial difficulties such as high costs to visit their families back home, support of a family not allowed to work due to immigration laws, tuition that is expensive by world standards, and large fees: visa fees by U.S. Citizenship and Immigration Services, and surveillance fees under the Student and Exchange Visitor Program of the United States Department of Homeland Security.[14]\nGraduate employee unions\n\nAt many universities, graduate students are employed by their university to teach classes or do research. While all graduate employees are graduate students, many graduate students are not employees. MBA students, for example, usually pay tuition and do not have paid teaching or research positions. In many countries graduate employees have collectively organized labor unions in order to bargain a contract with their university. In Canada, for example, almost all graduate employees are members of a CUPE local.\n\nIn the United States there are many graduate employee unions at public universities. The Coalition of Graduate Employee Unions lists 25 recognized unions at public universities on its website. Private universities, however, are covered under the National Labor Relations Act rather than state labor laws and until 2001 there were no recognized unions at private universities.\n\nMany graduate students see themselves as akin to junior faculty, but with significantly lower pay.[citation needed] Many graduate students feel that teaching takes time that would better be spent on research, and many point out that there is a vicious circle in the academic labor economy. Institutions that rely on cheap graduate student labor have no need to create expensive professorships, so graduate students who have taught extensively in graduate school can find it immensely difficult to get a teaching job when they have obtained their degree. Many institutions depend heavily on graduate student teaching: a 2003 report by agitators for a graduate student union at Yale,[15] for instance, claims that \"70% of undergraduate teaching contact hours at Yale are performed by transient teachers: graduate teachers, adjunct instructors, and other teachers not on the tenure track.\" The state of Michigan leads in terms of progressive policy regarding graduate student unions with five universities recognizing graduate employee unions: Central Michigan University, Michigan State University, the University of Michigan, Wayne State University, and Western Michigan University.\n\nThe United Auto Workers (under the slogan \"Uniting Academic Workers\") and the American Federation of Teachers are two international unions that represent graduate employees. Private universities' administrations often oppose their graduate students when they try to form unions, arguing that students should be exempt from labor laws intended for \"employees\". In some cases unionization movements have met with enough student opposition to fail. At the schools where graduate employees are unionized, which positions are unionized vary. Sometimes only one set of employees will unionize (e.g. teaching assistants, residential directors); at other times, most or all will. Typically, fellowship recipients, usually not employed by their university, do not participate.\n\nWhen negotiations fail, graduate employee unions sometimes go on strike. While graduate student unions can use the same types of strikes that other unions do, they have also made use of teach-ins, work-ins, marches, rallies, and grade strikes. In a grade strike, graduate students refuse to grade exams and papers and, if the strike lasts until the end of the academic term, also refuse to turn in final grades. Another form of job action is known as \"work-to-rule\", in which graduate student instructors work exactly as many hours as they are paid for and no more.", "skillName": "Graduate_school."}
{"id": 13, "category": "Education", "skillText": "A Master of Engineering degree (abbreviated MEng. or M.Eng.) or Master of Technology degree (abbreviated M.Tech. or MTech) can be either an academic or professional master's degree in the field of engineering.\n\nContents\n\n    1 International variations\n        1.1 Australia\n        1.2 Brazil\n        1.3 Canada\n        1.4 Colombia\n        1.5 Croatia\n        1.6 Finland\n        1.7 France\n        1.8 Germany\n        1.9 India\n        1.10 Italy\n        1.11 New Zealand\n        1.12 Nepal\n        1.13 Japan\n        1.14 Korea\n        1.15 Poland\n        1.16 Slovakia\n        1.17 Spain\n        1.18 Sweden\n        1.19 United Kingdom\n            1.19.1 Structure\n            1.19.2 History\n            1.19.3 Other undergraduate masters\n        1.20 United States\n    2 See also\n    3 References\n\nInternational variations\nAustralia\n\nIn Australia, the Master of Engineering degree is a research degree requiring completion of a thesis. Like the Master of Philosophy (M.Phil.), it is considered a lesser degree than Doctor of Philosophy (Ph.D.), and a higher degree than coursework master. It is not to be confused with Master of Engineering Science, Master of Engineering Studies or Master of Professional Engineering which are coursework master's degrees. Exceptions are Monash University which awards a Master of Engineering Science by either research or coursework, the University of Melbourne which offers a Master of Engineering by coursework,[1] and the University of Tasmania which offer a Master of Engineering Science by research.[2]\n\nThe University of Melbourne accepted the first intake of Master of Engineering students in February 2010. The coursework Master of Engineering is offered in 11 specialisations, and accepts graduates with three-year degrees in Science and Maths.\n\nThe entry requirement is completion of a bachelor's degree at or above the second class honours level. Some universities do not offer direct enrollment into Doctor of Philosophy degree and students must first enroll in a lesser research degree before \"upgrading\".\nBrazil\n\nIn Brazil, the rough equivalent of a Master of Engineering degree is a professional master's degree of typically two years length that involves mostly coursework and a thesis or research paper in applied engineering. Entrance to a Master of Engineering degree is a 5-year bachelor engineering degree. Another variant (academic master) is equivalent to a Master of Science, such degree is tied to extensive research and is valid for a Ph.D. entry. Both can be stricto sensu classified and it depends of the course provider institution.\nCanada\n\nIn Canada, the Master of Engineering degree is a professional degree of typically two years length that involves coursework and a thesis or research paper of significant depth. Entrance to a M.Eng. degree is a 4-year bachelor engineering degree. Some Canadian universities offer a Master of Engineering, or either a Master of Science or Master of Applied Science in engineering, or both. Master of Engineering degrees usually require more coursework and examination and less research, whereas Master of Applied Science degrees require less coursework and more research. However, this is not absolute since some universities only offer a Master of Engineering and some only offer a Master of Science or Master of Applied Science. Some universities differentiate only by the requirement to be resident and therefore assist in teaching/tutoring for a Master of Science Degree.\nColombia\n\nIn Colombia, the Master of Engineering, which takes a minimum of two years, is a postgraduate program that follows the undergraduate of five years. Depending upon the emphasis is called Ms.Eng. with emphasis in Energy, Chemistry, Environment, and so on. At the end, it is required to make a publication of the developed work in a recognized journal of scientific spreading as a requirement for the degree.\nCroatia\n\nIntroduced with the Bologna process, the Croatian Master of Engineering is typically a two-year program and a direct continuation of a three-year Bachelor course. The degree is abbreviated mag. ing. and followed by the field of study (for example: mag. ing. računarstva – Master of Computer Engineering)\nFinland\n\nThere are two distinct degrees in Finland, a taught university degree (diplomi-insinööri) and a polytechnic master's degree's (insinööri (ylempi AMK)).[3] While the former is translated as \"Master of Science in Technology\", the term \"Master of Engineering\" is predominantly used by Universities of Applied Sciences, which offer master's degree programmes to holders of polytechnic bachelor's degrees (insinööri (amk)). As European Bologna process directs, in order to get a M.Eng. degree, B.Sc engineers have to additionally study full-time one or two years and finalize a Master's thesis. Most of the M.Eng. degree programs are taught in Finnish, but some Swedish and English language programs also exist.[4]\nFrance\n\nIn France, two diploma exist for 5 years of study in the field of engineering: the Master's diploma in engineering (diplôme de master en sciences de l'ingénieur) which is usually delivered by Universities, and the Engineer's degree (\"diplôme d'ingénieur\") which can only be delivered by some Engineering schools called grandes écoles—very selective schools that are generally smaller than universities—and provides \"a level of education comparable to a master's degree in engineering in the United States\" (AACRAO \n).\n\nThe Engineer's degree usually prepare students for professional careers. Courses always include management, economy or more generally transverse courses. Training periods in industry or in laboratories are also required. The Master's diploma in engineering offers a more focused approach on a field of engineering. A Ph.D. program can be joined by acquiring a Master's diploma in engineering or an Engineer's degree.\nGermany\n\nIn Germany, the local engineer's degrees (Diplomingenieur (Dipl.-Ing.), a first degree after 5 years of study at a university and Dipl.-Ing. (FH), the engineering degree offered by Fachhochschulen after 4 years of study) were abolished in most universities in 2010, and were replaced by postgraduate master's degrees (M.Sc. and M.Eng.).\n\nThe first Master of Engineering courses were introduced in Germany in 2000 as result of the Bologna process. This type of master's degree is offered by German Fachhochschulen (Universities of Applied Sciences) universities and is typically a two-year program with application-oriented coursework and an applied research thesis. The entry requirement is the successful completion of a bachelor's degree, or an equivalent from before the Bologna process, with good marks.\nIndia\n\nIn India, a Master of Engineering or Master of Technology or Master of Science in Technology or Master of Science in Engineering degree is a postgraduate program in engineering or technology field. This is generally a 2-year (2.5[5] years for M.Sc.Eng.) specialization program in a specific branch of engineering or a technical field. Students typically enter the M.Eng./M.Tech./M.Sc.Eng.[5][6][7]/M.Sc.Tech. programs after completing a 4-year undergraduate program in engineering resulting in the award of a Bachelor of Engineering or Bachelor of Technology degree, or a 5-year program in Science or Applied Sciences resulting in the award of a Master of Science degree.\n\nThe M.Eng./M.Tech./M.Sc.Eng./M.Sc.Tech. programs in India are usually structured as an Engineering Research degree, lesser than Ph.D. and considered to be parallel to M.Phil. and M.Sc. degrees in Humanities and Science respectively. M.Eng. and M.Sc.Eng. programs in India were started by some well known institutions[8] with the aim of producing Research Engineers who can also get the position of \"Technologist\" in the Industries and Research Institutes. In electrical engineering, for example, areas of specialization might include: power systems, energy engineering, electrical machines, instrumentation and control, high voltage or power electronics, telecommunications, communication networks, signal processing, microelectronics...\nItaly\n\nIn Italy, the local engineer's degrees (Laurea in Ingegneria), a first degree after 5 years of study at a university were abolished in most universities in 2008. The equivalent of a Master of Engineering degree is a professional master's degree (Laurea Magistrale) of two years length that involves mostly coursework and a thesis paper in applied engineering. Entrance to a Master of Engineering degree is a 3-year bachelor engineering degree (Laurea).\nNew Zealand\n\nIn New Zealand, the Master of Engineering degree is a research based degree requiring completion of a thesis or a thesis (mainly) plus related taught courses in almost universities.[9][10] Similar to the Master of Philosophy (M.Phil.) in engineering or technology, it is considered a lesser degree than Doctor of Philosophy (Ph.D.), and a higher degree than coursework master. It is not to be confused with Master of Engineering Studies which is coursework master's degree.\n\nHowever, in Auckland University of Technology (AUT), this degree can be achieved only by coursework based,[11] which can be a coursework master's degree.\nNepal\n\nIn Nepal, Master of Engineering or Master of Technology degree is a postgraduate program in engineering or technology. This is generally a 2-year specialization program in a specific branch of engineering or a technical field. Students typically enter the ME/MTech programs after completing a 4-year undergraduate program in engineering resulting in the award of a Bachelor of Engineering or Bachelor of Technology degree. Kathmandu University, Tribhuwan University, Pokhara University and Purwanchal University provides these Engineering courses. IOE(Institute Of Engineering)provides M.Sc. cources in engineering at pulchowk campus(central campus of IOE)under the affiliation of Tribhuwan University.For more detail www.ioe.edu.np \nJapan\n\nIn Japan,the master of engineering (工学修士 Kogaku Shushi?), which takes a minimum of two years, is a postgraduate program that follows the undergraduate of four years. It is a research degree between the bachelor's degree and the doctor's degree, and requires completion of a thesis.\nKorea\n\nIn South Korea, the master of engineering, which takes a minimum of two years, is a postgraduate program that follows the undergraduate of four years. It is commonly awarded for specializations in the field of engineering rather than the science. For example, the degree \"master of science in computer science\" differs from the degree \"master of engineering in computer science\" in that the latter one is mainly concentrated on the applicability of the design with strong relation with the hardware rather than the software. Generally, the master of engineering program includes both coursework and research.\nPoland\n\nMagister inżynier (mgr inż., literally: master engineer) academic degree that can be obtained after 2 years post-graduate education (for students having already B.Eng.—inż.), or formerly (until full adaptation of Bologna process by university) through integrated 5 years B.Eng.–M.Eng. (or B.Sc.–M.Sc) programme, giving double degree mgr inż.\nSlovakia\n\nFIIT STU Software engineering IEE accreditation\n\nFEI STU engineering IEE accreditation\nSpain\n\nThe \"Master en Ingeniería Informática\" or \"The Master en Ingeniería Mecatrónica\" in Spain are degrees (Postgraduate Studies) which were recently implanted in the Spanish Educational System. They use the ECTS (European Credit Transfer and Accumulation System) which was designed by the European Union (Bologna Process). They are approximately equivalent to the second half of the old Engineering (\"Ingeniería\") academic degree, which spanned 5 full-time yearly courses (\"Licenciatura\"), or 4 in the posterior increased-credit-payload condensed form (Ingeniería) offered by some universities that preceded the application of the Bologna system. Not to be mistaken by the Technical Engineering degree (\"Ingeniería Técnica\") of a shorter duration: 3 years with standard credit payload or 2 years and a half including a bridge course that gave access to the higher engineering studies. The technical engineering degree is approximately similar to the new Degree within the Bologna system.\n\nEducation in Engineering had traditionally not too many specialities (Naval, Aerospatial, Industrial, Telecommunications, Informatics, Nuclear, and a few others) but after Bologna the Degrees and Masters branched out numerously, giving birth to hundreds of different combinations, aggravated by the possibility to take a Degree on one university and specialty, and then to take a Master´s degree on another specialty, perhaps at a different university, having usually passed an individual interview by the Dean and/or a \"Propedeutics\" exam or test of some sort, designed to guarantee a solid basis upon which to build further knowledge, that must take into consideration the degree of relationship between two branches of knowledge and among the subjects being taught at both degree at the origin and the destination.\n\nEngineering studies in Spain are among the hardest university studies in Spain, as shown by official statistics, which, at a priori equal level of competence from students, or even slightly higher (due to cutoff scores from the selective tests \"Selectividad\" used in admissions) reveal very high rates for dropout when compared to other studies, particularly, nut not only those among the Humanities, as well as increased, sometimes duplicated number of years to complete the degrees, overall much lower average scores when compared to other careers such as Medicine or Magistery and a consequent much lower rated of Honour Badges (\"Matrícula de Honor\") being awarded per 100 students. From 1990 to 2010, the number of students taking Informatics Engineering studies (including Computer Science, Software Engineering and IT Project Management subjects) increased steadily, outnumbering other Engineering egresates, and slightly declined after that due mainly to the worldwide deceipts of Y2K bug and the dot-com bubble, followed by the Global Financial Crisis. However, this collective was one of the most numerous among Engineers in Spain during the second decade.\n\nDuring the first decades of the 21st century, Spain greatly boosted its rate of university degrees per 1000 people, which now ranks among the highest in Europe. Furthermore, students proceeded into longer study and research careers, and the rates of postgraduates, Master holders, Multi-Master holders, Ph.D. and Postdocs increased noticeably. Nonetheless, due to the severe and prolonged 2008 economic crisis, the average salary these people perceive is not in line with their skills and qualification. This circumstance fostered a strong migratory movement or \"brain drain\" from the country outwards. Currently there exists a very low cost of opportunity in hiring Spanish Engineers and the Return of Investment of such a recruiting is very high when compared to hiring in other European, American or Western nations, adding an unparalled factor of technical skills acquired through struggling through adverse economic downturns.\nSweden\n\nThe \"Master of Engineering\" title was introduced in some Swedish universities proceeding the Bologna process. The title \"civilingenjör\" (literally translated \"Civil engineer\", but the English term \"Civil engineer\" is not equivalent to \"civilingenjör\") is the equivalent of a M.Eng. as well as the \"Master of Science in Engineering\" title. A Master of Science in Engineering is awarded after a minimum of 5 years of studies. Before 2007-07-01, it was awarded after a minimum of 4½ year of studies. Students starting with their studies before 2007-07-01, but finishing them before 2015-06-30 and after 2007-07-01, may choose to obtain the title either after 4½ year or after 5 years.\nUnited Kingdom\n\nIn the United Kingdom the M.Eng. degree is the normative university-level qualification taken by people wishing to become chartered engineers registered with the Engineering Council (EngC). The M.Eng. degree represents the minimum educational standard required to become a chartered engineer, but there are other equally satisfactory ways to demonstrate this standard such as the completion of a B.Eng. Honours and a subsequent postgraduate diploma or M.Sc., or by completion of the Engineering Council Postgraduate Diploma. The UK M.Eng. (undergraduate degree) is typically equivalent to the European Diplom Ingenieur (Dipl.-Ing.) and Civilingenjör degrees.\n\nEngC's minimum requirement for entry to a recognised M.Eng. course is BBB at A-level, compared to CCC for a B.Eng. Honours course. Universities are free to set higher entry requirements if they wish. Some universities, such as Oxford, Cambridge and Imperial only admit students to study for the M.Eng. degree. (Their courses usually allow a student to leave with a bachelor's degree after three years, but these shortened degrees are not ECUK-recognised and therefore do not count towards the educational requirements for becoming a chartered engineer.) Other universities, such as the University of Greenwich, University of Surrey, Coventry University and Brunel University, admit students to read for B.Eng. Honours and M.Eng. courses and allow students to change between the two during the early years of the course. The Open University offers the M.Eng. degree as a postgraduate qualification but requires students to complete its course within four years of completing a B.Eng. Honours degree.\n\nThe Master of Engineering (M.Eng.) is the highest award for undergraduate studies in engineering. In England, Northern Ireland and Wales this is a four-year course or a 'sandwich' five-year course (with one year spent working in industry). In Scotland, it is a five-year course. The Bachelor of Engineering degree (B.Eng.) is usually a three-year course (four in Scotland), or can also include a year in industry. Many universities offer the B.Eng., and may then allow a transfer onto the M.Eng. The Engineering Council Graduate Diploma is set at the same level as the final year of a British B.Eng. and its Postgraduate Diploma is set at the same level as the final year of a British M.Eng. The Graduateship in engineering, awarded by the City & Guilds of London Institute (Institution Established in 1878 recognized by Royal Charter n.117 year 1900), is mapped to a British Bachelor of Engineering (Honours)—B.Eng. (Honours)—degree. The Post Graduate Diploma is mapped to a British Master of Engineering (M.Eng.) degree. The Membership in Engineering (MCGI)(NQF at Level 7) is a strategic Management/Chartered professional level and a Post Graduate Diploma, mapped to a British master's degree, awarded by the City & Guilds of London Institute. This will be supported by a minimum of ten years of seasoned experience (peer reviewed) in areas as the Engineering + a British bachelor/graduateship (or by CEng).\n\nEngineers who have been awarded a B.Eng.(Ordinary) or B.Eng.(Honours) and have appropriate training and experience in the work place are able to apply to become an Incorporated Engineer (IEng). If an engineer has studied beyond the B.Eng. for an M.Sc. or has an M.Eng., they may apply to become a Chartered Engineer (CEng), once they have completed the required amount of post graduate work-based competency training and experience. Competency and training requirements are met over a period of 4–8 years in practice for a total of 8–12 years education, training and professional responsibility. Formal structured post graduate training schemes such as the monitored professional development IMechE enable the Engineer in training to satisfy the requirements for Chartered Engineer faster.[12]\n\nChartered Engineer and Incorporated Engineer titles awarded by the Engineering Council UK, are very broadly equivalent to (but not the same as) North American Professional Engineer (PEng / PE) and Professional Technologist (PTech) designations, but with often a far greater geographical recognition. However P.Eng/PE serve a very different purpose than the CEng qualification. PE/P.Eng are licenses to practice engineering in the public domain with legal liability at the state or provincial level. Unlike C.Eng they are not qualifications or titles. Under government legislations they allow one to engage in professional practice in a defined geographic region. For example, in Ontario the P.Eng license is issued within the Professional Engineers Act (established in 1922). Despite the Washington accord PE/PEng does not equal C.Eng. The ability of a C.Eng to practice engineering in the public domain in North America is determined on a case by case basis usually by state or province. Agreements to recognize qualifications between EngC or Engineers Ireland and Engineers Canada or the USA ABET are not recognized by individual states or provinces.\nStructure\n\nIn terms of course structure, M.Eng. degrees usually follow the pattern familiar from bachelor's degrees with lectures, laboratory work, coursework and exams each year. There is usually a substantial project to be completed in the fourth year which may have a research element to it, and a more teaching-based project to be completed in the third year. At the end of the third year, there is usually a threshold of academic performance in examinations to allow progression to the final year. At some universities, the structure of the final year is rather different from that of the first three, for example, at the University of York, the final year for the Computer Systems and Software programme consists entirely of project work and intensive advanced seminar courses rather than traditional lectures and problem classes. Final results are, in most cases, awarded on the standard British undergraduate degree classification scale, although some universities award something structurally similar to 'Distinction', 'Merit', 'Pass' or 'Fail' as this is often the way that taught postgraduate master's degrees are classified.\nHistory\n\nAt some universities in the UK in the early 1980s, the M.Eng. was awarded as a taught postgraduate degree after 12 months of study. Its entry requirements would typically be like those for other taught postgraduate courses, including holding an undergraduate degree, and its format would be similar to the modern M.Eng. although, as with many postgraduate master's degrees, the project would extend over a longer period. M.Eng. courses in their modern, undergraduate form were introduced in the mid-1980s in response to growing competition from technical-degree graduates from continental Europe, where undergraduate bachelor's degree courses are often longer than the usual three years in the UK. There was a feeling among recent graduates, the engineering institutions, employers and universities, that the longer and more in-depth study offered on the continent needed to be made available to UK students as well. Since to obtain a taught master's degree in the UK typically took an additional year beyond a bachelor's degree, it was decided that this extra year would be integrated into the undergraduate program and, instead of pursuing both a bachelor's and master's degree, students would proceed directly to a master's degree.\n\nSince its introduction, the M.Eng. has become the degree of choice for most undergraduate engineers, as was intended. The most common exception to this is international students who, because of the substantially higher fees they are charged, sometimes opt to take the tradition B.Eng./B.Sc. route where that is available[citation needed]. Most of the engineering institutions have now made an M.Eng. the minimum academic standard necessary to become a Chartered Engineer. Students who graduated before the changes in the rules will still be allowed to use their bachelor's degree for this purpose and those who have earned a bachelor's degree since the changes can usually take some additional courses (known as 'further learning') over time to reach an equivalent standard to the M.Eng. Some older universities such as Durham[13] and Cambridge allow students to obtain the B.Eng. degree after the third year before continuing on to the fourth year.\nOther undergraduate masters\n\nThe M.Eng. is one of a number of 'new' undergraduate master's degrees recently introduced in the UK; they are also commonly available in engineering, mathematics, computer science, physics, chemistry and biology.\nUnited States\n\nIn the United States, the Master of Engineering degree is generally a professional degree offered as a coursework-based alternative to the traditional research-based Master of Science. It is typically a two-year program, entered after the completion of a 4-year bachelor's degree and many universities allow students to choose between the Master of Engineering and the Master of Science. The Master of Engineering degree is offered at many leading universities in the United States on either a full-time and part-time (weekends or evenings) basis[14] and is considered a terminal degree in the field of engineering.\n\nSome M.Eng. degree programs require a scholarly project in addition to coursework. Some Master of Engineering programs require additional courses beyond those required for Master of Science students in order to better prepare students for professional careers. Some Master of Engineering programs highly encourage students to participate in collaborative consulting projects.[15] These courses may include topics such as business fundamentals, management and leadership.[16]", "skillName": "Master_of_Engineering."}
{"id": 14, "category": "Education", "skillText": "The Professional Science Master’s (PSM) Degree is a graduate degree designed to allow students to pursue advanced training in science or mathematics while simultaneously developing workplace skills. PSM programs are interdisciplinary in nature, preparing students for fields such as forensic science, computational chemistry, applied mathematics and bioinformatics. PSM degrees can be completed in two years of full-time study including an internship.\n\nContents\n\n    1 History\n    2 References\n    3 Bibliography\n    4 External links\n\nHistory\n\nRecognizing that traditional graduate-level science training may not be suitable for non-academic careers, the Alfred P. Sloan Foundation, in 1997, began to support master’s-level degree programs designed to provide science, technology, engineering, and mathematics (\"STEM\") students with a pathway into science-based careers.[1] These Professional Science Master’s degrees combine a science or mathematics curriculum with a professional component designed to provide graduates with the necessary skills for a career in business, government, or nonprofit agencies.[2]\n\nOriginally funding fourteen campuses, the Sloan Foundation expanded its support directly or indirectly to over fifty institutions, collectively offering over 100 different PSM programs.[1] In 2005, the Foundation funded the Council of Graduate Schools to be an “institutional base for PSM growth, with the goal of making the degree a normal, recognized, widely accepted academic offering”.[1] In furtherance of this objective, the Sloan Foundation also provided support to found the National Professional Science Master’s Association, a professional organization of PSM directors and alumni intended to “provide a collective voice for PSM degree programs”.[3]\n\nIn 2007, Congress passed the America COMPETES Act which placed special emphasis on improving America’s economic competitiveness by strengthening STEM education. The COMPETES Act specifically mentioned the importance of the PSM degree to the nation’s overall competitiveness.[4] Additionally, a 2008 report issued by the National Research Council of the National Academies urged the continued expansion of the PSM degree. In 2009, the National Science Foundation, under the auspices of the American Recovery and Reinvestment Act, facilitated funding of twenty-two different PSM programs by appropriating funds for a Science Master’s program.\nReferences\n\nAlfred P. Sloan Foundation [Sloan], 2008\nSimms, 2006\nNational Professional Science Master’s Association, 2010, para. 5\n\n    CGS, 2007\n\nBibliography\n\nAlfred P. Sloan Foundation. (2008). Science Education: Professional Science Master’s Degree. Retrieved August 3, 2010, from http://www.sloan.org/program/15 \n.\n\n______________________. (2008). Professional Science Master’s Degree: History. Retrieved August 3, 2010, from http://www.sloan.org/program/15/page/67 \n.\n\nCommittee on Enhancing the Master’s Degree in Natural Sciences. (2008). Science Professionals: Master’s Education for a Competitive World. Washington, D.C.: National Research Council of the National Academies. Retrieved August 3, 2010, from http://www.nap.edu/catalog.php?record_id=12064 \n.\n\nCouncil of Graduate Schools. (2007). Statement on the America COMPETES Act. Retrieved August 3, 2010,from http://www.cgsnet.org/portals/0/pdf/GR_AmericaCOMPETES_0307.pdf \n.\n\nNational Science Foundation. (2009). Program Solicitation: Science Master’s Program. Retrieved August 3, 2010, from http://www.nsf.gov/pubs/2009/nsf09607/nsf09607.htm \n.\n\nNational Professional Science Master’s Association. (2010). About Us. Retrieved August 3, 2010, from http://www.npsma.org/mc/page.do?sitePageId=101812&orgId=npsma \n.\n\nSimms, Leslie. (2006). Professional Master’s Education: A CGS Guide to Establishing Programs. Washington, D.C.: Council of Graduate Schools.", "skillName": "Professional_Science_Master's_Degree."}
{"id": 15, "category": "Education", "skillText": "The Master of Laws is a postgraduate academic degree, pursued by those either holding an undergraduate academic law degree, a professional law degree, or an undergraduate degree in a related subject. In some jurisdictions the \"Master of Laws\" is the basic professional degree for admission into legal practice. It is commonly abbreviated LL.M., from the Latin Legum Magister (meaning \"master of laws\").\n\nContents\n\n    1 Background on legal education in common law countries\n    2 International situation\n    3 Types of LL.M. degrees\n    4 Requirements\n        4.1 Australia\n        4.2 Canada\n        4.3 China\n        4.4 Germany\n        4.5 Hong Kong\n        4.6 India\n        4.7 Ireland\n        4.8 Italy\n        4.9 Pakistan\n        4.10 Portugal\n        4.11 South Africa\n        4.12 United Kingdom\n            4.12.1 Oxbridge\n        4.13 United States\n            4.13.1 Programs for foreign legal graduates\n            4.13.2 International law and other LL.M. programs\n    5 See also\n    6 References\n    7 External links\n\nBackground on legal education in common law countries\n\nTo become a lawyer and practice law in most states and countries, a person must first obtain a law degree. While in most common law countries a Bachelor of Laws (or LL.B.) is required, the U.S. and Canada generally require a professional doctorate, or Juris Doctor, to practice law.[1][2]\n\nThe Juris Doctor (J.D.) is a professional doctorate[3][4][5][6][7][8] and first professional[9][10] graduate degree[11][12][13] in law. The degree is earned by completing law school in the United States, Canada, Australia, and other common law countries. Many who hold the degree of Juris Doctor are professionals committed to the practice of law, and may choose to focus their practice on criminal law, tort, family law, corporate law, and/or a wide range of other areas.\n\nThe majority of individuals holding a J.D. must pass an examination in order to be licensed to practice law within their respective jurisdictions.[14][15][16][17]\n\nIf a person wishes to gain specialized knowledge through research in a particular area of law, he or she can continue his or her studies after an LL.B or J.D. in an LL.M. program. The word legum is the genitive plural form of the Latin word lex and means \"of the laws\". When used in the plural, it signifies a specific body of laws, as opposed to the general collective concept embodied in the word jus, from which the words \"juris\" and \"justice\" derive.\n\nThe highest research degree in law is the S.J.D. (or J.S.D., depending on the institution), and it is equivalent to the Doctor of Philosophy in Law (PhD or DPhil depending on the law school in UK), Doctorat en Droit (in France), or the Doktor der Rechtswissenschaften (Dr.iur.) in Germany. There are also variant doctoral degrees, such as the D.C.L. (Doctor of Civil Law) degree bestowed by McGill University in Canada. Most schools require an LL.M. before admission to a SJD or a PhD in law degree program. Like the PhD, the SJD degree generally requires a dissertation that is graded (often by two graders), orally defended (by an exam known as Viva Voce) and then often published as a book or series of articles.\n\nThe \"Doctor of Laws\" (LL.D.) degree in the United States of America is usually an honorary degree.\nInternational situation\n\nHistorically, the LL.M. degree is an element particular to the education system of English speaking countries, which is based on a distinction between Bachelor's and Master's degrees. Over the past years, however, specialized LL.M. programs have been introduced in many European countries, even where the Bologna process has not yet been fully implemented.\n\nDenmark, Sweden, Norway, Cyprus, Italy and Switzerland require a Master's with an additional two to five years to become a lawyer.\n\nAs of 2014, Spain requires a master's degree in addition to a 4 years' degree to become a lawyer.\n\nIn Finland, an LL.M. is the standard graduate degree required to practice law.[18] No other qualifications are required.[19]\n\nTo be allowed to practice law in the Netherlands, one needs an LL.M. degree with a specific (set of) course(s) in litigation law. The Dutch Order of Lawyers (NOVA) require these courses for every potential candidate lawyer who wants to be conditionally written in the district court for three years. After receiving all the diplomas prescribed by NOVA and under supervision of a \"patroon\" (master), a lawyer is eligible to have his own practice and is unconditionally written in a court for life but he/she will need to continually update his/her knowledge.\nTypes of LL.M. degrees\n\nThere are a wide range of LL.M. programs available worldwide, allowing students to focus on almost any area of the law. Most universities offer only a small number of LL.M. programs. One of the most popular LL.M. degrees in the United States is tax law, sometimes referred to as an MLT (Master of Laws in Taxation).\n\nOther LL.M. degree programs include bankruptcy law, banking law, commercial law, criminal law, dispute resolution, entertainment and media law, environmental law, estate planning (usually as a sub-specialty of tax law), financial services law, human rights law, information technology law, insurance law, intellectual property law, international law, law and economics, litigation, maritime law, military law, patent law, prosecutorial sciences, real estate law, social care law, telecommunications law, trade law, Trial Advocacy. Some law schools allow LL.M. students to freely design their own program of study from the school's many upper-level courses and seminars, including commercial and corporate, international, constitutional, and human rights law.\n\nIn Europe, LL.M. programs in European law are recently very popular, often referred to as LL.M. Eur (Master of European Law).\n\nSome LL.M. programs, particularly in the United States, and also in China, focus on teaching foreign lawyers the basic legal principles of the host country (a \"comparative law\" degree).\n\nMoreover, some programs are conducted in more than one language, give the students the opportunity to undertake classes in differing languages.\nRequirements\n\nLL.M. programs are usually only open to those students who have first obtained a degree in law. There are exceptions to this but an undergraduate degree or extensive experience in a related field is still required. Full-time LL.M. programs usually last one year and vary in their graduation requirements. Most programs require students to write a thesis. Some programs are research oriented with little classroom time (similar to a M.Phil.), while others require students to take a set number of classes (similar to a taught degree or M.Sc.).\n\nLL.M. degrees are often earned by students wishing to develop more concentrated expertise in a particular area of law. Pursuing an LL.M. degree may also allow law students to build a professional network. Some associations provide LL.M. degree holders with structures designed to strengthen their connections among peers and to access a competitive business environment, much like an MBA degree.\n\nMany LL.M. programs require a thesis.\nAustralia\n\nIn Australia, the LL.M. is generally only open to law graduates. However, some universities permit a non-law graduate to undertake variants of the degree.\n\nThe shortage of graduate program/articles places has resulted in some LL.B. graduates proceeding directly to an LL.M. course prior to seeking graduate employment.\n\nUnique variants of the LL.M. exist, such as the Master of Legal Practice (M.L.P.) available at the Australian National University, which serves as an equivalent to the Graduate Diploma in Legal Practice law graduates must obtain before being able to be admitted as a barrister or solicitor. Other variants of the LL.M. are more similar to the LL.M. available in the wider Commonwealth but under a different title, for example Master of Commercial Law, or Master of Human Rights Law. These courses are usually more specialised than a standard LL.M.\nCanada\n\nIn Canada, the LL.M. is generally only open to law graduates holding an LL.B., B.C.L., or a J.D. as a first degree. Students can choose to take research based LL.M. degrees or course based LL.M. degrees. Research based LL.M. degrees are one- or two-year programs that require students to write a thesis that makes a significant contribution to their field of research. Course based LL.M. degrees do not require a significant research paper. An LL.M. can be studied part-time, and at some schools, through distance learning. LL.M. degrees can be general, or students can choose to pursue a specialized area of research.\n\nCanadian law graduates pursue LL.M. degrees because they would like to pursue a career in academia or because they would like to deepen their knowledge in a specific area of the law. Canadian law graduates in most of the provinces in Canada must complete an internship with a law firm (known as \"articling\") and a professional legal training course, as well as pass professional exams in order to be called to the bar in a province.[20]\n\nForeign trained lawyers who wish to practice in Canada will first need to have their education and experience assessed by the Federation of Law Societies of Canada's National Committee on Accreditation. Upon having received a certificate of accreditation from the National Committee on Accreditation, foreign law graduates would then have to obtain articles with a law firm, take the professional legal training course, and pass the professional exams to be called to the bar in a province.\n\nThe University of British Columbia's LLM in Common Law is an example of one of a few LLM courses that help to prepare students for the professional exams.\nChina\n\nThe LL.M. is available at China University of Political Science and Law, and the entrance requirements are: a native English speaker, or near native English, with any bachelor's degree. The course is flexible and allows students to study Mandarin and assists with organizing work experience in Beijing and other cities in China. It normally takes two years, but can be completed in one and a half years if students take the required credits in time.[21]\n\nThe flagship of the China-EU School of Law (CESL) in Beijing is a Double Master Programme including a Master of Chinese Law and a Master of European and International Law (with special focus on Chinese Law). The Master of European and International Law is taught in English, open for international students and can be studied as a single master programme.[22]\n\nBeijing Foreign Studies University has launched an online LLM for international professionals. The course is taken over two years, with the first covering online lessons through video and assignments, the second year is for the dissertation and an online defense is required at the end. Students are required to attend Beijing for an introductory week in September to enroll and meet students and staff. Students also have the opportunity to take work experience at a top five law firm in China.[23][24]\nGermany\n\nIn Germany, the LL.M is seen as an advanced legal qualification of supplementary character. As such, Master of Laws programmes are generally open not only to law graduates, but also to graduates of related subjects and/or those displaying a genuine interest in and link to the particular LL.M programme in question. Some graduates choose to undertake their LL.M directly following their \"erstes juristisches Staatsexamen\" (the \"first state examination\" constitutes the first stage of the official German legal training and completes the German undergraduate law degree), an alternative undergraduate course, or their \"zweites juristisches Staatsexamen\" (that is, the second and final stage of the official German legal training, following which graduates are referred to as \"Volljuristen\" who then have access to practice in different branches of the legal profession). On the other hand, many professionals now take career breaks in order to study for an LL.M, in particular for subjects of growing importance or those with constantly changing dynamics, such as European law or media law for example.\nHong Kong\n\nLL.M. degree programmes are offered by the law faculties of The University of Hong Kong, the Open University of Hong Kong, The Chinese University of Hong Kong and the City University of Hong Kong. An LL.B. degree is usually required for admission, but for some specialised programmes, such as the LL.M. in Human Rights programme offered by HKU, requires an undergraduate degree in laws or any related discipline.\nIndia\n\nIn India, the thrust of legal education is on the undergraduate law degrees with most of those opting for the undergraduate law degree either going forward to enroll themselves with the Bar Council of India and start practicing as Advocates or giving legal advice without being eligible to appear in courts (a consequence of non-enrollment). Similar to the United Kingdom, a master's degree in Law in India is basically opted to specialize in particular areas of law. Traditionally the most popular areas of specialization in these master's degrees in law in India have been constitutional law, family law and taxation law.\n\nHowever, with the establishment of the specialized autonomous law schools in India in 1987 (the first was the National Law School of India University) much emphasis is being given at the master's level of legal education in India. With the establishment of these universities, focus in specialization has been shifted to newer areas such as corporate law, intellectual property law, international trade law etc. Master's degree of Law in India was in earlier times of 2 years but at present it is of only 1 year.\nIreland\n\nA number of universities and colleges in Ireland offer LL.M. Master of Laws programmes, such as Trinity College Dublin, University College Cork who have an LL.M. e-Law programme, National University of Ireland, Galway (NUIG) who offer an LL.M in Public Law, National University of Ireland, Maynooth (NUIM), who offer an LLM programme and an LLM in International Business Law (this is a dual degree with the Catholic University of Lyon) and Griffith College in Dublin and Cork who offer LL.M. programmes in International, Commercial and Human Rights Law. Hibernia College offer a completely online LL.M. in International Business Law validated by Birmingham City University.[25]\nItaly\n\nItaly offers both master programs in Italian and in English, depending on the school. They are often called \"laurea specialistica\", that is, the second step of the Bologna plan (European curriculum), and in this case they last two years. For example, the University of Milan \noffers a 2 years LLM on Sustainable Development. In Alto Adige programs are also taught in German, as in Bolzano \n.\n\nIn Italy the term \"master\" often refers to a vocational master, 6 or 12 months long, on specific areas, such as \"law and internet security\", or \"law of administrative management\", is often taught part-time to allow professionals already working in the field to improve their skills.\nPakistan\n\nIn Pakistan, University of the Punjab, Lahore, International Islamic University, Islamabad, S.M.Law College, Karachi, GC University, Faisalabad, University of Sargodha are LL.M. degree awarding institutions. In University of the Punjab, procedure of obtaining LL.M. degree is very difficult. Duration is of two years plus mandatory thesis on the proposed topic. A student has to get 60% marks in order to qualify for LL.M.degree. That is the reason only 136 students have been awarded LL.M. degree from the University of the Punjab since 1981 when the LL.M. degree course was started.\nPortugal\n\nThe Master of Laws programmes offered in Portugal are extremely varied but haven't, for the most part, adopted the designation LL.M., being more commonly called Mestrado em Direito (Master's Degree in Law), like the ones at Coimbra University's Faculty of Law \nand Lusíada University of Porto \n. Albeit the classical Mestrado em Direito takes two years to finish and involves a scientific dissertation, there are some shorter variants. A few Mestrados with an international theme have specifically adopted the LL.M designation: the LL.M in European and Transglobal Business Law at the School of Law of the University of Minho \nand the LL.M. Law in a European and Global Context and the Advanced LL.M. in International Business Law, both at the Católica Global School of Law \n, in Lisbon.\nSouth Africa\nSee also: Legal education in South Africa\nUniversity of Pretoria Faculty of Law\n\nIn South Africa, the LL.M. is a postgraduate degree offered both as a course-based, and as a research-based Master's. In the former case, the degree comprises advanced coursework in a specific area of law as well as (limited) related research, usually in the form of a \"short dissertation\", while in the latter, the degree is entirely thesis (\"dissertation\") based. The first type, typically, comprises practice-oriented \"training\", while the second type is theory-oriented, often preparing students for admission to LL.D. or Ph.D. programmes; see Doctor of law#South Africa. The research Master's essentially reflects an ability to conduct independent research, whereas a Doctoral thesis is, in addition, an original contribution in the field of study.[26] Admission is generally limited to LL.B. graduates, although holders of other law degrees, such as the BProc, may be able to apply if admitted as attorneys and / or by completing supplementary LL.B. coursework.[27]\nUnited Kingdom\n\nIn the United Kingdom, an LL.M. programme is open to those holding a recognised legal qualification, generally an undergraduate degree in Laws or a CPE. They do not have to be or intend to be legal practitioners. An LL.M. is not a sufficient qualification in itself to practise as a solicitor or barrister, since this requires completion of the Legal Practice Course, Bar Professional Training Course, or, if in Scotland, the Diploma in Legal Practice but is an opportunity to gain specialist knowledge of a particular area of law and/or an understanding of the legal systems of other nations. As with other degrees, an LL.M. can be studied on a part-time basis at many institutions and in some circumstances by distance learning.\n\nMost institutions allow those without a first degree in law onto their LL.M. programme although there are still minimum educational requirements, such as an undergraduate degree, or evidence of substantial professional experience in a related field. Examples of such programmes include the Master of Studies in Legal Research at Oxford, the LL.M. degrees at the University of Edinburgh and LL.M.s at the University of Leicester[28] In addition, Queen's University offers an LL.M. suite, accessible to legal and social science graduates, leading to specialisms in sustainable development, corporate governance, devolution or human rights. Northumbria University offers an innovative approach to an LL.M. qualification to students starting the master's programme as undergraduates. Students completing this four-year programme graduate with a combined LL.M. and Legal Practice Course professional qualification or BPTC.\n\n\nOxbridge\n\nThe Universities of Oxford and Cambridge have taken slightly different approaches to other British universities to postgraduate legal study, as they have in other areas.[29]\n\nThe University of Cambridge has a taught postgraduate law course, which formerly conferred an LL.B. on successful candidates (undergraduates studying law at Cambridge received a B.A.). In 1982 the LL.B. for postgraduate students was replaced with a more conventional LL.M. to avoid confusion with undergraduate degrees in other universities.[30] Additionally in 2012 the University of Cambridge introduced the M.C.L. (Masters of Corporate Law) aimed at postgraduate students with interests in corporate law.[30]\n\nThe University of Oxford unconventionally names its taught masters of laws B.C.L. (Bachelor of Civil Law) and M.Jur. (Magister Juris), and its research masters either MPhil (Master of Philosophy) or MSt (Master of Studies).[31] Oxford continues to name its principal postgraduate law degree the B.C.L. for largely historic reasons, as the B.C.L. is one of the oldest (and therefore within the Oxford hierarchy, most senior) degrees, having been conferred since the sixteenth century.[32] The M.Jur. was introduced in 1991.[32] At present there is no LL.M. degree conferred by the University.[31] Oxford claims that the B.C.L. is \"the most highly regarded taught masters-level qualification in the common law world\".[33]\nUnited States\n\nIn the United States the acquisition of an LL.M. degree is often, a way to specialize in an area of law such as tax law, business law, international business law, health law, trial advocacy, environmental law or intellectual property. A number of schools have combined J.D.-LL.M. programs, while others offer the degree through online study. Some LLM programs feature a general study of American law.\nPrograms for foreign legal graduates\n\nAn LL.M. degree from an ABA-approved law school also allows a foreign lawyer to become eligible to apply for admission to the bar (license to practice) in certain states. Each state has different rules relating to the admittance of foreign-educated lawyers to state bar associations.\n\nAn LL.M. degree from an ABA-approved law school qualifies a foreign legal graduate to take the bar exam in Alabama, California, New Hampshire, New York, Texas, as well as in the independent republic of Palau.[34]\n\nIn addition, legal practice in the home jurisdiction plus a certain amount of coursework at an accredited law school qualifies a foreign legal graduate to take the bar exam in Alaska, the District of Columbia, Massachusetts, Missouri, Pennsylvania, Rhode Island, Tennessee, Utah and West Virginia. However, a number of states, including Arizona, Florida, Georgia, New Jersey and North Carolina only recognize JD degrees from accredited law schools as qualification to take the bar.[35]\n\nNew York allows foreign lawyers from civil law countries to sit for the New York bar exam once they have completed a minimum of 24 credit hours (usually but not necessarily in an LL.M. program) at an ABA-approved law school involving at least two basic subjects tested on the New York bar exam. However, beginning for those who take the bar exam in July 2013, applicants will be required to complete 24 credits of law school coursework, including 12 credits in specific areas of law.[36] Lawyers from common-law countries face more lenient restrictions and may not need to study at an ABA-approved law school. Foreign lawyers from both civil law and common law jurisdictions, however, are required to demonstrate that they have successfully completed a course of law studies of at least three years that would fulfill the educational requirements to bar admission in their home country.[37]\nInternational law and other LL.M. programs\n\nAs of 2008, there is one LL.M. degree in International Law offered by The Fletcher School of Law and Diplomacy at Tufts University, the oldest school of international affairs in the United States. Given that the degree specializes in international law, and is not teaching a first degree in U.S. law (the J.D. degree), the program has not sought ABA accreditation.\n\nThe Notre Dame Law School at the University of Notre Dame offers an LL.M in International Human Rights Law to JD graduates from ABA-accredited US schools or LL.B or equivalent from accredited non-US schools.[38]\n\nBoth Duke University School of Law and Cornell Law School offer J.D. students the opportunity to simultaneously pursue an LL.M. in International and Comparative Law.\n\nThe University of Nebraska-Lincoln College of Law provides an LL.M. in Space, Cyber & Telecommunications Law, the only program providing focused study in these three areas. The program was established using a grant from NASA and a partnership with the U.S. Air Force Strategic Command.\n\nThe College of Law at the University of Tulsa offers an LL.M. in American Indian and Indigenous Peoples Law to JD graduates from ABA-accredited US schools or LL.B or equivalent from accredited non-US schools.[39]\n\nThere is one institution that offers an ABA-approved LL.M, that does not offer the first degree in law (the J.D. degree); The U.S. Army Judge Advocate General's Legal Center and School offers an officer's resident graduate course, a specialized program beyond the first degree in law, leading to an LL.M. in Military law, with concentrations in Administrative and Civil Law, Government Contract and Fiscal Law, Criminal Law, and Operational and International Law.[40][41]", "skillName": "Master_of_Laws."}
{"id": 16, "category": "Biomedical", "skillText": "Duties of Biomedical Engineers\n\nBiomedical engineers typically do the following:\n\n    Design equipment and devices, such as artificial internal organs, replacements for body parts, and machines for diagnosing medical problems\n    Install, adjust, maintain, repair, or provide technical support for biomedical equipment\n    Evaluate the safety, efficiency, and effectiveness of biomedical equipment\n    Train clinicians and other personnel on the proper use of equipment\n    Work with life scientists, chemists, and medical scientists to research the engineering aspects of the biological systems of humans and animals\n    Prepare procedures, write technical reports, publish research papers, and make recommendations based on their research findings\n    Present research findings to scientists, nonscientist executives, clinicians, hospital management, engineers, other colleagues, and the public\n\nBiomedical engineers design instruments, devices, and software used in healthcare; bring together knowledge from many technical sources to develop new procedures; or conduct research needed to solve clinical problems.\n\nThey often serve a coordinating function, using their background in both engineering and medicine. For example, they may create products for which an indepth understanding of living systems and technology is essential. They frequently work in research and development or in quality assurance.\n\nBiomedical engineers design electrical circuits, software to run medical equipment, or computer simulations to test new drug therapies. In addition, they design and build artificial body parts, such as hip and knee joints. In some cases, they develop the materials needed to make the replacement body parts. They also design rehabilitative exercise equipment.\n\nThe work of these engineers spans many professional fields. For example, although their expertise is based in engineering and biology, they often design computer software to run complicated instruments, such as three-dimensional x-ray machines. Alternatively, many of these engineers use their knowledge of chemistry and biology to develop new drug therapies. Others draw heavily on mathematics and statistics to build models to understand the signals transmitted by the brain or heart.\n\nThe following are examples of specialty areas within the field of biomedical engineering:\n\nBioinstrumentation uses electronics, computer science, and measurement principles to develop devices used in the diagnosis and treatment of disease.\n\nBiomaterials is the study of naturally occurring or laboratory-designed materials that are used in medical devices or as implantation materials.\n\nBiomechanics involves the study of mechanics, such as thermodynamics, to solve biological or medical problems.\n\nClinical engineering applies medical technology to optimize healthcare delivery.\n\nRehabilitation engineering is the study of engineering and computer science to develop devices that assist individuals with physical and cognitive impairments.\n\nSystems physiology uses engineering tools to understand how systems within living organisms, from bacteria to humans, function and respond to changes in their environment.\n\nSome people with training in biomedical engineering become professors. For more information, see the profile on postsecondary teachers.\nBiomedical Engineering Work Environment[About this section] [To Top]\n\nBiomedical engineers held about 22,100 jobs in 2014. The industries that employed the most biomedical engineers were as follows:\nMedical equipment and supplies manufacturing \t23%\nResearch and development in the physical, engineering, and life sciences \t16\nPharmaceutical and medicine manufacturing \t12\nNavigational, measuring, electromedical, and control instruments manufacturing \t8\nHospitals; state, local, and private \t8\n\nBiomedical engineers work in a variety of settings. Some work in hospitals, where therapy occurs, and others work in laboratories, doing research. Still others work in manufacturing settings, where they design biomedical engineering products. Yet other biomedical engineers work in commercial offices, where they make or support business decisions.\n\nBiomedical engineers work in teams with scientists, healthcare workers, or other engineers. Where and how they work depends on the project. For example, a biomedical engineer who has developed a new device designed to help a person with a disability to walk again might have to spend hours in a hospital to determine whether the device works as planned. If the engineer finds a way to improve the device, he or she might have to return to the manufacturer to help alter the manufacturing process in order to improve the design.", "skillName": "BiomedicalEngineers2."}
{"id": 17, "category": "Biomedical", "skillText": "Biomedicine (i.e. Medical biology) is a branch of medical science that applies biological and other natural-science principles to clinical practice.[1] The branch especially applies to biology and physiology.[2] Biomedicine also can relate to many other categories in health and biological related fields. It has been the dominant health system for more than a century.[3][4][5][6]\n\nIt includes many biomedical disciplines and areas of specialty that typically contain the \"bio-\" prefix such as:\n\n    molecular biology, biochemistry, biotechnology, cell biology, embryology,\n    nanobiotechnology, biological engineering, laboratory medical biology,\n    cytogenetics, genetics, gene therapy,\n    bioinformatics, biostatistics, systems biology, neuroscience,\n    microbiology, virology, immunology, parasitology,\n    physiology, pathology, anatomy,\n    toxicology, and many others that generally concern life sciences as applied to medicine.\n\nMedical biology[7] is the cornerstone of modern health care and laboratory diagnostics. It concerns a wide range of scientific and technological approaches: from an in vitro diagnostics[8][9] to the in vitro fertilisation,[10] from the molecular mechanisms of a cystic fibrosis to the population dynamics of the HIV virus, from the understanding molecular interactions to the study of the carcinogenesis,[11] from a single-nucleotide polymorphism (SNP) to the gene therapy.\n\nMedical biology based on molecular biology combines all issues of developing molecular medicine[12] into large-scale structural and functional relationships of the human genome, transcriptome, proteome, physiome and metabolome with the particular point of view of devising new technologies for prediction, diagnosis and therapy [13]\n\nBiomedicine involves the study of (patho-) physiological processes with methods from biology and physiology. Approaches range from understanding molecular interactions to the study of the consequences at the in vivo level. These processes are studied with the particular point of view of devising new strategies for diagnosis and therapy.[14][15]\n\nDepending on the severity of the disease, biomedicine pinpoints a problem within a patient and fixes the problem through medical intervention. Medicine focuses on curing diseases rather than improving one's health.[16]\n\nContents\n\n    1 Molecular Biology\n    2 Biochemistry\n    3 See also\n    4 References\n    5 External links\n\nMolecular Biology\n\nMolecular biology is the a process of synthesis and regulation of a cell’s DNA, RNA, and protein. Molecular biology consists of different techniques including Polymerase chain reaction, Gel electrophoresis, and macromolecule blotting to manipulate DNA.\n\nPolymerase chain reaction is done by placing a mixture of the desired DNA, DNA polymerase, primers, and nucleotide bases into a machine. The machine heats up and cools down at various temperatures to break the hydrogen bonds binding the DNA and allows the nucleotide bases to be added onto the two DNA templates after it has been separated.[17]\n\nGel electrophoresis is a technique used to identify similar DNA between two unknown samples of DNA. This process is done by first preparing an agarose gel. This jelly-like sheet will have wells for DNA to be poured into. An electric current is applied so that the DNA, which is negatively charged due to its phosphate groups is attracted to the positive electrode. Different rows of DNA will move at different speeds because some DNA pieces are larger than others. Thus if two DNA samples show a similar pattern on the gel electrophoresis, one can tell that these DNA samples match.[18]\n\nMacromolecule blotting is a process performed after gel electrophoresis. An alkaline solution is prepared in a container. A sponge is placed into the solution and an agaros gel is placed on top of the sponge. Next, nitrocellulose paper is placed on top of the agarose gel and a paper towels are added on top of the nitrocellulose paper to apply pressure. The alkaline solution is drawn upwards towards the paper towel. During this process, the DNA denatures in the alkaline solution and is carried upwards to the nitrocellulose paper. The paper is then placed into a plastic bag and filled with a solution full of the DNA fragments, called the probe, found in the desired sample of DNA. The probes anneal to the complimentary DNA of the bands already found on the nitrocellulose sample. Afterwards, probes are washed off and the only ones present are the ones that have annealed to complimentary DNA on the paper. Next the paper is stuck onto an x ray film. The radioactivity of the probes creates black bands on the film, called an autoradiograph. As a result, only similar patterns of DNA to that of the probe are present on the film. This allows us the compare similar DNA sequences of multiple DNA samples. The overall process results in a precise reading of similarities in both similar and different DNA sample.[19]\nBiochemistry\n\nBiochemistry is the science of the chemical processes which takes place within living organisms. Living organisms need essential elements to survive, consisting of carbon, hydrogen, nitrogen, oxygen, calcium, and phosphorus. These elements make up the four big macromolecules that living organisms need to survive- carbohydrates, lipids, proteins, and nucleic acids.[20]\n\nCarbohydrates, made up of carbon, hydrogen, and oxygen, are energy storing molecules. The simplest one of carbohydrates is glucose, C6H12O6, is used in cellular respiration to produce ATP, adenosine triphosphate, which supplies cells with energy.[20]\n\nProteins are chains of amino acids that function to contract skeletal muscle, function catalysts, transport molecules, and storage molecules. Proteins can facilitate biochemical processes, by lowering the activation energy of a reaction. Hemoglobins are also proteins, that carry oxygen to the cells in an organisms body.[20]\n\nLipids, also known as fats, also serve to store energy, but in the long term. Due to their unique structure, lipids provide more than twice the amount of energy that carbohydrates do. Lipids can be used as insulation, as it is present below the layer of skin in living organisms. Moreover, lipids can be used in hormone production to maintain a healthy hormonal balance and provide structure to your cell walls.[20]\n\nNucleic acids are used to store DNA in every living organism. The two types of nucleic acids are DNA and RNA. DNA is the main genetic information storing substance found oftentimes in the nucleus, which controls the processes that the cell undergoes. DNA consists of two complimentary antiparallel strands consisting varying patterns of nucleotides. RNA is a single strand of DNA, which is transcribed from DNA and used for DNA translation, which is the process for making proteins out of RNA sequences.[20]\nSee also\n\n    Cardiophysics\n    Laboratory diagnostics\n    Medicinal chemistry\n    Medical physics\n    The Cancer Genome Atlas[21]\n    The Convention on Human Rights and Biomedicine\n    The Human Genome Project[22][23]\n    The Human Physiome Project", "skillName": "Biomedicine."}
{"id": 18, "category": "Biomedical", "skillText": "Biomedical engineering (BME) is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g. diagnostic or therapeutic). This field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical and biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy.[1] Biomedical engineering has only recently emerged as its own study, compared to many other engineering fields. Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EEGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.\n\nContents\n\n    1 History\n        1.1 Origin\n        1.2 Major milestones\n    2 Bioinformatics\n    3 Biomechanics\n    4 Biomaterial\n    5 Biomedical optics\n    6 Tissue engineering\n    7 Genetic engineering\n    8 Neural engineering\n    9 Pharmaceutical engineering\n    10 Medical devices\n        10.1 Medical imaging\n        10.2 Implants\n        10.3 Bionics\n    11 Clinical engineering\n    12 Rehabilitation engineering\n    13 Regulatory issues\n        13.1 RoHS II\n        13.2 IEC 60601\n    14 Training and certification\n        14.1 Education\n        14.2 Licensure/certification\n    15 Career prospects\n    16 Founding figures\n    17 See also\n    18 References\n    19 Further reading\n    20 External links\n\nHistory\n\nBiomedical engineering has existed for centuries, perhaps even thousands of years. Researchers said the wear on the bottom surface suggests that it could be the oldest known limb prosthesis.\n\n    1895: Wilhelm Roentgen accidentally discovered that a cathode-ray tube could make a sheet of paper coated with barium platinocyanide glow, even when the tube and the paper were in separate rooms. Roentgen decided the tube must be emitting some kind of penetrating rays, which he called “X” rays. This set off a flurry of research into the tissue-penetrating and tissue-destroying properties of X-rays, a line of research that ultimately produced the modern array of medical imaging technologies and virtually eliminated the need for exploratory surgery.\n\nOrigin\n\nBiomedical engineering originated during World War II.[inconsistent] Biologists were needed to do work involving advances on radar technology, which led them to the electronic developments in medicine. Due to these developments, the next generation of biologists could not benefit from this technology, as they couldn’t understand it. A bridge was needed to fill the gap between technical knowledge and biology. Doctors and biologists who were interested in engineering and electrical engineers interested in biology became the first bioengineers. Those primarily concerned with medicine became the first biomedical engineers. The unique mix of engineering, medicine and science in biomedical engineering emerged alongside biophysics and medical physics early this century.\nMajor milestones\n\nBiomedical engineering achievements range from early devices, such as crutches, platform shoes, and wooden teeth to more modern equipment, including pacemakers, heart-lung machine, dialysis machines, diagnostic equipment, imaging technologies of every kind, and artificial organs, medical implants and advanced prosthetics.\n\n    1791: Luigi Galvani invented the frog galvanoscope.\n    1851: Hermann von Helmholtz invented the ophthalmoscope.\n    1881: Samuel von Basch invented the blood pressure meter (also known as sphygmomanometer).\n    1895: Conrad Roentgen (Germany) discovered the X-ray using gas discharged tubes.\n    1896: Henry Becquerel (France) discovered X-rays were emitted from uranium ore.\n    1901: Roentgen received the Nobel Prize for discovery of X-Rays.\n    1903: Willem Einthoven invented the electrocardiogram (ECG).\n    1921: First formal training in biomedical engineering was started at Oswalt Institute for Physics in Medicine, Frankfurt, Germany.\n    1927: Invention of the Drinker respirator.\n    1929: Hans Berger invents the electroencephalogram (EEG).\n    1930: X-rays were being used to visualize most organ systems using radio-opaque materials, refrigeration, permitted blood banks.\n    Mid 1930s – early 1940s: Antibiotics, sulfanilamide and pencillin reduced cross-infection in hospitals.\n    1940: Cardiac catheterization.\n    1943: The International Bio-Physical Society was formed.\n    1948: The first conference of Engineering in Medicine & Biology was held in the United States.\n    1950: Electron microscope.\n    1950s – early 1960s: Nuclear medicine.\n    1953: Cardiopulmonary bypass (heart–lung machine).\n    1969: Case Western Reserve created a MD/PhD program\n    1970: computer tomography (CT) and magnetic resonance imaging (MRI).\n    1975: Whitaker Foundation was founded.\n    1980: Gamma camera, positron emission tomography (PET) and SPECT.\n    1997: First Indigenous endovascular coronary stent (Kalam-Raju stent) was developed by the Care Foundation.\n    Biomedical engineering has provided advances in medical technology to improve human health. As per the statistics of the National Academy of Engineering, about 32000 biomedical engineers are currently working in various areas of health care technology.\n\nBioinformatics\nMain article: Bioinformatics\n\nBioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data.\n\nBioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis \"pipelines\" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.\nBiomechanics\nMain article: Biomechanics\n\nSee Biomechanics.\nBiomaterial\nMain article: Biomaterial\n\nA biomaterial is any matter, surface, or construct that interacts with living systems. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science or biomaterials engineering. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.\nBiomedical optics\nMain article: Biomedical optics\nTissue engineering\nMain article: Tissue engineering\n\nTissue engineering, like genetic engineering (see below), is a major segment of biotechnology - which overlaps significantly with BME.\n\nOne of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. Biomedical engineers are currently researching methods of creating such organs. Researchers have grown solid jawbones[2] and tracheas[3] from human stem cells towards this end. Several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients.[4] Bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.[5]\nMicromass cultures of C3H-10T1/2 cells at varied oxygen tensions stained with Alcian blue.\nGenetic engineering\nMain article: Genetic engineering\n\nGenetic engineering, recombinant DNA technology, genetic modification/manipulation (GM) and gene splicing are terms that apply to the direct manipulation of an organism's genes. Unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. Genetic engineering techniques have found success in numerous applications. Some examples include the improvement of crop technology (not a medical application, but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.\nNeural engineering\n\nNeural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. Neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.\nPharmaceutical engineering\n\nPharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of Chemical Engineering, and Pharmaceutical Analysis. It may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment. The ISPE is an international body that certifies this now rapidly emerging interdisciplinary science.\nMedical devices\nMain articles: Medical devices, medical equipment, and Medical technology\n\nThis is an extremely broad category—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism.\n\nA medical device is intended for use in:\n\n    the diagnosis of disease or other conditions, or\n    in the cure, mitigation, treatment, or prevention of disease.\n\nSome examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.\nBiomedical instrumentation amplifier schematic used in monitoring low voltage biological signals, an example of a biomedical engineering application of electronic engineering to electrophysiology.\n\nStereolithography is a practical example of medical modeling being used to create physical objects. Beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies,[6] treatments,[7] patient monitoring,[8] of complex diseases.\n\nMedical devices are regulated and classified (in the US) as follows (see also Regulation):\n\n    Class I devices present minimal potential for harm to the user and are often simpler in design than Class II or Class III devices. Devices in this category include tongue depressors, bedpans, elastic bandages, examination gloves, and hand-held surgical instruments and other similar types of common equipment.\n    Class II devices are subject to special controls in addition to the general controls of Class I devices. Special controls may include special labeling requirements, mandatory performance standards, and postmarket surveillance. Devices in this class are typically non-invasive and include X-ray machines, PACS, powered wheelchairs, infusion pumps, and surgical drapes.\n    Class III devices generally require premarket approval (PMA) or premarket notification (510k), a scientific review to ensure the device's safety and effectiveness, in addition to the general controls of Class I. Examples include replacement heart valves, hip and knee joint implants, silicone gel-filled breast implants, implanted cerebellar stimulators, implantable pacemaker pulse generators and endosseous (intra-bone) implants.\n\nMedical imaging\nMain article: Medical imaging\n\nMedical/biomedical imaging is a major segment of medical devices. This area deals with enabling clinicians to directly or indirectly \"view\" things not visible in plain sight (such as due to their size, and/or location). This can involve utilizing ultrasound, magnetism, UV, radiology, and other means.\nAn MRI scan of a human head, an example of a biomedical engineering application of electrical engineering to diagnostic imaging. Click here to view an animated sequence of slices.\n\nImaging technologies are often essential to medical diagnosis, and are typically the most complex equipment found in a hospital including: fluoroscopy, magnetic resonance imaging (MRI), nuclear medicine, positron emission tomography (PET), PET-CT scans, projection radiography such as X-rays and CT scans, tomography, ultrasound, optical microscopy, and electron microscopy.\nImplants\n\nAn implant is a kind of medical device made to replace and act as a missing biological structure (as compared with a transplant, which indicates transplanted biomedical tissue). The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone or apatite depending on what is the most functional. In some cases, implants contain electronics, e.g. artificial pacemakers and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.\nArtificial limbs: The right arm is an example of a prosthesis, and the left arm is an example of myoelectric control.\nA prosthetic eye, an example of a biomedical engineering application of mechanical engineering and biocompatible materials to ophthalmology.\nBionics\nFurther information: Bionics § In medicine\n\nArtificial body part replacements are one of the many applications of bionics. Concerned with the intricate and thorough study of the properties and function of human body systems, bionics may be applied to solve some engineering problems. Careful study of the different functions and processes of the eyes, ears, and other organs paved the way for improved cameras, television, radio transmitters and receivers, and many other useful tools. These developments have indeed made our lives better, but the best contribution that bionics has made is in the field of biomedical engineering (the building of useful replacements for various parts of the human body). Modern hospitals now have available spare parts to replace body parts badly damaged by injury or disease. Biomedical engineers work hand in hand with doctors to build these artificial body parts.\nClinical engineering\nMain article: Clinical engineering\n\nClinical engineering is the branch of biomedical engineering dealing with the actual implementation of medical equipment and technologies in hospitals or other clinical settings. Major roles of clinical engineers include training and supervising biomedical equipment technicians (BMETs), selecting technological products/services and logistically managing their implementation, working with governmental regulators on inspections/audits, and serving as technological consultants for other hospital staff (e.g. physicians, administrators, I.T., etc.). Clinical engineers also advise and collaborate with medical device producers regarding prospective design improvements based on clinical experiences, as well as monitor the progression of the state of the art so as to redirect procurement patterns accordingly.\n\nTheir inherent focus on practical implementation of technology has tended to keep them oriented more towards incremental-level redesigns and reconfigurations, as opposed to revolutionary research & development or ideas that would be many years from clinical adoption; however, there is a growing effort to expand this time-horizon over which clinical engineers can influence the trajectory of biomedical innovation. In their various roles, they form a \"bridge\" between the primary designers and the end-users, by combining the perspectives of being both 1) close to the point-of-use, while 2) trained in product and process engineering. Clinical engineering departments will sometimes hire not just biomedical engineers, but also industrial/systems engineers to help address operations research/optimization, human factors, cost analysis, etc. Also see safety engineering for a discussion of the procedures used to design safe systems.\nRehabilitation engineering\nMain article: Rehabilitation engineering\n\nRehabilitation engineering is the systematic application of engineering sciences to design, develop, adapt, test, evaluate, apply, and distribute technological solutions to problems confronted by individuals with disabilities. Functional areas addressed through rehabilitation engineering may include mobility, communications, hearing, vision, and cognition, and activities associated with employment, independent living, education, and integration into the community.[1]\n\nWhile some rehabilitation engineers have master's degrees in rehabilitation engineering, usually a subspecialty of Biomedical engineering, most rehabilitation engineers have undergraduate or graduate degrees in biomedical engineering, mechanical engineering, or electrical engineering. A Portuguese university provides an undergraduate degree and a master's degree in Rehabilitation Engineering and Accessibility.[2][4] Qualification to become a Rehab' Engineer in the UK is possible via a University BSc Honours Degree course such as Health Design & Technology Institute, Coventry University.[5]\n\nThe rehabilitation process for people with disabilities often entails the design of assistive devices such as Walking aids intended to promote inclusion of their users into the mainstream of society, commerce, and recreation.\nSchematic representation of a normal ECG trace showing sinus rhythm; an example of widely used clinical medical equipment (operates by applying electronic engineering to electrophysiology and medical diagnosis).\nRegulatory issues\n\nRegulatory issues have been constantly increased in the last decades to respond to the many incidents caused by devices to patients. For example, from 2008 to 2011, in US, there were 119 FDA recalls of medical devices classified as class I. According to U.S. Food and Drug Administration (FDA), Class I recall is associated to “a situation in which there is a reasonable probability that the use of, or exposure to, a product will cause serious adverse health consequences or death“ [9]\n\nRegardless of the country-specific legislation, the main regulatory objectives coincide worldwide.[10] For example, in the medical device regulations, a product must be: 1) safe and 2) effective and 3) for all the manufactured devices\n\nA product is safe if patients, users and third parties do not run unacceptable risks of physical hazards (death, injuries, …) in its intended use. Protective measures have to be introduced on the devices to reduce residual risks at acceptable level if compared with the benefit derived from the use of it.\n\nA product is effective if it performs as specified by the manufacturer in the intended use. Effectiveness is achieved through clinical evaluation, compliance to performance standards or demonstrations of substantial equivalence with an already marketed device.\n\nThe previous features have to be ensured for all the manufactured items of the medical device. This requires that a quality system shall be in place for all the relevant entities and processes that may impact safety and effectiveness over the whole medical device lifecyle.\n\nThe medical device engineering area is among the most heavily regulated fields of engineering, and practicing biomedical engineers must routinely consult and cooperate with regulatory law attorneys and other experts. The Food and Drug Administration (FDA) is the principal healthcare regulatory authority in the United States, having jurisdiction over medical devices, drugs, biologics, and combination products. The paramount objectives driving policy decisions by the FDA are safety and effectiveness of healthcare products that have to be assured through a quality system in place as specified under 21 CFR 829 regulation. In addition, because biomedical engineers often develop devices and technologies for \"consumer\" use, such as physical therapy devices (which are also \"medical\" devices), these may also be governed in some respects by the Consumer Product Safety Commission. The greatest hurdles tend to be 510K \"clearance\" (typically for Class 2 devices) or pre-market \"approval\" (typically for drugs and class 3 devices).\n\nIn the European context, safety effectiveness and quality is ensured through the \"Conformity Assessment\" that is defined as \"the method by which a manufacturer demonstrates that its device complies with the requirements of the European Medical Device Directive\". The directive specifies different procedures according to the class of the device ranging from the simple Declaration of Conformity (Annex VII) for Class I devices to EC verification (Annex IV), Production quality assurance (Annex V), Product quality assurance (Annex VI) and Full quality assurance (Annex II). The Medical Device Directive specifies detailed procedures for Certification. In general terms, these procedures include tests and verifications that are to be contained in specific deliveries such as the risk management file, the technical file and the quality system deliveries. The risk management file is the first deliverable that conditions the following design and manufacturing steps. Risk management stage shall drive the product so that product risks are reduced at an acceptable level with respect to the benefits expected for the patients for the use of the device. The technical file contains all the documentation data and records supporting medical device certification. FDA technical file has similar content although organized in different structure. The Quality System deliverables usually includes procedures that ensure quality throughout all product life cycle. The same standard (ISO EN 13485) is usually applied for quality management systems in US and worldwide.\nImplants, such as artificial hip joints, are generally extensively regulated due to the invasive nature of such devices.\n\nIn the European Union, there are certifying entities named \"Notified Bodies\", accredited by European Member States. The Notified Bodies must ensure the effectiveness of the certification process for all medical devices apart from the class I devices where a declaration of conformity produced by the manufacturer is sufficient for marketing. Once a product has passed all the steps required by the Medical Device Directive, the device is entitled to bear a CE marking, indicating that the device is believed to be safe and effective when used as intended, and, therefore, it can be marketed within the European Union area.\n\nThe different regulatory arrangements sometimes result in particular technologies being developed first for either the U.S. or in Europe depending on the more favorable form of regulation. While nations often strive for substantive harmony to facilitate cross-national distribution, philosophical differences about the optimal extent of regulation can be a hindrance; more restrictive regulations seem appealing on an intuitive level, but critics decry the tradeoff cost in terms of slowing access to life-saving developments.\nRoHS II\n\nDirective 2011/65/EU, better known as RoHS 2 is a recast of legislation originally introduced in 2002. The original EU legislation “Restrictions of Certain Hazardous Substances in Electrical and Electronics Devices” (RoHS Directive 2002/95/EC) was replaced and superseded by 2011/65/EU published in July 2011 and commonly known as RoHS 2. RoHS seeks to limit the dangerous substances in circulation in electronics products, in particular toxins and heavy metals, which are subsequently released into the environment when such devices are recycled.\n\nThe scope of RoHS 2 is widened to include products previously excluded, such as medical devices and industrial equipment. In addition, manufacturers are now obliged to provide conformity risk assessments and test reports – or explain why they are lacking. For the first time, not only manufacturers, but also importers and distributors share a responsibility to ensure Electrical and Electronic Equipment within the scope of RoHS comply with the hazardous substances limits and have a CE mark on their products.\nIEC 60601\n\nThe new International Standard IEC 60601 for home healthcare electro-medical devices defining the requirements for devices used in the home healthcare environment. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series.\n\nThe mandatory date for implementation of the EN European version of the standard is June 1, 2013. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.\nTraining and certification\nEducation\n\nBiomedical engineers require considerable knowledge of both engineering and biology, and typically have a Master's (M.S., M.Tech, M.S.E., or M.Eng.) or a Doctoral (Ph.D.) degree in BME (Biomedical Engineering) or another branch of engineering with considerable potential for BME overlap. As interest in BME increases, many engineering colleges now have a Biomedical Engineering Department or Program, with offerings ranging from the undergraduate (B.Tech,B.S., B.Eng or B.S.E.) to doctoral levels. Biomedical engineering has only recently been emerging as its own discipline rather than a cross-disciplinary hybrid specialization of other disciplines; and BME programs at all levels are becoming more widespread, including the Bachelor of Science in Biomedical Engineering which actually includes so much biological science content that many students use it as a \"pre-med\" major in preparation for medical school. The number of biomedical engineers is expected to rise as both a cause and effect of improvements in medical technology.[11]\n\nIn the U.S., an increasing number of undergraduate programs are also becoming recognized by ABET as accredited bioengineering/biomedical engineering programs. Over 65 programs are currently accredited by ABET.[12][13]\n\nIn Canada and Australia, accredited graduate programs in Biomedical Engineering are common, for example in Universities such as McMaster University, and the first Canadian undergraduate BME program at Ryerson University offering a four-year B.Eng program.[14][15][16][17] The Polytechnique in Montreal is also offering a bachelors's degree in biomedical engineering.\n\nAs with many degrees, the reputation and ranking of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees are also linked to the institution's graduate or research programs, which have some tangible factors for rating, such as research funding and volume, publications and citations. With BME specifically, the ranking of a university's hospital and medical school can also be a significant factor in the perceived prestige of its BME department/program.\n\nGraduate education is a particularly important aspect in BME. While many engineering fields (such as mechanical or electrical engineering) do not need graduate-level training to obtain an entry-level job in their field, the majority of BME positions do prefer or even require them.[18] Since most BME-related professions involve scientific research, such as in pharmaceutical and medical device development, graduate education is almost a requirement (as undergraduate degrees typically do not involve sufficient research training and experience). This can be either a Masters or Doctoral level degree; while in certain specialties a Ph.D. is notably more common than in others, it is hardly ever the majority (except in academia). In fact, the perceived need for some kind of graduate credential is so strong that some undergraduate BME programs will actively discourage students from majoring in BME without an expressed intention to also obtain a master's degree or apply to medical school afterwards.\n\nGraduate programs in BME, like in other scientific fields, are highly varied, and particular programs may emphasize certain aspects within the field. They may also feature extensive collaborative efforts with programs in other fields (such as the University's Medical School or other engineering divisions), owing again to the interdisciplinary nature of BME. M.S. and Ph.D. programs will typically require applicants to have an undergraduate degree in BME, or another engineering discipline (plus certain life science coursework), or life science (plus certain engineering coursework).\n\nEducation in BME also varies greatly around the world. By virtue of its extensive biotechnology sector, its numerous major universities, and relatively few internal barriers, the U.S. has progressed a great deal in its development of BME education and training opportunities. Europe, which also has a large biotechnology sector and an impressive education system, has encountered trouble in creating uniform standards as the European community attempts to supplant some of the national jurisdictional barriers that still exist. Recently, initiatives such as BIOMEDEA have sprung up to develop BME-related education and professional standards.[19] Other countries, such as Australia, are recognizing and moving to correct deficiencies in their BME education.[20] Also, as high technology endeavors are usually marks of developed nations, some areas of the world are prone to slower development in education, including in BME.\nLicensure/certification\nSee also: Professional engineer\n\nEngineering licensure in the US is largely optional, and rarely specified by branch/discipline. As with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered Professional Engineer (PE), but in practice such a license is not required to practice in the majority of situations (due to an exception known as the private industry exemption, which effectively applies to the vast majority of American engineers). This is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine.\n\nBiomedical engineering is regulated in some countries, such as Australia, but registration is typically only recommended and not required.[21]\n\nIn the UK, mechanical engineers working in the areas of Medical Engineering, Bioengineering or Biomedical engineering can gain Chartered Engineer status through the Institution of Mechanical Engineers. The Institution also runs the Engineering in Medicine and Health Division.[22] The Institute of Physics and Engineering in Medicine (IPEM) has a panel for the accreditation of MSc courses in Biomedical Engineering and Chartered Engineering status can also be sought through IPEM.\n\nThe Fundamentals of Engineering exam – the first (and more general) of two licensure examinations for most U.S. jurisdictions—does now cover biology (although technically not BME). For the second exam, called the Principles and Practices, Part 2, or the Professional Engineering exam, candidates may select a particular engineering discipline's content to be tested on; there is currently not an option for BME with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). However, the Biomedical Engineering Society (BMES) is, as of 2009, exploring the possibility of seeking to implement a BME-specific version of this exam to facilitate biomedical engineers pursuing licensure.\n\nBeyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. One such example is the Certified Clinical Engineer (CCE) certification for Clinical engineers.\nCareer prospects\n\nIn 2012 there were about 19,400 biomedical engineers employed in the US, and the field was predicted to grow by 27% (Much faster than average) from 2012-2022.[23] Biomedical engineering has the highest percentage of women engineers compared to other common engineering professions.\nFounding figures\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2016) (Learn how and when to remove this template message)\n\n    Forrest Bird – aviator and pioneer in the invention of mechanical ventilators\n    Kenneth R. Diller – Chaired and Endowed Professor in Engineering, University of Texas at Austin. Founded the BME department at UT Austin. Pioneer in bioheat transfer, mass transfer, and biotransport\n    Y. C. Fung – professor emeritus at the University of California, San Diego, considered by many to be the founder of modern biomechanics[24]\n    Leslie Geddes (deceased) – professor emeritus at Purdue University, electrical engineer, inventor, and educator of over 2000 biomedical engineers, received a National Medal of Technology in 2006 from President George Bush[25] for his more than 50 years of contributions that have spawned innovations ranging from burn treatments to miniature defibrillators, ligament repair to tiny blood pressure monitors for premature infants, as well as a new method for performing cardiopulmonary resuscitation (CPR).\n    Willem Johan Kolff (deceased) – pioneer of hemodialysis as well as in the field of artificial organs\n    Robert Langer – Institute Professor at MIT, runs the largest BME laboratory in the world, pioneer in drug delivery and tissue engineering[26]\n    Herbert Lissner (deceased) – Professor of Engineering Mechanics at Wayne State University. Initiated studies on blunt head trauma and injury thresholds beginning in 1939 in collaboration with E.S. Gurdjian, a neurosurgeon at Wayne State's School of Medicine. Individual for whom the American Society of Mechanical Engineers' top award in Biomedical Engineering, the Herbert R. Lissner Medal, is named.\n    John James Rickard Macleod (deceased) – one of the co-discoverers of insulin at Case Western Reserve University.\n    Alfred E. Mann – Physicist, entrepreneur and philanthropist. A pioneer in the field of Biomedical Engineering.[27]\n    Nicholas A. Peppas – Chaired Professor in Engineering, University of Texas at Austin, pioneer in drug delivery, biomaterials, hydrogels and nanobiotechnology.\n    Robert Plonsey – professor emeritus at Duke University, pioneer of electrophysiology[28]\n    Robert M. Nerem – professor emeritus at Georgia Institute of Technology. Pioneer in regenerative tissue, biomechanics, and author of over 300 published works. His works have been cited more than 20,000 times cumulatively.\n    Otto Schmitt (deceased) – biophysicist with significant contributions to BME, working with biomimetics\n    Ascher Shapiro (deceased) – Institute Professor at MIT, contributed to the development of the BME field, medical devices (e.g. intra-aortic balloons)\n    Frederick Thurstone (deceased) – professor emeritus at Duke University, pioneer of diagnostic ultrasound[29]\n    John G. Webster – professor emeritus at the University of Wisconsin–Madison, a pioneer in the field of instrumentation amplifiers for the recording of electrophysiological signals\n    U. A. Whitaker (deceased) – provider of the Whitaker Foundation, which supported research and education in BME by providing over $700 million to various universities, helping to create 30 BME programs and helping finance the construction of 13 buildings[30]\n\nSee also\n\n    Biomechanics\n    Biomedicine\n    Cardiophysics\n    Medical physics\n    Physiome", "skillName": "BiomedicalEngineering."}
{"id": 19, "category": "Biomedical", "skillText": "What Biomedical Engineers Do\nAbout this section\nBiomedical engineers\nBiomedical engineers install, maintain, or provide technical support for biomedical equipment.\n\nBiomedical engineers combine engineering principles with medical and biological sciences to design and create equipment, devices, computer systems, and software used in healthcare.\nDuties\n\nBiomedical engineers typically do the following:\n\n    Design equipment and devices, such as artificial internal organs, replacements for body parts, and machines for diagnosing medical problems\n    Install, adjust, maintain, repair, or provide technical support for biomedical equipment\n    Evaluate the safety, efficiency, and effectiveness of biomedical equipment\n    Train clinicians and other personnel on the proper use of equipment\n    Work with life scientists, chemists, and medical scientists to research the engineering aspects of the biological systems of humans and animals\n    Prepare procedures, write technical reports, publish research papers, and make recommendations based on their research findings\n    Present research findings to scientists, nonscientist executives, clinicians, hospital management, engineers, other colleagues, and the public\n\nBiomedical engineers design instruments, devices, and software used in healthcare; bring together knowledge from many technical sources to develop new procedures; or conduct research needed to solve clinical problems.\n\nThey often serve a coordinating function, using their background in both engineering and medicine. For example, they may create products for which an indepth understanding of living systems and technology is essential. They frequently work in research and development or in quality assurance.\n\nBiomedical engineers design electrical circuits, software to run medical equipment, or computer simulations to test new drug therapies. In addition, they design and build artificial body parts, such as hip and knee joints. In some cases, they develop the materials needed to make the replacement body parts. They also design rehabilitative exercise equipment.\n\nThe work of these engineers spans many professional fields. For example, although their expertise is based in engineering and biology, they often design computer software to run complicated instruments, such as three-dimensional x-ray machines. Alternatively, many of these engineers use their knowledge of chemistry and biology to develop new drug therapies. Others draw heavily on mathematics and statistics to build models to understand the signals transmitted by the brain or heart.\n\nThe following are examples of specialty areas within the field of biomedical engineering:\n\nBioinstrumentation uses electronics, computer science, and measurement principles to develop devices used in the diagnosis and treatment of disease.\n\nBiomaterials is the study of naturally occurring or laboratory-designed materials that are used in medical devices or as implantation materials.\n\nBiomechanics involves the study of mechanics, such as thermodynamics, to solve biological or medical problems.\n\nClinical engineering applies medical technology to optimize healthcare delivery.\n\nRehabilitation engineering is the study of engineering and computer science to develop devices that assist individuals with physical and cognitive impairments.\n\nSystems physiology uses engineering tools to understand how systems within living organisms, from bacteria to humans, function and respond to changes in their environment.\n\nSome people with training in biomedical engineering become professors. For more information, see the profile on postsecondary teachers.\n<- Summary\nWork Environment ->\nWork Environment\nAbout this section\nBiomedical engineers\nBiomedical engineers work in laboratory and clinical settings.\n\nBiomedical engineers held about 22,100 jobs in 2014. The industries that employed the most biomedical engineers were as follows:\nMedical equipment and supplies manufacturing \t23%\nResearch and development in the physical, engineering, and life sciences \t16\nPharmaceutical and medicine manufacturing \t12\nNavigational, measuring, electromedical, and control instruments manufacturing \t8\nHospitals; state, local, and private \t8\n\nBiomedical engineers work in a variety of settings. Some work in hospitals, where therapy occurs, and others work in laboratories, doing research. Still others work in manufacturing settings, where they design biomedical engineering products. Yet other biomedical engineers work in commercial offices, where they make or support business decisions.\n\nBiomedical engineers work in teams with scientists, healthcare workers, or other engineers. Where and how they work depends on the project. For example, a biomedical engineer who has developed a new device designed to help a person with a disability to walk again might have to spend hours in a hospital to determine whether the device works as planned. If the engineer finds a way to improve the device, he or she might have to return to the manufacturer to help alter the manufacturing process in order to improve the design.", "skillName": "_BiomedicalEngineers."}
{"id": 20, "category": "Math_Stats_tools", "skillText": "Statistics is the study of the collection, analysis, interpretation, presentation, and organization of data.[1] In applying statistics to, e.g., a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1]\n\nSome popular definitions are:\n\n    Merriam-Webster dictionary defines statistics as \"classified facts representing the conditions of a people in a state – especially the facts that can be stated in numbers or any other tabular or classified arrangement[2]\".\n    Statistician Sir Arthur Lyon Bowley defines statistics as \"Numerical statements of facts in any department of inquiry placed in relation to each other[3]\".\n\nWhen census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\n\nTwo main statistical methodologies are used in data analysis: descriptive statistics, which summarizes data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draws conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[4] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\n\nA standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and a synthetic data drawn from idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\").[5] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]\n\nMeasurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\nStatistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.\nContents\n\n    1 Scope\n        1.1 Mathematical statistics\n    2 Overview\n    3 Data collection\n        3.1 Sampling\n        3.2 Experimental and observational studies\n    4 Types of data\n    5 Terminology and theory of inferential statistics\n        5.1 Statistics, estimators and pivotal quantities\n        5.2 Null hypothesis and alternative hypothesis\n        5.3 Error\n        5.4 Interval estimation\n        5.5 Significance\n        5.6 Examples\n    6 Misuse\n        6.1 Misinterpretation: correlation\n    7 History of statistical science\n    8 Applications\n        8.1 Applied statistics, theoretical statistics and mathematical statistics\n        8.2 Machine learning and data mining\n        8.3 Statistics in society\n        8.4 Statistical computing\n        8.5 Statistics applied to mathematics or the arts\n    9 Specialized disciplines\n    10 See also\n    11 References\n    12 Further reading\n    13 External links\n\nScope\n\nStatistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data,[6] or as a branch of mathematics.[7] Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.[8][9]\nMathematical statistics\nMain article: Mathematical statistics\n\nMathematical statistics is the application of mathematics to statistics, which was originally conceived as the science of the state — the collection and analysis of facts about a country: its economy, land, military, population, and so forth. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[10][11]\nOverview\n\nIn applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as \"all persons living in a country\" or \"every atom composing a crystal\".\n\nIdeally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).\n\nWhen a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.\nData collection\nSampling\n\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting the use of data through statistical models. To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\n\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.\nExperimental and observational studies\n\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data – like natural experiments and observational studies[12] – for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\nExperiments\n\nThe basic steps of a statistical experiment are:\n\n    Planning the research, including finding the number of replicates of the study, using the following information: preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.\n    Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.\n    Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol.\n    Further examining the data set in secondary analyses, to suggest new hypotheses for future study.\n    Documenting and presenting the results of the study.\n\nExperiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[13]\nObservational study\n\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a case-control study, and then look for the number of cases of lung cancer in each group.\nTypes of data\nMain articles: Statistical data type and Levels of measurement\n\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\n\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\n\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977)[14] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[15] described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998),[16] van den Berg (1991).[17]\n\nThe issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer\" (Hand, 2004, p. 82).[18]\nTerminology and theory of inferential statistics\nStatistics, estimators and pivotal quantities\n\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[19] The population being examined is described by a probability distribution that may have unknown parameters.\n\nA statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.\n\nConsider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\n\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\n\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\n\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\n\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\nNull hypothesis and alternative hypothesis\n\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[20][21]\n\nThe best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\n\nWhat statisticians call an alternative hypothesis is simply an hypothesis that contradicts the null hypothesis.\nError\n\nWorking from a null hypothesis, two basic forms of error are recognized:\n\n    Type I errors where the null hypothesis is falsely rejected giving a \"false positive\".\n    Type II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\".\n\nStandard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\n\nA statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\n\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\nA least squares fit: in red the points to be fitted, in blue the fitted line.\n\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\n\nMeasurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.[22]\nInterval estimation\nMain article: Interval estimation\nConfidence intervals: the red line is true value for the mean in this example, the blue lines are random confidence intervals for 100 realizations.\n\nMost studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\n\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\nSignificance\nMain article: Statistical significance\n\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\nIn this graph the black line is probability distribution for the test statistic, the critical region is the set of values to the right of the observed data point (observed value of the test statistic) and the p-value is represented by the green area.\n\nThe standard approach[19] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\n\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\n\nWhile in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.\n\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\n\n    A difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.\n    Fallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.[23]\n    Rejecting the null hypothesis does not automatically prove the alternative hypothesis.\n    As everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.\n\nExamples\n\nSome well-known statistical tests and procedures are:\n\n    Analysis of variance (ANOVA)\n    Chi-squared test\n    Correlation\n    Factor analysis\n    Mann–Whitney U\n    Mean square weighted deviation (MSWD)\n    Pearson product-moment correlation coefficient\n    Regression analysis\n    Spearman's rank correlation coefficient\n    Student's t-test\n    Time series analysis\n    Conjoint Analysis\n\nMisuse\nMain article: Misuse of statistics\n\nMisuse of statistics can produce subtle, but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\n\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\n\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[24] A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics[24] outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[25]\n\nWays to avoid misuse of statistics include using proper diagrams and avoiding bias.[26] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[27] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[26] Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[27] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[28] According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"[29]\n\nTo assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[30]\n\n    Who says so? (Does he/she have an axe to grind?)\n    How does he/she know? (Does he/she have the resources to know the facts?)\n    What’s missing? (Does he/she give us a complete picture?)\n    Did someone change the subject? (Does he/she offer us the right answer to the wrong problem?)\n    Does it make sense? (Is his/her conclusion logical and consistent with what we already know?)\n\nThe confounding variable problem: X and Y may be correlated, not because there is causal relationship between them, but because both depend on a third variable Z. Z is called a confounding factor.\nMisinterpretation: correlation\n\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)\nHistory of statistical science\nGerolamo Cardano, the earliest pioneer on the mathematics of probability.\nMain articles: History of statistics and Founders of statistics\n\nStatistical methods date back at least to the 5th century BC.\n\nSome scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[31] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n\nIts mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel.[32] The method of least squares was first described by Adrien-Marie Legendre in 1805.\nKarl Pearson, a founder of mathematical statistics.\n\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages.[33] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics – height, weight, eyelash length among others.[34] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[35] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[36] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[37]\n\nRonald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".[38][39]\n\nThe second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance, which was the first to use the statistical term, variance, his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[40][41][42][43] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[44] In his 1930 book The Genetical Theory of Natural Selection he applied statistics to various biological concepts such as Fisher's principle[45]). Nevertheless, A. W. F. Edwards has remarked that it is \"probably the most celebrated argument in evolutionary biology\".[45] (about the sex ratio), the Fisherian runaway,[46][47][48][49][50][51] a concept in sexual selection about a positive feedback runaway affect found in evolution.\n\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[52]\n\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.[53]\nApplications\nApplied statistics, theoretical statistics and mathematical statistics\n\n\"Applied statistics\" comprises descriptive statistics and the application of inferential statistics.[54][55] Theoretical statistics concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\nMachine learning and data mining\n\nThere are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis.\nStatistics in society\n\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.\nStatistical computing\ngretl, an example of an open source statistical package\nMain article: Computational statistics\n\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\n\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available.\nStatistics applied to mathematics or the arts\n\nTraditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.\n\n    In number theory, scatter plots of data generated by a distribution function may be transformed with familiar tools used in statistics to reveal underlying patterns, which may then lead to hypotheses.\n    Methods of statistics including predictive methods in forecasting are combined with chaos theory and fractal geometry to create video works that are considered to have great beauty.[citation needed]\n    The process art of Jackson Pollock relied on artistic experiments whereby underlying distributions in nature were artistically revealed.[citation needed] With the advent of computers, statistical methods were applied to formalize such distribution-driven natural processes to make and analyze moving video art.[citation needed]\n    Methods of statistics may be used predicatively in performance art, as in a card trick based on a Markov process that only works some of the time, the occasion of which can be predicted using statistical methodology.\n    Statistics can be used to predicatively create art, as in the statistical or stochastic music invented by Iannis Xenakis, where the music is performance-specific. Though this type of artistry does not always come out as expected, it does behave in ways that are predictable and tunable using statistics.\n\nSpecialized disciplines\nMain article: List of fields of application of statistics\n\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n\n    Actuarial science (assesses risk in the insurance and finance industries)\n    Applied information economics\n    Astrostatistics (statistical evaluation of astronomical data)\n    Biostatistics\n    Business statistics\n    Chemometrics (for analysis of data from chemistry)\n    Data mining (applying statistics and pattern recognition to discover knowledge from data)\n    Data science\n    Demography\n    Econometrics (statistical analysis of economic data)\n    Energy statistics\n    Engineering statistics\n    Epidemiology (statistical analysis of disease)\n    Geography and Geographic Information Systems, specifically in Spatial analysis\n    Image processing\n    Medical Statistics\n    Psychological statistics\n    Reliability engineering\n    Social statistics\n    Statistical Mechanics\n\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\n\n    Bootstrap / Jackknife resampling\n    Multivariate statistics\n    Statistical classification\n    Structured data analysis (statistics)\n    Structural equation modelling\n    Survey methodology\n    Survival analysis\n    Statistics in various sports, particularly baseball - known as Sabermetrics - and cricket\n\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.\nSee also\nLibrary resources about\nStatistics\n\n    Resources in your library \n\nMain article: Outline of statistics\n\n    Abundance estimation\n    Data science\n    Glossary of probability and statistics\n    List of academic statistical associations\n    List of important publications in statistics\n    List of national and international statistical services\n    List of statistical packages (software)\n    List of statistics articles\n    List of university statistical consulting centers\n    Notation in probability and statistics\n\nFoundations and major areas of statistics\n\n    Foundations of statistics\n    List of statisticians\n    Official statistics\n    Multivariate analysis of variance", "skillName": "Statistics."}
{"id": 21, "category": "Math_Stats_tools", "skillText": "This article itemizes the various lists of mathematics topics. Some of these lists link to hundreds of articles; some link only to a few. The template to the right includes links to alphabetical lists of all mathematical articles. This article brings together the same content organized in a manner better suited for browsing.\n\nThe purpose of this list is not similar to that of the Mathematics Subject Classification formulated by the American Mathematical Society. Many mathematics journals ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The subject codes so listed are used by the two major reviewing databases, Mathematical Reviews and Zentralblatt MATH. This list has some items that would not fit in such a classification, such as list of exponential topics and list of factorial and binomial topics, which may surprise the reader with the diversity of their coverage.\n\nContents\n\n    1 Areas of basic mathematics\n    2 Areas of advanced mathematics\n        2.1 Pure mathematics\n            2.1.1 Algebra\n            2.1.2 Calculus and analysis\n            2.1.3 Geometry and topology\n            2.1.4 Combinatorics\n            2.1.5 Logic\n            2.1.6 Number theory\n        2.2 Applied mathematics\n            2.2.1 Dynamical systems and differential equations\n            2.2.2 Mathematical physics\n            2.2.3 Computation\n            2.2.4 Information theory and signal processing\n            2.2.5 Probability and statistics\n            2.2.6 Game theory\n            2.2.7 Operations research\n    3 Methodology\n    4 Mathematical statements\n    5 General concepts\n    6 Mathematical objects\n    7 Equations named after people\n    8 About mathematics\n        8.1 Mathematicians\n        8.2 Work of particular mathematicians\n    9 Reference tables\n        9.1 Integrals\n    10 Journals\n    11 Meta-lists\n    12 Others\n    13 Notes\n    14 External links and references\n\nAreas of basic mathematics\n\nThese lists include topics typically taught in secondary education or in the first year of university.\n\n    Outline of discrete mathematics\n    List of calculus topics\n    List of geometry topics\n        Outline of geometry\n        List of trigonometry topics\n            Outline of trigonometry\n            List of trigonometric identities\n    List of topics in logic\n\nAreas of advanced mathematics\n\nSee also Areas of mathematics.\n\nAs a rough guide this list is divided into pure and applied sections although in reality these branches are overlapping and intertwined.\nPure mathematics\nAlgebra\n\nAlgebra includes the study of algebraic structures, which are sets and operations defined on these sets satisfying certain axioms. The field of algebra is further divided according to which structure is studied; for instance, group theory concerns an algebraic structure called group.\n\n    Outline of algebra\n    List of abstract algebra topics\n    List of algebraic structures\n    List of Boolean algebra topics\n    List of category theory topics\n    List of commutative algebra topics\n    List of homological algebra topics\n    List of group theory topics\n    List of representation theory topics\n    List of linear algebra topics\n    List of reciprocity laws\n    Glossary of field theory\n    Glossary of group theory\n    Glossary of linear algebra\n    Glossary of ring theory\n    List of cohomology theories\n\nCalculus and analysis\nFourier series approximation of square wave in five steps.\n\nCalculus studies the computation of limits, derivatives, and integrals of functions of real numbers, and in particular studies instantaneous rates of change. Analysis evolved from calculus.\n\n    List of complex analysis topics\n    List of functional analysis topics\n        List of vector spaces in mathematics\n    List of integration and measure theory topics\n    List of harmonic analysis topics\n        List of Fourier analysis topics\n    List of multivariable calculus topics\n    List of q-analogs\n    List of real analysis topics\n    List of variational topics\n    Glossary of tensor theory\n    List of mathematical series\n    See also Dynamical systems and differential equations section below.\n\nGeometry and topology\nFord circles—A circle rests upon each fraction in lowest terms. Each touches its neighbors without crossing.\n\nGeometry is initially the study of spatial figures like circles and cubes, though it has been generalized considerably. Topology developed from geometry; it looks at those properties that do not change even when the figures are deformed by stretching and bending, like dimension.\n\n    List of geometry topics\n    List of geometric shapes\n    List of curves topics\n    List of triangle topics\n    List of circle topics\n        List of topics related to pi\n    List of general topology topics\n    List of differential geometry topics\n    List of algebraic geometry topics\n        List of algebraic surfaces\n    List of algebraic topology topics\n        List of cohomology theories\n    List of geometric topology topics\n    List of knot theory topics\n    List of Lie group topics\n    Glossary of differential geometry and topology\n    Glossary of general topology\n    List of mathematical properties of points\n    Glossary of Riemannian and metric geometry\n    Glossary of scheme theory\n\nCombinatorics\n\nCombinatorics concerns the study of discrete (and usually finite) objects. Aspects include \"counting\" the objects satisfying certain criteria (enumerative combinatorics), deciding when the criteria can be met, and constructing and analyzing objects meeting the criteria (as in combinatorial designs and matroid theory), finding \"largest\", \"smallest\", or \"optimal\" objects (extremal combinatorics and combinatorial optimization), and finding algebraic structures these objects may have (algebraic combinatorics).\n\n    Outline of combinatorics\n    List of graph theory topics\n    Glossary of graph theory\n\nLogic\nVenn diagrams are illustrations of set theoretical, mathematical or logical relationships.\n\nLogic is the foundation which underlies mathematical logic and the rest of mathematics. It tries to formalize valid reasoning. In particular, it attempts to define what constitutes a proof.\n\n    List of Boolean algebra topics\n    List of first-order theories\n    List of large cardinal properties\n    List of mathematical logic topics\n    Glossary of order theory\n    List of set theory topics\n\nNumber theory\n\nNumber theory studies the natural, or whole, numbers. One of the central concepts in number theory is that of the prime number, and there are many questions about primes that appear simple but whose resolution continues to elude mathematicians.\n\n    List of algebraic number theory topics\n    List of number theory topics\n    List of recreational number theory topics\n    Glossary of arithmetic and Diophantine geometry\n    List of prime numbers—not just a table, but a list of various kinds of prime numbers (each with an accompanying table)\n    List of zeta functions\n\nApplied mathematics\nDynamical systems and differential equations\nPhase portrait of a continuous-time dynamical system, the Van der Pol oscillator.\n\nA differential equation is an equation involving an unknown function and its derivatives.\n\nIn a dynamical system, a fixed rule describes the time dependence of a point in a geometrical space. The mathematical models used to describe the swinging of a clock pendulum, the flow of water in a pipe, or the number of fish each spring in a lake are examples of dynamical systems.\n\n    List of dynamical systems and differential equations topics\n    List of partial differential equation topics\n    List of nonlinear partial differential equations\n\nMathematical physics\n\nMathematical physics is concerned with \"the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories\".1\n\n    List of mathematical topics in classical mechanics\n    List of mathematical topics in quantum theory\n    List of mathematical topics in relativity\n    List of string theory topics\n    Index of wave articles\n\nComputation\nRay tracing is a process based on computational mathematics.\n\nThe fields of mathematics and computing intersect both in computer science, the study of algorithms and data structures, and in scientific computing, the study of algorithmic methods for solving problems in mathematics, science and engineering.\n\n    List of algorithm general topics\n    List of computability and complexity topics\n    Lists for computational topics in geometry and graphics\n        List of combinatorial computational geometry topics\n        List of computer graphics and descriptive geometry topics\n        List of numerical computational geometry topics\n    List of computer vision topics\n    List of formal language and literal string topics\n    List of numerical analysis topics\n    List of terms relating to algorithms and data structures\n\nInformation theory and signal processing\n\nInformation theory is a branch of applied mathematics and electrical engineering involving the quantification of information. Historically, information theory was developed to find fundamental limits on compressing and reliably communicating data.\n\nSignal processing is the analysis, interpretation, and manipulation of signals. Signals of interest include sound, images, biological signals such as ECG, radar signals, and many others. Processing of such signals includes filtering, storage and reconstruction, separation of information from noise, compression, and feature extraction.\n\n    List of information theory topics\n    List of algebraic coding theory topics\n    List of cryptography topics\n\nProbability and statistics\nThe \"bell curve\"—the probability density function of the normal distribution.\nMain article: Lists of statistics topics\n\nProbability theory is the formalization and study of the mathematics of uncertain events or knowledge. The related field of mathematical statistics develops statistical theory with mathematics. Statistics, the science concerned with collecting and analyzing data, is an autonomous discipline (and not a subdiscipline of applied mathematics).\n\n    List of probability topics\n    List of stochastic processes topics\n    List of probability distributions\n    Catalog of articles in probability theory\n    List of statistics topics\n    Outline of regression analysis\n\nGame theory\n\nGame theory is a branch of mathematics that uses models to study interactions with formalized incentive structures (\"games\"). It has applications in a variety of fields, including economics, evolutionary biology, political science, social psychology and military strategy.\n\n    Glossary of game theory\n    List of games in game theory\n\nOperations research\n\nOperations research is the study and use of mathematical models, statistics and algorithms to aid in decision-making, typically with the goal of improving or optimizing performance of real-world systems.\n\n    List of network theory topics\n    List of knapsack problems\n\nMethodology\n\n    List of graphical methods\n    List of mathematics-based methods\n    List of rules of inference\n\nMathematical statements\n\nA mathematical statement amounts to a proposition or assertion of some mathematical fact, formula, or construction. Such statements include axioms and the theorems that may be proved from them, conjectures that may be unproven or even unprovable, and also algorithms for computing the answers to questions that can be expressed mathematically.\n\n    List of algorithms\n    List of axioms\n    List of conjectures\n        Erdős conjecture — a list of conjectures by Paul Erdős\n    Combinatorial principles\n    List of equations\n    List of formulae involving pi\n    List of mathematical identities\n    List of inequalities\n    List of lemmas\n    List of mathematical proofs\n    List of NP-complete problems\n    List of statements undecidable in ZFC\n    List of undecidable problems\n    List of theorems\n        List of fundamental theorems\n\nGeneral concepts\n\n    List of convexity topics\n    List of dualities\n    List of exceptional set concepts\n    List of exponential topics\n    List of factorial and binomial topics\n    List of fractal topics\n    List of logarithm topics\n    List of numeral system topics\n    List of order topics\n    List of partition topics\n    List of polynomial topics\n    List of properties of sets of reals\n    List of transforms\n    List of permutation topics\n\nMathematical objects\n\nAmong mathematical objects are numbers, functions, sets, a great variety of things called \"spaces\" of one kind or another, algebraic structures such as rings, groups, or fields, and many other things.\n\n    List of mathematical examples\n    List of curves\n    List of complex reflection groups\n    List of complexity classes\n    List of examples in general topology\n    List of finite simple groups\n    List of Fourier-related transforms\n    List of mathematical functions\n    List of mathematical knots and links\n    List of manifolds\n    List of mathematical shapes\n    List of matrices\n    List of numbers\n    List of polygons, polyhedra and polytopes\n    List of regular polytopes\n    List of simple Lie groups\n    List of small groups\n    List of special functions and eponyms\n    List of algebraic surfaces\n    List of surfaces\n    Table of Lie groups\n\nEquations named after people\n\n    Scientific equations named after people\n\nAbout mathematics\n\n    List of mathematical societies\n    List of letters used in mathematics and science\n    List of mathematics competitions\n    List of mathematics history topics\n    List of publications in mathematics\n\nMathematicians\nMain article: List of mathematicians\n\nMathematicians study and research in all the different areas of mathematics. The publication of new discoveries in mathematics continues at an immense rate in hundreds of scientific journals, many of them devoted to mathematics and many devoted to subjects to which mathematics is applied (such as theoretical computer science and theoretical physics).\n\n    List of geometers\n    List of logicians\n    List of game theorists\n    List of mathematicians\n    List of mathematical probabilists\n    List of statisticians\n\nWork of particular mathematicians\nSee also: Category:Lists of things named after mathematicians\n\n    List of things named after Niels Henrik Abel\n    List of things named after Archimedes\n    List of things named after Emil Artin\n    List of things named after Thomas Bayes\n    List of things named after Élie Cartan\n    List of things named after Augustin-Louis Cauchy\n    List of things named after Arthur Cayley\n    List of things named after Richard Dedekind\n    List of things named after Pierre Deligne\n    List of things named after Peter Gustav Lejeune Dirichlet\n    List of things named after Albert Einstein\n    List of things named after Euclid\n    List of things named after Leonhard Euler\n    List of things named after Paul Erdős\n    List of things named after Fibonacci\n    List of things named after Ferdinand Georg Frobenius\n    List of things named after Carl Friedrich Gauss\n    List of things named after Évariste Galois\n    List of things named after Jacques Hadamard\n    List of things named after Erich Hecke\n    List of things named after Charles Hermite\n    List of things named after W. V. D. Hodge\n    List of things named after Carl Gustav Jacob Jacobi\n    List of things named after Felix Klein\n    List of things named after Joseph-Louis Lagrange\n    List of things named after Pierre-Simon Laplace\n    List of things named after Adrien-Marie Legendre\n    List of things named after Gottfried Leibniz\n    List of things named after John Milnor\n    List of things named after Henri Poincaré\n    List of things named after Siméon Denis Poisson\n    List of things named after Pythagoras\n    List of things named after Srinivasa Ramanujan\n    List of things named after Bernhard Riemann\n    List of things named after Jean-Pierre Serre\n    List of things named after James Joseph Sylvester\n    List of things named after Alfred Tarski\n    List of things named after Karl Weierstrass\n    List of things named after André Weil\n    List of things named after Hermann Weyl\n    List of things named after Ernst Witt\n    List of things named after John von Neumann\n\nReference tables\n\n    List of mathematical reference tables\n    List of moments of inertia\n    Table of derivatives\n\nIntegrals\n\nIn calculus, the integral of a function is a generalization of area, mass, volume, sum, and total. The following pages list the integrals of many different functions.\n\n    Lists of integrals\n    List of integrals of exponential functions\n    List of integrals of hyperbolic functions\n    List of integrals of inverse hyperbolic functions\n    List of integrals of inverse trigonometric functions\n    List of integrals of irrational functions\n    List of integrals of logarithmic functions\n    List of integrals of rational functions\n    List of integrals of trigonometric functions\n\nJournals\n\n    List of mathematics journals\n    List of mathematics education journals\n    Category:History of science journals\n    Category:Philosophy of science literature\n\nMeta-lists\n\n    List of important publications in mathematics\n    List of important publications in statistics\n    List of mathematical theories\n    List of mathematics categories\n    Table of mathematical symbols\n    Table of logic symbols\n\nOthers\n\n    Lists of unsolved problems in mathematics", "skillName": "Lists_of_mathematics_topics."}
{"id": 22, "category": "Math_Stats_tools", "skillText": "Statistical software are specialized computer programs for analysis in statistics and econometrics.\n\nContents\n\n    1 Open-source\n    2 Public domain\n    3 Freeware\n    4 Proprietary\n        4.1 Add-ons\n    5 See also\n    6 References\n    7 External links\n\nOpen-source\ngretl is an example of an open-source statistical package\n\n    ADaMSoft – a generalized statistical software with data mining algorithms and methods for data management\n    ADMB – a software suite for non-linear statistical modeling based on C++ which uses automatic differentiation\n    Bayesian Filtering Library\n    Chronux – for neurobiological time series data\n    CBEcon – web-based econometrics and statistical software\n    DataMelt (DMelt) – Java-based statistical analysis framework for scientists and engineers. It includes an IDE\n    DAP – free replacement for SAS\n    DynStats – a on-line statistical laboratory\n    Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI) a software framework for developing data mining algorithms in Java\n    Fityk – nonlinear regression software (GUI and command line)\n    GNU Octave – programming language very similar to MATLAB with statistical features\n    gretl – gnu regression, econometrics and time-series library\n    intrinsic Noise Analyzer (iNA) – For analyzing intrinsic fluctuations in biochemical systems\n    JASP – A free software alternative to IBM SPSS Statistics with additional option for Bayesian methods\n    Just another Gibbs sampler (JAGS) – a program for analyzing Bayesian hierarchical models using Markov chain Monte Carlo developed by Martyn Plummer. It is similar to WinBUGS\n    JMulTi\n    LDT - Automatic Time Series Analysis with Stationary VAR Models\n    LIBSVM – C++ support vector machine libraries\n    MLPACK (C++ library) – open-source library for machine learning, exploits C++ language features to provide maximum performance and flexibility while providing a simple and consistent application programming interface (API)\n    Mondrian – data analysis tool using interactive statistical graphics with a link to R\n    Neurophysiological Biomarker Toolbox - Matlab toolbox for data-mining of neurophysiological biomarkers\n    OpenBUGS\n    OpenEpi – A web-based, open-source, operating-independent series of programs for use in epidemiology and statistics based on JavaScript and HTML\n    OpenNN – A software library written in the programming language C++ which implements neural networks, a main area of deep learning research\n    OpenMx – A package for structural equation modeling running in R (programming language)\n    Orange, a data mining, machine learning, and bioinformatics software\n    Pandas – High-performance computing (HPC) data structures and data analysis tools for Python in Python and Cython (statsmodels, scikit-learn)\n    Perl Data Language – Scientific computing with Perl\n    Ploticus – software for generating a variety of graphs from raw data\n    PSPP – A free software alternative to IBM SPSS Statistics\n    R – free implementation of the S (programming language)\n        Programming with Big Data in R (pbdR) – a series of R packages enhanced by SPMD parallelism for big data analysis\n        R Commander – GUI interface for R\n        Rattle GUI – GUI interface for R\n        Revolution Analytics – production-grade software for the enterprise big data analytics\n        RStudio – GUI interface and development environment for R\n    ROOT – an open-source C++ system for data storage, processing and analysis, developed by CERN and used to find the Higgs boson\n    Salstat - menu-driven statistics software\n    Scilab – uses GPL-compatible CeCILL license\n    SciPy – Python library for scientific computing that contains the stats sub-package which is partly based on the venerable |STAT (a.k.a. PipeStat, formerly UNIX|STAT) software\n        scikit-learn - extends SciPy with a host of machine learning models (classification, clustering, regression, etc.)\n        statsmodels - extends SciPy with statistical models and tests (regression, plotting, example datasets, generalized linear model (GLM), time series analysis, autoregressive–moving-average model (ARMA), vector autoregression (VAR), non-parametric statistics, ANOVA, empirical likelihood)\n    Shogun (toolbox) – open-source, large-scale machine learning toolbox that provides several SVM (Support Vector Machine) implementations (like libSVM, SVMlight) under a common framework and interfaces to Octave, MATLAB, Python, R\n    Simfit – simulation, curve fitting, statistics, and plotting\n    SOCR\n    SOFA Statistics – desktop GUI program focused on ease of use, learn as you go, and beautiful output\n    Stan (software) – open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. It’s somewhat like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors\n    Statistical Lab – R-based and focusing on educational purposes\n    Torch (machine learning) – a deep learning software library written in Lua (programming language)\n    Weka (machine learning) – a suite of machine learning software written at the University of Waikato\n\nPublic domain\n\n    CSPro\n    Epi Info\n    X-12-ARIMA\n\nFreeware\n\n    BV4.1\n    GeoDA\n    MaxStat Lite – general statistical software\n    MINUIT\n    WinBUGS – Bayesian analysis using Markov chain Monte Carlo methods\n    Winpepi – package of statistical programs for epidemiologists\n\nProprietary\n\n    Analytica - visual analytics and statistics package\n    Angoss - products KnowledgeSEEKER and KnowledgeSTUDIO incorporate several data mining algorithms\n    ASReml – for restricted maximum likelihood analyses\n    BMDP – general statistics package\n    Data Applied – for building statistical models\n    DB Lytix - 800+ in-database models\n    EViews – for econometric analysis\n    FAME (database) – a system for managing time-series databases\n    GAUSS – programming language for statistics\n    Genedata – software solution for integration and interpretation of experimental data in the life science R&D\n    GenStat – general statistics package\n    GLIM – early package for fitting generalized linear models\n    GraphPad InStat – very simple with lots of guidance and explanations\n    GraphPad Prism – biostatistics and nonlinear regression with clear explanations\n    IMSL Numerical Libraries – software library with statistical algorithms\n    JMP – visual analysis and statistics package\n    LIMDEP – comprehensive statistics and econometrics package\n    LISREL – statistics package used in structural equation modeling\n    Maple – programming language with statistical features\n    Mathematica – a software package with statistical particularlyŋ features\n    MATLAB – programming language with statistical features\n    MaxStat Pro – general statistical software\n    MedCalc – for biomedical sciences\n    Microfit – econometrics package, time series\n    Minitab – general statistics package\n    MLwiN – multilevel models (free to UK academics)\n    NAG Numerical Library – comprehensive math and statistics library\n    Neural Designer – commercial deep learning package\n    NCSS – general statistics package\n    NLOGIT – comprehensive statistics and econometrics package\n    NMath Stats – statistical package for .NET Framework\n    O-Matrix – programming language\n    OriginPro – statistics and graphing, programming access to NAG library\n    PASS Sample Size Software (PASS) – power and sample size software from NCSS\n    Plotly – plotting library and styling interface for analyzing data and creating browser-based graphs. Available for R, Python, MATLAB, Julia, and Perl\n    Primer-E Primer – environmental and ecological specific\n    PV-WAVE – programming language comprehensive data analysis and visualization with IMSL statistical package\n    Qlucore Omics Explorer - interactive and visual data analysis software\n    Quantum Programming Language – part of the SPSS MR product line, mostly for data validation and tabulation in Marketing and Opinion Research\n    RapidMiner – machine learning toolbox\n    Regression Analysis of Time Series (RATS) – comprehensive econometric analysis package\n    SAS (software) – comprehensive statistical package\n    SHAZAM (Econometrics and Statistics Software) – comprehensive econometrics and statistics package\n    Simul - econometric tool for multidimensional (multi-sectoral, multi-regional) modeling\n    SigmaStat – package for group analysis\n    SmartPLS - statistics package used in partial least squares path modeling (PLS) and PLS-based structural equation modeling\n    SOCR – online tools for teaching statistics and probability theory\n    Speakeasy (computational environment) – numerical computational environment and programming language with many statistical and econometric analysis features\n    SPSS Modeler – comprehensive data mining and text analytics workbench\n    SPSS Statistics – comprehensive statistics package that stands for \"Statistical Package for the Social Sciences\"\n    Stata – comprehensive statistics package\n    Statgraphics – general statistics package to include cloud computing and Six Sigma for use in business development, process improvement, data visualization and statistical analysis, design of experiment, point processes, geospatial analysis, regression, and time series analysis are all included within this complete statistical package.\n    Statistica – comprehensive statistics package\n    StatsDirect – statistics package designed for biomedical, public health and general health science uses\n    StatXact – package for exact nonparametric and parametric statistics\n    Systat – general statistics package\n    SuperCROSS - comprehensive statistics package with ad-hoc, cross tabulation analysis\n    S-PLUS – general statistics package\n    Unistat – general statistics package that can also work as Excel add-in\n    The Unscrambler - free-to-try commercial multivariate analysis software for Windows\n    Wolfram Language[1] - the computer language that evolved from the program Mathematica. It has similar statistical capabilities as Mathematica.\n    World Programming System (WPS) – statistical package that supports the SAS language\n    XploRe\n\nAdd-ons\n\n    Analyse-it – add-on to Microsoft Excel for statistical analysis\n    NumXL – add-on to Microsoft Excel for statistical and time series analysis\n    SigmaXL – add-on to Microsoft Excel for statistical and graphical analysis\n    SPC XL – add-on to Microsoft Excel for general statistics\n    Statgraphics Sigma Express - add-on to Microsoft Excel for Six Sigma statistical analysis\n    SUDAAN – add-on to SAS and SPSS for statistical surveys\n    XLfit add-on to Microsoft Excel for curve fitting and statistical analysis\n\nSee also\n\n    Comparison of statistical packages\n    Econometric software\n    Free statistical software\n    List of computer algebra systems\n    List of graphing software\n    List of numerical analysis software\n    List of numerical libraries\n    Mathematical software\n    Psychometric software", "skillName": "List_of_statistical_packages."}
{"id": 23, "category": "Math_Stats_tools", "skillText": "Mathematics (from Greek μάθημα máthēma, “knowledge, study, learning”) is the study of topics such as quantity (numbers),[2] structure,[3] space,[2] and change.[4][5][6] There is a range of views among mathematicians and philosophers as to the exact scope and definition of mathematics.[7][8]\n\nMathematicians seek out patterns[9][10] and use them to formulate new conjectures. Mathematicians resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity for as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.\n\nRigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.[11]\n\nGalileo Galilei (1564–1642) said, \"The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language, and the letters are triangles, circles and other geometrical figures, without which means it is humanly impossible to comprehend a single word. Without these, one is wandering about in a dark labyrinth.\"[12] Carl Friedrich Gauss (1777–1855) referred to mathematics as \"the Queen of the Sciences\".[13] Benjamin Peirce (1809–1880) called mathematics \"the science that draws necessary conclusions\".[14] David Hilbert said of mathematics: \"We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise.\"[15] Albert Einstein (1879–1955) stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\"[16]\n\nMathematics is essential in many fields, including natural science, engineering, medicine, finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians also engage in pure mathematics, or mathematics for its own sake, without having any application in mind. There is no clear line separating pure and applied mathematics, and practical applications for what began as pure mathematics are often discovered.[17]\nContents\n\n    1 History\n        1.1 Etymology\n    2 Definitions of mathematics\n        2.1 Mathematics as science\n    3 Inspiration, pure and applied mathematics, and aesthetics\n    4 Notation, language, and rigor\n    5 Fields of mathematics\n        5.1 Foundations and philosophy\n        5.2 Pure mathematics\n        5.3 Applied mathematics\n    6 Mathematical awards\n    7 See also\n    8 Notes\n    9 References\n    10 Further reading\n    11 External links\n\nHistory\nMain article: History of mathematics\n\nThe history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals,[18] was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.\nGreek mathematician Pythagoras (c. 570 – c. 495 BC), commonly credited with discovering the Pythagorean theorem\nMayan numerals\n\nAs evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.[19]\n\nEvidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[20] The earliest uses of mathematics were in trading, land measurement, painting and weaving patterns and the recording of time.\n\nIn Babylonian mathematics elementary arithmetic (addition, subtraction, multiplication and division) first appears in the archaeological record. Numeracy pre-dated writing and numeral systems have been many and diverse, with the first known written numerals created by Egyptians in Middle Kingdom texts such as the Rhind Mathematical Papyrus.[citation needed]\n\nBetween 600 and 300 BC the Ancient Greeks began a systematic study of mathematics in its own right with Greek mathematics.[21]\nPersian mathematician Al-Khwarizmi ( c. 780 - c. 850 ), the inventor of the Algebra.\n\nDuring the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics: most of them include the contributions from Persian mathematicians such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.\n\nMathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, \"The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.\"[22]\nEtymology\n\nThe word mathematics comes from the Greek μάθημα (máthēma), which, in the ancient Greek language, means \"that which is learnt\",[23] \"what one gets to know\", hence also \"study\" and \"science\", and in modern Greek just \"lesson\". The word máthēma is derived from μανθάνω (manthano), while the modern Greek equivalent is μαθαίνω (mathaino), both of which mean \"to learn\". In Greece, the word for \"mathematics\" came to have the narrower and more technical meaning \"mathematical study\" even in Classical times.[24] Its adjective is μαθηματικός (mathēmatikós), meaning \"related to learning\" or \"studious\", which likewise further came to mean \"mathematical\". In particular, μαθηματικὴ τέχνη (mathēmatikḗ tékhnē), Latin: ars mathematica, meant \"the mathematical art\".\n\nSimilarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί) - which at the time meant \"teachers\" rather than \"mathematicians\" in the modern sense.\n\nIn Latin, and in English until around 1700, the term mathematics more commonly meant \"astrology\" (or sometimes \"astronomy\") rather than \"mathematics\"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations: a particularly notorious one is Saint Augustine's warning that Christians should beware of mathematici meaning astrologers, which is sometimes mistranslated as a condemnation of mathematicians.[25]\n\nThe apparent plural form in English, like the French plural form les mathématiques (and the less commonly used singular derivative la mathématique), goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural τα μαθηματικά (ta mathēmatiká), used by Aristotle (384–322 BC), and meaning roughly \"all things mathematical\"; although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from the Greek.[26] In English, the noun mathematics takes singular verb forms. It is often shortened to maths or, in English-speaking North America, math.[27]\nDefinitions of mathematics\nMain article: Definitions of mathematics\nLeonardo Fibonacci, the Italian mathematician who established the Hindu–Arabic numeral system to the Western World\n\nAristotle defined mathematics as \"the science of quantity\", and this definition prevailed until the 18th century.[28] Starting in the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions.[29] Some of these definitions emphasize the deductive character of much of mathematics, some emphasize its abstractness, some emphasize certain topics within mathematics. Today, no consensus on the definition of mathematics prevails, even among professionals.[7] There is not even consensus on whether mathematics is an art or a science.[8] A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable.[7] Some just say, \"Mathematics is what mathematicians do.\"[7]\n\nThree leading types of definition of mathematics are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought.[30] All have severe problems, none has widespread acceptance, and no reconciliation seems possible.[30]\n\nAn early definition of mathematics in terms of logic was Benjamin Peirce's \"the science that draws necessary conclusions\" (1870).[31] In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's \"All Mathematics is Symbolic Logic\" (1903).[32]\n\nIntuitionist definitions, developing from the philosophy of mathematician L.E.J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is \"Mathematics is the mental activity which consists in carrying out constructs one after the other.\"[30] A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.\n\nFormalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as \"the science of formal systems\".[33] A formal system is a set of symbols, or tokens, and some rules telling how the tokens may be combined into formulas. In formal systems, the word axiom has a special meaning, different from the ordinary meaning of \"a self-evident truth\". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.\nMathematics as science\nCarl Friedrich Gauss, known as the prince of mathematicians\n\nGauss referred to mathematics as \"the Queen of the Sciences\".[13] In the original Latin Regina Scientiarum, as well as in German Königin der Wissenschaften, the word corresponding to science means a \"field of knowledge\", and this was the original meaning of \"science\" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of \"science\" to natural science follows the rise of Baconian science, which contrasted \"natural science\" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\"[16] More recently, Marcus du Sautoy has called mathematics \"the Queen of Science ... the main driving force behind scientific discovery\".[34]\n\nMany philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper.[35] However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians[who?] that mathematics cannot be reduced to logic alone, and Karl Popper concluded that \"most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently.\"[36] Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.\n\nAn alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. The theoretical physicist J.M. Ziman proposed that science is public knowledge, and thus includes mathematics.[37] Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.\n\nThe opinions of mathematicians on this matter are varied. Many mathematicians[who?] feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others[who?] feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). It is common to see universities divided into sections that include a division of Science and Mathematics, indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.[citation needed]\nInspiration, pure and applied mathematics, and aesthetics\nMain article: Mathematical beauty\nIsaac Newton\nGottfried Wilhelm von Leibniz\nIsaac Newton (left) and Gottfried Wilhelm Leibniz (right), developers of infinitesimal calculus\n\nMathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.[38]\n\nSome mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the \"purest\" mathematics often turns out to have practical applications, is what Eugene Wigner has called \"the unreasonable effectiveness of mathematics\".[39] As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages.[40] Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.\n\nFor those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G.H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic.[41] Mathematicians often strive to find proofs that are particularly elegant, proofs from \"The Book\" of God according to Paul Erdős.[42][43] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.\nNotation, language, and rigor\nMain article: Mathematical notation\nLeonhard Euler, who created and popularized much of the mathematical notation used today\n\nMost of the mathematical notation in use today was not invented until the 16th century.[44] Before that, mathematics was written out in words, limiting mathematical discovery.[45] Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. It is compressed: a few symbols contain a great deal of information. Like musical notation, modern mathematical notation has a strict syntax and encodes information that would be difficult to write in any other way.\n\nMathematical language can be difficult to understand for beginners. Common words such as or and only have more precise meanings than in everyday speech. Moreover, words such as open and field have specialized mathematical meanings. Technical terms such as homeomorphism and integrable have precise meanings in mathematics. Additionally, shorthand phrases such as iff for \"if and only if\" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as \"rigor\".\n\nMathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken \"theorems\", based on fallible intuitions, of which many instances have occurred in the history of the subject.[46] The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.[47]\n\nAxioms in traditional thought were \"self-evident truths\", but that conception is problematic.[48] At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.[49]\nFields of mathematics\nSee also: Areas of mathematics and Glossary of areas of mathematics\nAn abacus, a simple calculating tool used since ancient times\n\nMathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.\nFoundations and philosophy\n\nIn order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase \"crisis of foundations\" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930.[50] Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.\n\nMathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proved are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science,[citation needed] as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.\n\nTheoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the \"P = NP?\" problem, one of the Millennium Prize Problems.[51] Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.\n\n    p ⇒ q {\\displaystyle p\\Rightarrow q\\,} p\\Rightarrow q\\, \tVenn A intersect B.svg \tCommutative diagram for morphism.svg \tDFAexample.svg\n    Mathematical logic \tSet theory \tCategory theory \tTheory of computation\n\nPure mathematics\nQuantity\n\nThe study of quantity starts with numbers, first the familiar natural numbers and integers (\"whole numbers\") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.\n\nAs the number system is further developed, the integers are recognized as a subset of the rational numbers (\"fractions\"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of \"infinity\". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.\n\n    1 , 2 , 3 , … {\\displaystyle 1,2,3,\\ldots \\!} 1,2,3,\\ldots \\! \t… , − 2 , − 1 , 0 , 1 , 2 … {\\displaystyle \\ldots ,-2,-1,0,1,2\\,\\ldots \\!} \\ldots ,-2,-1,0,1,2\\,\\ldots \\! \t− 2 , 2 3 , 1.21 {\\displaystyle -2,{\\frac {2}{3}},1.21\\,\\!} -2,{\\frac {2}{3}},1.21\\,\\! \t− e , 2 , 3 , π {\\displaystyle -e,{\\sqrt {2}},3,\\pi \\,\\!} -e,{\\sqrt {2}},3,\\pi \\,\\! \t2 , i , − 2 + 3 i , 2 e i 4 π 3 {\\displaystyle 2,i,-2+3i,2e^{i{\\frac {4\\pi }{3}}}\\,\\!} 2,i,-2+3i,2e^{i{\\frac {4\\pi }{3}}}\\,\\!\n    Natural numbers \tIntegers \tRational numbers \tReal numbers \tComplex numbers\n\nStructure\n\nMany mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.\n\nBy its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.\n\n    ( 1 , 2 , 3 ) ( 1 , 3 , 2 ) ( 2 , 1 , 3 ) ( 2 , 3 , 1 ) ( 3 , 1 , 2 ) ( 3 , 2 , 1 ) {\\displaystyle {\\begin{matrix}(1,2,3)&(1,3,2)\\\\(2,1,3)&(2,3,1)\\\\(3,1,2)&(3,2,1)\\end{matrix}}} {\\begin{matrix}(1,2,3)&(1,3,2)\\\\(2,1,3)&(2,3,1)\\\\(3,1,2)&(3,2,1)\\end{matrix}} \tElliptic curve simple.svg \tRubik's cube.svg \tGroup diagdram D6.svg \tLattice of the divisibility of 60.svg \tBraid-modular-group-cover.svg\n    Combinatorics \tNumber theory \tGroup theory \tGraph theory \tOrder theory \tAlgebra\n\nSpace\n\nThe study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.\n\n    Illustration to Euclid's proof of the Pythagorean theorem.svg \tSinusvåg 400px.png \tHyperbolic triangle.svg \tTorus.png \tMandel zoom 07 satellite.jpg \tMeasure illustration.png\n    Geometry \tTrigonometry \tDifferential geometry \tTopology \tFractal geometry \tMeasure theory\n\nChange\n\nUnderstanding and describing change is a common theme in the natural sciences, and calculus was developed as a powerful tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.\nIntegral as region under curve.svg \tVector field.svg \tNavier Stokes Laminar.svg \tLimitcycle.svg \tLorenz attractor.svg \tConformal grid after Möbius transformation.svg\nCalculus \tVector calculus \tDifferential equations \tDynamical systems \tChaos theory \tComplex analysis\nApplied mathematics\n\nApplied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, \"applied mathematics\" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the \"formulation, study, and use of mathematical models\" in science, engineering, and other areas of mathematical practice.\n\nIn the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.\nStatistics and other decision sciences\n\nApplied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) \"create data that makes sense\" with random sampling and with randomized experiments;[52] the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians \"make sense of the data\" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.[53]\n\nStatistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[54] Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.[55]\nComputational mathematics\n\nComputational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.\nGravitation space source.png \tBernoullisLawDerivationDiagram.svg \tComposite trapezoidal rule illustration small.svg \tMaximum boxed.png \tTwo red dice 01.svg \tOldfaithful3.png \tCaesar3.svg\nMathematical physics \tFluid dynamics \tNumerical analysis \tOptimization \tProbability theory \tStatistics \tCryptography\nMarket Data Index NYA on 20050726 202628 UTC.png \tArbitrary-gametree-solved.svg \tSignal transduction pathways.svg \tCH4-structure.svg \tGDP PPP Per Capita IMF 2008.svg \tSimple feedback control loop2.svg\nMathematical finance \tGame theory \tMathematical biology \tMathematical chemistry \tMathematical economics \tControl theory\nMathematical awards\n\nArguably the most prestigious award in mathematics is the Fields Medal,[56][57] established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.\n\nThe Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.\n\nA famous list of 23 open problems, called \"Hilbert's problems\", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the \"Millennium Prize Problems\", was published in 2000. A solution to each of these problems carries a $1 million reward, and only one (the Riemann hypothesis) is duplicated in Hilbert's problems.\nSee also\n\n    iconMathematics portal \n\n    Lists of mathematics topics\n    Mathematics and art\n    Mathematics education\n    Relationship between mathematics and physics\n    Science, Technology, Engineering, and Mathematics", "skillName": "Mathematics."}
{"id": 24, "category": "Math_Stats_tools", "skillText": "Advanced analytics tools R, SPSS, Matlab\nData Mining tools RapidMiner\nR is a programming language and software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing.The R language is widely used among statisticians and data miners for developing statistical software[5] and data analysis.[6] Polls, surveys of data miners, and studies of scholarly literature databases show that R's popularity has increased substantially in recent years.[7]\nR is a GNU project. The source code for the R software environment is written primarily in C, Fortran, and R.[9] R is freely available under the GNU General Public License, and pre-compiled binary versions are provided for various operating systems. While R has a command line interface, there are several graphical front-ends available\nMathlab\nPython\nTableau  R\nSAS\nScripting language Octave\nStatistical tools and data mining techniques\nStatistical computing and languages WEKA, KNIME, IBM SPSS\nOpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks,[1] a main area of deep learning research. The library is open source, licensed under the GNU Lesser General Public License.\n\nContents\n\n    1 Characteristics\n    2 History\n    3 Applications\n    4 Related libraries\n    5 See also\n    6 References\n    7 External links\n\nCharacteristics\n\nThe software implements any number of layers of non-linear processing units for supervised learning. This deep architecture allows the design of neural networks with universal approximation properties. Additionally, it allows multiprocessing programming by means of OpenMP, in order to increase computer performance.\n\nOpenNN contains data mining algorithms as a bundle of functions. These can be embedded in other software tools, using an application programming interface, for the integration of the predictive analytics tasks. In this regard, a graphical user interface is missing but some functions can be supported by specific visualization tools.[2]\nHistory\n\nThe development started in 2003 at the International Center for Numerical Methods in Engineering (CIMNE), within the research project funded by the European Union called RAMFLOOD.[3] Then it continued as part of similar projects. At present, OpenNN is being developed by the startup company Artelnics.[4]\n\nIn 2014, Big Data Analytics Today rated OpenNN as the #1 brain inspired artificial intelligence project.[5] Also, during the same year, ToppersWorld selected OpenNN among the top 5 open source data mining tools.[6]\nApplications\nOpenNN is a general purpose artificial intelligence software package.[7] It uses machine learning techniques for solving data mining and predictive analytics tasks in different fields. For instance, the library has been applied in the engineering,[8] energy,[9] or chemistry[10] sectors.\n\nJMP (pronounced \"jump\") is a computer program for statistics developed by the JMP business unit of SAS Institute. It was launched in 1989[1] to take advantage of the graphical user interface introduced by the Macintosh. It has since been improved and made available for the Windows operating system. JMP is used in applications such as Six Sigma, quality control and engineering, design of experiments and scientific research.\n\nThe software consists of five products: JMP, JMP Pro, JMP Clinical, JMP Genomics and the JMP Graph Builder App for the iPad. A scripting language is also available. The software is focused on exploratory analytics, whereby users investigate and explore data, rather than testing a hypothesis.\n\nContents\n\n    1 History\n    2 Software\n    3 JMP Scripting Language (JSL)\n    4 Notable applications\n    5 See also\n    6 References\n    7 Further reading\n    8 External links\n\nHistory\nVersion 1.0 of JMP from 1989\n\nJMP was developed in the 1980s by John Sall and a team of developers to make use of the graphical user interface introduced by the 1984 Apple Macintosh.[2] It originally stood for \"John's Macintosh Project\"[3] or “John’s Macintosh Product”[4] and was first released in October 1989.[2] It was used mostly by scientists and engineers for design of experiments (DOE), quality and productivity support (Six Sigma), and reliability modeling.[5] Semiconductor manufacturers were also among JMP’s early adopters.[6]\n\nInteractive graphics and other features were added in 1991[7][8] with version 2.0. Version 2 was twice the size as the original, though it was still delivered on a floppy disk. It required 2 MB of memory and came with 700 pages of documentation.[9] Support for Microsoft Windows was added in 1994.[4][10] JMP was re-written[11] with version 3 in 1999.[12][13] Version 4, released in 2002, could import data from a wider variety of data sources[14] and added support for surface plots.[8] Version 4 also added time series forecasting and new smoothing models, such as the seasonal smoothing method, called Winter's Method, and ARIMA (Autoregressive Integrated Moving Average). It was also the first version to support JSL, JMP Scripting Language.[15]\n\nIn 2005, data mining tools like a decision tree and neural net were added with version 5[16] as well as Linux support, which was later withdrawn in JMP 9.[5] Later in 2005, JMP 6 was introduced.[6][17] JMP began integrating with SAS in version 7.0 in 2007 and has strengthened this integration ever since. Users can write SAS code in JMP, connect to SAS servers, and retrieve and use data from SAS. Support for bubble plots was added in version 7.[5][18] JMP 7 also improved data visualization and diagnostics.[19]\n\nJMP 8 was released in 2009 with new drag-and-drop features and a 64-bit version to take advantage of advances in the Mac operating system.[20] It also added a new user interface for building graphs, tools for choice experiments and support for Life Distributions.[21] According to Scientific Computing, the software had improvements in \"graphics, QA, ease-of-use, SAS integration and data management areas.\"[22] JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel.[23][24] The main screen was rebuilt and enhancements were made to simulations, graphics and a new Degradation platform.[25] In March 2012, version 10 made improvements in data mining, predictive analytics, and automated model building.[26][27]\nSoftware\nScreenshot of different data displays in JMP\n\nJMP consists of JMP, JMP Pro, JMP Clinical and JMP Genomics,[27] as well as the Graph Builder iPad App.[28] JMP Clinical and JMP Genomics combine JMP with SAS software.[27]\n\nJMP software is partly focused on exploratory data analysis and visualization. It is designed for users to investigate data to learn something unexpected, as opposed to confirming a hypothesis.[4][27][29] JMP links statistical data to graphics representing them, so users can drill down or up to explore the data and various visual representations of it.[14][30][31] Its primary applications are for designed experiments and analyzing statistical data from industrial processes.[6]\n\nJMP is a desktop application with a wizard-based user interface, while SAS can be installed on servers. It runs in-memory, instead of on disk storage.[27] According to a review in Pharmaceutical Statistics, JMP is often used as a graphical front-end for a SAS system, which performs the statistical analysis and tabulations.[32] JMP Genomics, used for analyzing and visualizing genomics data,[33] requires a SAS component to operate and can access SAS/Genetics and SAS/STAT procedures or invoke SAS macros.[32] JMP Clinical, used for analyzing clinical trial data, can package SAS code within the JSL scripting language and convert SAS code to JMP.[18]\n\nJMP is also the name of the SAS Institute business unit that develops JMP. As of 2011 it had 180 employees and 250,000 users.[27]\nJMP Scripting Language (JSL)\n\nThe JMP Scripting Language (JSL) is an interpreted language for recreating analytic results and for automating or extending the functionality of JMP software.[34]:29 JSL was first introduced in JMP version 4 in 2000.[35]:1 JSL has a LISP-like syntax, structured as a series of expressions. All [rpgramming elements, including if-then statemenst and loops, are implemented as JSL functions. Data tables, display elements and analyses are represented by objects in JSL that are manipulated with named messages. Users may write JSL scripts to perform analyses and visualizations not available in the point-and-click interface or to automate a series of commands, such as weekly reports.[34] SAS, R, and Matlab code can also be executed using JSL.[36]\nNotable applications\nJMP being used in the WildTrack FIT system\n\nIn 2007, a wildlife monitoring organization, WildTrack, started using JMP with the Footprint Identification Technology (FIT) system to identify individual endangered animals by their footprints.[37] In 2009, the Chicago Botanic Garden used JMP to analyze DNA data from tropical breadfruit. Researchers determined that the seedless, starchy fruit was created by the deliberate hybridization of two fruits, the breadnut and the dugdug.[38] The Herzenberg Laboratory at Stanford has integrated JMP with the Fluorescence Activated Cell Sorter (FACS). The FACS system is used to study HIV, cancer, stem-cells and oceanography.[39]\n\nData mining is an interdisciplinary subfield of computer science.[1][2][3] It is the computational process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems.[1] The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.[1] Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[4]\n\nThe term is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[5] It also is a buzzword[6] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[7] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[8] Often the more general terms (large scale) data analysis and analytics – or, when referring to actual methods, artificial intelligence and machine learning – are more appropriate.\n\nThe actual data mining task is the automatic or semi-automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.\n\nThe related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.\n\nContents\n\n    1 Etymology\n    2 Background\n    3 Process\n        3.1 Pre-processing\n        3.2 Data mining\n        3.3 Results validation\n    4 Research\n    5 Standards\n    6 Notable uses\n    7 Privacy concerns and ethics\n        7.1 Situation in Europe\n        7.2 Situation in the United States\n    8 Copyright Law\n        8.1 Situation in Europe\n        8.2 Situation in the United States\n    9 Software\n        9.1 Free open-source data mining software and applications\n        9.2 Proprietary data-mining software and applications\n        9.3 Marketplace surveys\n    10 See also\n    11 References\n    12 Further reading\n    13 External links\n\nEtymology\n\nIn the 1960s, statisticians used terms like \"Data Fishing\" or \"Data Dredging\" to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"Data Mining\" appeared around 1990 in the database community. For a short time in 1980s, a phrase \"database mining\"™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[9] researchers consequently turned to \"data mining\". Other terms used include Data Archaeology, Information Harvesting, Information Discovery, Knowledge Extraction, etc. Gregory Piatetsky-Shapiro coined the term \"Knowledge Discovery in Databases\" for the first workshop on the same topic (KDD-1989) \nand this term became more popular in AI and Machine Learning Community. However, the term data mining became more popular in the business and press communities.[10] Currently, Data Mining and Knowledge Discovery are used interchangeably. Since about 2007, \"Predictive Analytics\" and since 2011, \"Data Science\" terms were also used to describe this field.\n\nIn the Academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding Editor-in-Chief. Later he started the SIGKDDD Newsletter SIGKDD Explorations.[11] The KDD International conference became the primary highest quality conference in Data Mining with an acceptance rate of research paper submissions below 18%. The Journal Data Mining and Knowledge Discovery is the primary research journal of the field.\nBackground\n\nThe manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology has dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns[12] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever larger data sets.\nProcess\n\nThe Knowledge Discovery in Databases (KDD) process is commonly defined with the stages:\n\n    (1) Selection\n    (2) Pre-processing\n    (3) Transformation\n    (4) Data Mining\n    (5) Interpretation/Evaluation.[4]\n\nIt exists, however, in many variations on this theme, such as the Cross Industry Standard Process for Data Mining (CRISP-DM) which defines six phases:\n\n    (1) Business Understanding\n    (2) Data Understanding\n    (3) Data Preparation\n    (4) Modeling\n    (5) Evaluation\n    (6) Deployment\n\nor a simplified process such as (1) pre-processing, (2) data mining, and (3) results validation.\n\nPolls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[13] The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[14][15] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[16]\nPre-processing\n\nBefore data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.\nData mining\n\nData mining involves six common classes of tasks:[4]\n\n    Anomaly detection (Outlier/change/deviation detection) – The identification of unusual data records, that might be interesting or data errors that require further investigation.\n    Association rule learning (Dependency modelling) – Searches for relationships between variables. For example, a supermarket might gather data on customer purchasing habits. Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis.\n    Clustering – is the task of discovering groups and structures in the data that are in some way or another \"similar\", without using known structures in the data.\n    Classification – is the task of generalizing known structure to apply to new data. For example, an e-mail program might attempt to classify an e-mail as \"legitimate\" or as \"spam\".\n    Regression – attempts to find a function which models the data with the least error.\n    Summarization – providing a more compact representation of the data set, including visualization and report generation.\n\nResults validation\nAn example of data produced by data dredging through a bot operated by statistician Tyler Viglen, apparently showing a close link between the best word winning a spelling bee competition and the number of people in the United States killed by venomous spiders. The similarity in trends is obviously a coincidence.\n\nData mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening.[citation needed]\nWiki letter w.svg\n\tThis section is missing information about non-classification tasks in data mining. It only covers machine learning. Please expand the section to include this information. Further details may exist on the talk page. (September 2011)\n\nThe final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.\n\nIf the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.\nResearch\n\nThe premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[17][18] Since 1989 this ACM SIG has hosted an annual international conference and published its proceedings,[19] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".[20]\n\nComputer science conferences on data mining include:\n\n    CIKM Conference – ACM Conference on Information and Knowledge Management\n    DMIN Conference – International Conference on Data Mining\n    DMKD Conference – Research Issues on Data Mining and Knowledge Discovery\n    DSAA Conference – IEEE International Conference on Data Science and Advanced Analytics\n    ECDM Conference – European Conference on Data Mining\n    ECML-PKDD Conference – European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases\n    EDM Conference – International Conference on Educational Data Mining\n    INFOCOM Conference – IEEE INFOCOM\n    ICDM Conference – IEEE International Conference on Data Mining\n    KDD Conference – ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n    MLDM Conference – Machine Learning and Data Mining in Pattern Recognition\n    PAKDD Conference – The annual Pacific-Asia Conference on Knowledge Discovery and Data Mining\n    PAW Conference – Predictive Analytics World\n    SDM Conference – SIAM International Conference on Data Mining (SIAM)\n    SSTD Symposium – Symposium on Spatial and Temporal Databases\n    WSDM Conference – ACM Conference on Web Search and Data Mining\n\nData mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases\nStandards\n\nThere have been some efforts to define standards for the data mining process, for example the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006, but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.\n\nFor exchanging the extracted models – in particular for use in predictive analytics – the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[21]\nNotable uses\nMain article: Examples of data mining\nSee also: Category:Applied data mining.\n\nData mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.\nPrivacy concerns and ethics\n\nWhile the term \"data mining\" itself has no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[22]\n\nThe ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[23] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[24][25]\n\nData mining requires data preparation which can uncover information or patterns which may compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[26] This is not data mining per se, but a result of the preparation of data before – and for the purposes of – the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[27][28][29]\n\nIt is recommended that an individual is made aware of the following before data are collected:[26]\n\n    the purpose of the data collection and any (known) data mining projects;\n    how the data will be used;\n    who will be able to mine the data and use the data and their derivatives;\n    the status of security surrounding access to the data;\n    how collected data can be updated.\n\nData may also be modified so as to become anonymous, so that individuals may not readily be identified.[26] However, even \"de-identified\"/\"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[30]\n\nThe inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial, emotional, or bodily harm to the indicated individual. In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies.[31]\nSituation in Europe\n\nEurope has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.-E.U. Safe Harbor Principles currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's Global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement have failed.[citation needed]\nSituation in the United States\n\nIn the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week', \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is undermined by the complexity of consent forms that are required of patients and participants, which approach a level of incomprehensibility to average individuals.\"[32] This underscores the necessity for data anonymity in data aggregation and mining practices.\n\nU.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. Use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.\nCopyright Law\nSituation in Europe\n\nDue to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. Where a database is pure data in Europe there is likely to be no copyright, but database rights may exist so data mining becomes subject to regulations by the Database Directive. On the recommendation of the Hargreaves review this led to the UK government to amend its copyright law in 2014[33] to allow content mining as a limitation and exception. Only the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[34] The focus on the solution to this legal issue being licences and not limitations and exceptions led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[35]\nSituation in the United States\n\nBy contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that content mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one being text and data mining.[36]\nSoftware\nSee also: Category:Data mining and machine learning software.\nFree open-source data mining software and applications\n\nThe following applications are available under free/open source licenses. Public access to application sourcecode is also available.\n\n    Carrot2: Text and search results clustering framework.\n    Chemicalize.org: A chemical structure miner and web search engine.\n    ELKI: A university research project with advanced cluster analysis and outlier detection methods written in the Java language.\n    GATE: a natural language processing and language engineering tool.\n    KNIME: The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\n    Massive Online Analysis (MOA): a real-time big data stream mining with concept drift tool in the Java programming language.\n    ML-Flex: A software package that enables users to integrate with third-party machine-learning packages written in any programming language, execute classification analyses in parallel across multiple computing nodes, and produce HTML reports of classification results.\n    MLPACK library: a collection of ready-to-use machine learning algorithms written in the C++ language.\n    NLTK (Natural Language Toolkit): A suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python language.\n    OpenNN: Open neural networks library.\n    Orange: A component-based data mining and machine learning software suite written in the Python language.\n    R: A programming language and software environment for statistical computing, data mining, and graphics. It is part of the GNU Project.\n    SCaViS: Java cross-platform data analysis framework developed at Argonne National Laboratory.\n    scikit-learn is an open source machine learning library for the Python programming language\n    SenticNet API \n    : A semantic and affective resource for opinion mining and sentiment analysis.\n    Torch: An open source deep learning library for the Lua programming language and scientific computing framework with wide support for machine learning algorithms.\n    UIMA: The UIMA (Unstructured Information Management Architecture) is a component framework for analyzing unstructured content such as text, audio and video – originally developed by IBM.\n    Weka: A suite of machine learning software applications written in the Java programming language.\n\nProprietary data-mining software and applications\n\nThe following applications are available under proprietary licenses.\n\n    Angoss KnowledgeSTUDIO: data mining tool provided by Angoss.\n    Clarabridge: enterprise class text analytics solution.\n    HP Vertica Analytics Platform: data mining software provided by HP.\n    IBM SPSS Modeler: data mining software provided by IBM.\n    KXEN Modeler: data mining tool provided by KXEN.\n    LIONsolver: an integrated software application for data mining, business intelligence, and modeling that implements the Learning and Intelligent OptimizatioN (LION) approach.\n    Megaputer Intelligence: data and text mining software is called PolyAnalyst.\n    Microsoft Analysis Services: data mining software provided by Microsoft.\n    NetOwl: suite of multilingual text and entity analytics products that enable data mining.\n    OpenText™ Big Data Analytics: Visual Data Mining & Predictive Analysis by Open Text Corporation\n    Oracle Data Mining: data mining software by Oracle.\n    PSeven: platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining provided by DATADVANCE.\n    Qlucore Omics Explorer: data mining software provided by Qlucore.\n    RapidMiner: An environment for machine learning and data mining experiments.\n    SAS Enterprise Miner: data mining software provided by the SAS Institute.\n    STATISTICA Data Miner: data mining software provided by StatSoft.\n    Tanagra: A visualisation-oriented data mining software, also for teaching.\n\nMarketplace surveys\n\nSeveral researchers and organizations have conducted reviews of data mining tools and surveys of data miners. These identify some of the strengths and weaknesses of the software packages. They also provide an overview of the behaviors, preferences and views of data miners. Some of these reports include:\n\n    Hurwitz Victory Index: Report for Advanced Analytics as a market research assessment tool, it highlights both the diverse uses for advanced analytics technology and the vendors who make those applications possible.Recent-research \n    2011 Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery[37]\n    Rexer Analytics Data Miner Surveys (2007–2013)[38]\n    Forrester Research 2010 Predictive Analytics and Data Mining Solutions report[39]\n    Gartner 2008 \"Magic Quadrant\" report[40]\n    Robert A. Nisbet's 2006 Three Part Series of articles \"Data Mining Tools: Which One is Best For CRM?\"[41]\n    Haughton et al.'s 2003 Review of Data Mining Software Packages in The American Statistician[42]\n    Goebel & Gruenwald 1999 \"A Survey of Data Mining a Knowledge Discovery Software Tools\" in SIGKDD Explorations[43]", "skillName": "Math_Stats_tools."}
{"id": 25, "category": "capital_market", "skillText": "cial services are the economic services provided by the finance industry, which encompasses a broad range of businesses that manage money, including credit unions, banks, credit-card companies, insurance companies, accountancy companies, consumer-finance companies, stock brokerages, investment funds and some government-sponsored enterprises.[1]\n\nContents\n\n    1 History\n    2 Banks\n        2.1 Commercial banking services\n        2.2 Investment banking services\n    3 Foreign exchange services\n    4 Investment services\n    5 Insurance\n    6 Other financial services\n    7 Financial exports\n    8 Financial crime\n    9 Market share\n        9.1 US\n    10 See also\n    11 References\n    12 Further reading\n    13 External links\n\nHistory\nSee also: Global financial system § History of international financial architecture\n\nThe term \"financial services\" became more prevalent in the United States partly as a result of the Gramm-Leach-Bliley Act of the late 1990s, which enabled different types of companies operating in the U.S. financial services industry at that time to merge.[2]\n\nCompanies usually have two distinct approaches to this new type of business. One approach would be a bank which simply buys an insurance company or an investment bank, keeps the original brands of the acquired firm, and adds the acquisition to its holding company simply to diversify its earnings. Outside the U.S. (e.g., in Japan), non-financial services companies are permitted within the holding company. In this scenario, each company still looks independent, and has its own customers, etc. In the other style, a bank would simply create its own brokerage division or insurance division and attempt to sell those products to its own existing customers, with incentives for combining all things with one company.\nBanks\nMain article: Bank\nCommercial banking services\n\nA \"commercial bank\" is what is commonly referred to as simply a bank. The term \"commercial\" is used to distinguish it from an \"investment bank,\" a type of financial services entity which, instead of lending money directly to a business, helps businesses raise money from other firms in the form of bonds (debt) or stock (equity).\n\nThe primary operations of banks include:\n\n    Keeping money safe while also allowing withdrawals when needed\n    Issuance of chequebooks so that bills can be paid and other kinds of payments can be delivered by post\n    Provide personal loans, commercial loans, and mortgage loans (typically loans to purchase a home, property or business)\n    Issuance of credit cards and processing of credit card transactions and billing\n    Issuance of debit cards for use as a substitute for cheques\n    Allow financial transactions at branches or by using Automatic Teller Machines (ATMs)\n    Provide wire transfers of funds and Electronic fund transfers between banks\n    Facilitation of standing orders and direct debits, so payments for bills can be made automatically\n    Provide overdraft agreements for the temporary advancement of the bank's own money to meet monthly spending commitments of a customer in their current account.\n    Provide internet banking system to facilitate the customers to view and operate their respective accounts through internet.\n    Provide charge card advances of the bank's own money for customers wishing to settle credit advances monthly.\n    Provide a check guaranteed by the bank itself and prepaid by the customer, such as a cashier's check or certified check.\n    Notary service for financial and other documents\n    Accepting the deposits from customer and provide the credit facilities to them.\n    Sell investment products like mutual funds etc.\n\nInvestment banking services\n\n    Capital markets services - underwriting debt and equity, assist company deals (advisory services, underwriting, mergers and acquisitions and advisory fees), and restructure debt into structured finance products.\n    Private banking - Private banks provide banking services exclusively to high-net-worth individuals. Many financial services firms require a person or family to have a certain minimum net worth to qualify for private banking service.[3] Private banks often provide more personal services, such as wealth management and tax planning, than normal retail banks.[4]\n    Brokerage services - facilitating the buying and selling of financial securities between a buyer and a seller. In today's (2014) stock brokers, brokerages services are offered online to self trading investors throughout the world who have the option of trading with 'tied' online trading platforms offered by a banking institution or with online trading platforms sometimes offered in a group by so-called online trading portals.\n\nForeign exchange services\n\nForeign exchange services are provided by many banks and specialist foreign exchange brokers around the world. Foreign exchange services include:\n\n    Currency exchange - where clients can purchase and sell foreign currency banknotes.\n    Wire transfer - where clients can send funds to international banks abroad.\n    Remittance - where clients that are migrant workers send money back to their home country.\n\nInvestment services\n\n    Investment management - the term usually given to describe companies which run collective investment funds. Also refers to services provided by others, generally registered with the Securities and Exchange Commission as Registered Investment Advisors. Investment banking financial services focus on creating capital through client investments.\n    Hedge fund management - Hedge funds often employ the services of \"prime brokerage\" divisions at major investment banks to execute their trades.\n    Custody services - the safe-keeping and processing of the world's securities trades and servicing the associated portfolios. Assets under custody in the world are approximately US$100 trillion.[5]\n\nInsurance\nMain article: Insurance\n\n    Insurance brokerage - Insurance brokers shop for insurance (generally corporate property and casualty insurance) on behalf of customers. Recently a number of websites have been created to give consumers basic price comparisons for services such as insurance, causing controversy within the industry.[6]\n    Insurance underwriting - Personal lines insurance underwriters actually underwrite insurance for individuals, a service still offered primarily through agents, insurance brokers, and stock brokers. Underwriters may also offer similar commercial lines of coverage for businesses. Activities include insurance and annuities, life insurance, retirement insurance, health insurance, and property insurance and casualty insurance.\n    F&I - Finance & Insurance, a service still offered primarily at asset dealerships. The F&I manager encompasses the financing and insuring of the asset which is sold by the dealer. F&I is often called \"the second gross\" in dealerships who have adopted the model\n    Reinsurance - Reinsurance is insurance sold to insurers themselves, to protect them from catastrophic losses.\n\nOther financial services\n\n    Bank cards - include both credit cards and debit cards. According to the Nilson Report, JP Morgan Chase is the largest issuer of bank cards.[7]\n    Credit card machine services and networks - Companies which provide credit card machine and payment networks call themselves \"merchant card providers\".\n    Intermediation or advisory services - These services involve stock brokers (private client services) and discount brokers. Stock brokers assist investors in buying or selling shares. Primarily internet-based companies are often referred to as discount brokerages, although many now have branch offices to assist clients. These brokerages primarily target individual investors. Full service and private client firms primarily assist and execute trades for clients with large amounts of capital to invest, such as large companies, wealthy individuals, and investment management funds.\n    Private equity - Private equity funds are typically closed-end funds, which usually take controlling equity stakes in businesses that are either private, or taken private once acquired. Private equity funds often use leveraged buyouts (LBOs) to acquire the firms in which they invest. The most successful private equity funds can generate returns significantly higher than provided by the equity markets\n    Venture capital is a type of private equity capital typically provided by professional, outside investors to new, high-growth-potential companies in the interest of taking the company to an IPO or trade sale of the business.\n    Angel investment - An angel investor or angel (known as a business angel or informal investor in Europe), is an affluent individual who provides capital for a business start-up, usually in exchange for convertible debt or ownership equity. A small but increasing number of angel investors organize themselves into angel groups or angel networks to share resources and pool their investment capital.\n    Conglomerates - A financial services company, such as a universal bank, that is active in more than one sector of the financial services market e.g. life insurance, general insurance, health insurance, asset management, retail banking, wholesale banking, investment banking, etc. A key rationale for the existence of such businesses is the existence of diversification benefits that are present when different types of businesses are aggregated i.e. bad things don't always happen at the same time. As a consequence, economic capital for a conglomerate is usually substantially less than economic capital is for the sum of its parts.\n    Financial market utilities - Organisations that are part of the infrastructure of financial services, such as stock exchanges, clearing houses, derivative and commodity exchanges and payment systems such as real-time gross settlement systems or interbank networks.\n    Debt resolution is a consumer service that assists individuals that have too much debt to pay off as requested, but do not want to file bankruptcy and wish to pay off their debts owed. This debt can be accrued in various ways including but not limited to personal loans, credit cards or in some cases merchant accounts.\n    Factoring\n\nFinancial exports\n\nA financial export is a financial service provided by a domestic firm (regardless of ownership) to a foreign firm or individual. While financial services such as banking, insurance and investment management are often seen as a domestic service, an increasing proportion of financial services are now being handled abroad, in other financial centres, for a variety of reasons. Some smaller financial centres, such as Bermuda, Luxembourg, and the Cayman Islands, lack sufficient size for a domestic financial services sector and have developed a role providing services to non-residents as offshore financial centres. The increasing competitiveness of financial services has meant that some countries, such as Japan, which were self-sufficient have increasingly imported financial services.\n\nThe leading financial exporter, in terms of exports less imports, is the United Kingdom, which had $95 billion of financial exports in 2014.[8] The UK's position is helped by both unique institutions (such as Lloyd's of London for insurance, the Baltic Exchange for shipping etc.) and an environment that attracts foreign firms; many international corporations have global or regional headquarters in the London and are listed on the London Stock Exchange, and many banks and other financial institutions operate there or in Edinburgh.[9][10]\nFinancial crime\n\nFraud within the financial industry costs the UK (regulated by the FCA) an estimated £14bn a year and it is believed a further £25bn is laundered by British institutions.[11]\nMarket share\nUS\n\nAs of 2004 the financial services industry (finance industry) represented 20% of the market capitalization of the S&P 500 in the United States.[citation needed]\n\nThe U.S. finance industry comprised only 10% of total non-farm business profits in 1947, but it grew to 50% by 2010. Over the same period, finance industry income as a proportion of GDP rose from 2.5% to 7.5%, and the finance industry's proportion of all corporate income rose from 10% to 20%. The mean earnings per employee hour in finance relative to all other sectors has closely mirrored the share of total U.S. income earned by the top 1% income earners since 1930. The mean salary in New York City's finance industry rose from $80,000 in 1981 to $360,000 in 2011, while average New York City salaries rose from $40,000 to $70,000. In 1988, there were about 12,500 U.S. banks with less than $300 million in deposits, and about 900 with more deposits, but by 2012, there were only 4,200 banks with less than $300 million in deposits in the U.S., and over 1,801 with more.\n\nThe financial services industry constitutes the largest group of companies in the world in terms of earnings and equity market capitalization. However it is not the largest category in terms of revenue or number of employees. It is also a slow growing and extremely fragmented industry, with the largest company (Citigroup), only having a 3% US market share.[12] In contrast, the largest home improvement store in the US, Home Depot, has a 30% market share, and the largest coffee house Starbucks has a market share of 32%.\nSee also\nBook icon \t\n\n    Book: Finance\n\n    Alternative financial services\n    Financial analyst\n    Financial data vendors\n    Financial markets\n    Financial technology\n    Financialization", "skillName": "Financial_services."}
{"id": 26, "category": "capital_market", "skillText": "Financial economics is the branch of economics characterized by a \"concentration on monetary activities\", in which \"money of one type or another is likely to appear on both sides of a trade\".[1] Its concern is thus the interrelation of financial variables, such as prices, interest rates and shares, as opposed to those concerning the real economy. It has two main areas of focus: asset pricing (or \"investment theory\") and corporate finance; the first being the perspective of providers of capital and the second of users of capital.\n\nThe subject is concerned with \"the allocation and deployment of economic resources, both spatially and across time, in an uncertain environment\".[2] It therefore centers on decision making under uncertainty in the context of the financial markets, and the resultant economic and financial models and principles, and is concerned with deriving testable or policy implications from acceptable assumptions. It is built on the foundations of microeconomics and decision theory.\n\nFinancial econometrics is the branch of financial economics that uses econometric techniques to parameterise these relationships. Mathematical finance is related in that it will derive and extend the mathematical or numerical models suggested by financial economics. Note though that the emphasis there is mathematical consistency, as opposed to compatibility with economic theory.\n\nFinancial economics is usually taught at the postgraduate level; see Master of Financial Economics. Recently, specialist undergraduate degrees are offered in the discipline.[3]\n\nNote that this article provides an overview and survey of the field: for derivations and more technical discussion, see the specific articles linked.\n\nContents\n\n    1 Underlying economics\n        1.1 Present value, expectation and utility\n        1.2 Arbitrage-free pricing and equilibrium\n        1.3 State prices\n    2 Resultant models\n        2.1 Certainty\n        2.2 Uncertainty\n    3 Extensions\n        3.1 Portfolio theory\n        3.2 Derivative pricing\n        3.3 Corporate finance theory\n    4 Challenges and criticism\n        4.1 Departures from normality\n        4.2 Departures from rationality\n    5 See also\n    6 References\n    7 Bibliography\n    8 External links\n\nUnderlying economics\nJEL classification codes\nIn the Journal of Economic Literature classification codes, Financial Economics is one of the 19 primary classifications, at JEL: G. It follows Monetary and International Economics and precedes Public Economics. Detailed subclassifications are linked in the following footnote.[4]\n\nThe New Palgrave Dictionary of Economics (2008, 2nd ed.) also uses the JEL codes to classify its entries in v. 8, Subject Index, including Financial Economics at pp. 863–64. The corresponding footnotes below have links to entry abstracts of The New Palgrave Online \nfor each primary or secondary JEL category (10 or fewer per page, similar to Google searches):\n\n    JEL: G – Financial Economics[5]\n    JEL: G0 – General[6]\n    JEL: G1 – General Financial Markets[7]\n    JEL: G2 – Financial institutions and Services[8]\n    JEL: G3 – Corporate finance and Governance[9]\n\nTertiary category entries can also be searched.[10]\n\nAs above, the discipline essentially explores how rational investors would apply decision theory to the problem of investment. The subject is thus built on the foundations of microeconomics and decision theory, and derives several key results for the application of decision making under uncertainty to the financial markets.\nPresent value, expectation and utility\n\nUnderlying all of financial economics are the concepts of present value and expectation.[11] Calculating their present value allows the decision maker to aggregate the cashflows (or other returns) to be produced by the asset in the future, to a single value at the date in question, and to thus more readily compare two opportunities; this concept is therefore the starting point for financial decision making. Its history is correspondingly early - it was developed in a book on compound interest by Richard Witt (\"Arithmetical Questions\" (1613)), and these ideas were further developed by Johan de Witt and Edmond Halley.\n\nAn immediate extension is to then combine probabilities with present value, leading to the expected value criterion which sets asset value as a function of the sizes of the expected payouts and the probabilities of their occurrence. These ideas originate with Blaise Pascal and Pierre de Fermat.\n\nThis decision method, however, fails to consider risk aversion (\"as any student of finance knows\" [11]). In other words, since individuals receive greater utility from an extra dollar when they are poor and less utility when comparatively rich, the approach is to therefore \"adjust\" the weight assigned to the various outcomes (\"states\") correspondingly. (Some investors may in fact be risk seeking as opposed to risk averse, but the same logic would apply). Choice under uncertainty here, may then be characterized as the maximization of expected utility. More formally, the resulting expected utility hypothesis states that, if certain axioms are satisfied, the subjective value associated with a gamble by an individual is that individual's statistical expectation of the valuations of the outcomes of that gamble. The impetus for these ideas arise from various inconsistencies observed under the expected value framework, such as the St. Petersburg paradox (see also Ellsberg paradox). The development here originally due to Daniel Bernoulli, and later formalized by John von Neumann and Oskar Morgenstern.\nArbitrage-free pricing and equilibrium\n\nThe concepts of arbitrage-free “Rational” pricing and equilibrium are then coupled with the above to derive \"classical\" financial economics. Rational pricing is the assumption in financial economics that asset prices (and hence asset pricing models) will reflect the arbitrage-free price of the asset, as any deviation from this price will be \"arbitraged away\". This assumption is useful in pricing fixed income securities, particularly bonds, and is fundamental to the pricing of derivative instruments.\n\nEconomic equilibrium is, in general, a state where economic forces such as supply and demand are balanced and in the absence of external influences these equilibrium values of economic variables will not change. General equilibrium deals with the behavior of supply, demand, and prices in a whole economy with several or many interacting markets, by seeking to prove that a set of prices exists that will result in an overall equilibrium (this is in contrast to partial equilibrium, which only analyzes single markets.)\n\nThe two concepts are linked as follows: Where market prices do not allow for profitable arbitrage, i.e. comprise an arbitrage-free market, so these prices are also said to constitute an \"arbitrage equilibrium\". Intuitively, this may be seen by considering that where an arbitrage opportunity does exist, then prices can be expected to change, and are, therefore, not in equilibrium.[12] An arbitrage equilibrium is thus a precondition for a general economic equilibrium.\n\nThe immediate, and formal, extension of this idea, the Fundamental theorem of asset pricing, shows that where markets are as above - and are additionally (implicitly and correspondingly) complete - we may then conduct financial decisioning by constructing a risk neutral probability measure corresponding to the market. \"Complete\" here means that there is a price for every asset in every possible state of the world, and that the complete set of possible bets on future states-of-the-world can therefore be constructed with existing assets (assuming no friction). The derivation will then precede by arbitrage arguments:[11][12] For the logic see Rational pricing#Risk neutral valuation, where, in a simplified environment, the market has only two possible states - up and down - and where \"p\" and \"1-p\" comprise the corresponding (i.e. implied) probabilities, and in turn, the derived distribution (or measure).\n\nWith this measure in place, the expected, i.e. required, return of any security (or portfolio) will then equal the riskless return, plus an “adjustment for risk,” [11] i.e. a security specific risk premium, compensating for the extent to which the cashflows are unpredictable (perfectly predictable, being risk free). All pricing models are then essentially a variant on this, given specific assumptions and / or conditions.[11][13] This approach is consistent with the above, but with the expectation based on \"the market\" (i.e. arbitrage free, and, per the theorem, therefore in equilibrium) as opposed to individual preferences.\n\nThus, continuing the example, to value a specific security, its forecasted cashflows in the up- and down-states are multiplied through by \"p\" and \"1-p\", and are then discounted at the risk-free rate plus an appropriate premium. In general, this premium may be derived by the CAPM (or extensions) as will be seen under #Uncertainty. Note that in corporate finance and accounting contexts, it is the most likely (i.e. \"average\") overall cashflows which are forecasted, as opposed to those state by state, see John Burr Williams' proposal below; the discounting methodology is unchanged.\nState prices\n\nWith the above relationship established, the further specialized Arrow–Debreu model may be derived. This important result suggests that, under certain economic conditions, there must be a set of prices such that aggregate supplies will equal aggregate demands for every commodity in the economy. The analysis here is often undertaken assuming a Representative agent.\n\nThe Arrow–Debreu model applies to economies with maximally complete markets, in which there exists a market for every time period and forward prices for every commodity at all time periods. A direct extension, then, is the concept of a state price security (also called an Arrow-Debreu security), a contract that agrees to pay one unit of a numeraire (a currency or a commodity) if a particular state occurs (\"up\" and \"down\" in the simplified example above) at a particular time in the future and pays zero numeraire in all the other states. The price of this security is the state price of this particular state of the world. Correspondingly, the state price vector is the vector of state prices for all states.\n\nState prices find immediate application in Financial Economics, as will be seen below: for example, any derivatives contract whose settlement value is a function of an underlying whose value is uncertain at contract date can be decomposed as a linear combination of its Arrow-Debreu securities, and thus as a weighted sum of its state prices. Analogously, for a continuous random variable indicating a continuum of possible states, the value is found by integrating over the state price density; see Stochastic discount factor. These concepts are extended to Martingale pricing and the related Risk-neutral measure.\nResultant models\nModigliani–Miller Proposition II with risky debt. As leverage (D/E) increases, the WACC (k0) stays constant.\nEfficient Frontier. The hyperbola is sometimes referred to as the 'Markowitz Bullet', and its upward sloped portion is the efficient frontier if no risk-free asset is available. With a risk-free asset, the straight line is the efficient frontier. The graphic displays the CAL, Capital allocation line, formed when the risky asset is a single-asset rather than the market, in which case the line is the CML.\nThe Capital market line is the tangent line drawn from the point of the risk-free asset to the feasible region for risky assets. The tangency point M represents the market portfolio. The CML results from the combination of the market portfolio and the risk-free asset (the point L). Addition of leverage (the point R) creates levered portfolios that are also on the CML.\nThe capital asset pricing model (CAPM)\n\n    E ( R i ) = R f + β i ( E ( R m ) − R f ) {\\displaystyle E(R_{i})=R_{f}+\\beta _{i}(E(R_{m})-R_{f})} {\\displaystyle E(R_{i})=R_{f}+\\beta _{i}(E(R_{m})-R_{f})}\n\nSecurity market line: the representation of the CAPM displaying the expected rate of return of an individual security as a function of its systematic, non-diversifiable risk.\nSimulated geometric Brownian motions with parameters from market data.\nThe Black–Scholes equation\n\n    ∂ V ∂ t + 1 2 σ 2 S 2 ∂ 2 V ∂ S 2 + r S ∂ V ∂ S − r V = 0 {\\displaystyle {\\frac {\\partial V}{\\partial t}}+{\\frac {1}{2}}\\sigma ^{2}S^{2}{\\frac {\\partial ^{2}V}{\\partial S^{2}}}+rS{\\frac {\\partial V}{\\partial S}}-rV=0} {\\frac {\\partial V}{\\partial t}}+{\\frac {1}{2}}\\sigma ^{2}S^{2}{\\frac {\\partial ^{2}V}{\\partial S^{2}}}+rS{\\frac {\\partial V}{\\partial S}}-rV=0\n\nThe Black–Scholes formula for the value of a call option:\n\n    C ( S , t ) = N ( d 1 ) S − N ( d 2 ) K e − r ( T − t ) d 1 = 1 σ T − t [ ln ⁡ ( S K ) + ( r + σ 2 2 ) ( T − t ) ] d 2 = d 1 − σ T − t {\\displaystyle {\\begin{aligned}C(S,t)&=N(d_{1})S-N(d_{2})Ke^{-r(T-t)}\\\\d_{1}&={\\frac {1}{\\sigma {\\sqrt {T-t}}}}\\left[\\ln \\left({\\frac {S}{K}}\\right)+\\left(r+{\\frac {\\sigma ^{2}}{2}}\\right)(T-t)\\right]\\\\d_{2}&=d_{1}-\\sigma {\\sqrt {T-t}}\\\\\\end{aligned}}} {\\begin{aligned}C(S,t)&=N(d_{1})S-N(d_{2})Ke^{-r(T-t)}\\\\d_{1}&={\\frac {1}{\\sigma {\\sqrt {T-t}}}}\\left[\\ln \\left({\\frac {S}{K}}\\right)+\\left(r+{\\frac {\\sigma ^{2}}{2}}\\right)(T-t)\\right]\\\\d_{2}&=d_{1}-\\sigma {\\sqrt {T-t}}\\\\\\end{aligned}}\n\nBinomial Lattice with CRR formulae\n\nApplying the preceding economic concepts, we may then derive various economic- and financial models and principles. As above, the two usual areas of focus are Asset Pricing and Corporate Finance, the first being the perspective of providers of capital, the second of users of capital. Here, and for (almost) all other financial economics models, the questions addressed are typically framed in terms of \"time, uncertainty, options, and information\",[1] as will be seen below.\n\n    Time: money now is traded for money in the future.\n    Uncertainty (or risk): The amount of money to be transferred in the future is uncertain.\n    Options: one party to the transaction can make a decision at a later time that will affect subsequent transfers of money.\n    Information: knowledge of the future can reduce, or possibly eliminate, the uncertainty associated with future monetary value (FMV).\n\nApplying this framework, with the above concepts, leads to the required models. This derivation begins with the assumption of \"no uncertainty\" and is then expanded to incorporate the other considerations. (This division sometimes denoted \"deterministic\" and \"random\",[14] or \"stochastic\".)\nCertainty\n\nA starting point here is “Investment under certainty\". The Fisher separation theorem, asserts that the objective of a corporation will be the maximization of its present value, regardless of the preferences of its shareholders. Related is the Modigliani-Miller theorem, which shows that, under certain conditions, the value of a firm is unaffected by how that firm is financed, and depends neither on its dividend policy nor its decision to raise capital by issuing stock or selling debt. The proof here proceeds using arbitrage arguments, and acts as a benchmark for evaluating the effects of factors outside the model that do affect value.\n\nThe mechanism for determining (corporate) value is provided by The Theory of Investment Value (John Burr Williams), which proposes that the value of an asset should be calculated using “evaluation by the rule of present worth”. Thus, for a common stock, the intrinsic, long-term worth is the present value of its future net cashflows, in the form of dividends. What remains to be determined is the appropriate discount rate. Later developments show that, \"rationally\", i.e. in the formal sense, the appropriate discount rate here will (should) depend on the asset's riskiness relative to the overall market, as opposed to its owners' preferences; see below.\n\nBond valuation, in that cashflows (coupons and return of principal) are deterministic, may proceed in the same fashion.[14] In many treatments, in fact, bond valuation precedes equity valuation where cashflows (dividends) are not \"known\" per se: Williams and onward allow for forecasting assumptions - based on historic ratios or published policy - as to these, and cashflows are then treated as deterministic. (See comments under Financial modeling#Accounting, as well as Corporate finance#Quantifying uncertainty.)\n\nIt will be noted that these \"certainty\" results are all commonly employed under corporate finance; uncertainty is the focus of \"asset pricing models\", as follows.\nUncertainty\n\nFor \"choice under uncertainty\", the twin assumptions of rationality and market efficiency lead to modern portfolio theory (MPT) with its Capital asset pricing model (CAPM) — an equilibrium-based result — and to the Black–Scholes–Merton theory (BSM; often, simply Black-Scholes) for option pricing — an arbitrage-free result.\n\nBriefly, and intuitively - and consistent with #Arbitrage-free pricing and equilibrium above - the linkage is as follows.[15] Given the ability to profit from private information, self-interested traders are motivated to acquire and act on their private information. In doing so, traders contribute to more and more efficient market prices: the Efficient Market Hypothesis (EMH). The EMH (implicitly) assumes that average expectations constitute an \"optimal forecast\", i.e. prices using all available information, are identical to the best guess of the future: the assumption of rational expectations. (Note that the EMH does allow that when faced with new information, some investors may overreact and some may underreact, but what is required, however, is that investors' reactions follow a normal distribution - so that the net effect on market prices cannot be reliably exploited to make an abnormal profit.) In the competitive limit, market prices will reflect all available information and prices can only move in response to news:[16] the random walk hypothesis. Thus, if prices of financial assets are (broadly) correct, i.e. efficient, then deviations from these (equilibrium) values could not last for long.\n\nUnder these conditions investors can then be assumed to act rationally: their investment decision must be calculated or a loss is sure to follow. Also, where an arbitrage opportunity presents itself, then investors will exploit it, reinforcing this equilibrium. Here, as under the certainty-case above, the specific assumption as to pricing is that prices are calculated as the present value of expected future dividends,[13][16] as based on currently available information. What is required though is a theory for determining the appropriate discount rate given this uncertainty: this is provided by the MPT and its CAPM. Relatedly, rationality — in the sense of arbitrage-exploitation — gives rise to Black-Scholes; option values here ultimately consistent with the CAPM.\n\nIn general, then, while portfolio theory studies how investors should balance risk and return when investing in many assets or securities, the CAPM is more focused, describing how, in equilibrium, markets set the prices of assets in relation to how risky they are. The theory demonstrates that if one can construct an efficient frontier — i.e. each combination of assets offering the best possible expected level of return for its level of risk, see diagram — then mean-variance efficient portfolios can be formed simply as a combination of holdings of the risk-free asset and the \"market portfolio\" (the Mutual fund separation theorem), with the combinations here plotting as the capital market line, or CML. Then given this CML, the theory shows that the required return on risky securities will be independent of the investor's utility function, and solely determined by their covariance with aggregate risk (“beta”). As seen in the formula aside, this result is consistent with the preceding, equaling the riskless return plus an adjustment for risk.[13] Note an important implication here: under the assumptions of MPT, an asset may be priced without reference to the investor's level of risk aversion, thus providing a readily determined discount rate for corporate finance decision makers as above.[17] (The efficient frontier was introduced by Harry Markowitz. The CAPM was derived by Jack Treynor (1961, 1962), William F. Sharpe (1964), John Lintner (1965) and Jan Mossin (1966) independently.)\n\nBlack-Scholes provides a mathematical model of a financial market containing derivative instruments, and the resultant formula for the price of European-styled options. The model is expressed as the Black–Scholes equation, a partial differential equation describing the changing price of the option over time; it is derived assuming log-normal, geometric Brownian motion. The key financial insight behind the equation is that one can perfectly hedge the option by buying and selling the underlying asset in just the right way and consequently \"eliminate risk\", absenting the risk adjustment from the pricing ( V {\\displaystyle V} V, the value, or price, of the option, grows at r {\\displaystyle r} r, the risk-free rate).[11][13] This hedge, in turn, implies that there is only one right price — in an arbitrage-free sense — for the option. And this price is returned by the Black–Scholes option pricing formula. (The formula, and hence the price, is consistent with the equation, since the formula is the solution to the equation.) Since the formula is without reference to the share's expected return, Black-Scholes entails (assumes) risk neutrality, consistent with the \"elimination of risk\" here. Relatedly, therefore, the pricing formula may also be derived directly via risk neutral expectation; see Brownian model of financial markets. (BSM is consistent with \"previous versions of the formula\" of Louis Bachelier and Edward O. Thorp.[18])\n\nAs mentioned, it can be shown that the two models are consistent; then, as is to be expected, \"classical\" financial economics is thus unified. Here, the Black Scholes equation may alternatively be derived from the CAPM, and the price obtained from the Black-Scholes model is thus consistent with the expected return from the CAPM.[19] The Black-Scholes theory, although built on Arbitrage-free pricing, is therefore consistent with the equilibrium based capital asset pricing. Both models, in turn, are ultimately consistent with the Arrow-Debreu theory, and may be derived via state-pricing,[11] further explaining, and if required demonstrating, this unity.\nExtensions\n\nMore recent work further generalizes and / or extends these models.\nPortfolio theory\n\n    See also: Post-modern portfolio theory; Mathematical finance#Risk and portfolio management: the P world.\n\nMulti-factor models such as the Fama–French three-factor model and the Carhart four-factor model, propose factors other than market return as relevant in pricing. The Intertemporal CAPM and Consumption-based CAPM similarly extend the basic model, respectively incorporating intertemporal portfolio choice and consumption (in the economic sense). The single-index model is a more simple asset pricing model than the CAPM. It assumes, only, a correlation between security and market returns, without (numerous) other economic assumptions. It is useful in that it simplifies the estimation of correlation between securities, significantly reducing the inputs for building the correlation matrix required for portfolio optimization.\n\nThe Black–Litterman model starts with the equilibrium assumption, which is then modified to take into account the 'views' (i.e., the specific opinions about asset returns) of the investor in question to arrive at a bespoke asset allocation. The arbitrage pricing theory returns the required (expected) return of a financial asset, modeled as a linear function of various macro-economic factors, and specifying how arbitrage should bring incorrectly priced assets back into line.\nDerivative pricing\n\n    See also: Mathematical finance#Derivatives pricing: the Q world.\n\nAs regards derivative pricing, the binomial options pricing model provides a discretized version of Black-Scholes, useful for the valuation of American styled options; discretized models of this type are built using state-prices (as above), while exotic derivatives although modeled in continuous time via Monte Carlo methods for option pricing are also priced using risk neutral expected value. Various other numeric techniques have also been developed. The theoretical framework too has been extended such that martingale pricing is now the standard approach. Since the work of Breeden and Litzenberger in 1978,[20] a large number of researchers have used options to extract Arrow–Debreu prices for a variety of applications in financial economics. Developments relating to complexities in return and / or volatility are discussed below.\n\nDrawing on these techniques, derivative models for various other underlyings and applications have also been developed, all based on the same logic. Real options valuation allows that option holders can influence the option's underlying; models for employee stock option valuation explicitly assume non-rationality on the part of option holders; Credit derivatives allow that payment obligations / delivery requirements might not be honored. Exotic derivatives are now routinely valued.\n\nSimilarly, beginning with Oldrich Vasicek, various short rate models, as well as the HJM and BGM forward rate-based techniques, allow for an extension of these to fixed income- and interest rate derivatives. (The Vasicek and CIR models are equilibrium-based, while Ho–Lee and subsequent models are based on arbitrage-free pricing.) Bond valuation is relatedly extended: the Arbitrage-free pricing approach discounts each cashflow at the market derived rate as opposed to an overall rate as above; the Stochastic calculus approach allows for rates that are \"random\" (while returning a price that is similarly arbitrage free); Lattice models for Hybrid Securities allow for non-deterministic cashflows (and stochastic rates).\n\nAs above, (OTC) derivative pricing has relied on the BSM risk neutral pricing framework, under the assumptions of funding at the risk free rate and the ability to perfectly replicate derivatives so as to fully hedge. Post the financial crisis of 2008 issues such as counterparty credit risk, funding costs and costs of capital are considered in the valuation.[21] The purpose of these calculations is twofold: primarily to hedge for possible losses due to counterparty default; but also, to determine (and hedge) the amount of capital required under Basel III. There are two (emergent) techniques. When the deal is collateralized then the “fair-value” is computed as before, but using the Overnight Index Swap (OIS) curve for discounting. (The OIS is chosen here as it reflects the rate for overnight unsecured lending between banks, and is thus considered a good indicator of the interbank credit markets.) When the deal is not collateralized then a CVA (Credit Valuation Adjustment) is added to this value. [22] The CVA reflects the market value of counterparty credit risk, while other Valuation Adjustments for Debit, Funding, regulatory Capital and Margin (DVA, FVA, KVA and MVA) may also be added. These are collectively referred to as “XVA”.\n\nSwap pricing is relatedly and further modified. Previously, swaps were valued off a single interest rate curve such that the two legs (fixed and floating for interest rate swaps) had the same value at initiation; see further under Rational pricing. Post crisis, to accommodate credit risk, valuation is now under a two-curve framework, where one curve (OIS) is used for discounting, and a curve that matches the maturity of the underlying floating rate is used for the projection of forward rates. Further, since the basis spread between LIBOR rates of different maturities widened during the crisis, forecast curves are often constructed for each LIBOR tenor used in floating rate derivative legs. This approach is then referred to as a “multi-curve” framework.[23] Currency basis will require additional curves.\nCorporate finance theory\n\nCorporate finance theory has also been extended: mirroring the above developments, asset-valuation and decisioning no longer need assume \"certainty\". As discussed above, Monte Carlo methods in finance, introduced by David B. Hertz in 1964, allow financial analysts to construct \"stochastic\" or probabilistic corporate finance models, as opposed to the traditional static and deterministic models; see Corporate finance#Quantifying uncertainty. Relatedly, Real Options theory, as discussed, allows for owner - i.e. managerial - actions that impact underlying value; by incorporating option pricing logic, these actions are then applied to a distribution of future outcomes, changing with time, which then determine the \"project's\" valuation today.\n\nOther extensions here include [24] agency theory, which analyses the difficulties in motivating corporate management (the \"agent\") to act in the best interests of shareholders (the \"principal\"), rather than in their own interests. Clean surplus accounting and the related residual income valuation provide a model that returns price as a function of earnings, expected returns, and change in book value, as opposed to dividends. This approach, to some extent, arises due to the implicit contradiction of seeing value as a function of dividends, while also holding that dividend policy cannot influence value per Modigliani and Miller’s “Irrelevance principle”; see Dividend policy#Irrelevance of dividend policy.\nChallenges and criticism\n\n    See also: Capital asset pricing model#Problems of CAPM; Modern portfolio theory#Criticisms; Black–Scholes model#Criticism; Financial mathematics#Criticism; Efficient-market hypothesis#Criticism and behavioral finance; Rational expectations#Criticisms; as well as.[25]\n\nAs above, there is a very close link between the random walk hypothesis, with the associated expectation that price changes should follow a normal distribution, on the one hand, and market efficiency and rational expectations, on the other. Note, however, that (wide) departures from these are commonly observed, and there are thus, respectively, two main sets of challenges presented here.\nDepartures from normality\nImplied volatility surface. The Z-axis represents implied volatility in percent, and X and Y axes represent the option delta, and the days to maturity.\n\nThe first set of challenges: As discussed, the assumptions that market prices follow a random walk and / or that asset returns are normally distributed are fundamental. Empirical evidence, however, suggests that these assumptions may not hold (see Kurtosis risk, Skewness risk, Long tail) and that in practice, traders, analysts and particularly risk managers frequently modify the \"standard models\" (see Model risk). In fact, Benoît Mandelbrot had discovered already in the 1960s that changes in financial prices do not follow a Gaussian distribution, the basis for much option pricing theory, although this observation was slow to find its way into mainstream financial economics.\n\nFinancial models with long-tailed distributions and volatility clustering have been introduced to overcome problems with the realism of the above “classical” financial models; while jump diffusion models allow for (option) pricing incorporating \"jumps\" in the spot price. Risk managers, similarly, complement (or substitute) the standard value at risk models with historical simulations, mixture models, principal component analysis, extreme value theory, as well as models for volatility clustering.[26] For further discussion see Fat-tailed distribution#Applications in economics, and Value at risk#Criticism.\n\nClosely related is the volatility smile, where implied volatility is observed to differ as a function of strike price (i.e. moneyness), true only if the price-change distribution is non-normal, unlike that assumed by BSM. The term structure of volatility describes how (implied) volatility differs for related options with different maturities. An implied volatility surface is then a three-dimensional surface plot of volatility smile and term structure. These empirical phenomena negate the assumption of constant volatility — and log-normality— upon which Black-Scholes is built;[18] see Black–Scholes model#The volatility smile.\n\nApproaches developed here in response include local volatility and stochastic volatility (the Heston, SABR and CEV models, amongst others). Alternatively, implied-binomial and -trinomial trees instead of directly modelling volatility, return a lattice consistent with — in an arbitrage-free sense — (all) observed prices, facilitating the pricing of other, i.e. non-quoted, strike/maturity combinations. Edgeworth binomial trees allow for a specified (i.e. non-Gaussian) skew and kurtosis in the spot price. Priced here, options with differing strikes will return differing implied volatilities, and the tree can thus be calibrated to the smile if required.[27] Similarly purposed closed-form models include: Jarrow and Rudd (1982); Corrado and Su (1996); Backus, Foresi, and Wu (2004).[28]\n\nAs above, additional to log-normality in returns, BSM assumes the ability to perfectly replicate derivatives so as to fully hedge, and hence to discount at the risk-free rate. This, in turn, is built on the assumption of a credit-risk-free environment. The post crisis reality, however, differs, necessitating the various XVA adjustments to the derivative valuation, as described. Note that these adjustments are additional to any smile or surface effect. This is valid as the smile is built on price data relating to fully collateralized positions, and there is therefore no \"double counting\" of credit risk (etc.) when including XVA.\nDepartures from rationality\nMarket anomalies and Economic puzzles\n\n    Calendar effect\n        January effect\n        Santa Claus rally\n        Sell in May\n    Closed-end fund puzzle\n    Dividend puzzle\n    Equity home bias puzzle\n    Equity premium puzzle\n    Forward premium anomaly\n    Low-volatility anomaly\n    Post-earnings-announcement drift\n    Real exchange-rate puzzles\n\n    See also Financial Modelers' Manifesto; Physics envy; Unreasonable ineffectiveness of mathematics#Economics and finance.\n\nThe second set of challenges: As seen, a common assumption is that financial decision makers act rationally; see Homo economicus. Recently, however, researchers in experimental economics and experimental finance have challenged this assumption empirically. These assumptions are also challenged theoretically, by behavioral finance, a discipline primarily concerned with the limits to rationality of economic agents.\n\nConsistent with, and complementary to these findings, various persistent market anomalies have been documented, these being price and/or return distortions - e.g. size premiums - which appear to contradict the efficient-market hypothesis; calendar effects are the best known group here. Related to these are various of the economic puzzles, concerning phenomena similarly contradicting the theory. The equity premium puzzle, as one example, arises in that the difference between the observed returns on stocks as compared to government bonds is consistently higher than the risk premium rational equity investors should demand, an \"abnormal return\". For further context see Random walk hypothesis#A non-random walk hypothesis, and sidebar for specific instances.\n\nMore generally, and particularly following the financial crisis of 2007–2010, financial economics and mathematical finance have been subjected to deeper criticism; notable here is Nassim Nicholas Taleb, who claims that the prices of financial assets cannot be characterized by the simple models currently in use, rendering much of current practice at best irrelevant, and, at worst, dangerously misleading; see Black swan theory, Taleb distribution. A topic of general interest studied in recent years has thus been financial crises,[29] and the failure of financial economics to model these.\n\nAreas of research attempting to explain (or at least model) these phenomena, and crises, include noise trading, market microstructure, and Heterogeneous agent models. The latter is extended to agent-based computational economics, where price is treated as an emergent phenomenon, resulting from the interaction of the various market participants (agents). The noisy market hypothesis argues that prices can be influenced by speculators and momentum traders, as well as by insiders and institutions that often buy and sell stocks for reasons unrelated to fundamental value; see Noise (economic). The adaptive market hypothesis is an attempt to reconcile the efficient market hypothesis with behavioral economics, by applying the principles of evolution to financial interactions. An information cascade, alternatively, shows market participants engaging in the same acts as others (\"herd behavior\"), despite contradictions with their private information. See also George Soros' approach, #Reflexivity, financial markets, and economic theory, as well as Hyman Minsky's \"financial instability hypothesis\".\n\nNote however, that on the obverse, various studies have shown that despite these departures from efficiency, asset prices do typically exhibit a random walk and that one cannot therefore consistently outperform market averages.[30] The practical implication, therefore, is that passive investing (e.g. via low-cost index funds) should, on average, serve better than any other active strategy.[31] Burton Malkiel's A Random Walk Down Wall Street - first published in 1973, and in its 12th edition as of 2015 - is a widely read popularization of these arguments. (See also John C. Bogle’s Common Sense on Mutual Funds; but compare Warren Buffett's The Superinvestors of Graham-and-Doddsville.) Note also that institutionally inherent limits to arbitrage — as opposed to factors directly contradictory to the theory — are sometimes proposed as an explanation for these departures from efficiency.\nSee also\nBook icon \t\n\n    Book: Finance\n\n    Category:Finance theories\n    Category:Financial economists\n    Deutsche Bank Prize in Financial Economics\n    Financial modeling\n    Fischer Black Prize\n    List of unsolved problems in economics#Financial economics\n    Outline of economics\n    Outline of finance", "skillName": "Financial_economics."}
{"id": 27, "category": "capital_market", "skillText": "Finance capitalism or financial capitalism is the subordination of processes of production to the accumulation of money profits in a financial system.[1]\n\nFinancial capitalism is thus a form of capitalism where the intermediation of saving to investment becomes a dominant function in the economy, with wider implications for the political process and social evolution:[2] since the late 20th century it has become the predominant force in the global economy,[3] whether in neoliberal or other form.[4]\n\nContents\n\n    1 Characteristics\n        1.1 Social implications\n    2 Historical developments\n    3 Opponents\n    4 See also\n    5 References\n    6 Further reading\n    7 External links\n\nCharacteristics\n\nFinance capitalism is characterized by a predominance of the pursuit of profit from the purchase and sale of, or investment in, currencies and financial products such as bonds, stocks, futures and other derivatives. It also includes the lending of money at interest; and is seen by Marxist analysts (from whom the term finance capitalism originally derived) as being exploitative by supplying income to non-laborers.[5] Academic defenders of the economic concept of capitalism, such as Eugen von Böhm-Bawerk, see such profits as part of the roundabout process by which it grows and hedges against inevitable risks.[6]\n\nIn financial capitalism, financial intermediaries become large concerns, ranging from banks to investment firms. Where deposit banks attract savings and lend out money, while investment banks obtain funds on the interbank market to re-lend for investment purposes, investment firms, by comparison, act on behalf of other concerns, by selling their equities or securities to investors, for investment purposes.[7]\nSocial implications\n\nThe meaning of the term financial capitalism goes beyond the importance of financial intermediation in the modern capitalist economy. It also encompasses the significant influence of the wealth holders on the political process and the aims of economic policy.[8]\n\nThomas Palley has argued that the 21st century predominance of finance capital has led to a preference for speculation – Casino Capitalism – over investment for entrepreneurial growth in the global economy.[9]\nHistorical developments\n\nRudolf Hilferding is credited with first bringing the term finance capitalism into prominence, with his (1910) study of the links between German trusts, banks, and monopolies before World War I – a study subsumed by Lenin into his wartime analysis of the imperialist relations of the great world powers.[10] Lenin concluded of the banks at that time that they were “the chief nerve centres of the whole capitalist system of national economy”:[11] for the Comintern, the phrase \"dictatorship of finance capitalism\"[12] became a regular one.\n\nIn such a traditional Marxist perspective, finance capitalism is seen as a dialectical outgrowth of industrial capitalism, and part of the process by which the whole capitalist phase of history comes to an end. In a fashion similar to the views of Thorstein Veblen, finance capitalism is contrasted with industrial capitalism, where profit is made from the manufacture of goods.\n\nBraudel would later point to two earlier periods when finance capitalism had emerged in human history – with the Genoese in the 16th century and the Dutch in the 17th and 18th centuries – although at those points it was from commercial capitalism that it developed.[13] Giovanni Arrighi extended Braudel's analysis to suggest that a predominance of finance capitalism is a recurring, long-term phenomenon, whenever a previous phase of commercial/industrial capitalist expansion reaches a plateau.[14]\n\nWhereas by mid-century the industrial corporation had displaced the banking system as the prime economic symbol of success,[15] the late twentieth-century growth of derivatives and of a novel banking model[16] ushered in a new (and historically fourth) period of finance capitalism.[17]\n\nFredric Jameson has seen the globalised abstractions of this current phase of financial capitalism as underpinning the cultural manifestations of postmodernism.[18]\nOpponents\n\nFascists were vocal in their opposition to finance capitalism.[19]\n\nC. H. Sisson saw the underlying theme of The Cantos of Ezra Pound as the depredations of finance capital: “the monstrous aberration of a world in which reality is distorted, down to a detail never so comprehensively implicated before, by the pull of a fictitious money”.[20]\n\nGottfried Feder opposed financial capitalism and the concentration of capital by bankers in Germany.\nSee also\n\n    Corporatism\n    Economics of fascism\n    Financialization\n    Financial capital\n    Financial Instability Hypothesis\n    Late capitalism\n    Late modernity\n    Post-Fordism\n    Speculation", "skillName": "FinanceCapitalism."}
{"id": 28, "category": "capital_market", "skillText": "Capital markets are financial markets for the buying and selling of long-term debt or equity-backed securities. These markets channel the wealth of savers to those who can put it to long-term productive use, such as companies/ governments making long-term investments.[a] Capital markets are defined as markets in which money is provided for periods longer than a year.[1] Financial regulators, such as the UK's Bank of England (BoE) or the U.S. Securities and Exchange Commission (SEC), oversee the capital markets in their jurisdictions to protect investors against fraud, among other duties.\n\nModern capital markets are almost invariably hosted on computer-based electronic trading systems; most can be accessed only by entities within the financial sector or the treasury departments of governments and corporations, but some can be accessed directly by the public.[b] There are many thousands of such systems, most serving only small parts of the overall capital markets. Entities hosting the systems include stock exchanges, investment banks, and government departments. Physically the systems are hosted all over the world, though they tend to be concentrated in financial centres like London, New York, and Hong Kong.\n\nA key division within the capital markets is between the primary markets and secondary markets. In primary markets, new stock or bond issues are sold to investors, often via a mechanism known as underwriting. The main entities seeking to raise long-term funds on the primary capital markets are governments (which may be municipal, local or national) and business enterprises (companies). Governments issue only bonds, whereas companies often issue either equity or bonds. The main entities purchasing the bonds or stock include pension funds, hedge funds, sovereign wealth funds, and less commonly wealthy individuals and investment banks trading on their own behalf. In the secondary markets, existing securities are sold and bought among investors or traders, usually on an exchange, over-the-counter, or elsewhere. The existence of secondary markets increases the willingness of investors in primary markets, as they know they are likely to be able to swiftly cash out their investments if the need arises.[2]\n\nA second important division falls between the stock markets (for equity securities, also known as shares, where investors acquire ownership of companies) and the bond markets (where investors become creditors).[2]\n\nContents\n\n    1 Difference between money markets and capital markets\n    2 Difference between regular bank lending and capital markets\n    3 Examples of capital market transactions\n        3.1 A government raising money on the primary markets\n        3.2 A company raising money on the primary markets\n        3.3 Trading on the secondary markets\n    4 Size of the global capital markets\n    5 Forecasting and analyses\n    6 Capital controls\n    7 See also\n    8 Notes\n    9 References\n\nDifference between money markets and capital markets\nFinance\nCDS volume outstanding.png  \nMarkets\n\n    Bond Commodity Derivatives Foreign exchange Money Over-the-counter Private equity Real estate Spot Stock \n\nParticipants\n\n    Investor\n        institutional Retail Speculator \n\nInstruments\n\n    Cash Credit line Deposit Derivative Futures contract Loan \n\n    Option (call exotic put) \n\n    Security Stock \n\nTime deposit\n(certificate of deposit)\nCorporate\n\n        Accounting Audit Capital budgeting \n    Credit rating agency\n        Risk management Financial statement \n    Leveraged buyout\n    Mergers and acquisitions\n        Structured finance Venture capital \n\nPersonal\n\n    Credit / Debt\n    Employment contract\n    Financial planning\n        Retirement Student loan \n\nPublic\nGovernment spending\n\n    Final consumption expenditure\n        Operations Redistribution \n    Transfer payment\n\nGovernment revenue\n\n        Taxation Deficit spending \n        Budget (balance) Debt \n    Non-tax revenue\n    Warrant of payment\n\nBanks and banking\n\n    Central bank Deposit account Fractional-reserve banking Loan Money supply \n\n    Lists of banks\n\nRegulation · Standards\n\n    Bank regulation\n    Basel Accords\n    International Financial Reporting Standards\n    ISO 31000\n    Professional certification\n    Fund governance\n    Accounting scandals\n\nEconomic history\n\n    Private equity and venture capital\n    Recession\n    Stock market bubble\n    Stock market crash\n\n    v t e \n\nThe money markets are used for the raising of short term finance, sometimes for loans that are expected to be paid back as early as overnight. Whereas the capital markets are used for the raising of long term finance, such as the purchase of shares, or for loans that are not expected to be fully paid back for at least a year.[1]\n\nFunds borrowed from the money markets are typically used for general operating expenses, to cover brief periods of liquidity. For example, a company may have inbound payments from customers that have not yet cleared, but may wish to immediately pay out cash for its payroll. When a company borrows from the primary capital markets, often the purpose is to invest in additional physical capital goods, which will be used to help increase its income. It can take many months or years before the investment generates sufficient return to pay back its cost, and hence the finance is long term.[2]\n\nTogether, money markets and capital markets form the financial markets as the term is narrowly understood.[c] The capital market is concerned with long term finance. In the widest sense, it consists of a series of channels through which the savings of the community are made available for industrial and commercial enterprises and public authorities.\nDifference between regular bank lending and capital markets\n\nRegular bank lending is not usually classed as a capital market transaction, even when loans are extended for a period longer than a year. A key difference is that with a regular bank loan, the lending is not securitised (i.e., it doesn't take the form of resalable security like a share or bond that can be traded on the markets). A second difference is that lending from banks and similar institutions is more heavily regulated than capital market lending. A third difference is that bank depositors and shareholders tend to be more risk averse than capital market investors. The previous three differences all act to limit institutional lending as a source of finance. Two additional differences, this time favoring lending by banks, are that banks are more accessible for small and medium companies, and that they have the ability to create money as they lend. In the 20th century, most company finance apart from share issues was raised by bank loans. But since about 1980 there has been an ongoing trend for disintermediation, where large and credit worthy companies have found they effectively have to pay out less in interest if they borrow direct from capital markets rather than banks. The tendency for companies to borrow from capital markets instead of banks has been especially strong in the US. According to Lena Komileva writing for The Financial Times, Capital Markets overtook bank lending as the leading source of long term finance in 2009 – this reflects the additional risk aversion and regulation of banks following the 2008 financial crisis.[3]\nExamples of capital market transactions\nA government raising money on the primary markets\nOne Churchill Place, Barclays' HQ in Canary Wharf, London. Barclays is a major player in the world's primary and secondary bond markets.\n\nWhen a government wants to raise long term finance it will often sell bonds to the capital markets. In the 20th and early 21st century, many governments would use investment banks to organize the sale of their bonds. The leading bank would underwrite the bonds, and would often head up a syndicate of brokers, some of whom might be based in other investment banks. The syndicate would then sell to various investors. For developing countries, a multilateral development bank would sometimes provide an additional layer of underwriting, resulting in risk being shared between the investment bank(s), the multilateral organization, and the end investors. However, since 1997 it has been increasingly common for governments of the larger nations to bypass investment banks by making their bonds directly available for purchase over the Internet. Many governments now sell most of their bonds by computerized auction. Typically large volumes are put up for sale in one go; a government may only hold a small number of auctions each year. Some governments will also sell a continuous stream of bonds through other channels. The biggest single seller of debt is the US Government; there are usually several transactions for such sales every second,[d] which corresponds to the continuous updating of the US real time debt clock.[4][5][6]\nA company raising money on the primary markets\n\nWhen a company wants to raise money for long-term investment, one of its first decisions is whether to do so by issuing bonds or shares. If it chooses shares, it avoids increasing its debt, and in some cases the new shareholders may also provide non monetary help, such as expertise or useful contacts. On the other hand, a new issue of shares can dilute the ownership rights of the existing shareholders, and if they gain a controlling interest, the new shareholders may even replace senior managers. From an investor's point of view, shares offer the potential for higher returns and capital gains if the company does well. Conversely, bonds are safer if the company does poorly, as they are less prone to severe falls in price, and in the event of bankruptcy, bond owners are usually paid before shareholders.\n\nWhen a company raises finance from the primary market, the process is more likely to involve face-to-face meetings than other capital market transactions. Whether they choose to issue bonds or shares,[e] companies will typically enlist the services of an investment bank to mediate between themselves and the market. A team from the investment bank often meets with the company's senior managers to ensure their plans are sound. The bank then acts as an underwriter, and will arrange for a network of brokers to sell the bonds or shares to investors. This second stage is usually done mostly through computerized systems, though brokers will often phone up their favored clients to advise them of the opportunity. Companies can avoid paying fees to investment banks by using a direct public offering, though this is not a common practice as it incurs other legal costs and can take up considerable management time.[4][7]\nTrading on the secondary markets\nAn Electronic trading platform being used at the Deutsche Börse. Most 21st century capital market transactions are executed electronically, sometimes a human operator is involved, and sometimes unattended computer systems execute the transactions, as happens in algorithmic trading.\n\nMost capital market transactions take place on the secondary market. On the primary market, each security can be sold only once, and the process to create batches of new shares or bonds is often lengthy due to regulatory requirements. On the secondary markets, there is no limit on the number of times a security can be traded, and the process is usually very quick. With the rise of strategies such as high-frequency trading, a single security could in theory be traded thousands of times within a single hour.[f] Transactions on the secondary market don't directly help raise finance, but they do make it easier for companies and governments to raise finance on the primary market, as investors know if they want to get their money back in a hurry, they will usually be easily able to re-sell their securities. Sometimes however secondary capital market transactions can have a negative effect on the primary borrowers – for example, if a large proportion of investors try to sell their bonds, this can push up the yields for future issues from the same entity. An extreme example occurred shortly after Bill Clinton began his first term as President of the United States; Clinton was forced to abandon some of the spending increases he'd promised in his election campaign due to pressure from the bond markets. In the 21st century, several governments have tried to lock in as much as possible of their borrowing into long dated bonds, so they are less vulnerable to pressure from the markets. Following the financial crisis of 2007–08, the introduction of Quantitative easing further reduced the ability of private actors to push up the yields of government bonds, at least for countries with a Central bank able to engage in substantial Open market operations.[4][6][7][8]\n\nA variety of different players are active in the secondary markets. Regular individuals account for a small proportion of trading, though their share has slightly increased; in the 20th century it was mostly only a few wealthy individuals who could afford an account with a broker, but accounts are now much cheaper and accessible over the internet. There are now numerous small traders who can buy and sell on the secondary markets using platforms provided by brokers which are accessible via web browsers. When such an individual trades on the capital markets, it will often involve a two-stage transaction. First they place an order with their broker, then the broker executes the trade. If the trade can be done on an exchange, the process will often be fully automated. If a dealer needs to manually intervene, this will often mean a larger fee. Traders in investment banks will often make deals on their bank's behalf, as well as executing trades for their clients. Investment banks will often have a division (or department) called capital markets: staff in this division try to keep aware of the various opportunities in both the primary and secondary markets, and will advise major clients accordingly. Pension and sovereign wealth funds tend to have the largest holdings, though they tend to buy only the highest grade (safest) types of bonds and shares, and often don't trade all that frequently. According to a 2012 Financial Times article, hedge funds are increasingly making most of the short term trades in large sections of the capital market (like the UK and US stock exchanges), which is making it harder for them to maintain their historically high returns, as they are increasingly finding themselves trading with each other rather than with less sophisticated investors.[4][6][7][9]\n\nThere are several ways to invest in the secondary market without directly buying shares or bonds. A common method is to invest in mutual funds[g] or exchange-traded funds. It's also possible to buy and sell derivatives that are based on the secondary market; one of the most common being contract for difference – these can provide rapid profits, but can also cause buyers to lose more money than they originally invested.[4]\nSize of the global capital markets\n\nAll figures given are in Billions of US$ and are sourced to the IMF. There is no universally recognized standard for measuring all of these figures, so other estimates may vary. A GDP column is included as a comparison.\nYear[10] \tStocks \tBonds \tBank assets[11] \tTotal of stocks, bonds and bank assets.[12] \tWorld GDP\n2013[13] \t62,552.00 \t99,788.80 \t120,421.60 \t282,762.40 \t74,699.30\n2012[14] \t52494.90 \t99,134.20 \t116,956.10 \t268,585.20 \t72,216.40\n2011[15] \t47,089.23 \t98,388.10 \t110,378.24 \t255,855.57 \t69,899.22\nForecasting and analyses\n\nA great deal of work goes into analysing capital markets and predicting their future movements. This includes academic study; work from within the financial industry for the purposes of making money and reducing risk; and work by governments and multilateral institutions for the purposes of regulation and in understanding the impact of capital markets on the wider economy. Methods range from the gut instincts of experienced traders, to various forms of stochastic calculus and algorithms such as Stratonovich-Kalman-Bucy filtering.[16][17][18]\nCapital controls\nMain article: Capital control\n\nCapital controls are measures imposed by a state's government aimed at managing capital account transactions – in other words, capital market transactions where one of the counter-parties[h] involved is in a foreign country. Whereas domestic regulatory authorities try to ensure that capital market participants trade fairly with each other, and sometimes to ensure institutions like banks don't take excessive risks, capital controls aim to ensure that the macroeconomic effects of the capital markets don't have a net negative impact on the nation in question. Most advanced nations like to use capital controls sparingly if at all, as in theory allowing markets freedom is a win-win situation for all involved: investors are free to seek maximum returns, and countries can benefit from investments that will develop their industry and infrastructure. However, sometimes capital market transactions can have a net negative effect – for example, in a financial crisis, there can be a mass withdrawal of capital, leaving a nation without sufficient foreign currency to pay for needed imports. On the other hand, if too much capital is flowing into a country, it can push up inflation and the value of the nation's currency, making its exports uncompetitive. Some nations such as India have also used capital controls to ensure that their citizens' money is invested at home, rather than abroad.[19]", "skillName": "CapitalMarket."}
{"id": 29, "category": "capital_market", "skillText": "Mathematical finance, also known as quantitative finance, is a field of applied mathematics, concerned with financial markets. Generally, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock (see: Valuation of options; Financial modeling). The fundamental theorem of arbitrage-free pricing is one of the key theorems in mathematical finance, while the Black–Scholes equation and formula are amongst the key results.[1]\n\nMathematical finance also overlaps heavily with the fields of computational finance and financial engineering. The latter focuses on applications and modeling, often by help of stochastic asset models (see: Quantitative analyst), while the former focuses, in addition to analysis, on building tools of implementation for the models. In general, there exist two separate branches of finance that require advanced quantitative techniques: derivatives pricing on the one hand, and risk- and portfolio management on the other.[2]\n\nMany universities offer degree and research programs in mathematical finance; see Master of Mathematical Finance.\n\nContents\n\n    1 History: Q versus P\n        1.1 Derivatives pricing: the Q world\n        1.2 Risk and portfolio management: the P world\n    2 Criticism\n    3 Mathematical finance articles\n        3.1 Mathematical tools\n        3.2 Derivatives pricing\n    4 See also\n    5 Notes\n    6 References\n\nHistory: Q versus P\n\nThere exist two separate branches of finance that require advanced quantitative techniques: derivatives pricing, and risk and portfolio management. One of the main differences is that they use different probabilities, namely the risk-neutral probability (or arbitrage-pricing probability), denoted by \"Q\", and the actual (or actuarial) probability, denoted by \"P\".\nDerivatives pricing: the Q world\nThe Q world Goal \t\"extrapolate the present\"\nEnvironment \trisk-neutral probability Q {\\displaystyle \\mathbb {Q} } \\mathbb {Q}\nProcesses \tcontinuous-time martingales\nDimension \tlow\nTools \tItō calculus, PDEs\nChallenges \tcalibration\nBusiness \tsell-side\nMain article: Risk-neutral measure\nFurther information: Black–Scholes model, Brownian model of financial markets, and Martingale pricing\n\nThe goal of derivatives pricing is to determine the fair price of a given security in terms of more liquid securities whose price is determined by the law of supply and demand. The meaning of \"fair\" depends, of course, on whether one considers buying or selling the security. Examples of securities being priced are plain vanilla and exotic options, convertible bonds, etc.\n\nOnce a fair price has been determined, the sell-side trader can make a market on the security. Therefore, derivatives pricing is a complex \"extrapolation\" exercise to define the current market value of a security, which is then used by the sell-side community. Quantitative derivatives pricing was initiated by Louis Bachelier in The Theory of Speculation (published 1900), with the introduction of the most basic and most influential of processes, the Brownian motion, and its applications to the pricing of options. Bachelier modeled the time series of changes in the logarithm of stock prices as a random walk in which the short-term changes had a finite variance. This causes longer-term changes to follow a Gaussian distribution.[3]\n\nThe theory remained dormant until Fischer Black and Myron Scholes, along with fundamental contributions by Robert C. Merton, applied the second most influential process, the geometric Brownian motion, to option pricing. For this M. Scholes and R. Merton were awarded the 1997 Nobel Memorial Prize in Economic Sciences. Black was ineligible for the prize because of his death in 1995.[4]\n\nThe next important step was the fundamental theorem of asset pricing by Harrison and Pliska (1981), according to which the suitably normalized current price P0 of a security is arbitrage-free, and thus truly fair, only if there exists a stochastic process Pt with constant expected value which describes its future evolution:[5]\n\n    P 0 = E 0 ( P t ) {\\displaystyle P_{0}=\\mathbf {E} _{0}(P_{t})} P_{{0}}={\\mathbf {E}}_{{0}}(P_{{t}})\n    \t\n\n     \n    \t\n\n     \n    \t\n\n     \n\n     \n    \t\n\n    (1 )\n\nA process satisfying (1) is called a \"martingale\". A martingale does not reward risk. Thus the probability of the normalized security price process is called \"risk-neutral\" and is typically denoted by the blackboard font letter \" Q {\\displaystyle \\mathbb {Q} } \\mathbb {Q} \".\n\nThe relationship (1) must hold for all times t: therefore the processes used for derivatives pricing are naturally set in continuous time.\n\nThe quants who operate in the Q world of derivatives pricing are specialists with deep knowledge of the specific products they model.\n\nSecurities are priced individually, and thus the problems in the Q world are low-dimensional in nature. Calibration is one of the main challenges of the Q world: once a continuous-time parametric process has been calibrated to a set of traded securities through a relationship such as (1), a similar relationship is used to define the price of new derivatives.\n\nThe main quantitative tools necessary to handle continuous-time Q-processes are Itō’s stochastic calculus and partial differential equations (PDE’s).\nRisk and portfolio management: the P world\nThe P world Goal \t\"model the future\"\nEnvironment \treal-world probability P {\\displaystyle \\mathbb {P} } \\mathbb {P}\nProcesses \tdiscrete-time series\nDimension \tlarge\nTools \tmultivariate statistics\nChallenges \testimation\nBusiness \tbuy-side\n\nRisk and portfolio management aims at modeling the statistically derived probability distribution of the market prices of all the securities at a given future investment horizon.\nThis \"real\" probability distribution of the market prices is typically denoted by the blackboard font letter \" P {\\displaystyle \\mathbb {P} } \\mathbb {P} \", as opposed to the \"risk-neutral\" probability \" Q {\\displaystyle \\mathbb {Q} } \\mathbb {Q} \" used in derivatives pricing.\nBased on the P distribution, the buy-side community takes decisions on which securities to purchase in order to improve the prospective profit-and-loss profile of their positions considered as a portfolio.\n\nFor their pioneering work, Markowitz and Sharpe, along with Merton Miller, shared the 1990 Nobel Memorial Prize in Economic Sciences, for the first time ever awarded for a work in finance.\n\nThe portfolio-selection work of Markowitz and Sharpe introduced mathematics to investment management. With time, the mathematics has become more sophisticated. Thanks to Robert Merton and Paul Samuelson, one-period models were replaced by continuous time, Brownian-motion models, and the quadratic utility function implicit in mean–variance optimization was replaced by more general increasing, concave utility functions.[6] Furthermore, in more recent years the focus shifted toward estimation risk, i.e., the dangers of incorrectly assuming that advanced time series analysis alone can provide completely accurate estimates of the market parameters.[7]\n\nMuch effort has gone into the study of financial markets and how prices vary with time. Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow Theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of \"technical analysis\" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics.\nCriticism\nSee also: Financial economics #Challenges and criticism and Financial models with long-tailed distributions and volatility clustering\n\nOver the years, increasingly sophisticated mathematical models and derivative pricing strategies have been developed, but their credibility was damaged by the financial crisis of 2007–2010.\nContemporary practice of mathematical finance has been subjected to criticism from figures within the field notably by Paul Wilmott and Nassim Nicholas Taleb, a professor of financial engineering at Polytechnic Institute of New York University, in his book The Black Swan.[8] Taleb claims that the prices of financial assets cannot be characterized by the simple models currently in use, rendering much of current practice at best irrelevant, and, at worst, dangerously misleading. Wilmott and Emanuel Derman published the Financial Modelers' Manifesto in January 2008[9] which addresses some of the most serious concerns.\nBodies such as the Institute for New Economic Thinking are now attempting to develop new theories and methods.[10]\n\nIn general, modeling the changes by distributions with finite variance is, increasingly, said to be inappropriate.[11] In the 1960s it was discovered by Benoît Mandelbrot that changes in prices do not follow a Gaussian distribution, but are rather modeled better by Lévy alpha-stable distributions.[12] The scale of change, or volatility, depends on the length of the time interval to a power a bit more than 1/2. Large changes up or down are more likely than what one would calculate using a Gaussian distribution with an estimated standard deviation. But the problem is that it does not solve the problem as it makes parametrization much harder and risk control less reliable.[8]\nMathematical finance articles\n\n    See also Outline of finance: § Financial mathematics; § Mathematical tools; § Derivatives pricing.\n\nMathematical tools\n\n    Asymptotic analysis\n    Calculus\n    Copulas\n    Differential equations\n    Expected value\n    Ergodic theory\n    Feynman–Kac formula\n    Fourier transform\n    Gaussian copulas\n    Girsanov's theorem\n    Itô's lemma\n    Martingale representation theorem\n    Mathematical models\n\n\t\n\n    Monte Carlo method\n    Numerical analysis\n    Real analysis\n    Partial differential equations\n    Probability\n    Probability distributions\n        Binomial distribution\n        Log-normal distribution\n    Quantile functions\n        Heat equation\n    Radon–Nikodym derivative\n    Risk-neutral measure\n\n\t\n\n    Stochastic calculus\n        Brownian motion\n        Lévy process\n    Stochastic differential equations\n    Stochastic volatility\n        Numerical partial differential equations\n            Crank–Nicolson method\n            Finite difference method\n    Value at risk\n    Volatility\n        ARCH model\n        GARCH model\n\nDerivatives pricing\n\n    The Brownian Motion Model of Financial Markets\n    Rational pricing assumptions\n        Risk neutral valuation\n        Arbitrage-free pricing\n    Forward Price Formula\n    Futures contract pricing\n    Swap Valuation\n\n\t\n\n    Options\n        Put–call parity (Arbitrage relationships for options)\n        Intrinsic value, Time value\n        Moneyness\n        Pricing models\n            Black–Scholes model\n            Black model\n            Binomial options model\n                Implied binomial tree\n                Edgeworth binomial tree\n            Monte Carlo option model\n            Implied volatility, Volatility smile\n            SABR Volatility Model\n            Markov Switching Multifractal\n            The Greeks\n            Finite difference methods for option pricing\n            Vanna Volga method\n            Trinomial tree\n                Implied trinomial tree\n            Garman-Kohlhagen model\n        Pricing of American options\n            Barone-Adesi and Whaley\n            Bjerksund and Stensland\n            Black's approximation\n            Optimal stopping\n            Roll-Geske-Whaley\n\n\t\n\n    Interest rate derivatives\n        Black model\n            caps and floors\n            swaptions\n            Bond options\n        Short-rate models\n            Rendleman-Bartter model\n            Vasicek model\n            Ho-Lee model\n            Hull–White model\n            Cox–Ingersoll–Ross model\n            Black–Karasinski model\n            Black–Derman–Toy model\n            Kalotay–Williams–Fabozzi model\n            Longstaff–Schwartz model\n            Chen model\n        Forward rate-based models\n            LIBOR market model (Brace–Gatarek–Musiela Model, BGM)\n            Heath–Jarrow–Morton Model (HJM)\n\nSee also\nBook icon \t\n\n    Book: Finance\n\n    Computational finance\n    Quantitative behavioral finance\n    Derivative (finance), list of derivatives topics\n    Modeling and analysis of financial markets\n    Technical analysis\n    International Swaps and Derivatives Association\n    Fundamental financial concepts – topics\n    Model (economics)\n    List of finance topics\n    List of economics topics, List of economists\n    List of accounting topics\n    Statistical finance\n    Brownian model of financial markets\n    Master of Mathematical Finance\n    Financial economics", "skillName": "Mathematical_finance."}
{"id": 30, "category": "capital_market", "skillText": "Financial instruments are monetary contracts between parties. They can be created, traded, modified and settled. They can be cash (currency), evidence of an ownership interest in an entity (share), or a contractual right to receive or deliver cash (bond).\n\nInternational Accounting Standards IAS 32 and 39 define a financial instrument as \"any contract that gives rise to a financial asset of one entity and a financial liability or equity instrument of another entity\".[1]\n\nContents\n\n    1 Types\n    2 Measuring gain or loss\n    3 See also\n    4 References\n    5 External links\n\nTypes\n\nFinancial instruments can be either cash instruments or derivative instruments:\n\n    Cash instruments —instruments whose value is determined directly by the markets. They can be securities, which are readily transferable, and instruments such as loans and deposits, where both borrower and lender have to agree on a transfer.\n    Derivative instruments —instruments which derive their value from the value and characteristics of one or more underlying entities such as an asset, index, or interest rate. They can be exchange-traded derivatives and over-the-counter (OTC) derivatives.[2]\n\nAlternatively, financial instruments may be categorized by \"asset class\" depending on whether they are equity based (reflecting ownership of the issuing entity) or debt based (reflecting a loan the investor has made to the issuing entity). If it is debt, it can be further categorised into short term (less than one year) or long term. Foreign exchange instruments and transactions are neither debt- nor equity-based and belong in their own category.\nAsset class \tInstrument type\nSecurities \tOther cash \tExchange-traded derivatives \n\tOTC derivatives \nDebt (long term)\n> 1 year \tBonds \tLoans \tBond futures\nOptions on bond futures \tInterest rate swaps\nInterest rate caps and floors\nInterest rate options\nExotic derivatives\nDebt (short term)\n≤ 1 year \tBills, e.g. T-bills\nCommercial paper \tDeposits\nCertificates of deposit \tShort-term interest rate futures \tForward rate agreements\nEquity \tStock \tN/A \tStock options\nEquity futures \tStock options\nExotic derivatives\nForeign exchange \tN/A \tSpot foreign exchange \tCurrency futures \tForeign exchange options\nOutright forwards\nForeign exchange swaps\nCurrency swaps\n\nSome instruments defy categorization into the above matrix, for example repurchase agreements.\nMeasuring gain or loss\n\nThe gain or loss on a financial instrument is as follows:\nInstrument Type\nCategories \tMeasurement \tGains and losses\nAssets \tLoans and receivables \tAmortized costs \tNet income when asset is derecognized or impaired (foreign exchange and impairment recognized in net income immediately)\nAssets \tAvailable for sale financial assets \tDeposit account – fair value \tOther comprehensive income (impairment recognized in net income immediately)\nSee also\n\n    Off-balance-sheet issues", "skillName": "Financial_instrument."}
{"id": 31, "category": "capital_market", "skillText": "Corporate finance is the area of finance dealing with the sources of funding and the capital structure of corporations and the actions that managers take to increase the value of the firm to the shareholders, as well as the tools and analysis used to allocate financial resources. The primary goal of corporate finance is to maximize or increase shareholder value.[1] Although it is in principle different from managerial finance which studies the financial management of all firms, rather than corporations alone, the main concepts in the study of corporate finance are applicable to the financial problems of all kinds of firms.\n\nInvestment analysis (or capital budgeting) is concerned with the setting of criteria about which value-adding projects should receive investment funding, and whether to finance that investment with equity or debt capital. Working capital management is the management of the company's monetary funds that deal with the short-term operating balance of current assets and current liabilities; the focus here is on managing cash, inventories, and short-term borrowing and lending (such as the terms on credit extended to customers).[citation needed]\n\nThe terms corporate finance and corporate financier are also associated with investment banking. The typical role of an investment bank is to evaluate the company's financial needs and raise the appropriate type of capital that best fits those needs. Thus, the terms \"corporate finance\" and \"corporate financier\" may be associated with transactions in which capital is raised in order to create, develop, grow or acquire businesses. Recent legal and regulatory developments in the U.S. will likely alter the makeup of the group of arrangers and financiers willing to arrange and provide financing for certain highly leveraged transactions.[2]\n\nFinancial management overlaps with the financial function of the Accounting profession. However, financial accounting is the reporting of historical financial information, while financial management is concerned with the allocation of capital resources to increase a firm's value to the shareholders.\n\nContents\n\n    1 Outline of corporate finance\n    2 Capital structure\n        2.1 Capitalization structure\n        2.2 Sources of capital\n            2.2.1 Debt capital\n            2.2.2 Equity capital\n            2.2.3 Preferred stock\n    3 Investment and project valuation\n        3.1 Valuing flexibility\n        3.2 Quantifying uncertainty\n    4 Dividend policy\n    5 Working capital management\n        5.1 Working capital\n        5.2 Management of working capital\n    6 Relationship with other areas in finance\n        6.1 Investment banking\n        6.2 Financial risk management\n    7 See also\n    8 References\n    9 Further reading\n    10 Bibliography\n\nOutline of corporate finance\n\nThe primary goal of financial management is to maximize or to continually increase shareholder value.[3] Maximizing shareholder value requires managers to be able to balance capital funding between investments in projects that increase the firm's long term profitability and sustainability, along with paying excess cash in the form of dividends to shareholders. Managers of growth companies (i.e. firms that earn high rates of return on invested capital) will use most of the firm's capital resources and surplus cash on investments and projects so the company can continue to expand its business operations into the future. When companies reach maturity levels within their industry (i.e. companies that earn approximately average or lower returns on invested capital), managers of these companies will use surplus cash to payout dividends to shareholders. Managers must do an analysis to determine the appropriate allocation of the firm's capital resources and cash surplus between projects and payouts of dividends to shareholders, as well as paying back creditor related debt.[3][4]\n\nChoosing between investment projects will be based upon several inter-related criteria. (1) Corporate management seeks to maximize the value of the firm by investing in projects which yield a positive net present value when valued using an appropriate discount rate in consideration of risk. (2) These projects must also be financed appropriately. (3) If no growth is possible by the company and excess cash surplus is not needed to the firm, then financial theory suggests that management should return some or all of the excess cash to shareholders (i.e., distribution via dividends).[5]\n\nThis \"capital budgeting\" is the planning of value-adding, long-term corporate financial projects relating to investments funded through and affecting the firm's capital structure. Management must allocate the firm's limited resources between competing opportunities (projects).[6]\n\nCapital budgeting is also concerned with the setting of criteria about which projects should receive investment funding to increase the value of the firm, and whether to finance that investment with equity or debt capital.[7] Investments should be made on the basis of value-added to the future of the corporation. Projects that increase a firm's value may include a wide variety of different types of investments, including but not limited to, expansion policies, or mergers and acquisitions. When no growth or expansion is possible by a corporation and excess cash surplus exists and is not needed, then management is expected to pay out some or all of those surplus earnings in the form of cash dividends or to repurchase the company's stock through a share buyback program.[8][9]\nCapital structure\nCapitalization structure\nDomestic credit to private sector in 2005.\nMain article: Capital structure\nFurther information: Security (finance)\n\nAchieving the goals of corporate finance requires that any corporate investment be financed appropriately.[10] The sources of financing are, generically, capital self-generated by the firm and capital from external funders, obtained by issuing new debt and equity (and hybrid- or convertible securities). As above, since both hurdle rate and cash flows (and hence the riskiness of the firm) will be affected, the financing mix will impact the valuation of the firm. There are two interrelated considerations here:\n\n    Management must identify the \"optimal mix\" of financing – the capital structure that results in maximum firm value,[11] (See Balance sheet, WACC) but must also take other factors into account (see trade-off theory below). Financing a project through debt results in a liability or obligation that must be serviced, thus entailing cash flow implications independent of the project's degree of success. Equity financing is less risky with respect to cash flow commitments, but results in a dilution of share ownership, control and earnings. The cost of equity (see CAPM and APT) is also typically higher than the cost of debt - which is, additionally, a deductible expense – and so equity financing may result in an increased hurdle rate which may offset any reduction in cash flow risk.[12]\n    Management must attempt to match the long-term financing mix to the assets being financed as closely as possible, in terms of both timing and cash flows. Managing any potential asset liability mismatch or duration gap entails matching the assets and liabilities respectively according to maturity pattern (\"Cashflow matching\") or duration (\"immunization\"); managing this relationship in the short-term is a major function of working capital management, as discussed below. Other techniques, such as securitization, or hedging using interest rate- or credit derivatives, are also common. See Asset liability management; Treasury management; Credit risk; Interest rate risk.\n\nMuch of the theory here, falls under the umbrella of the Trade-Off Theory in which firms are assumed to trade-off the tax benefits of debt with the bankruptcy costs of debt when choosing how to allocate the company's resources. However economists have developed a set of alternative theories about how managers allocate a corporation's finances. One of the main alternative theories of how firms manage their capital funds is the Pecking Order Theory (Stewart Myers), which suggests that firms avoid external financing while they have internal financing available and avoid new equity financing while they can engage in new debt financing at reasonably low interest rates. Also, Capital structure substitution theory hypothesizes that management manipulates the capital structure such that earnings per share (EPS) are maximized. An emerging area in finance theory is right-financing whereby investment banks and corporations can enhance investment return and company value over time by determining the right investment objectives, policy framework, institutional structure, source of financing (debt or equity) and expenditure framework within a given economy and under given market conditions. One of the more recent innovations in this area from a theoretical point of view is the Market timing hypothesis. This hypothesis, inspired in the behavioral finance literature, states that firms look for the cheaper type of financing regardless of their current levels of internal resources, debt and equity.\nSources of capital\nFurther information: Security (finance)\nDebt capital\nFurther information: Bankruptcy and Financial distress\n\nCorporations may rely on borrowed funds (debt capital or credit) as sources of investment to sustain ongoing business operations or to fund future growth. Debt comes in several forms, such as through bank loans, notes payable, or bonds issued to the public. Bonds require the corporations to make regular interest payments (interest expenses) on the borrowed capital until the debt reaches its maturity date, therein the firm must pay back the obligation in full. Debt payments can also be made in the form of sinking fund provisions, whereby the corporation pays annual installments of the borrowed debt above regular interest charges. Corporations that issue callable bonds are entitled to pay back the obligation in full whenever the company feels it is in their best interest to pay off the debt payments. If interest expenses cannot be made by the corporation through cash payments, the firm may also use collateral assets as a form of repaying their debt obligations (or through the process of liquidation).\nEquity capital\n\nCorporations can alternatively sell shares of the company to investors to raise capital. Investors, or shareholders, expect that there will be an upward trend in value of the company (or appreciate in value) over time to make their investment a profitable purchase. Shareholder value is increased when corporations invest equity capital and other funds into projects (or investments) that earn a positive rate of return for the owners. Investors prefer to buy shares of stock in companies that will consistently earn a positive rate of return on capital in the future, thus increasing the market value of the stock of that corporation. Shareholder value may also be increased when corporations payout excess cash surplus (funds from retained earnings that are not needed for business) in the form of dividends.\nPreferred stock\n\nPreferred stock is an equity security which may have any combination of features not possessed by common stock including properties of both an equity and a debt instrument, and is generally considered a hybrid instrument. Preferreds are senior (i.e. higher ranking) to common stock, but subordinate to bonds in terms of claim (or rights to their share of the assets of the company).[13]\n\nPreferred stock usually carries no voting rights,[14] but may carry a dividend and may have priority over common stock in the payment of dividends and upon liquidation. Terms of the preferred stock are stated in a \"Certificate of Designation\".\n\nSimilar to bonds, preferred stocks are rated by the major credit-rating companies. The rating for preferreds is generally lower, since preferred dividends do not carry the same guarantees as interest payments from bonds and they are junior to all creditors.[15]\n\nPreferred stock is a special class of shares which may have any combination of features not possessed by common stock. The following features are usually associated with preferred stock:[16]\n\n    Preference in dividends\n    Preference in assets, in the event of liquidation\n    Convertibility to common stock.\n    Callability, at the option of the corporation\n    Nonvoting\n\nInvestment and project valuation\nFurther information: Business valuation, stock valuation, and fundamental analysis\n\nIn general,[17] each project's value will be estimated using a discounted cash flow (DCF) valuation, and the opportunity with the highest value, as measured by the resultant net present value (NPV) will be selected (applied to Corporate Finance by Joel Dean in 1951). This requires estimating the size and timing of all of the incremental cash flows resulting from the project. Such future cash flows are then discounted to determine their present value (see Time value of money). These present values are then summed, and this sum net of the initial investment outlay is the NPV. See Financial modeling.\n\nThe NPV is greatly affected by the discount rate. Thus, identifying the proper discount rate – often termed, the project \"hurdle rate\"[18] – is critical to choosing good projects and investments for the firm. The hurdle rate is the minimum acceptable return on an investment – i.e., the project appropriate discount rate. The hurdle rate should reflect the riskiness of the investment, typically measured by volatility of cash flows, and must take into account the project-relevant financing mix.[19] Managers use models such as the CAPM or the APT to estimate a discount rate appropriate for a particular project, and use the weighted average cost of capital (WACC) to reflect the financing mix selected. (A common error in choosing a discount rate for a project is to apply a WACC that applies to the entire firm. Such an approach may not be appropriate where the risk of a particular project differs markedly from that of the firm's existing portfolio of assets.)\n\nIn conjunction with NPV, there are several other measures used as (secondary) selection criteria in corporate finance. These are visible from the DCF and include discounted payback period, IRR, Modified IRR, equivalent annuity, capital efficiency, and ROI. Alternatives (complements) to NPV include Residual Income Valuation, MVA / EVA (Joel Stern, Stern Stewart & Co) and APV (Stewart Myers). See list of valuation topics.\nValuing flexibility\nMain articles: Real options analysis and decision tree\n\nIn many cases, for example R&D projects, a project may open (or close) various paths of action to the company, but this reality will not (typically) be captured in a strict NPV approach.[20] Some analysts account for this uncertainty by adjusting the discount rate (e.g. by increasing the cost of capital) or the cash flows (using certainty equivalents, or applying (subjective) \"haircuts\" to the forecast numbers).[21][22] Even when employed, however, these latter methods do not normally properly account for changes in risk over the project's lifecycle and hence fail to appropriately adapt the risk adjustment.[23] Management will therefore (sometimes) employ tools which place an explicit value on these options. So, whereas in a DCF valuation the most likely or average or scenario specific cash flows are discounted, here the \"flexible and staged nature\" of the investment is modelled, and hence \"all\" potential payoffs are considered. See further under Real options valuation. The difference between the two valuations is the \"value of flexibility\" inherent in the project.\n\nThe two most common tools are Decision Tree Analysis (DTA)[24] and Real options valuation (ROV);[25] they may often be used interchangeably:\n\n    DTA values flexibility by incorporating possible events (or states) and consequent management decisions. (For example, a company would build a factory given that demand for its product exceeded a certain level during the pilot-phase, and outsource production otherwise. In turn, given further demand, it would similarly expand the factory, and maintain it otherwise. In a DCF model, by contrast, there is no \"branching\" – each scenario must be modelled separately.) In the decision tree, each management decision in response to an \"event\" generates a \"branch\" or \"path\" which the company could follow; the probabilities of each event are determined or specified by management. Once the tree is constructed: (1) \"all\" possible events and their resultant paths are visible to management; (2) given this \"knowledge\" of the events that could follow, and assuming rational decision making, management chooses the branches (i.e. actions) corresponding to the highest value path probability weighted; (3) this path is then taken as representative of project value. See Decision theory#Choice under uncertainty.\n    ROV is usually used when the value of a project is contingent on the value of some other asset or underlying variable. (For example, the viability of a mining project is contingent on the price of gold; if the price is too low, management will abandon the mining rights, if sufficiently high, management will develop the ore body. Again, a DCF valuation would capture only one of these outcomes.) Here: (1) using financial option theory as a framework, the decision to be taken is identified as corresponding to either a call option or a put option; (2) an appropriate valuation technique is then employed – usually a variant on the Binomial options model or a bespoke simulation model, while Black Scholes type formulae are used less often; see Contingent claim valuation. (3) The \"true\" value of the project is then the NPV of the \"most likely\" scenario plus the option value. (Real options in corporate finance were first discussed by Stewart Myers in 1977; viewing corporate strategy as a series of options was originally per Timothy Luehrman, in the late 1990s.) See also Option pricing approaches under Business valuation.\n\nQuantifying uncertainty\nFurther information: Sensitivity analysis, Scenario planning, and Monte Carlo methods in finance\n\nGiven the uncertainty inherent in project forecasting and valuation,[24][26] analysts will wish to assess the sensitivity of project NPV to the various inputs (i.e. assumptions) to the DCF model. In a typical sensitivity analysis the analyst will vary one key factor while holding all other inputs constant, ceteris paribus. The sensitivity of NPV to a change in that factor is then observed, and is calculated as a \"slope\": ΔNPV / Δfactor. For example, the analyst will determine NPV at various growth rates in annual revenue as specified (usually at set increments, e.g. -10%, -5%, 0%, 5%....), and then determine the sensitivity using this formula. Often, several variables may be of interest, and their various combinations produce a \"value-surface[disambiguation needed \n]\",[27] (or even a \"value-space\",) where NPV is then a function of several variables. See also Stress testing.\n\nUsing a related technique, analysts also run scenario based forecasts of NPV. Here, a scenario comprises a particular outcome for economy-wide, \"global\" factors (demand for the product, exchange rates, commodity prices, etc...) as well as for company-specific factors (unit costs, etc...). As an example, the analyst may specify various revenue growth scenarios (e.g. 0% for \"Worst Case\", 10% for \"Likely Case\" and 20% for \"Best Case\"), where all key inputs are adjusted so as to be consistent with the growth assumptions, and calculate the NPV for each. Note that for scenario based analysis, the various combinations of inputs must be internally consistent (see discussion at Financial modeling), whereas for the sensitivity approach these need not be so. An application of this methodology is to determine an \"unbiased\" NPV, where management determines a (subjective) probability for each scenario – the NPV for the project is then the probability-weighted average of the various scenarios; see First Chicago Method. (See also rNPV, where cash flows, as opposed to scenarios, are probability-weighted.)\n\nA further advancement which \"overcomes the limitations of sensitivity and scenario analyses by examining the effects of all possible combinations of variables and their realizations\" [28] is to construct stochastic[29] or probabilistic financial models – as opposed to the traditional static and deterministic models as above.[26] For this purpose, the most common method is to use Monte Carlo simulation to analyze the project's NPV. This method was introduced to finance by David B. Hertz in 1964, although it has only recently become common: today analysts are even able to run simulations in spreadsheet based DCF models, typically using a risk-analysis add-in, such as @Risk or Crystal Ball. Here, the cash flow components that are (heavily) impacted by uncertainty are simulated, mathematically reflecting their \"random characteristics\". In contrast to the scenario approach above, the simulation produces several thousand random but possible outcomes, or trials, \"covering all conceivable real world contingencies in proportion to their likelihood;\" [30] see Monte Carlo Simulation versus \"What If\" Scenarios. The output is then a histogram of project NPV, and the average NPV of the potential investment – as well as its volatility and other sensitivities – is then observed. This histogram provides information not visible from the static DCF: for example, it allows for an estimate of the probability that a project has a net present value greater than zero (or any other value).\n\nContinuing the above example: instead of assigning three discrete values to revenue growth, and to the other relevant variables, the analyst would assign an appropriate probability distribution to each variable (commonly triangular or beta), and, where possible, specify the observed or supposed correlation between the variables. These distributions would then be \"sampled\" repeatedly – incorporating this correlation – so as to generate several thousand random but possible scenarios, with corresponding valuations, which are then used to generate the NPV histogram. The resultant statistics (average NPV and standard deviation of NPV) will be a more accurate mirror of the project's \"randomness\" than the variance observed under the scenario based approach. These are often used as estimates of the underlying \"spot price\" and volatility for the real option valuation as above; see Real options valuation: Valuation inputs. A more robust Monte Carlo model would include the possible occurrence of risk events (e.g., a credit crunch) that drive variations in one or more of the DCF model inputs.\nDividend policy\nMain article: Dividend policy\n\nDividend policy is concerned with financial policies regarding the payment of a cash dividend in the present or paying an increased dividend at a later stage. Whether to issue dividends,[31] and what amount, is determined mainly on the basis of the company's unappropriated profit (excess cash) and influenced by the company's long-term earning power. When cash surplus exists and is not needed by the firm, then management is expected to pay out some or all of those surplus earnings in the form of cash dividends or to repurchase the company's stock through a share buyback program.\n\nIf there are no NPV positive opportunities, i.e. projects where returns exceed the hurdle rate, and excess cash surplus is not needed, then – finance theory suggests – management should return some or all of the excess cash to shareholders as dividends. This is the general case, however there are exceptions. For example, shareholders of a \"growth stock\", expect that the company will, almost by definition, retain most of the excess cash surplus so as to fund future projects internally to help increase the value of the firm.\n\nManagement must also choose the form of the dividend distribution, generally as cash dividends or via a share buyback. Various factors may be taken into consideration: where shareholders must pay tax on dividends, firms may elect to retain earnings or to perform a stock buyback, in both cases increasing the value of shares outstanding. Alternatively, some companies will pay \"dividends\" from stock rather than in cash; see Corporate action. Financial theory suggests that the dividend policy should be set based upon the type of company and what management determines is the best use of those dividend resources for the firm to its shareholders. As a general rule, shareholders of growth companies would prefer managers to retain earnings and pay no dividends (use excess cash to reinvest into the company's operations), whereas shareholders of value or secondary stocks would prefer the management of these companies to payout surplus earnings in the form of cash dividends when a positive return cannot be earned through the reinvestment of undistributed earnings. A share buyback program may be accepted when the value of the stock is greater than the returns to be realized from the reinvestment of undistributed profits. In all instances, the appropriate dividend policy is usually directed by that which maximizes long-term shareholder value.\nWorking capital management\nMain article: Working capital\n\nManaging the corporation's working capital position to sustain ongoing business operations is referred to as working capital management.[32][33] These involve managing the relationship between a firm's short-term assets and its short-term liabilities. In general this is as follows: As above, the goal of Corporate Finance is the maximization of firm value. In the context of long term, capital budgeting, firm value is enhanced through appropriately selecting and funding NPV positive investments. These investments, in turn, have implications in terms of cash flow and cost of capital. The goal of Working Capital (i.e. short term) management is therefore to ensure that the firm is able to operate, and that it has sufficient cash flow to service long term debt, and to satisfy both maturing short-term debt and upcoming operational expenses. In so doing, firm value is enhanced when, and if, the return on capital exceeds the cost of capital; See Economic value added (EVA). Managing short term finance and long term finance is one task of a modern CFO.\nWorking capital\n\nWorking capital is the amount of funds which are necessary to an organization to continue its ongoing business operations, until the firm is reimbursed through payments for the goods or services it has delivered to its customers.[34] Working capital is measured through the difference between resources in cash or readily convertible into cash (Current Assets), and cash requirements (Current Liabilities). As a result, capital resource allocations relating to working capital are always current, i.e. short term. In addition to time horizon, working capital management differs from capital budgeting in terms of discounting and profitability considerations; they are also \"reversible\" to some extent. (Considerations as to Risk appetite and return targets remain identical, although some constraints – such as those imposed by loan covenants – may be more relevant here).\n\nThe (short term) goals of working capital are therefore not approached on the same basis as (long term) profitability, and working capital management applies different criteria in allocating resources: the main considerations are (1) cash flow / liquidity and (2) profitability / return on capital (of which cash flow is probably the most important).\n\n    The most widely used measure of cash flow is the net operating cycle, or cash conversion cycle. This represents the time difference between cash payment for raw materials and cash collection for sales. The cash conversion cycle indicates the firm's ability to convert its resources into cash. Because this number effectively corresponds to the time that the firm's cash is tied up in operations and unavailable for other activities, management generally aims at a low net count. (Another measure is gross operating cycle which is the same as net operating cycle except that it does not take into account the creditors deferral period.)\n    In this context, the most useful measure of profitability is Return on capital (ROC). The result is shown as a percentage, determined by dividing relevant income for the 12 months by capital employed; Return on equity (ROE) shows this result for the firm's shareholders. As above, firm value is enhanced when, and if, the return on capital exceeds the cost of capital.\n\nManagement of working capital\n\nGuided by the above criteria, management will use a combination of policies and techniques for the management of working capital.[35] These policies aim at managing the current assets (generally cash and cash equivalents, inventories and debtors) and the short term financing, such that cash flows and returns are acceptable.[33]\n\n    Cash management. Identify the cash balance which allows for the business to meet day to day expenses, but reduces cash holding costs.\n    Inventory management. Identify the level of inventory which allows for uninterrupted production but reduces the investment in raw materials – and minimizes reordering costs – and hence increases cash flow. Note that \"inventory\" is usually the realm of operations management: given the potential impact on cash flow, and on the balance sheet in general, finance typically \"gets involved in an oversight or policing way\".[36]:714 See Supply chain management; Just In Time (JIT); Economic order quantity (EOQ); Dynamic lot size model; Economic production quantity (EPQ); Economic Lot Scheduling Problem; Inventory control problem; Safety stock.\n    Debtors management. There are two inter-related roles here: Identify the appropriate credit policy, i.e. credit terms which will attract customers, such that any impact on cash flows and the cash conversion cycle will be offset by increased revenue and hence Return on Capital (or vice versa); see Discounts and allowances. Implement appropriate Credit scoring policies and techniques such that the risk of default on any new business is acceptable given these criteria.\n    Short term financing. Identify the appropriate source of financing, given the cash conversion cycle: the inventory is ideally financed by credit granted by the supplier; however, it may be necessary to utilize a bank loan (or overdraft), or to \"convert debtors to cash\" through \"factoring\".\n\nRelationship with other areas in finance\nInvestment banking\n\nUse of the term \"corporate finance\" varies considerably across the world. In the United States it is used, as above, to describe activities, analytical methods and techniques that deal with many aspects of a company's finances and capital. In the United Kingdom and Commonwealth countries, the terms \"corporate finance\" and \"corporate financier\" tend to be associated with investment banking – i.e. with transactions in which capital is raised for the corporation.[37] These may include\n\n    Raising seed, start-up, development or expansion capital\n    Mergers, demergers, acquisitions or the sale of private companies\n    Mergers, demergers and takeovers of public companies, including public-to-private deals\n    Management buy-out, buy-in or similar of companies, divisions or subsidiaries – typically backed by private equity\n    Equity issues by companies, including the flotation of companies on a recognised stock exchange in order to raise capital for development and/or to restructure ownership\n    Raising capital via the issue of other forms of equity, debt and related securities for the refinancing and restructuring of businesses\n    Financing joint ventures, project finance, infrastructure finance, public-private partnerships and privatisations\n    Secondary equity issues, whether by means of private placing or further issues on a stock market, especially where linked to one of the transactions listed above.\n    Raising debt and restructuring debt, especially when linked to the types of transactions listed above\n\nFinancial risk management\nMain article: Financial risk management\n\n    See also: Credit risk; Default (finance); Financial risk; Interest rate risk; Liquidity risk; Operational risk; Settlement risk; Value at Risk; Volatility risk; Insurance.\n\nRisk management [29][38] is the process of measuring risk and then developing and implementing strategies to manage (\"hedge\") that risk. Financial risk management, typically, is focused on the impact on corporate value due to adverse changes in commodity prices, interest rates, foreign exchange rates and stock prices (market risk). It will also play an important role in short term cash- and treasury management; see above. It is common for large corporations to have risk management teams; often these overlap with the internal audit function. While it is impractical for small firms to have a formal risk management function, many still apply risk management informally. See also Enterprise risk management.\n\nThe discipline typically focuses on risks that can be hedged using traded financial instruments, typically derivatives; see Cash flow hedge, Foreign exchange hedge, Financial engineering. Because company specific, \"over the counter\" (OTC) contracts tend to be costly to create and monitor, derivatives that trade on well-established financial markets or exchanges are often preferred. These standard derivative instruments include options, futures contracts, forward contracts, and swaps; the \"second generation\" exotic derivatives usually trade OTC. Note that hedging-related transactions will attract their own accounting treatment: see Hedge accounting, Mark-to-market accounting, FASB 133, IAS 39.\n\nThis area is related to corporate finance in two ways. Firstly, firm exposure to business and market risk is a direct result of previous capital financial investments. Secondly, both disciplines share the goal of enhancing, or preserving, firm value. There is a fundamental debate [39] relating to \"Risk Management\" and shareholder value. Per the Modigliani and Miller framework, hedging is irrelevant since diversified shareholders are assumed to not care about firm-specific risks, whereas, on the other hand hedging is seen to create value in that it reduces the probability of financial distress. A further question, is the shareholder's desire to optimize risk versus taking exposure to pure risk (a risk event that only has a negative side, such as loss of life or limb). The debate links the value of risk management in a market to the cost of bankruptcy in that market.\nSee also\nBook icon \t\n\n    Book: Finance\n\n\tWikiversity has learning materials about Corporate finance\n\n    Financial accounting\n    Stock market\n    Security (finance)\n    Growth stock\n    Financial planning\n    Investment bank\n    Venture capital\n    Financial statement analysis\n    Corporate tax\n    Corporate governance\n\nLists:\n\n    List of accounting topics\n    List of finance topics\n        List of corporate finance topics\n        List of valuation topics", "skillName": "Corporate_finance."}
{"id": 32, "category": "capital_market", "skillText": "A financial market is a market in which people trade financial securities, commodities, and other fungible items of value at low transaction costs and at prices that reflect supply and demand. Securities include stocks and bonds, and commodities include precious metals or agricultural products.\n\nIn economics, typically, the term market means the aggregate of possible buyers and sellers of a certain good or service and the transactions between them.\n\nThe term \"market\" is sometimes used for what are more strictly exchanges, organizations that facilitate the trade in financial securities, e.g., a stock exchange or commodity exchange. This may be a physical location (like the NYSE, BSE, NSE) or an electronic system (like NASDAQ). Much trading of stocks takes place on an exchange; still, corporate actions (merger, spinoff) are outside an exchange, while any two companies or people, for whatever reason, may agree to sell stock from the one to the other without using an exchange.\n\nTrading of currencies and bonds is largely on a bilateral basis, although some bonds trade on a stock exchange, and people are building electronic systems for these as well, similar to stock exchanges.\n\nContents\n\n    1 Types of financial markets\n    2 Raising capital\n        2.1 Lenders\n            2.1.1 Individuals & Doubles\n        2.2 Companies\n        2.3 Borrowers\n    3 Derivative products\n    4 Analysis of financial markets\n    5 Financial market slang\n    6 Role in the economy\n    7 Functions of financial markets\n    8 Components of financial market\n        8.1 Based on market levels\n        8.2 Based on security types\n    9 See also\n    10 Notes\n    11 References\n    12 External links\n\nTypes of financial markets\n\nWithin the financial sector, the term \"financial markets\" is often used to refer just to the markets that are used to raise finance: for long term finance, the Capital markets; for short term finance, the Money markets. Another common use of the term is as a catchall for all the markets in the financial sector, as per examples in the breakdown below.\n\n    Capital markets which to consist of:\n        Stock markets, which provide financing through the issuance of shares or common stock, and enable the subsequent trading thereof.\n        Bond markets, which provide financing through the issuance of bonds, and enable the subsequent trading thereof.\n    Commodity markets, which facilitate the trading of commodities.\n    Money markets, which provide short term debt financing and investment.\n    Derivatives markets, which provide instruments for the management of financial risk.[1]\n    Futures markets, which provide standardized forward contracts for trading products at some future date; see also forward market.\n    Foreign exchange markets, which facilitate the trading of foreign exchange.\n    Spot market\n    Interbanks market\n\nThe capital markets may also be divided into primary markets and secondary markets. Newly formed (issued) securities are bought or sold in primary markets, such as during initial public offerings. Secondary markets allow investors to buy and sell existing securities. The transactions in primary markets exist between issuers and investors, while secondary market transactions exist among investors.\n\nLiquidity is a crucial aspect of securities that are traded in secondary markets. Liquidity refers to the ease with which a security can be sold without a loss of value. Securities with an active secondary market mean that there are many buyers and sellers at a given point in time. Investors benefit from liquid securities because they can sell their assets whenever they want; an illiquid security may force the seller to get rid of their asset at a large discount.\nRaising capital\n\nFinancial markets attract funds from investors and channel them to corporations—they thus allow corporations to finance their operations and achieve growth. Money markets allow firms to borrow funds on a short term basis, while capital markets allow corporations to gain long-term funding to support expansion (known as maturity transformation).\n\nWithout financial markets, borrowers would have difficulty finding lenders themselves. Intermediaries such as banks, Investment Banks, and Boutique Investment Banks can help in this process. Banks take deposits from those who have money to save. They can then lend money from this pool of deposited money to those who seek to borrow. Banks popularly lend money in the form of loans and mortgages.\n\nMore complex transactions than a simple bank deposit require markets where lenders and their agents can meet borrowers and their agents, and where existing borrowing or lending commitments can be sold on to other parties. A good example of a financial market is a stock exchange. A company can raise money by selling shares to investors and its existing shares can be bought or sold.\n\nThe following table illustrates where financial markets fit in the relationship between lenders and borrowers:\nRelationship between lenders and borrowers\nLenders \tFinancial Intermediaries \tFinancial Markets \tBorrowers\nIndividuals\nCompanies \tBanks\nInsurance Companies\nPension Funds\nMutual Funds\n\tInterbank\nStock Exchange\nMoney Market\nBond Market\nForeign Exchange \tIndividuals\nCompanies\nCentral Government\nMunicipalities\nPublic Corporations\nLenders\n\nThe lender temporarily gives money to somebody else, on the condition of getting back the principal amount together with some interest/profit or charge.\nIndividuals & Doubles\n\nMany individuals are not aware that they are lenders, but almost everybody does lend money in many ways. A person lends money when he or she:\n\n    Puts money in a savings account at a bank\n    Contributes to a pension plan\n    Pays premiums to an insurance company\n    Invests in government bonds\n\nCompanies\n\nCompanies tend to be lenders of capital. When companies have surplus cash that is not needed for a short period of time, they may seek to make money from their cash surplus by lending it via short term markets called money markets. Alternatively, such companies may decide to return the cash surplus to their shareholders (e.g. via a share repurchase or dividend payment).\nBorrowers\n\n    Individuals borrow money via bankers' loans for short term needs or longer term mortgages to help finance a house purchase.\n    Companies borrow money to aid short term or long term cash flows. They also borrow to fund modernization or future business expansion.\n    Governments often find their spending requirements exceed their tax revenues. To make up this difference, they need to borrow. Governments also borrow on behalf of nationalized industries, municipalities, local authorities and other public sector bodies. In the UK, the total borrowing requirement is often referred to as the Public sector net cash requirement (PSNCR).\n\nGovernments borrow by issuing bonds. In the UK, the government also borrows from individuals by offering bank accounts and Premium Bonds. Government debt seems to be permanent. Indeed, the debt seemingly expands rather than being paid off. One strategy used by governments to reduce the value of the debt is to influence inflation.\n\nMunicipalities and local authorities may borrow in their own name as well as receiving funding from national governments. In the UK, this would cover an authority like Hampshire County Council.\n\nPublic Corporations typically include nationalized industries. These may include the postal services, railway companies and utility companies.\n\nMany borrowers have difficulty raising money locally. They need to borrow internationally with the aid of Foreign exchange markets.\n\nBorrowers having similar needs can form into a group of borrowers. They can also take an organizational form like Mutual Funds. They can provide mortgage on weight basis. The main advantage is that this lowers the cost of their borrowings.\nDerivative products\n\nDuring the 1980s and 1990s, a major growth sector in financial markets was the trade in so called derivative products \n, or derivatives for short.\n\nIn the financial markets, stock prices, bond prices, currency rates, interest rates and dividends go up and down, creating risk. Derivative products are financial products which are used to control risk or paradoxically exploit risk.[2] It is also called financial economics.\n\nDerivative products or instruments help the issuers to gain an unusual profit from issuing the instruments. For using the help of these products a contract has to be made. Derivative contracts are mainly 4 types:[3]\n\n    Future\n    Forward\n    Option\n    Swap\n\nSeemingly, the most obvious buyers and sellers of currency are importers and exporters of goods. While this may have been true in the distant past,[when?] when international trade created the demand for currency markets, importers and exporters now represent only 1/32 of foreign exchange dealing, according to the Bank for International Settlements.[4]\n\nThe picture of foreign currency transactions today shows:\n\n    Banks/Institutions\n    Speculators\n    Government spending (for example, military bases abroad)\n    Importers/Exporters\n    Tourists\n\nAnalysis of financial markets\n\n    See Statistical analysis of financial markets, statistical finance\n\nMuch effort has gone into the study of financial markets and how prices vary with time. Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of \"technical analysis\" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics, who claim that the evidence points rather to the random walk hypothesis, which states that the next change is not correlated to the last change. The role of human psychology in price variations also plays a significant factor. Large amounts of volatility often indicate the presence of strong emotional factors playing into the price. Fear can cause excessive drops in price and greed can create bubbles. In recent years the rise of algorithmic and high-frequency program trading has seen the adoption of momentum, ultra-short term moving average and other similar strategies which are based on technical as opposed to fundamental or theoretical concepts of market Behaviour.\n\nThe scale of changes in price over some unit of time is called the volatility. It was discovered by Benoît Mandelbrot that changes in prices do not follow a Gaussian distribution, but are rather modeled better by Lévy stable distributions. The scale of change, or volatility, depends on the length of the time unit to a power a bit more than 1/2. Large changes up or down are more likely than what one would calculate using a Gaussian distribution with an estimated standard deviation.\nFinancial market slang\n\n    Poison pill, when a company issues more shares to prevent being bought out by another company, thereby increasing the number of outstanding shares to be bought by the hostile company making the bid to establish majority.\n    Bips, meaning \"bps\" or basis points. A basis point is a financial unit of measurement used to describe the magnitude of percent change in a variable. One basis point is the equivalent of one hundredth of a percent. For example, if a stock price were to rise 100bit/s, it means it would increase 1%.\n    Quant, a quantitative analyst with advanced training in mathematics and statistical methods.\n    Rocket scientist, a financial consultant at the zenith of mathematical and computer programming skill. They are able to invent derivatives of high complexity and construct sophisticated pricing models. They generally handle the most advanced computing techniques adopted by the financial markets since the early 1980s. Typically, they are physicists and engineers by training.\n    IPO, stands for initial public offering, which is the process a new private company goes through to \"go public\" or become a publicly traded company on some index.\n    White Knight, a friendly party in a takeover bid. Used to describe a party that buys the shares of one organization to help prevent against a hostile takeover of that organization by another party.\n    Round-tripping\n    Smurfing, a deliberate structuring of payments or transactions to conceal it from regulators or other parties, a type of money laundering that is often illegal.\n    Spread, the difference between the highest bid and the lowest offer.\n\nRole in the economy\n\nOne of the important sustainability requisite for the accelerated development of an economy is the existence of a dynamic financial market. A financial market helps the economy in the following manner.\n\n    Saving mobilization: Obtaining funds from the savers or surplus units such as household individuals, business firms, public sector units, central government, state governments etc. is an important role played by financial markets.\n    Investment: Financial markets play a crucial role in arranging to invest funds thus collected in those units which are in need of the same.\n    National Growth: An important role played by financial market is that, they contribute to a nation's growth by ensuring unfettered flow of surplus funds to deficit units. Flow of funds for productive purposes is also made possible.\n    Entrepreneurship growth: Financial market contribute to the development of the entrepreneurial claw by making available the necessary financial resources.\n    Industrial development: The different components of financial markets help an accelerated growth of industrial and economic development of a country, thus contributing to raising the standard of living and the society of well-being.\n\nFunctions of financial markets\n\n    Intermediary functions: The intermediary functions of financial markets include the following:\n        Transfer of resources: Financial markets facilitate the transfer of real economic resources from lenders to ultimate borrowers.\n        Enhancing income: Financial markets allow lenders to earn interest or dividend on their surplus invisible funds, thus contributing to the enhancement of the individual and the national income.\n        Productive usage: Financial markets allow for the productive use of the funds borrowed. The enhancing the income and the gross national production.\n        Capital formation: Financial markets provide a channel through which new savings flow to aid capital formation of a country.\n        Price determination: Financial markets allow for the determination of price of the traded financial assets through the interaction of buyers and sellers. They provide a sign for the allocation of funds in the economy based on the demand and to the supply through the mechanism called price discovery process.\n        Sale mechanism: Financial markets provide a mechanism for selling of a financial asset by an investor so as to offer the benefit of marketability and liquidity of such assets.\n        Information: The activities of the participants in the financial market result in the generation and the consequent dissemination of information to the various segments of the market. So as to reduce the cost of transaction of financial assets.\n    Financial Functions\n        Providing the borrower with funds so as to enable them to carry out their investment plans.\n        Providing the lenders with earning assets so as to enable them to earn wealth by deploying the assets in production debentures.\n        Providing liquidity in the market so as to facilitate trading of funds.\n        Providing liquidity to commercial bank\n        Facilitating credit creation\n        Promoting savings\n        Promoting investment\n        Facilitating balanced economic growth\n        Improving trading floors\n\nComponents of financial market\nBased on market levels\n\n    Primary market: Primary market is a market for new issues or new financial claims. Hence it’s also called new issue market. The primary market deals with those securities which are issued to the public for the first time.\n    Secondary market: It’s a market for secondary sale of securities. In other words, securities which have already passed through the new issue market are traded in this market. Generally, such securities are quoted in the stock exchange and it provides a continuous and regular market for buying and selling of securities.\n\nSimply put, primary market is the market where the newly started company issued shares to the public for the first time through IPO (initial public offering). Secondary market is the market where the second hand securities are sold (securitCommodity Marketies).\nBased on security types\n\n    Money market: Money market is a market for dealing with financial assets and securities which have a maturity period of up to one year. In other words, it’s a market for purely short term funds.\n    Capital market: A capital market is a market for financial assets which have a long or indefinite maturity. Generally it deals with long term securities which have a maturity period of above one year. Capital market may be further divided into: (a) industrial securities market (b) Govt. securities market and (c) long term loans market.\n        Equity markets: A market where ownership of securities are issued and subscribed is known as equity market. An example of a secondary equity market for shares is the Bombay stock exchange.\n        Debt market: The market where funds are borrowed and lent is known as debt market. Arrangements are made in such a way that the borrowers agree to pay the lender the original amount of the loan plus some specified amount of interest.\n    Derivative markets: A market where financial instruments are derived and traded based on an underlying asset such as commodities or stocks.\n    Financial service market: A market that comprises participants such as commercial banks that provide various financial services like ATM. Credit cards. Credit rating, stock broking etc. is known as financial service market. Individuals and firms use financial services markets, to purchase services that enhance the working of debt and equity markets.\n    Depository markets: A depository market consists of depository institutions that accept deposit from individuals and firms and uses these funds to participate in the debt market, by giving loans or purchasing other debt instruments such as treasure bills.\n    Non-depository market: Non-depository market carry out various functions in financial markets ranging from financial intermediary to selling, insurance etc. The various constituency in non-depositary markets are mutual funds, insurance companies, pension funds, brokerage firms etc.\n\nSee also\n\n    Finance capitalism\n    Financial services\n    Financial instrument\n    Financial market efficiency\n    Brownian Model of Financial Markets\n    Investment theory\n    Quantitative behavioral finance\n    Slippage (finance)\n    Stock investor\n    Financial Market Theory of Development", "skillName": "Financial_market."}
{"id": 33, "category": "capital_market", "skillText": "In the 1970s Eugene Fama defined an efficient financial market as \"one in which prices always fully reflect available information”.[1]\n\nThe most common type of efficiency referred to in financial markets is the allocative efficiency, or the efficiency of allocating resources. This includes producing the right goods for the right people at the right price.\n\nA trait of allocatively efficient financial market is that it channels funds from the ultimate lenders to the ultimate borrowers in a way that the funds are used in the most socially useful manner.\n\nContents\n\n    1 Market efficiency levels\n    2 Efficient-market hypothesis (EMH)\n        2.1 Random walk theory\n            2.1.1 Evidence\n                2.1.1.1 Evidence of financial market efficiency\n                2.1.1.2 Evidence of financial market inefficiency\n    3 Market efficiency types\n    4 Conclusion\n    5 References\n    6 Bibliography\n\nMarket efficiency levels\n\nEugene Fama identified three levels of market efficiency:\n\n1. Weak-form efficiency\n\nPrices of the securities instantly and fully reflect all information of the past prices. This means future price movements cannot be predicted by using past prices. It is simply to say that, past data on stock prices are of no use in predicting future stock price changes. Everything is random. In this kind of market, should simply use a \"buy-and-hold\" strategy.\n\n2. Semi-strong efficiency\n\nAsset prices fully reflect all of the publicly available information. Therefore, only investors with additional inside information could have advantage on the market. Any price anomalies are quickly found out and the stock market adjusts.\n\n3. Strong-form efficiency\n\nAsset prices fully reflect all of the public and inside information available. Therefore, no one can have advantage on the market in predicting prices since there is no data that would provide any additional value to the investors.\nEfficient-market hypothesis (EMH)\n\nFama also created the efficient-market hypothesis (EMH) theory, which states that in any given time, the prices on the market already reflect all known information, and also change fast to reflect new information.\n\nTherefore, no one could outperform the market by using the same information that is already available to all investors, except through luck.[2]\nRandom walk theory\n\nAnother theory related to the efficient market hypothesis created by Louis Bachelier is the \"random walk\" theory, which states that the prices in the financial markets evolve randomly and are not connected, they are independent of each other.\n\nTherefore, identifying trends or patterns of price changes in a market couldn't be used to predict the future value of financial instruments.\nEvidence\nEvidence of financial market efficiency\n\n    Predicting future asset prices is not always accurate (represents weak efficiency form)\n    Asset prices always reflect all new available information quickly (represents semi-strong efficiency form)\n    Investors can't outperform on the market often (represents strong efficiency form)\n\nEvidence of financial market inefficiency\n\n    There is a vast literature in academic finance dealing with the momentum effect that was identified by Jegadeesh and Titman.[3][4] Stocks that have performed relatively well (poorly) over the past 3 to 12 months continue to do well (poorly) over the next 3 to 12 months. The momentum strategy is long recent winners and shorts recent losers, and produces positive risk-adjusted average returns. Being simply based on past stock returns that are functions of past prices (dividends can be ignored), the momentum effect produces strong evidence against weak-form market efficiency,and has been observed in the stock returns of most countries, in industry returns, and in national equity market indices. Moreover, Fama has accepted that momentum is the premier anomaly.[5][6]\n    January effect (repeating and predictable price movements and patterns occur on the market)\n    Stock market crashes, Asset Bubbles, and Credit Bubbles[7][8]\n    Investors that often outperform on the market such as Warren Buffett,[9] institutional investors, and corporations trading in their own stock[10]\n    Certain consumer credit market prices don't adjust to legal changes that affect future losses [11]\n\nMarket efficiency types\n\nJames Tobin identified four efficiency types that could be present in a financial market:[12]\n\n1. Information arbitrage efficiency\n\nAsset prices fully reflect all of the privately available information (the least demanding requirement for efficient market, since arbitrage includes realizable, risk free transactions)\n\nArbitrage involves taking advantage of price similarities of financial instruments between 2 or more markets by trading to generate losses.\n\nIt involves only risk-free transactions and the information used for trading is obtained at no cost. Therefore, the profit opportunities are not fully exploited, and it can be said that arbitrage is a result of market inefficiency.\n\nThis reflects the semi-strong efficiency model.\n\n2. Fundamental valuation efficiency\n\nAsset prices reflect the expected past flows of payments associated with holding the assets (profit forecasts are correct, they attract investors)\n\nFundamental valuation involves lower risks and less profit opportunities. It refers to the accuracy of the predicted return on the investment.\n\nFinancial markets are characterized by predictability and inconsistent misalignments that force the prices to always deviate from their fundamental valuations.\n\nThis reflects the weak information efficiency model.\n\n3. Full insurance efficiency\n\nIt ensures the continuous delivery of goods and services in all contingencies.\n\n4. Functional/Operational efficiency\n\nThe products and services available at the financial markets are provided for the least cost and are directly useful to the participants.\n\nEvery financial market will contain a unique mixture of the identified efficiency types.\nConclusion\n\nFinancial market efficiency is an important topic in the world of finance. While most financiers believe the markets are neither 100% efficient, nor 100% inefficient, many disagree where on the efficiency line the world's markets fall.\n\nIt can be concluded that in reality a financial market can’t be considered to be extremely efficient, or completely inefficient.\n\nThe financial markets are a mixture of both, sometimes the market will provide fair returns on the investment for everyone, while at other times certain investors will generate above average returns on their investment.", "skillName": "Financial_market_efficiency."}
{"id": 34, "category": "capital_market", "skillText": "Financial regulation is a form of regulation or supervision, which subjects financial institutions to certain requirements, restrictions and guidelines, aiming to maintain the integrity of the financial system. This may be handled by either a government or non-government organization. Financial regulation has also influenced the structure of banking sectors, by decreasing borrowing costs and increasing the variety of financial products available.\n\nContents\n\n    1 Aims of regulation\n    2 Structure of supervision\n        2.1 Supervision of stock exchanges\n        2.2 Supervision of listed companies\n        2.3 Supervision of investment management\n        2.4 Supervision of banks and financial services providers\n    3 Authority by country\n        3.1 Unique jurisdictions\n    4 Regulatory reliance on credit rating agencies\n    5 See also\n    6 References\n    7 Further reading\n    8 External links\n\nAims of regulation\n\nThe objectives of financial regulators are usually:[1]\n\n    market confidence – to maintain confidence in the financial system\n    financial stability – contributing to the protection and enhancement of stability of the financial system\n    consumer protection – securing the appropriate degree of protection for consumers.\n    reduction of financial crime – reducing the extent to which it is possible for a regulated business to be used for a purpose connected with financial crime.\n    regulating foreign participation in the financial markets.\n\nStructure of supervision\n\nActs empower organizations, government or non-government, to monitor activities and enforce actions.[2] There are various setups and combinations in place for the financial regulatory structure around the global.[3][4] Leaf parts are in any case:\nSupervision of stock exchanges\nMain article: Securities commission\n\nExchange acts ensure that trading on the exchanges is conducted in a proper manner. Most prominent the pricing process, execution and settlement of trades, direct and efficient trade monitoring.[5][6]\nSupervision of listed companies\n\nFinancial regulators ensure that listed companies and market participants comply with various regulations under the trading acts. The trading acts demands that listed companies publish regular financial reports, ad hoc notifications or directors' dealings. Whereas market participants are required to Publish major shareholder notifications. The objective of monitoring compliance by listed companies with their disclosure requirements is to ensure that investors have access to essential and adequate information for making an informed assessment of listed companies and their securities.[7][8][9]\nSupervision of investment management\n\nAsset management supervision or investment acts ensures the frictionless operation of those vehicles.[10]\nSupervision of banks and financial services providers\nMain article: Bank regulation\n\nBanking acts lay down rules for banks which they have to observe when they are being established and when they are carrying on their business. These rules are designed to prevent unwelcome developments that might disrupt the smooth functioning of the banking system. Thus ensuring a strong and efficient banking system.[11][12]\nAuthority by country\nMain article: List of financial regulatory authorities by country\nNumber of countries having a banking crisis in each year since 1800. This is based on This Time is Different: Eight Centuries of Financial Folly [13] which covers only 70 countries. The general upward trend might be attributed to many factors. One of these is a gradual increase in the percent of people who receive money for their labor. The dramatic feature of this graph is the virtual absence of banking crises during the period of the Bretton Woods agreement, 1945 to 1971. This analysis is similar to Figure 10.1 in Reinhart and Rogoff (2009). For more details see the help file for \"bankingCrises\" in the Ecdat package available from the Comprehensive R Archive Network (CRAN).\n\nThe following is a short listing of regulatory authorities in various jurisdictions, for a more complete listing, please see list of financial regulatory authorities by country.\n\n    United States\n        U.S. Securities and Exchange Commission (SEC)\n        Financial Industry Regulatory Authority (FINRA)\n        Commodity Futures Trading Commission (CFTC)\n        Federal Reserve System (\"Fed\")\n        Federal Deposit Insurance Corporation (FDIC)\n        Office of the Comptroller of the Currency (OCC)\n        National Credit Union Administration (NCUA)\n        Office of Thrift Supervision (OTS) (dissolved in 2011)\n        Consumer Financial Protection Bureau (CFPB)\n    United Kingdom\n        Bank of England (BoE)\n        Prudential Regulation Authority (PRA)\n        Financial Conduct Authority (FCA)\n    Financial Services Agency (FSA), Japan\n    Federal Financial Supervisory Authority (BaFin), Germany\n    Autorité des marchés financiers (France) (AMF), France\n    Monetary Authority of Singapore (MAS), Singapore\n    Swiss Financial Market Supervisory Authority (FINMA), Switzerland\n    People's Republic of China\n        China Banking Regulatory Commission (CBRC)\n        China Insurance Regulatory Commission (CIRC)\n        China Securities Regulatory Commission (CSRC)\n\nUnique jurisdictions\n\nIn most cases, financial regulatory authorities regulate all financial activities. But in some cases, there are specific authorities to regulate each sector of the finance industry, mainly banking, securities, insurance and pensions markets, but in some cases also commodities, futures, forwards, etc. For example, in Australia, the Australian Prudential Regulation Authority (APRA) supervises banks and insurers, while the Australian Securities and Investments Commission (ASIC) is responsible for enforcing financial services and corporations laws.\n\nSometimes more than one institution regulates and supervises the banking market, normally because, apart from regulatory authorities, central banks also regulate the banking industry. For example, in the USA banking is regulated by a lot of regulators, such as the Federal Reserve System, the Federal Deposit Insurance Corporation, the Office of the Comptroller of the Currency, the National Credit Union Administration, the Office of Thrift Supervision, as well as regulators at the state level.[14]\n\nIn the European Union, the European System of Financial Supervision consists of the European Banking Authority (EBA), the European Securities and Markets Authority (ESMA) and the European Insurance and Occupational Pensions Authority (EIOPA) as well as the European Systemic Risk Board. The Eurozone countries are forming a Single Supervisory Mechanism under the European Central Bank as a prelude to Banking union.\n\nIn addition, there are also associations of financial regulatory authorities. At the international level, there is the International Organization of Securities Commissions (IOSCO), the International Association of Insurance Supervisors, the Basel Committee on Banking Supervision, the Joint Forum, and the Financial Stability Board, where national authorities set standards through consensus-based decision-making processes.[15]\n\nThe structure of financial regulation has changed significantly in the past two decades, as the legal and geographic boundaries between markets in banking, securities, and insurance have become increasingly \"blurred\" and globalized.\nRegulatory reliance on credit rating agencies\n\nThink-tanks such as the World Pensions Council (WPC) have argued that most European governments pushed dogmatically for the adoption of the Basel II recommendations, adopted in 2005, transposed in European Union law through the Capital Requirements Directive (CRD), effective since 2008. In essence, they forced European banks, and, more importantly, the European Central Bank itself e.g. when gauging the solvency of EU-based financial institutions, to rely more than ever on the standardized assessments of credit risk marketed by two private US agencies- Moody’s and S&P, thus using public policy and ultimately taxpayers’ money to strengthen an anti-competitive duopolistic industry.\nSee also\n\n    Bank regulation\n    Finance\n    Financial repression\n    Global financial system\n    Group of Thirty\n    Insurance law\n    International Organization of Securities Commissions\n    International Centre for Financial Regulation\n    LabEx ReFi - European Laboratory on Financial Regulation\n    Regulatory capture\n    Securities Commission\n    Regulation of commodity markets\n    Virtual currency law in the United States", "skillName": "FinancialRegulation."}
{"id": 35, "category": "capital_market", "skillText": "Investment theory encompasses the body of knowledge used to support the decision-making process of choosing investments for various purposes. It includes portfolio theory, the capital asset pricing model, arbitrage pricing theory, efficient-market hypothesis, and rational pricing. It is near synonymous with \"asset pricing theory\", one major focus of financial economics; see Financial economics #Uncertainty.", "skillName": "Investment_theory."}
{"id": 36, "category": "capital_market", "skillText": "A stock exchange or bourse is an exchange where stock brokers and traders can buy and/or sell stocks (also called shares), bonds, and other securities. Stock exchanges may also provide facilities for issue and redemption of securities and other financial instruments, and capital events including the payment of income and dividends. Securities traded on a stock exchange include stock issued by listed companies, unit trusts, derivatives, pooled investment products and bonds. Stock exchanges often function as \"continuous auction\" markets, with buyers and sellers consummating transactions at a central location, such as the floor of the exchange.[2]\n\nTo be able to trade a security on a certain stock exchange, it must be listed there. Usually, there is a central location at least for record keeping, but trade is increasingly less linked to such a physical place, as modern markets use electronic networks, which gives them advantages of increased speed and reduced cost of transactions. Trade on an exchange is restricted to brokers who are members of the exchange. In recent years, various other trading venues, such as electronic communication networks, alternative trading systems and \"dark pools\" have taken much of the trading activity away from traditional stock exchanges.[3]\n\nThe initial public offering of stocks and bonds to investors is by definition done in the primary market and subsequent trading is done in the secondary market. A stock exchange is often the most important component of a stock market. Supply and demand in stock markets are driven by various factors that, as in all free markets, affect the price of stocks (see stock valuation).\n\nThere is usually no obligation for stock to be issued via the stock exchange itself, nor must stock be subsequently traded on the exchange. Such trading may be off exchange or over-the-counter. This is the usual way that derivatives and bonds are traded. Increasingly, stock exchanges are part of a global securities market.\n\nContents\n\n    1 History\n    2 Role of stock exchanges\n        2.1 Raising capital for businesses\n            2.1.1 Common forms of capital raising\n                2.1.1.1 Going public\n                2.1.1.2 Limited partnerships\n                2.1.1.3 Venture capital\n                2.1.1.4 Corporate partners\n        2.2 Mobilizing savings for investment\n        2.3 Facilitating company growth\n        2.4 Profit sharing\n        2.5 Corporate governance\n        2.6 Creating investment opportunities for small investors\n        2.7 Government capital-raising for development projects\n        2.8 Barometer of the economy\n    3 Major stock exchanges\n    4 Listing requirements\n        4.1 Requirements by stock exchange\n    5 Ownership\n    6 Other types of exchanges\n    7 See also\n    8 References\n    9 External links\n\nHistory\nAmong many other things, the Code of Hammurabi recorded interest-bearing loans.\n\nThe idea of debt dates back to the ancient world, as evidenced for example by ancient Mesopotamian clay tablets recording interest-bearing loans. There is little consensus among scholars as to when corporate stock was first traded. Some see the key event as the Dutch East India Company's founding in 1602, while others point to earlier developments. Economist Ulrike Malmendier of the University of California at Berkeley argues that a share market existed as far back as ancient Rome.\n\nIn the Roman Republic, which existed for centuries before the Empire was founded, there were societates publicanorum, organizations of contractors or leaseholders who performed temple-building and other services for the government. One such service was the feeding of geese on the Capitoline Hill as a reward to the birds after their honking warned of a Gallic invasion in 390 B.C. Participants in such organizations had partes or shares, a concept mentioned various times by the statesman and orator Cicero. In one speech, Cicero mentions \"shares that had a very high price at the time.\" Such evidence, in Malmendier's view, suggests the instruments were tradable, with fluctuating values based on an organization's success. The societas declined into obscurity in the time of the emperors, as most of their services were taken over by direct agents of the state.\n\nTradable bonds as a commonly used type of security were a more recent innovation, spearheaded by the Italian city-states of the late medieval and early Renaissance periods.\n\nThe Dutch East India Company, formed to build up the spice trade, operated as a colonial ruler in what is now Indonesia and beyond, a purview that included conducting military operations against wishes of the exploited natives and competing colonial powers. Control of the company was held tightly by its directors, with ordinary shareholders not having much influence on management or even access to the company's accounting statements.\n\nHowever, shareholders were rewarded well for their investment. The company paid an average dividend of over 16 percent per year from 1602 to 1650. Financial innovation in Amsterdam took many forms. In 1609, investors led by one Isaac Le Maire formed history's first bear syndicate, but their coordinated trading had only a modest impact in driving down share prices, which tended to be robust throughout the 17th century. By the 1620s, the company was expanding its securities issuance with the first use of corporate bonds.\nCourtyard of the Amsterdam Stock Exchange, circa 1670\n\nJoseph de la Vega, also known as Joseph Penso de la Vega and by other variations of his name, was an Amsterdam trader from a Spanish Jewish family and a prolific writer as well as a successful businessman in 17th-century Amsterdam. His 1688 book Confusion of Confusions explained the workings of the city's stock market. It was the earliest book about stock trading, taking the form of a dialogue between a merchant, a shareholder and a philosopher, the book described a market that was sophisticated but also prone to excesses, and de la Vega offered advice to his readers on such topics as the unpredictability of market shifts and the importance of patience in investment.\n\nWilliam sought to modernize England's finances to pay for its wars, and thus the kingdom's first government bonds were issued in 1693 and the Bank of England was set up the following year. Soon thereafter, English joint-stock companies began going public.\nLondon Stock Exchange in 1810\n\nLondon's first stockbrokers, however, were barred from the old commercial center known as the Royal Exchange, reportedly because of their rude manners. Instead, the new trade was conducted from coffee houses along Exchange Alley. By 1698, a broker named John Castaing, operating out of Jonathan's Coffee House, was posting regular lists of stock and commodity prices. Those lists mark the beginning of the London Stock Exchange.\n\nOne of history's greatest financial bubbles occurred in the next few decades. At the center of it were the South Sea Company, set up in 1711 to conduct English trade with South America, and the Mississippi Company, focused on commerce with France's Louisiana colony and touted by transplanted Scottish financier John Law, who was acting in effect as France's central banker. Investors snapped up shares in both, and whatever else was available. In 1720, at the height of the mania, there was even an offering of \"a company for carrying out an undertaking of great advantage, but nobody to know what it is.\"\n\nBy the end of that same year, share prices were collapsing, as it became clear that expectations of imminent wealth from the Americas were overblown. In London, Parliament passed the Bubble Act, which stated that only royally chartered companies could issue public shares. In Paris, Law was stripped of office and fled the country. Stock trading was more limited and subdued in subsequent decades. Yet the market survived, and by the 1790s shares were being traded in the young United States.\nRole of stock exchanges\nThe floor of the New York Stock Exchange\nLondon Stock Exchange, the City of London\nTokyo Stock Exchange, Tokyo\nBuenos Aires Stock Exchange\nThe offices of Bursa Malaysia, Malaysia's national stock exchange (known before demutualization as Kuala Lumpur Stock Exchange)\n\nStock exchanges have multiple roles in the economy. This may include the following:[4]\nRaising capital for businesses\n\nA stock exchange provides companies with the facility to raise capital for expansion through selling shares to the investing public.[5]\nCommon forms of capital raising\n\nBesides the borrowing capacity provided to an individual or firm by the banking system, in the form of credit or a loan, there are four common forms of capital raising used by companies and entrepreneurs. Most of these available options might be achieved, directly or indirectly, through a stock exchange.\nGoing public\n\nCapital intensive companies, particularly high tech companies, always need to raise high volumes of capital in their early stages. For this reason, the public market provided by the stock exchanges has been one of the most important funding sources for many capital intensive startups. After the 1990s and early-2000s hi-tech listed companies' boom and bust in the world's major stock exchanges, it has been much more demanding for the high-tech entrepreneur to take his/her company public, unless either the company already has products in the market and is generating sales and earnings, or the company has completed advanced promising clinical trials, earned potentially profitable patents or conducted market research which demonstrated very positive outcomes. This is quite different from the situation of the 1990s to early-2000s period, when a number of companies (particularly Internet boom and biotechnology companies) went public in the most prominent stock exchanges around the world, in the total absence of sales, earnings and any well-documented promising outcome. Anyway, every year a number of companies, including unknown highly speculative and financially unpredictable hi-tech startups, are listed for the first time in all the major stock exchanges – there are even specialized entry markets for these kind of companies or stock indexes tracking their performance (examples include the Alternext, CAC Small, SDAX, TecDAX, or most of the third market good companies).\nLimited partnerships\n\nA number of companies have also raised significant amounts of capital through R&D limited partnerships. Tax law changes that were enacted in 1987 in the United States changed the tax deductibility of investments in R&D limited partnerships. In order for a partnership to be of interest to investors today, the cash on cash return must be high enough to entice investors.\nVenture capital\n\nA third usual source of capital for startup companies has been venture capital. This source remains largely available today, but the maximum statistical amount that the venture company firms in aggregate will invest in any one company is not limitless (it was approximately $15 million in 2001 for a biotechnology company).\nCorporate partners\n\nA fourth alternative source of cash for a private company is a corporate partner, usually an established multinational company, which provides capital for the smaller company in return for marketing rights, patent rights, or equity. Corporate partnerships have been used successfully in a large number of cases.\nMobilizing savings for investment\n\nWhen people draw their savings and invest in shares (through an IPO or the issuance of new company shares of an already listed company), it usually leads to rational allocation of resources because funds, which could have been consumed, or kept in idle deposits with banks, are mobilized and redirected to help companies' management boards finance their organizations. This may promote business activity with benefits for several economic sectors such as agriculture, commerce and industry, resulting in stronger economic growth and higher productivity levels of firms.\nFacilitating company growth\n\nCompanies view acquisitions as an opportunity to expand product lines, increase distribution channels, hedge against volatility, increase their market share, or acquire other necessary business assets. A takeover bid or a merger agreement through the stock market is one of the simplest and most common ways for a company to grow by acquisition or fusion.\nProfit sharing\n\nBoth casual and professional stock investors, as large as institutional investors or as small as an ordinary middle-class family, through dividends and stock price increases that may result in capital gains, share in the wealth of profitable businesses. Unprofitable and troubled businesses may result in capital losses for shareholders.\nCorporate governance\n\nBy having a wide and varied scope of owners, companies generally tend to improve management standards and efficiency to satisfy the demands of these shareholders, and the more stringent rules for public corporations imposed by public stock exchanges and the government. Consequently, it is alleged that public companies (companies that are owned by shareholders who are members of the general public and trade shares on public exchanges) tend to have better management records than privately held companies (those companies where shares are not publicly traded, often owned by the company founders and/or their families and heirs, or otherwise by a small group of investors).\n\nDespite this claim, some well-documented cases are known where it is alleged that there has been considerable slippage in corporate governance on the part of some public companies. The dot-com bubble in the late 1990s, and the subprime mortgage crisis in 2007–08, are classical examples of corporate mismanagement. Companies like Pets.com (2000), Enron (2001), One.Tel (2001), Sunbeam (2001), Webvan (2001), Adelphia (2002), MCI WorldCom (2002), Parmalat (2003), American International Group (2008), Bear Stearns (2008), Lehman Brothers (2008), General Motors (2009) and Satyam Computer Services (2009) were among the most widely scrutinized by the media.\n\nTo assist in corporate governance many banks and companies worldwide utilize securities identification numbers (USIN) to identify, uniquely, their stocks, bonds and other securities. Adding an ISIN code helps to distinctly identify securities and the ISIN system is used worldwide by funds, companies, and governments.\n\nHowever, when poor financial, ethical or managerial records are known by the stock investors, the stock and the company tend to lose value. In the stock exchanges, shareholders of underperforming firms are often penalized by significant share price decline, and they tend as well to dismiss incompetent management teams.\nCreating investment opportunities for small investors\n\nAs opposed to other businesses that require huge capital outlay, investing in shares is open to both the large and small stock investors because a person buys the number of shares they can afford. Therefore, the Stock Exchange provides the opportunity for small investors to own shares of the same companies as large investors.\nGovernment capital-raising for development projects\n\nGovernments at various levels may decide to borrow money to finance infrastructure projects such as sewage and water treatment works or housing estates by selling another category of securities known as bonds. These bonds can be raised through the stock exchange whereby members of the public buy them, thus loaning money to the government. The issuance of such bonds can obviate, in the short term, direct taxation of citizens to finance development—though by securing such bonds with the full faith and credit of the government instead of with collateral, the government must eventually tax citizens or otherwise raise additional funds to make any regular coupon payments and refund the principal when the bonds mature.\nBarometer of the economy\n\nAt the stock exchange, share prices rise and fall depending, largely, on economic forces. Share prices tend to rise or remain stable when companies and the economy in general show signs of stability and growth. An economic recession, depression, or financial crisis could eventually lead to a stock market crash. Therefore, the movement of share prices and in general of the stock indexes can be an indicator of the general trend in the economy.\nMajor stock exchanges\n\nMajor stock exchanges (top 20 by market capitalization) of issued shares of domestic companies, as of 31 January 2015 (Monthly reports, World Federation of Exchanges \n)\nRank \tExchange \tEconomy \tHead­quarters \tMarket cap\n(USD bn) \tMonthly trade volume\n(USD bn) \tTime zone \tΔ \tDST \tOpen\n(local) \tClose\n(local) \tLunch\n(local) \tOpen\n(UTC) \tClose\n(UTC)\n1 \tNew York Stock Exchange \t United States \tNew York \t19,223 \t1,520 \tEST/EDT \t−5 \n\tMar–Nov \t09:30 \t16:00 \tNo \t14:30 \t21:00\n2 \tNASDAQ \t United States \tNew York \t6,831 \t1,183 \tEST/EDT \t−5 \n\tMar–Nov \t09:30 \t16:00 \tNo \t14:30 \t21:00\n3[6] \tLondon Stock Exchange Group \t United Kingdom\n Italy \tLondon \t6,187 \t165 \tGMT/BST \t+0 \n\tMar–Oct \t08:00 \t16:30 \tNo \t08:00 \t16:30\n4 \tJapan Exchange Group – Tokyo \t Japan \tTokyo \t4,485 \t402 \tJST \t+9 \n\t\t09:00 \t15:00 \t11:30–12:30 \t00:00 \t06:00\n5 \tShanghai Stock Exchange \t China \tShanghai \t3,986 \t1,278 \tCST \t+8 \n\t\t09:30 \t15:00 \t11:30–13:00 \t01:30 \t07:00\n6 \tHong Kong Stock Exchange \t Hong Kong \tHong Kong \t3,325 \t155 \tHKT \t+8 \n\t\t09:15 \t16:00 \t12:00–13:00 \t01:15 \t08:00\n7 \tEuronext \t European Union \tAmsterdam\nBrussels\nLisbon\nLondon\nParis \t3,321 \t184 \tCET/CEST \t+1 \n\tMar–Oct \t09:00 \t17:30 \tNo \t08:00 \t16:30\n8 \tShenzhen Stock Exchange \t China \tShenzhen \t2,285 \t800 \tCST \t+8 \n\t\t09:30 \t15:00 \t11:30–13:00 \t01:30 \t07:00\n9 \tTMX Group \t Canada \tToronto \t1,939 \t120 \tEST/EDT \t−5 \n\tMar–Nov \t09:30 \t16:00 \tNo \t14:30 \t21:00\n10 \tDeutsche Börse \t Germany \tFrankfurt \t1,762 \t142 \tCET/CEST \t+1 \n\tMar–Oct \t08:00 (Eurex)\n08:00 (floor)\n09:00 (Xetra) \t22:00 (Eurex)\n20:00 (floor)\n17:30 (Xetra) \tNo \t07:00 \t21:00\n11 \tBombay Stock Exchange \t India \tMumbai \t1,682 \t11.8 \tIST \t+5.5 \n\t\t09:15 \t15:30 \tNo \t03:45 \t10:00\n12 \tNational Stock Exchange of India \t India \tMumbai \t1,642 \t62.2 \tIST \t+5.5 \n\t\t09:15 \t15:30 \tNo \t03:45 \t10:00\n13 \tSIX Swiss Exchange \t  Switzerland \tZurich \t1,516 \t126 \tCET/CEST \t+1 \n\tMar–Oct \t09:00 \t17:30 \tNo \t08:00 \t16:30\n14 \tAustralian Securities Exchange \t Australia \tSydney \t1,272 \t55.8 \tAEST/AEDT \t+10 \n\tOct–Apr \t09:50 \t16:12 \tNo \t23:50 \t06:12\n15 \tKorea Exchange \t South Korea \tSeoul \t1,251 \t136 \tKST \t+9 \n\t\t09:00 \t15:00 \tNo \t00:00 \t06:00\n16 \tOMX Nordic Exchange \tSweden Northern Europe, Armenia \tStockholm \t1,212 \t63.2 \t\tvarious \t\t\t\t\t\t\n17 \tJSE Limited \t South Africa \tJohannesburg \t951 \t27.6 \tCAT \t+2 \n\t\t09:00 \t17:00 \tNo \t07:00 \t15:00\n18 \tBME Spanish Exchanges \t Spain \tMadrid \t942 \t94.0 \tCET/CEST \t+1 \n\tMar–Oct \t09:00 \t17:30 \tNo \t08:00 \t16:30\n19 \tTaiwan Stock Exchange \t Taiwan \tTaipei \t861 \t54.3 \tCST \t+8 \n\t\t09:00 \t13:30 \tNo \t01:00 \t05:30\n20 \tBM&F Bovespa \t Brazil \tSão Paulo \t824 \t51.1 \tBRT/BRST \t−3 \n\tOct–Feb \t10:00 \t17:30 \tNo \t13:00 \t20:00\n\n    Note: \"Δ\" to UTC, as well as \"Open (UTC)\" and \"Close (UTC)\" columns contain valid data only for standard time in a given time zone. During daylight saving time period, the UTC times will be one hour less and and Δs one hour more.\n\nListing requirements\n\nListing requirements are the set of conditions imposed by a given stock exchange upon companies that want to be listed on that exchange. Such conditions sometimes include minimum number of shares outstanding, minimum market capitalization, and minimum annual income.\nRequirements by stock exchange\n\nCompanies must meet an exchange's requirements to have their stocks and shares listed and traded there, but requirements vary by stock exchange:\n\n    New York Stock Exchange: To be listed on the New York Stock Exchange (NYSE), a company must have issued at least a million shares of stock worth $100 million and must have earned more than $10 million over the last three years.[7]\n    NASDAQ Stock Exchange: To be listed on the NASDAQ a company must have issued at least 1.25 million shares of stock worth at least $70 million and must have earned more than $11 million over the last three years.[8]\n    London Stock Exchange: The main market of the London Stock Exchange has requirements for a minimum market capitalization (£700,000), three years of audited financial statements, minimum public float (25 per cent) and sufficient working capital for at least 12 months from the date of listing.\n    Bombay Stock Exchange: Bombay Stock Exchange (BSE) has requirements for a minimum market capitalization of ₹250 million (US$3.7 million) and minimum public float equivalent to ₹100 million (US$1.5 million).[9]\n\nOwnership\n\nStock exchanges originated as mutual organizations, owned by its member stock brokers. There has been a recent trend for stock exchanges to demutualize, where the members sell their shares in an initial public offering. In this way the mutual organization becomes a corporation, with shares that are listed on a stock exchange. Examples are Australian Securities Exchange (1998), Euronext (merged with New York Stock Exchange), NASDAQ (2002), Bursa Malaysia (2004), the New York Stock Exchange (2005), Bolsas y Mercados Españoles, and the São Paulo Stock Exchange (2007). The Shenzhen and Shanghai stock exchanges can be characterized as quasi-state institutions insofar as they were created by government bodies in China and their leading personnel are directly appointed by the China Securities Regulatory Commission. Another example is Tashkent republican stock exchange (Uzbekistan) established in 1994, three years after collapse of Soviet Union, mainly state-owned but has a form of a public corporation (joint stock company). According to an Uzbek government decision (March 2012) 25 percent minus one share of Tashkent stock exchange was expected to be sold to Korea Exchange(KRX) in 2014.[10]\nOther types of exchanges\n\nIn the 19th century, exchanges were opened to trade forward contracts on commodities. Exchange traded forward contracts are called futures contracts. These commodity exchanges later started offering future contracts on other products, such as interest rates and shares, as well as options contracts. They are now generally known as futures exchanges.\nSee also\nBook icon \t\n\n    Book: Stock exchanges\n\n\tWikimedia Commons has media related to Stock exchanges.\n\tLook up bourse or stock exchange in Wiktionary, the free dictionary.\n\n    Auction\n    Capital market\n    Commodities exchange\n    Financial regulation\n    International Organization of Securities Commissions\n    Shareholder\n    Stag profit\n    Stock exchanges for developing countries\n    Stock investor\n    Stock market\n    Stock market data systems\n    Histoire des bourses de valeurs (French)\n\nLists:\n\n    List of stock exchanges\n    List of stock market indices\n    List of financial regulatory authorities by country\n    List of Swiss financial market legislation", "skillName": "Stock_Exchange."}
{"id": 37, "category": "capital_market", "skillText": "Finance is a field that deals with the study of investments. It includes the dynamics of assets and liabilities over time under conditions of different degrees of uncertainty and risk. Finance can also be defined as the science of money management. Finance aims to price assets based on their risk level and their expected rate of return. Finance can be broken into three different sub-categories: public finance, corporate finance and personal finance.\n\nContents\n\n    1 Areas of finance\n        1.1 Personal finance\n        1.2 Corporate finance\n            1.2.1 Financial services\n        1.3 Public finance\n    2 Capital\n    3 Financial theory\n        3.1 Financial economics\n        3.2 Financial mathematics\n        3.3 Experimental finance\n        3.4 Behavioral finance\n        3.5 Intangible asset finance\n    4 Professional qualifications\n    5 Unsolved problems in finance\n    6 See also\n    7 References\n    8 External links\n\nAreas of finance\n\n    Wall Street, the center of American finance.\n\n    London Stock Exchange, global center of finance.\n\nPersonal finance\nMain article: Personal finance\n\nQuestions in personal finance revolve around:\n\n    Protection against unforeseen personal events, as well as events in the wider economies\n    Transference of family wealth across generations (bequests and inheritance)\n    Effects of tax policies (tax subsidies and/or penalties) on management of personal finances\n    Effects of credit on individual financial standing\n    Development of a savings plan or financing for large purchases (auto, education, home)\n    Planning a secure financial future in an environment of economic instability\n\nPersonal finance may involve paying for education, financing durable goods such as real estate and cars, buying insurance, e.g. health and property insurance, investing and saving for retirement.\n\nPersonal finance may also involve paying for a loan, or debt obligations. The six key areas of personal financial planning, as suggested by the Financial Planning Standards Board, are:[1]\n\n    Financial position: is concerned with understanding the personal resources available by examining net worth and household cash flow. Net worth is a person's balance sheet, calculated by adding up all assets under that person's control, minus all liabilities of the household, at one point in time. Household cash flow totals up all the expected sources of income within a year, minus all expected expenses within the same year. From this analysis, the financial planner can determine to what degree and in what time the personal goals can be accomplished.\n    Adequate protection: the analysis of how to protect a household from unforeseen risks. These risks can be divided into the following: liability, property, death, disability, health and long term care. Some of these risks may be self-insurable, while most will require the purchase of an insurance contract. Determining how much insurance to get, at the most cost effective terms requires knowledge of the market for personal insurance. Business owners, professionals, athletes and entertainers require specialized insurance professionals to adequately protect themselves. Since insurance also enjoys some tax benefits, utilizing insurance investment products may be a critical piece of the overall investment planning.\n    Tax planning: typically the income tax is the single largest expense in a household. Managing taxes is not a question of if you will pay taxes, but when and how much. Government gives many incentives in the form of tax deductions and credits, which can be used to reduce the lifetime tax burden. Most modern governments use a progressive tax. Typically, as one's income grows, a higher marginal rate of tax must be paid. Understanding how to take advantage of the myriad tax breaks when planning one's personal finances can make a significant impact in which it can later save you money in the long term.\n    Investment and accumulation goals: planning how to accumulate enough money - for large purchases and life events - is what most people consider to be financial planning. Major reasons to accumulate assets include, purchasing a house or car, starting a business, paying for education expenses, and saving for retirement. Achieving these goals requires projecting what they will cost, and when you need to withdraw funds that will be necessary to be able to achieve these goals. A major risk to the household in achieving their accumulation goal is the rate of price increases over time, or inflation. Using net present value calculators, the financial planner will suggest a combination of asset earmarking and regular savings to be invested in a variety of investments. In order to overcome the rate of inflation, the investment portfolio has to get a higher rate of return, which typically will subject the portfolio to a number of risks. Managing these portfolio risks is most often accomplished using asset allocation, which seeks to diversify investment risk and opportunity. This asset allocation will prescribe a percentage allocation to be invested in stocks (either preferred stock and/or common stock), bonds (for example mutual bonds or government bonds, or corporate bonds), cash and alternative investments. The allocation should also take into consideration the personal risk profile of every investor, since risk attitudes vary from person to person.\n    Retirement planning is the process of understanding how much it costs to live at retirement, and coming up with a plan to distribute assets to meet any income shortfall. Methods for retirement plan include taking advantage of government allowed structures to manage tax liability including: individual (IRA) structures, or employer sponsored retirement plans.\n    Estate planning involves planning for the disposition of one's assets after death. Typically, there is a tax due to the state or federal government at one's death. Avoiding these taxes means that more of one's assets will be distributed to one's heirs. One can leave one's assets to family, friends or charitable groups.\n\nCorporate finance\nMain article: Corporate finance\n\nCorporate finance deals with the sources of funding and the capital structure of corporations and the actions that managers take to increase the value of the firm to the shareholders, as well as the tools and analysis used to allocate financial resources. Although it is in principle different from managerial finance which studies the financial management of all firms, rather than corporations alone, the main concepts in the study of corporate finance are applicable to the financial problems of all kinds of firms. Corporate finance generally involves balancing risk and profitability, while attempting to maximize an entity's assets, net incoming cash flow and the value of its stock, and generically entails three primary areas of capital resource allocation. In the first, \"capital budgeting\", management must choose which \"projects\" (if any) to undertake. The discipline of capital budgeting may employ standard business valuation techniques or even extend to real options valuation; see Financial modeling. The second, \"sources of capital\" relates to how these investments are to be funded: investment capital can be provided through different sources, such as by shareholders, in the form of equity (privately or via an initial public offering), creditors, often in the form of bonds, and the firm's operations (cash flow). Short-term funding or working capital is mostly provided by banks extending a line of credit. The balance between these elements forms the company's capital structure. The third, \"the dividend policy\", requires management to determine whether any unappropriated profit (excess cash) is to be retained for future investment / operational requirements, or instead to be distributed to shareholders, and if so, in what form. Short term financial management is often termed \"working capital management\", and relates to cash-, inventory- and debtors management.\n\nCorporate finance also includes within its scope business valuation, stock investing, or investment management. An investment is an acquisition of an asset in the hope that it will maintain or increase its value over time that will in hope give back a higher rate of return when it comes to disbursing dividends. In investment management – in choosing a portfolio – one has to use financial analysis to determine what, how much and when to invest. To do this, a company must:\n\n    Identify relevant objectives and constraints: institution or individual goals, time horizon, risk aversion and tax considerations;\n    Identify the appropriate strategy: active versus passive hedging strategy\n    Measure the portfolio performance\n\nFinancial management overlaps with the financial function of the accounting profession. However, financial accounting is the reporting of historical financial information, while financial management is concerned with the allocation of capital resources to increase a firm's value to the shareholders and increase their rate of return on the investments.\n\nFinancial risk management, an element of corporate finance, is the practice of creating and protecting economic value in a firm by using financial instruments to manage exposure to risk, particularly credit risk and market risk. (Other risk types include foreign exchange, shape, volatility, sector, liquidity, inflation risks, etc.) It focuses on when and how to hedge using financial instruments; in this sense it overlaps with financial engineering. Similar to general risk management, financial risk management requires identifying its sources, measuring it (see: Risk measure: Well known risk measures), and formulating plans to address these, and can be qualitative and quantitative. In the banking sector worldwide, the Basel Accords are generally adopted by internationally active banks for tracking, reporting and exposing operational, credit and market risks.[citation needed]\nFinancial services\nMain article: Financial services\n\nAn entity whose income exceeds its expenditure can lend or invest the excess income to help that excess income produce more income in the future. Though on the other hand, an entity whose income is less than its expenditure can raise capital by borrowing or selling equity claims, decreasing its expenses, or increasing its income. The lender can find a borrower—a financial intermediary such as a bank—or buy notes or bonds (corporate bonds, government bonds, or mutual bonds) in the bond market. The lender receives interest, the borrower pays a higher interest than the lender receives, and the financial intermediary earns the difference for arranging the loan.\n\nA bank aggregates the activities of many borrowers and lenders. A bank accepts deposits from lenders, on which it pays interest. The bank then lends these deposits to borrowers. Banks allow borrowers and lenders, of different sizes, to coordinate their activity.\n\nFinance is used by individuals (personal finance), by governments (public finance), by businesses (corporate finance) and by a wide variety of other organizations such as schools and non-profit organizations. In general, the goals of each of the above activities are achieved through the use of appropriate financial instruments and methodologies, with consideration to their institutional setting.\n\nFinance is one of the most important aspects of business management and includes analysis related to the use and acquisition of funds for the enterprise.\n\nIn corporate finance, a company's capital structure is the total mix of financing methods it uses to raise funds. One method is debt financing, which includes bank loans and bond sales. Another method is equity financing - the sale of stock by a company to investors, the original shareholders (they own a portion of the business) of a share. Ownership of a share gives the shareholder certain contractual rights and powers, which typically include the right to receive declared dividends and to vote the proxy on important matters (e.g., board elections). The owners of both bonds (either government bonds or corporate bonds) and stock (whether its preferred stock or common stock), may be institutional investors - financial institutions such as investment banks and pension funds  or private individuals, called private investors or retail investors.\nPublic finance\nMain article: Public finance\n\nPublic finance describes finance as related to sovereign states and sub-national entities (states/provinces, counties, municipalities, etc.) and related public entities (e.g. school districts) or agencies. It usually encompasses a long-term, strategic perspective regarding investment decisions that affect public entities.[2] These long-term, strategic periods usually encompass five or more years.[3] Public finance is primarily concerned with:\n\n    Identification of required expenditure of a public sector entity\n    Source(s) of that entity's revenue\n    The budgeting process\n    Debt issuance (municipal bonds) for public works projects\n\nCentral banks, such as the Federal Reserve System banks in the United States and Bank of England in the United Kingdom, are strong players in public finance, acting as lenders of last resort as well as strong influences on monetary and credit conditions in the economy.[4]\nCapital\nMain article: Financial capital\n\nCapital, in the financial sense, is the money that gives the business the power to buy goods to be used in the production of other goods or the offering of a service. (The capital has two types of resources, Equity and Debt).\n\nThe deployment of capital is decided by the budget. This may include the objective of business, targets set, and results in financial terms, e.g., the target set for sale, resulting cost, growth, required investment to achieve the planned sales, and financing source for the investment.\n\nA budget may be long term or short term. Long term budgets have a time horizon of 5–10 years giving a vision to the company; short term is an annual budget which is drawn to control and operate in that particular year.\n\nBudgets will include proposed fixed asset requirements and how these expenditures will be financed. Capital budgets are often adjusted annually (done every year) and should be part of a longer-term Capital Improvements Plan.\n\nA cash budget is also required. The working capital requirements of a business are monitored at all times to ensure that there are sufficient funds available to meet short-term expenses.\n\nThe cash budget is basically a detailed plan that shows all expected sources and uses of cash when it comes to spending it appropriately. The cash budget has the following six main sections:\n\n    Beginning Cash Balance - contains the last period's closing cash balance, in other words, the remaining cash from last years earnings.\n    Cash collections - includes all expected cash receipts (all sources of cash for the period considered, mainly sales)\n    Cash disbursements - lists all planned cash outflows for the period such as dividend, excluding interest payments on short-term loans, which appear in the financing section. All expenses that do not affect cash flow are excluded from this list (e.g. depreciation, amortization, etc.)\n    Cash excess or deficiency - a function of the cash needs and cash available. Cash needs are determined by the total cash disbursements plus the minimum cash balance required by company policy. If total cash available is less than cash needs, a deficiency exists.\n    Financing - discloses the planned borrowings and repayments of those planned borrowings, including interest.\n\nFinancial theory\nFinancial economics\nMain article: Financial economics\n\nFinancial economics is the branch of economics studying the interrelation of financial variables, such as prices, interest rates and shares, as opposed to goods and services. Financial economics concentrates on influences of real economic variables on financial ones, in contrast to pure finance. It centres on managing risk in the context of the financial markets, and the resultant economic and financial models. It essentially explores how rational investors would apply risk and return to the problem of an investment policy. Here, the twin assumptions of rationality and market efficiency lead to modern portfolio theory (the CAPM), and to the Black–Scholes theory for option valuation; it further studies phenomena and models where these assumptions do not hold, or are extended. \"Financial economics\", at least formally, also considers investment under \"certainty\" (Fisher separation theorem, \"theory of investment value\", Modigliani–Miller theorem) and hence also contributes to corporate finance theory. Financial econometrics is the branch of financial economics that uses econometric techniques to parameterize the relationships suggested.\n\nAlthough closely related, the disciplines of economics and finance are distinctive. The “economy” is a social institution that organizes a society’s production, distribution, and consumption of goods and services, all of which must be financed.\n\nEconomists make a number of abstract assumptions for purposes of their analyses and predictions. They generally regard financial markets that function for the financial system as an efficient mechanism (Efficient-market hypothesis). Instead, financial markets are subject to human error and emotion.[5] New research discloses the mischaracterization of investment safety and measures of financial products and markets so complex that their effects, especially under conditions of uncertainty, are impossible to predict. The study of finance is subsumed under economics as financial economics, but the scope, speed, power relations and practices of the financial system can uplift or cripple whole economies and the well-being of households, businesses and governing bodies within them—sometimes in a single day.\nFinancial mathematics\nMain article: Financial mathematics\n\nFinancial mathematics is a field of applied mathematics, concerned with financial markets. The subject has a close relationship with the discipline of financial economics, which is concerned with much of the underlying theory that is involved in financial mathematics. Generally, mathematical finance will derive, and extend, the mathematical or numerical models suggested by financial economics. In terms of practice, mathematical finance also overlaps heavily with the field of computational finance (also known as financial engineering). Arguably, these are largely synonymous, although the latter focuses on application, while the former focuses on modelling and derivation (see: Quantitative analyst). The field is largely focused on the modelling of derivatives, although other important subfields include insurance mathematics and quantitative portfolio problems. See Outline of finance: Mathematical tools; Outline of finance: Derivatives pricing.\nExperimental finance\nMain article: Experimental finance\n\nExperimental finance aims to establish different market settings and environments to observe experimentally and provide a lens through which science can analyze agents' behavior and the resulting characteristics of trading flows, information diffusion and aggregation, price setting mechanisms, and returns processes. Researchers in experimental finance can study to what extent existing financial economics theory makes valid predictions and therefore prove them, and attempt to discover new principles on which such theory can be extended and be applied to future financial decisions. Research may proceed by conducting trading simulations or by establishing and studying the behavior, and the way that these people act or react, of people in artificial competitive market-like settings.\nBehavioral finance\nMain article: Behavioral economics\n\nBehavioral finance studies how the psychology of investors or managers affects financial decisions and markets when making a decision that can impact either negatively or positively on one of their areas. Behavioral finance has grown over the last few decades to become central and very important to finance.[citation needed]\n\nBehavioral finance includes such topics as:\n\n    Empirical studies that demonstrate significant deviations from classical theories.\n    Models of how psychology affects and impacts trading and prices\n    Forecasting based on these methods.\n    Studies of experimental asset markets and use of models to forecast experiments.\n\nA strand of behavioral finance has been dubbed Quantitative Behavioral Finance, which uses mathematical and statistical methodology to understand behavioral biases in conjunction with valuation. Some of this endeavor has been led by Gunduz Caginalp (Professor of Mathematics and Editor of Journal of Behavioral Finance during 2001-2004) and collaborators including Vernon Smith (2002 Nobel Laureate in Economics), David Porter, Don Balenovich, Vladimira Ilieva, Ahmet Duran). Studies by Jeff Madura, Ray Sturm and others have demonstrated significant behavioral effects in stocks and exchange traded funds. Among other topics, quantitative behavioral finance studies behavioral effects together with the non-classical assumption of the finiteness of assets.\nIntangible asset finance\nMain article: Intangible asset finance\n\nIntangible asset finance is the area of finance that deals with intangible assets such as patents, trademarks, goodwill, reputation, etc.\n\nIntangible assets are divided into indefinite or definite. The brand name of a company is an indefinite asset; it stays with the company throughout its existence. A patent however, granted to that company for a limited amount of time would be a definite intangible asset.[6]\nProfessional qualifications\n\nThere are several related professional qualifications, that can lead to the field:\n\n    Generalist Finance qualifications:\n        Degrees: Master of Science in Finance (MSF), Master of Finance (M.Fin), Master of Financial Economics, Master of Applied Finance, Master of Liberal Arts in Finance (ALM.Fin)\n        Certifications: Chartered Financial Analyst (CFA), Certified Treasury Professional (CTP), Certified Valuation Analyst (CVA), Certified Patent Valuation Analyst (CPVA), Chartered Business Valuator (CBV), Certified International Investment Analyst (CIIA), Financial Risk Manager (FRM), Professional Risk Manager (PRM), Association of Corporate Treasurers (ACT), Certified Market Analyst (CMA/FAD) Dual Designation, Corporate Finance Qualification (CF), Chartered Alternative Investment Analyst (CAIA), Chartered Investment Manager (CIM)\n    Quantitative Finance qualifications: Master of Financial Engineering (MSFE), Master of Quantitative Finance (MQF), Master of Computational Finance (MCF), Master of Financial Mathematics (MFM), Certificate in Quantitative Finance (CQF).\n    Accountancy qualifications:\n        Qualified accountant: Chartered Certified Accountant (ACCA, UK certification), Chartered Accountant (ACA - England & Wales certification / CA - certification in Scotland and Commonwealth countries), Certified Public Accountant (CPA, US certification), ACMA/FCMA (Associate/Fellow Chartered Management Accountant) from Chartered Institute of Management Accountant (CIMA), UK.\n        Non-statutory qualifications: Chartered Cost Accountant CCA Designation from AAFM\n    Business qualifications: Master of Business Administration (MBA), Master of Management (MM), Master of Commerce (M.Comm), Master of Science in Management (MSM), Doctor of Business Administration (DBA)\n\nUnsolved problems in finance\n\nAs the debate to whether finance is an art or a science is still open,[7] there have been recent efforts to organize a list of unsolved problems in finance.\nSee also\n\n    Outline of finance\n    Financial crisis of 2007–2010\n    List of unsolved problems in finance", "skillName": "Finance."}
{"id": 38, "category": "capital_market", "skillText": "A stock trader or equity trader or share trader is a person or company involved in trading equity securities. Stock traders may be an agent, hedger, arbitrageur, speculator, stockbroker or investor. A stock investor is an individual or company who puts money to use by the purchase of equity securities, offering potential profitable returns, as interest, income, or appreciation in value (capital gains). This buy-and-hold long term strategy is passive in nature, as opposed to speculation, which is typically active in nature. Many stock speculators will trade bonds (and possibly other financial assets) as well. Stock speculation is a risky and complex occupation because the direction of the markets are generally unpredictable and lack transparency, also financial regulators are sometimes unable to adequately detect, prevent and remediate irregularities committed by malicious listed companies or other financial market participants. In addition, the financial markets are usually subjected to speculation.\n\nContents\n\n    1 Stock speculator vs stock investor\n        1.1 Stock trading as a profession/career\n            1.1.1 Risks and other costs\n                1.1.1.1 Notable cases\n    2 Methodology\n    3 Stock picking\n        3.1 The efficient-market hypothesis\n        3.2 Mandelbrot's fractal theory\n        3.3 Beating the market, fraud and scams\n    4 See also\n    5 References\n\nStock speculator vs stock investor\nA view of a computerized trading floor at the Frankfurt Stock Exchange\n\nStock speculators are often ambiguously categorized as stock traders, if trading in that capacity, as it sounds more acceptable to the general public. Individuals or firms trading equity (stock) on the stock markets as their principal capacity are often called stock traders. Stock speculators usually try to profit from short-term price volatility with trades lasting anywhere from several seconds to several weeks.\n\nThe stock speculator is usually a professional. Persons can call themselves full or part-time stock traders/investors while maintaining other professions. When a stock speculator/investor has clients, and acts as a money manager or adviser with the intention of adding value to their clients finances, he is also called a financial advisor or manager. In this case, the financial manager could be an independent professional or a large bank corporation employee. This may include managers dealing with investment funds, hedge funds, mutual funds, and pension funds, or other professionals in venture capital, equity investment, fund management, and wealth management. These organized investors, are sometimes referred to as institutional investors. Several different types of stock trading strategies or approaches exist including day trading, trend following, market making, scalping (trading), momentum trading, trading the news, and arbitrage.\n\nOn the other hand, stock investors are firms or individuals who purchase stocks with the intention of holding them for an extended period of time, usually several months to years, for passive income objectives such as dividend accumulation. They rely primarily on fundamental analysis for their investment decisions and fully recognize stock shares as part-ownership in the company. Many investors believe in the buy and hold strategy, which as the name suggests, implies that investors will buy stock ownership in a corporation and hold onto those stocks for the very long term, generally measured in years. This strategy was made popular in the equity bull market of the 1980s and 90s where buy-and-hold investors rode out short-term market declines and continued to hold as the market returned to its previous highs and beyond. However, during the 2001-2003 equity bear market (A bear market is defined as a 20% drop in a major index (e.g. DJIA or SPX) which lasts at least two months. The last bear market began on October 10, 2007 when the DJIA and SPX closed at bull market highs),[1] the buy-and-hold strategy lost some followers as broader market indexes like the NASDAQ saw their values decline by over 60%.[dubious – discuss]\nStock trading as a profession/career\nU.S. Securities and Exchange Commission headquarters in Washington, D.C.\n\nStock traders advise shareholders and help manage portfolios. Traders engage in buying and selling bonds, stocks, futures and shares in hedge funds. A stock trader also conducts extensive research and observation of how financial markets perform. This is accomplished through economic and microeconomic study; consequently, more advanced stock traders will delve into macroeconomics and industry specific technical analysis to track asset or corporate performance. Other duties of a stock trader include comparison of financial analysis to current and future regulation of his or her occupation.\n\nProfessional stock traders who work for a financial company, are required to complete an internship of up to four months before becoming established in their career field. In the United States, for example, internship is followed up by taking and passing a Financial Industry Regulatory Authority-administered Series 63 or 65 exam. Stock traders who pass demonstrate familiarity with U.S. Securities and Exchange Commission (SEC) compliant practices and regulation. Stock traders with experience usually obtain a four-year degree in a financial, accounting or economics field after licensure. Supervisory positions as a trader may usually require an MBA for advanced stock market analysis.\n\nThe U.S. Bureau of Labor Statistics (BLS)[2] reported that growth for stock and commodities traders was forecast to be greater than 21% between 2006 and 2016. In that period, stock traders would benefit from trends driven by pensions of baby boomers and their decreased reliance on Social Security. U.S. Treasury bonds would also be traded on a more fluctuating basis. Stock traders just entering the field suffer since few entry-level positions exist. While entry into this career field is very competitive, increased ownership of stocks and mutual funds drive substantial career growth of traders. Banks were also offering more opportunities for people of average means to invest and speculate in stocks. The BLS reported that stock traders had median annual incomes of $68,500. Experienced traders of stocks and mutual funds have the potential to earn more than $145,600 annually.\nRisks and other costs\nCrowd gathering on Wall Street after the Wall Street Crash of 1929\n\nContrary to a stockbroker, a professional who arranges transactions between a buyer and a seller, and gets a guaranteed commission for every deal executed, a professional trader may have a steep learning curve and his/her ultra-competitive performance based career may be cut short, especially during generalized stock market crashes. Stock market trading operations have a considerably high level of risk, uncertainty and complexity, especially for unwise and inexperienced stock traders/investors seeking an easy way to make money quickly. In addition, trading activities are not free. Stock speculators/investors face several costs such as commissions, taxes and fees to be paid for the brokerage and other services, like the buying/selling orders placed at the stock exchange. Depending on the nature of each national or state legislation involved, a large array of fiscal obligations must be respected, and taxes are charged by jurisdictions over those transactions, dividends and capital gains that fall within their scope. However, these fiscal obligations will vary from jurisdiction to jurisdiction. Among other reasons, there could be some instances where taxation is already incorporated into the stock price through the differing legislation that companies have to comply with in their respective jurisdictions; or that tax free stock market operations are useful to boost economic growth. Beyond these costs are the opportunity costs of money and time, currency risk, financial risk, and Internet, data and news agency services and electricity consumption expenses—all of which must be accounted for.\nNotable cases\n\nJerome Kerviel (Société Générale) and Kweku Adoboli (UBS), two rogue traders, worked in the same type of position, the Delta One desk - a table where derivatives are traded, and not single stocks or bonds. These types of operations are relatively simple and often reserved for novice traders who also specialize in exchange-traded funds (ETFs), financial products that mimic the performance of an index (i.e. either upward or downward). As they are easy to use, they facilitate portfolio diversification through the acquisition of contracts backed by a stock index or industry (e.g. commodities). The two traders were very familiar to control procedures. They worked in the back office, the administrative body of the bank that controls the regularity of operations, before moving to trading. According to the report of the Inspector General of Societe Generale, in 2005 and 2006 Kerviel \"led\" by taking 100 to 150 million-euro positions on the shares of Solarworld AG listed in Germany. Moreover, the \"unauthorized trading\" of Kweku Adoboli, similar to Kerviel, did not date back a long way. Adoboli had executed operations since October 2008 - his failure and subsequent arrest occurred in 2011.[3]\nMethodology\n\nStock speculators and investors usually need a stock broker such as a bank or a brokerage firm to access the stock market. Since the advent of Internet banking, an Internet connection is commonly used to manage positions. Using the Internet, specialized software, and a personal computer, stock speculators/investors make use of technical and fundamental analysis to help them in making decisions. They may use several information resources, some of which are strictly technical. Using the pivot points calculated from a previous day's trading, they attempt to predict the buy and sell points of the current day's trading session. These points give a cue to speculators, as to where prices will head for the day, prompting each speculator where to enter his trade, and where to exit. An added tool for the stock picker is the use of \"stock screens\". Stock screens allow the user to input specific parameters, based on technical and/or fundamental conditions, that he or she deems desirable. Primary benefit associated with stock screens is its ability to return a small group of stocks for further analysis, among tens of thousands, that fit the requirements requested. There is criticism on the validity of using these technical indicators in analysis, and many professional stock speculators do not use them.[citation needed] Many full-time stock speculators and stock investors, as well as most other people in finance, traditionally have a formal education and training in fields such as economics, finance, mathematics and computer science, which may be particularly relevant to this occupation – since stock trading is not an exact science, stock prices have in general a random or chaotic[4] behaviour and there is no proven technique for trading stocks profitably, the degree of knowledge in those fields is ultimately neglectable.\nStock picking\nThe efficient-market hypothesis\nTechnical analysis is the use of graphical and analytical patterns and data to attempt to predict future prices.\n\nAlthough many companies offer courses in stock picking, and numerous experts report success through technical analysis and fundamental analysis, many economists and academics state that because of the efficient-market hypothesis (EMH) it is unlikely that any amount of analysis can help an investor make any gains above the stock market itself. In the distribution of investors, many academics believe that the richest are simply outliers in such a distribution (i.e. in a game of chance, they have flipped heads twenty times in a row). When money is put into the stock market, it is done with the aim of generating a return on the capital invested. Many investors try not only to make a profitable return, but also to outperform, or beat, the market. However, market efficiency - championed in the EMH formulated by Eugene Fama in 1970, suggests that at any given time, prices fully reflect all available information on a particular stock and/or market.\n\nThus, according to the EMH, no investor has an advantage in predicting a return on a stock price because no one has access to information not already available to everyone else. In efficient markets, prices become not predictable but random, so no investment pattern can be discerned. A planned approach to investment, therefore, cannot be successful. This \"random walk\" of prices, commonly spoken about in the EMH school of thought, results in the failure of any investment strategy that aims to beat the market consistently. In fact, the EMH suggests that given the transaction costs involved in portfolio management, it would be more profitable for an investor to put his or her money into an index fund.\nMandelbrot's fractal theory\n\nIn 1963 Benoit Mandelbrot analyzed the variations of cotton prices on a time series starting in 1900. There were two important findings. First, price movements had very little to do with a normal distribution in which the bulk of the observations lies close to the mean (68% of the data are within one standard deviation). Instead, the data showed a great frequency of extreme variations. Second, price variations followed patterns that were indifferent to scale: the curve described by price changes for a single day was similar to a month’s curve. Surprisingly, these patterns of self-similarity were present during the entire period 1900-1960, a violent epoch that had seen a Great Depression and two world wars. Mandelbrot used his fractal theory to explain the presence of extreme events in Wall Street. In 2004 he published his book on the “misbehavior” of financial markets - The (Mis)behavior of Markets: A Fractal View of Risk, Ruin, and Reward. The basic idea that relates fractals to financial markets is that the probability of experiencing extreme fluctuations (like the ones triggered by herd behavior) is greater than what conventional wisdom wants us to believe. This of course delivers a more accurate vision of risk in the world of finance. The central objective in financial markets is to maximize income for a given level of risk. Standard models for this are based on the premise that the probability of extreme variations of asset prices is very low.\n\nThese models rely on the assumption that asset price fluctuations are the result of a well-behaved random or stochastic process. This is why mainstream models (such as the famous Black-Scholes model) use normal probabilistic distributions to describe price movements. For all practical purposes, extreme variations can be ignored. Mandelbrot thought this was an awful way to look at financial markets. For him, the distribution of price movements is not normal and has the property of kurtosis, where fat tails abound. This is a more faithful representation of financial markets: the movements of the Dow index for the past hundred years reveals a troubling frequency of violent movements. Still, conventional models used by the time of the 2008 financial crisis ruled out these extreme variations and considered they can only happen every 10,000 years. An obvious conclusion from Mandelbrot’s work is that greater regulation in financial markets is indispensable. Other contributions of his work for the study of stock market behaviour are the creation of new approaches to evaluate risk and avoid unanticipated financial collapses.[4]\nBeating the market, fraud and scams\n\nOutside of academia, the controversy surrounding market timing is primarily focused on day trading conducted by individual investors and the mutual fund trading scandals perpetrated by institutional investors in 2003. Media coverage of these issues has been so prevalent that many investors now dismiss market timing as a credible investment strategy. Unexposed insider trading, accounting fraud, embezzlement and pump and dump strategies are factors that hamper an efficient, rational, fair and transparent investing, because they may create fictitious company's financial statements and data, leading to inconsistent stock prices.\n\nThroughout the stock markets history, there have been dozens of scandals involving listed companies, stock investing methods and brokerage. A classical case related to insider trading of listed companies involved Raj Rajaratnam and its hedge fund management firm, the Galleon Group. On Friday October 16, 2009, he was arrested by the FBI and accused of conspiring with others in insider trading in several publicly traded companies. U.S. Attorney Preet Bharara put the total profits in the scheme at over $60 million, telling a news conference it was the largest hedge fund insider trading case in United States history.[5] A well publicized accounting fraud of a listed company involved Satyam. On January 7, 2009, its Chairman Raju resigned after publicly announcing his involvement in a massive accounting fraud. Ramalinga Raju was sent to the Hyderabad prison along with his brother and former board member Rama Raju, and the former CFO Vadlamani Srinivas. In Italy, Parmalat's Calisto Tanzi was charged with financial fraud and money laundering in 2008. Italians were shocked that such a vast and established empire could crumble so quickly. When the scandal was made known, the share price of Parmalat in the Milan Stock Exchange tumbled. Parmalat had sold itself credit-linked notes, in effect placing a bet on its own credit worthiness in order to conjure up an asset out of thin air. After his arrest, Tanzi reportedly admitted during questioning at Milan's San Vittore prison, that he diverted funds from Parmalat into Parmatour and elsewhere. The family football and tourism enterprises were financial disasters; as well as Tanzi's attempt to rival Berlusconi by buying Odeon TV, only to sell it at a loss of about €45 million. Tanzi was sentenced to 10 years in prison for fraud relating to the collapse of the dairy group. The other seven defendants, including executives and bankers, were acquitted. Another eight defendants settled out of court in September 2008.[6]\nWarren Buffett became known as one of the most successful and influential stock investors in history. His approach to investing is almost impossible for individual investors to duplicate because he uses leverage and a long-term approach that most people lack the will and wealth to follow.[7]\n\nDay trading sits at the extreme end of the investing spectrum from conventional buy-and-hold wisdom. It is the ultimate market-timing strategy. While all the attention that day trading attracts seems to suggest that the theory is sound, critics argue that, if that were so, at least one famous money manager would have mastered the system and claimed the title of \"the Warren Buffett of day trading\". The long list of successful investors that have become legends in their own time does not include a single individual that built his or her reputation by day trading.\n\nEven Michael Steinhardt, who made his fortune trading in time horizons ranging from 30 minutes to 30 days, claimed to take a long-term perspective on his investment decisions. From an economic perspective, many professional money managers and financial advisors shy away from day trading, arguing that the reward simply does not justify the risk. Despite the controversy, market timing is neither illegal nor unethical. Attempting to make a profit is the reason investors invest, and buy low and sell high is the general goal of most investors (although short-selling and arbitrage take a different approach, the success or failure of these strategies still depends on timing).\n\nThe problems with mutual fund trading that cast market timing in a negative light occurred because the prospectuses written by the mutual fund companies strictly forbid short-term trading. Despite this prohibition, special clients were allowed to do it anyway. So, the problem was not with the trading strategy but rather with the unethical and unfair implementation of that strategy, which permitted some investors to engage in it while excluding others. All of the world's greatest investors rely, to some extent, on market timing for their success. Whether they base their buy-sell decisions on fundamental analysis of the markets, technical analysis of individual companies, personal intuition, or all of the above, the ultimate reason for their success involves making the right trades at the right time. In most cases, those decisions involve extended periods of time and are based on buy-and-hold investment strategies. Value investing is a clear example, as the strategy is based on buying stocks that trade for less than their intrinsic values and selling them when their value is recognized in the marketplace. Most value investors are known for their patience, as undervalued stocks often remain undervalued for significant periods of time.\n\nSome investors choose a blend of technical, fundamental and environmental factors to influence where and when they invest. These strategists reject the 'chance' theory of investing, and attribute their higher level of returns to both insight and discipline.\nBernard Madoff claimed to make money by trading; in fact, it was all fictitious (see Ponzi scheme).\n\nFinancial fail and unsuccessful stories related with stock trading abound. Every year, a lot of money is wasted in non-peer-reviewed (and largely unregulated) publications and courses attended by credulous people that get persuaded and take the bill, hoping getting rich by trading on the markets. This allow widespread promotion of inaccurate and unproven trading methods for stocks, bonds, commodities, or Forex, while generating sizable revenues for unscrupulous authors, advisers and self-titled trading gurus. Most active money managers produce worse returns than an index, such as the S&P 500.[8]\n\nSpeculation in stocks is a risky and complex occupation because the direction of the markets are generally unpredictable and lack transparency, also financial regulators are sometimes unable to adequately detect, prevent and remediate irregularities committed by malicious listed companies or other financial market participants. In addition, the financial markets are usually subjected to speculation. This does not invalidate the well documented true and genuine stories of large success and consistent profitability of many individual stock investors and stock investing organizations along the history.\nSee also\n\n    Brokerage firm\n    Day trader\n    Dead cat bounce\n    Don't fight the tape\n    Fundamental analysis           \n    Option (finance)\n    Paper trading\n    Shareholder\n    Slippage (finance)\n    Stock\n    Stockbroker\n    Stock exchange\n    Stock market\n    Stock market data systems\n    Technical analysis\n    Technical analysis software\n    Trader (finance)\n    Value investing", "skillName": "Stock_Trader."}
{"id": 39, "category": "Statistics", "skillText": "20 \n. \nC\nHAP\nT E R   2 \nANCOVA \ngives \na more \npowerful \nlook \nat the \nIV-DV \nrelationship \nby \nminimizing \nerror \nvariance \n(cf. \nChapter \n3). \nThe \nstronger \nthe \nrelationship \nbetween \nthe \nDV \nand \nthe \ncovariate(s) \nis, \nthe \ngreater \nthe \npower \nof \nANCOVA \nover \nANOVA. \nANCOVA \nis  discussed \nin \nChapter \n8. \nANCOVA \nis \nalso \nused \nto \nadjust \nfor \ndifferences \namong \ngroups \nwhen \ngroups \nare \nnaturally \noccurring \nand \nrandom \nassignment \nto \nthem \nis \nnot \npossible. \nFor \nexample, \none \nmight \nask \nif \nattitude \ntoward \nabortion \n(the \nDV) \nvaries \nas \na function \nof \nreligious \naffiliation. \nHowever, \nit is  not \npossible \nto \nrandomly \nassign \npeople \nto \nreligious \naffiliation. \nIn \nthis \nsituation, \nthere \ncould \neasily \nbe \nother \nsystem­\natic \ndifferences \namong \ngroups, \nsuch \nas \nlevel \nof \neducation, \nthat \nare \nalso \nrelated \nto \nattitude \ntoward \nabortion. \nApparent \ndifferences among \nreligious \ngroups \nmight \nwell \nbe \ndue \nto \ndifferences \nin \neducation \nrather \nthan \ndifferences \nin \nreligious \naffiliation. \nTo \nget \na \"purer\" \nmeasure \nof \nthe \nrelationship \nbetween \nattitude \nand \nreligious \naffiliation, \nattitude \nscores \nare \nfirst \nadjusted \nfor \neducational \ndifferences, \nthat \nis, \neducation \nis \nused \nas \na  covariate. \nChapter \n8  also \ndiscusses \nthis \nsomewhat \nproblematical \nuse \nof \nANCOVA. \nWhen \nthere \nare \nmore \nthan \ntwo \ngroups, \nplanned \nor \npost \nhoc \ncomparisons \nare \navailable \nin \nANCOVA \njust \nas \nin \nANOVA. \nWith \nANCOVA, \nselected \nand/or \npooled \ngroup \nmeans \nare \nadjusted \nfor \ndifferences \non \ncovariates \nbefore \ndifferences \nin \nmeans \non \nthe \nDV \nare \nassessed. \n2.1.2.3 \nFactorial \nANOVA \nFactorial \nANOVA, \nreviewed \nin \nChapter \n3, \nis \nthe \nsubject \nof \nnumerous \nstatistics \ntexts \n(e.g., \nKeppel, \n1991; \nMyers \n& \nWell, \n1991; \nWiner, \n1971; \nTabachnick \n& \nFidell, \n2001) \nand \nis \nintroduced \nin \nmost ele­\nmentary \ntexts. \nAlthough \nthere \nis only \none \nDV \nin \nfactorial \nANOVA, \nits \nplace \nwithin \nthe general \nlinear \nmodel \nis  discussed \nin \nChapter \n17.\nL\n2.1.2.4 \nFactorial \nAN~OVA\nFactorial \nANCOVA \ndiffers from \none-way \nANCOVA \nonly \nin \nthat \nthere \nis more \nthan \none \nIV. \nThe \ndesir­\nability \nand \nuse \nof \ncovariates \nare \nthe \nsame. \nFor \ninstance, \nin \nthe \neducational \ntherapy \nexample \nof \nSec­\ntion \n2.1.2.2, \nanother \ninteresting \nIV \nmight \nbe \ngender \nof \nthe \nchild. \nThe \neffects \nof \ngender, \ntype \nof \neducational \ntherapy, \nand \ntheir \ninteraction \non \noutcome \nare \nassessed \nafter \nadjusting \nfor \nage \nand \nprior \ndegree \nof \nreading \ndisability. \nThe \ninteraction \nof \ngender \nwith \ntype \nof \ntherapy \nasks \nif \nboys \nand \ngirls \ndif­\nfer \nas \nto \nwhich \ntype \nof \neducational \ntherapy \nis  more \neffective \nafter \nadjustment \nfor \ncovariates. \n2.1.2.5 \nHotelling's \nT2 \nHotelling's \nT \n2 \nis \nused \nwhen \nthe \nIV \nhas \nonly \ntwo \ngroups \nand \nthere \nare \nseveral \nDVs. \nFor \nexample, \nthere \nmight \nbe \ntwo \nDV\ns, \nsuch \nas \nscore \non \nan \nacademic \nachievement \ntest \nand \nattention \nspan \nin \nthe \nclassroom, \nand \ntwo \nlevels \nof \ntype \nof \neducational \ntherapy, \nemphasis \non \nperceptual \ntraining \nversus \nemphasis \non \nacademic \ntraining. \nIt \nis  not \nlegitimate \nto \nuse \nseparate \nt \ntests \nfor \neach \nDV \nto \nlook \nfor \ndif­\nferences \nbetween \ngroups \nbecause \nthat \ninflates \nType \nI error \ndue \nto \nunnecessary \nmultiple \nsignificance \ntests \nwith \n(likely) \ncorrelated \nDVs. \nInstead, \nHotelling's \nT\n2 \nis  used \nto \nsee \nif \ngroups \ndiffer \non \nthe \ntwo \nDVs \ncombined. \nThe \nresearcher \nasks \nif \nthere \nare \nreliable \ndifferences \nin \nthe centroids \n(average \non \nthe \ncombined \nDVs) \nfor \nthe \ntwo \ngroups. \nHotelling's \nT \n2 \nis \na special \ncase \nof \nmultivariate \nanalysis \nof \nvariance, \njust \nas \nthe \nt \ntest \nis  a spe­\ncial \ncase \nof \nunivariate \nanalysis \nof \nvariance, \nwhen \nthe \nIV \nhas \nonly \ntwo \ngroups. Multivariate \nanalysis \nof \nvariance \nis \ndiscussed \nin \nChapter \n9. \n21 \nA \nGuide \nto \nStatistical \nTechniques \nI\" \n2.1.2.6 \nOne-\nWay \nMANOVA \n\\ \nMultivariate \nanalysis \nof \nvariance \nevaluates \ndifferences \namong \ncentroids \nfor \na set \nof \nDVs \nwhen \nthere \nare \ntwo \nor \nmore \nlevels \nof \nan \nIV \n(groups). \nMANOVA \nis  useful \nfor \nthe \neducational \ntherapy \nexample \nin \nthe \npreceding \nsection \nwith \ntwo \ngroups \nand \nalso \nwhen \nthere \nare \nmore \nthan two \ngroups \n(e.g., \nif \na non­\ntreatment control \ngroup \nis added). \nWith \nmore \nthan \ntwo \ngroups, \nplanned \nand \npost \nhoc \ncomparisons \nare \navailable. \nFor \nexample, \nif \na main \neffect \nof \ntreatment \nis  found \nin \nMANOVA, \nit might \nbe \ninteresting \nto \nask \npost \nhoc \nif \nthere \nare \ndifferences \nin \nthe \ncentroids \nof \nthe \ntwo \ngroups \ngiven \ndifferent \ntypes \nof \neducational \ntherapy, \nignoring \nthe \ncontrol \ngroup, \nand, \npossibly, \nif \nthe \ncentroid \nof \nthe \ncontrol \ngroup \ndiffers \nfrom \nthe \ncentroid \nof \nthe \ntwo \neducational \ntherapy \ngroups \ncombined. \nAny \nnumber \nof \nDVs \nmay \nbe \nused; \nthe \nprocedure \ndeals \nwith \ncorrelations \namong \nthem, \nand \nthe \nentire \nanalysis \nis  accomplished \nwithin \nthe \npreset \nlevel \nfor \nType \nI error. \nOnce \nreliable \ndifferences \nare \nfound, \ntechniques \nare \navailable \nto \nassess \nwhich \nDVs \nare \ninfluenced \nby \nwhich \nIV. \nFor \nexample, \nassignment \nto \ntreatment \ngroup \nmight affect \nthe \nacademic \nDV \nbut \nnot \nattention \nspan. \nMANOVA \nis \nalso \navailable \nwhen \nthere \nare \nwithin-subject \nIVs. \nFor \nexample, \nchildren \nmight \nbe \nmeasured \non \nboth \nDVs \nthree times: \n3,6, \nand \n9 months \nafter \ntherapy \nbegins. \nMANOVA \nis \ndiscussed \nin \nChapter \n9 and \na special case \nof \nit (profile \nanalysis. \nin \nwhich \nthe \nwithin-subjects \nIV \nis  treat \nmulti­\nvariately) \nin \nChapter \n10. \nProfile \nanalysis \nalso \nis \nan \nalternative \nto \none-way \nbetween-subjects \nMANOVA \nwhen \nthe \nDVs \nare \nall \nmeasured \non \nthe \nsame \nscale. \nDiscriminant \nfunction \nanalysis \nis  also \navailable \nfor \none-way \nbetween-subjects \ndesigns, \nas \ndescribed \nin \nSection \n2.1.3.1 \nand \nChapter \n11. \n2.1.2.7 \nOne-Way \nMANCOVA \n:...--\ns \nIn \naddition \nto \ndealing \nwith \nmultiple \nDVs, \nmultivariate \nanalysis \nof \nvariance \ncan be \napplied \nto \nprob­\nlems \nwhen \nthere \nare \none \nor \nmore \ncovariates. \nIn \nthis \ncase, \nMANOVA \nbecomes \nmultivariate \nanalysis \nof \ncovariance-MANCOVA. \nIn \nthe \neducational \ntherapy \nexample \nof \nSection \n2.1.2.6, \nit  might \nbe \nworthwhile \nto \nadjust \nthe \nDV \nscores \nfor \npretreatment \ndifferences \nin academic \nachievement \nand \natten­\ntion \nspan. \nHere \nthe \ncovariates \nare \npretests \nof \nthe \nDV\ns, \na  classic \nuse \nof \ncovariance \nanalysis. \nAfter \nadjustment \nfor \npretreatment \nscores, \ndifferences \nin \nposttest \nscores \n(DVs) \ncan be \nmore \nclearly \nattrib­\nuted \nto \ntreatment \n(the \ntwo \ntypes \nof \neducational \ntherapy \nplus \ncontrol \ngroup \nthat \nmake \nup \nthe \nIV). \nIn \nthe \none-way \nANCOVA \nexample \nof \nreligious \ngroups \nin \nSection \n2.1.2.2, \nit might \nbe \ninterest­\ning \nto \ntest \npolitical \nliberalism \nversus \nconservatism, \nand \nattitude \ntoward \necology, \nas \nwell \nas \nattitude \ntoward \nabortion, \nto \ncreate \nthree \nDV\ns. \nHere \nagain, \ndifferences \nin \nattitudes \nmight \nbe \nassociated \nwith \nboth \ndifferences \nin \nreligion \nand \ndifferences \nin \neducation \n(which, \nin \ntum, \nvaries \nwith \nreligious \naffili­\nation). \nIn \nthe \ncontext \nof \nMANCOVA, \neducation \nis  the \ncovariate, \nreligious \naffiliation \nthe \nIV, \nand \natti­\ntudes \nthe \nDV\ns. \nDifferences \nin attitudes \namong \ngroups \nwith \ndifferent \nreligious \naffiliation \nare \nassessed \nafter \nadjustment \nfor \ndifferences \nin education. \nIf\nthe \nIV \nhas \nmore \nthan \ntwo \nlevels, \nplanned \nand \npost \nhoc \ncomparisons \nare \nuseful, \nwith \nadjust­\nment \nfor \ncovariates. \nMANCOVA \n(Chapter \n9) \nis  available \nfor \nboth \nthe \nmain \nanalysis \nand \ncomparisons. \n2.1.2.8 \nFactorial \nMANOVA \nFactorial \nMANOVA \nis \nthe \nextension \nof \nMANOVA \nto \ndesigns \nwith \nmore \nthan \none \nIV \nand \nmultiple \nDVs. \nFor \nexample, \ngender \n(a \nbetween-subjects \nIV) \nmight \nbe \nadded \nto \ntype \nof \neducational \ntherapy \n(another \nbetween-subjects \nIV) \nwith \nboth \nacademic \nachievement \nand \nattention \nspan \nused \nas \nDVs. \nIn \n29 \nA Guide \nto \nStatistical \nTechniques \nTABLE \n2.1 \nContinued \nNumber \nNumber \nMajor \n(Kind) \nof \n(Kind) \nof \nResearch \nDependent \nIndependent \nAnalytic \nGoal \nof \nQuestion \nVariables \nVariables \nCovariates \nStrategy \nAnalysis \n-i\netermine \nhow \nNone \nN \nSurvival \nanalysis \nlong \nit takes \nfor \none \n-\n(life \ntables) \nsomething \nto \nhappen. \nOne \n(time) \nCreate \na linear \ncombination \nof\nOne \nor \nNone \nor \nSurvival \nanalysis_----i \nIVs \nand \nCVs \nto \nmore \nsome \n(with \npredictors) \npredict \ntime \nto \nan \nTime \nevent.\ncourse \nof \nevents \n~\n\". \n\"[Predict \nf~ture\n\" \nNone \nor \n1S\nTime-senes \nanalYi\ncourse \not \nDV \non\nT\nIme \n--\nsome \n(forecasting) \nbasis \nof \npast \n\\.\nOne \nI \ncourse \nof \nDY. \n(continuous) \nD \n\" \netermme\nOne \nor \nmore \n\"\"\" \n. \n)  g \nsome \n(mtervenhon) \nDV \nchanges \nWIth\nhme \n.\n.\nmterventlOn. \nommend \na flexible \napproach \nto \ndata \nanalysis \nin \nwhich \nboth \nunivariate \nand \nmultivariate \nprocedures \nare \nused \nto \nclarify \nthe \nresults. \n2.3 \nTechnique \nChapters \nChapters \n5 through \n16, \nthe \nbasic \ntechnique \nchapters, \nfollow \na common \nformat. \nFirst, \nthe \ntechnique \nis \ndescribed \nand \nthe \ngeneral \npurpose \nbriefly \ndiscussed. \nThen \nthe \nspecific \nkinds \nof \nquestions \nthat \ncan \nbe \nanswered \nthrough \napplication \nof \nthat \ntechnique \nare \nlisted. \nNext, \nboth \nthe \ntheoretical \nand \npractical \nlimitations \nof \nthe \ntechnique \nare \ndiscussed; \nthis \nsection \nlists \nassumptions \nparticularly \nassociated \nwith \nthe \ntechnique, \ndescribes \nmethods \nfor \nchecking \nthe \nassumptions \nfor \nyour \ndata \nset, \nand \ngives \nsugges­\ntions \nfor \ndealing \nwith \nviolations. \nThen \na small \nhypothetical \ndata \nset \nis used \nto \nillustrate \nthe \nstatistical \ndevelopment \nof \nthe \nprocedure. \nIt \nis recommended \nthat \nstudents \nfollow \nthe \nmatrix \ncalculations \nusing \na matrix \nalgebra \nprogram \navailable \nin \nthe \nthree \nstatistical \ncomputer \npackages \nor \na spreadsheet \npro­\ngram \nsuch \nas \nExcel \nor \nQuattro. \nSimple \nanalyses \nby \nprograms \nfrom \nthree \ncomputer \npackages follow. \nThe \nnext \nsection \ndescribes \nthe \nmajor \ntypes \nof \nthe \ntechnique, \nwhen \nappropriate. \nThen \nsome \nof \nthe \nmost \nimportant \nissues \nto \nbe \nconsidered \nwhen \nusing \nthe \ntechnique \nare \ncovered, \nincluding \nspecial \nstatistical \ntests, \ndata \nsnooping, \nand \nthe \nlike. \n30 \nC H A \nPTE\nR   2 \nThe \nnext \nsection \nshows \na step-by-step \napplication \nof \nthe \ntechnique \nto \nactual \ndata \ngathered, \nas \ndescribed \nin \nAppendix \nB. \nAssumptions \nare \ntested \nand \nviolations \ndealt \nwith, \nwhen \nnecessary. \nMajor \nhypotheses \nare \nevaluated, \nand \nfollow-up analyses \nare \nperformed \nas \nindicated. \nThen \na Results \nsection \nis  developed, \nas \nmight \nbe \nappropriate \nfor \nsubmission \nto \na professional \njournal. \nWhen \nmore \nthan \none \nmajor \ntype \nof \ntechnique \nis  available, \nthere \nare \nadditional \ncomplete \nexamples using \nreal \ndata. \nFinally, \na detailed \ncomparison \nof \nfeatures \navailable \nin \nthe \nSPSS, \nSAS, \nand \nSYSTAT \nprograms \nis  made. \nIn \nworking \nwith \nthese \ntechnique \nchapters, \nit is  suggested \nthat \nthe \nstudent/researcher \napply \nthe \nvarious \nanalyses \nto \nsome \ninteresting \nlarge \ndata \nset. \nMany \ndata \nbanks \nare \nreadily \naccessible \nthrough \ncomputer \ninstallations. \nFurther, \nalthough \nwe \nrecommend \nmethods \nof\nreporting \nmultivariate \nresults, \nit may be \ninappro­\npriate \nto \nreport \nthem \nfully \nin \nall \npublications. \nCertainly, \none \nwould \nat least \nwant \nto \nmention \nthat \nuni­\nvariate \nresults \nwere \nsupported \nand \nguided \nby \nmultivariate \ninference. \nBut \nthe \ndetails \nassociated \nwith \na full \ndisclosure \nof \nmultivariate \nresults \nat \na  colloquium, \nfor \ninstance, \nmight \nrequire \nmore \nattention \nthan \none \ncould \nreasonably \nexpect \nfrom \nan \naudience. \nLikewise, \na full \nmultivariate \nanalysis \nmay \nbe \nmore \nthan \nsome \njournals \nare \nwilling \nto \nprint. \n2.4 \nPreliminary \nCheck \nof \nthe \nData \nBefore \napplying \nany \ntechnique, \nor \nsometimes \neven \nbefore \nchoosing \na  technique, \nyou \nshould \ndeter­\nmine \nthe \nfit \nbetween \nyour \ndata \nand \nsome \nvery \nbasic \nassumptions \nunderlying \nmost \nof \nthe \nmultivariate \nstatistics. \nThough \neach \ntechnique \nhas \nspecific \nassumptions \nas \nwell, \nmost \nrequire \nconsideration \nof \nmaterial \nin \nChapter \n4.", "skillName": "A_Guide_to_Statistical_Techniques."}
{"id": 40, "category": "Statistics", "skillText": "Statistical Techniques\n\nStatistical studies allow analysts to estimate key parameters of cost or production models. Econometric analyses require a large data set to ensure reliable results.  Obtaining the number of observations needed to derive an efficient and unbiased estimate of cost (or production) structures can often prove to be a difficult task.  Regression results are sensitive to model specification (for example, a linear vs. a non-linear functional form).  In addition, for some models, the interpretation of the error term becomes important.\n\n\nThe early studies tended to utilize Ordinary Least Squares (OLS) to estimate cost functions for firms.  Due to data limitations, most of these studies were cross-sectional in nature.  Besides using data from only a single year researchers utilized data from England and Wales or from the United States. These academic studies often focused on the relative performance of private vs. publicly-owned water and sewerage utilities.  In addition, they investigated the extent of scale economies and economies of joint production (providing both water and sewerage services).  In some cases, they considered the impacts of residential vs. industrial/commercial customers.\n\n\nAs data from Brazil, Peru, and other emerging nations became available, additional country studies were publishedoften using more advanced econometric (parametric) or non-parametric data analysis techniques. Studies of utilities in France, Italy, and other nations began to appear in the academic literature. Techniques associated with Stochastic Frontier Analysis began to be applied to both production functions and cost functions.  Panel data facilitated the incorporation of customer density, topology, and other variables.\n\n\nThe most commonly used parametric methods are ordinary least squares (OLS) ,  corrected ordinary least squares (COLS) models and  Stochastic Frontier Analysis (SFA). The main difference between these models is that COLS attributes all the deviations to inefficiency while SFA models attribute part of the deviations to inefficiency and part of the deviations to random noise. In other words, the SFA models take both inefficiency and random noise into account. The most widely used stochastic frontier models include the stochastic production frontier model, stochastic cost frontier model, and stochastic distance function model. Before selecting a specific model, analysts have to make an initial choice between the two most widely used functional forms: Cobb-Douglas function and translog function.\n\n\nOrdinary least squares (OLS) models\n\nOLS techniques can be used to perform benchmarking that relates individual firm performance relative to what would be expected: an estimate of an average production or cost function of a sample of firms.  Average benchmarking methods may be used to compare firms with relatively similar costs or when there is a lack of sufficient data of comparable firms for the application of frontier methods.  Basically, the method refers to the estimation of a regression functional form for costs or production using the OLS approach.  Linear regression analysis seeks to derive a relationship between firm performance (in terms of output or total cost) and market conditions and characteristics of the production processes.  Statistical analysis can isolate the impacts of specific conditions or levels of outputso the roles of multiple independent variables can be determined.  Data from the firms being compared can then be used to arrive at expected dimensions of firm performance, given the variables characterizing each firm.\n\n\nThe technique of regression analysis is defined by the following steps: 1) selecting both the cost (or output) measure and exogenous variables, 2) estimating a cost (or production) function for the industry, and 3) calculating the efficiency coefficient for each firm within the industry.  Predicted versus actual output provides a measure of relative performance.  The quality of these results can then be statistically evaluated to provide the policy-maker with a framework for evaluating firms. The linear vs. non-linear issue can be examined by including parameters that capture scale economies or diseconomies. \n\n    Advantages:  The statistical method reveals information about cost structures and distinguishes between different variables’ roles in affecting output.  Coefficients can be interpreted in terms of cost drivers or how inputs contribute to output.\n\n    Disadvantages:  Large data set is necessary in order to obtain reliable results.  The regression results are sensitive to functional form if the error term is not adequately interpreted, which can lead to widely varying conclusions, depending on how the regression is initially set up. \n\n    Application:  The UK water regulator OFWAT applies mean and average methods to the operating costs (OPEX) and capital expenditures (CAPEX) of water utilities when determining the price caps every five years. OFWAT has developed an efficiency analysis relying on mean and average methods that is a key part of its price determination process.\n\n\nCorrected ordinary least squares (COLS) models\n\nA slightly different approach than OLS involves shifting the line towards the best performing company, which is called Corrected Least Squares methodology (COLS).  In a general sense, COLS is merely a shifted average function. Two steps are needed, one to get the expected value of the error term and another to shift or to “center” the equation.\n\n\nWhen using OLS or COLS it is good practice to perform Quantile analysis. Quantile analysis helps to overcome the possible effect of outliers on the estimated mean allowing the analyst to detect the presence of performers on specific or extreme quantiles such as the lower (25%) or the upper (75%) quantiles.\n\n    Advantages: The statistical method reveals information about cost structures and distinguishes between different variables’ roles in affecting output.  The adjustment turns the OLS into a “frontier” approach.\n\n    Disadvantages: As with OLS, a large data set is necessary in order to obtain reliable results.  The regression results are sensitive to functional form if the error term is not adequately interpreted, which can lead to widely varying conclusions depending on how the regression is initially set up.  Furthermore, the results are especially sensitive to outliers, since the “best” performer along any dimension serves as the anchor for the estimate.  Thus, the performance scores are very sensitive to outliers.\n\n    Application:  Most studies that analyze frontier relationships utilize Stochastic Frontier Analysis (SFA).  Some simplicity is then lost, but tests of the sources of different types of errors can be identified with SFA.\n\n\n\nOrdinary least squares (OLS) models\n\nStochastic Frontier Analysis attempts to estimate an efficient frontier which does incorporates the possibility of measurement error or chance factors in its estimation. To separate inefficiency and noise, strong assumptions are needed on the distribution of noise among each observed firm.  Stochastic frontiers may be classified as Production, Cost, and Input Distance frontiers.\n\n\nA production frontier reveals technical relationships between inputs and outputs of firms and represents an alternative when cost frontiers can not be calculated due to lack of data. The estimated output is the maximum possible output for given inputs of an individual firm.  The output difference obtained in the estimation is interpreted as technical inefficiency of each individual firm.  On a production frontier, variable returns to scale is the sensible option and appropriate scale efficiency changes need to be included when calculating total factor productivity.\n\n\nA cost frontier shows costs as a function of the level of output/s and the prices of inputs. It is useful when trying to access the wedge between tariff and minimum costs. Conceptually, the minimum cost function defines a frontier showing costs technically possible associated with various levels of inputs and control variables.  Total cost frontier rather than variable or expenditure cost frontier is preferable to account for substitutability of factor inputs.  Separate models for CAPEX and OPEX do not allow for allocation of expenditures between operating and capital expenditure.  Cost efficiency contains the effects of technical and allocative efficiency.\n\n\nEach approach (production or cost) may yield different results.  The difference will be larger if large allocative distortions are present.  In this case, the parameters of the cost frontier will be biased.  An important factor to consider when choosing between a cost frontier and a production frontier is that usually regulated firms are required to provide the service at a preset tariff and they must meet demand.  In this sense, firms are not allowed to choose their own level of output which makes output an exogenous variable.  The regulated firm maximizes benefits by minimizing its costs of producing a given level of output.  Cost is the choice variable for the firm so a cost frontier approach is a more sensible choice.\n\n\nFinally, an input distance frontier is the natural option for regulated industries where output quantity is exogenous and input quantities are endogenous, and when the nature of the technology is multiple outputs or there is not data available on price of inputs.  This is the case for water and sewerage as different outputs under the same firm where their provision comes from shared inputs which jointly determine the production function.\n\n\nA distance function may have either an input or an output orientation.  An input orientation looks at how much the input vector may be proportionally contracted with the output vector held fixed. An output orientation looks at how much the output vector may be proportionally expanded with the input vector held fixed.   Input distance functions can be estimated by either stochastic or DEA methods. The advantage of a distance frontier with regard to a cost frontier is that firm is not assumed to be minimizing costs. With respect to production frontier is that it avoids the endogenous problem.\n\n    Advantages of Stochastic Frontiers:  Accounts for data noise such as data errors and omitted variables.  Standard statistical tests can be used to test hypotheses on model specification and significance of the variables included on the model.  It is also more amenable to modeling effects of other variables (e.g., environment, quality)\n\n    Disadvantages of Stochastic Frontiers: There is a need of functional form and production technology specification. Also, the separation of noise and inefficiency relies on strong assumptions on the distribution of the error term\n\n    Application: A number of studies utilize these techniques, such as the relative efficiency of public and private water companies in East Asia and the Pacific.", "skillName": "StatisticalTechniques."}
{"id": 41, "category": "Statistics", "skillText": "Statistics is the study of the collection, analysis, interpretation, presentation, and organization of data.[1] In applying statistics to, e.g., a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1]\n\nSome popular definitions are:\n\n    Merriam-Webster dictionary defines statistics as \"classified facts representing the conditions of a people in a state – especially the facts that can be stated in numbers or any other tabular or classified arrangement[2]\".\n    Statistician Sir Arthur Lyon Bowley defines statistics as \"Numerical statements of facts in any department of inquiry placed in relation to each other[3]\".\n\nWhen census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\n\nTwo main statistical methodologies are used in data analysis: descriptive statistics, which summarizes data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draws conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[4] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\n\nA standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and a synthetic data drawn from idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\").[5] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]\n\nMeasurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\nStatistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.\nContents\n\n    1 Scope\n        1.1 Mathematical statistics\n    2 Overview\n    3 Data collection\n        3.1 Sampling\n        3.2 Experimental and observational studies\n    4 Types of data\n    5 Terminology and theory of inferential statistics\n        5.1 Statistics, estimators and pivotal quantities\n        5.2 Null hypothesis and alternative hypothesis\n        5.3 Error\n        5.4 Interval estimation\n        5.5 Significance\n        5.6 Examples\n    6 Misuse\n        6.1 Misinterpretation: correlation\n    7 History of statistical science\n    8 Applications\n        8.1 Applied statistics, theoretical statistics and mathematical statistics\n        8.2 Machine learning and data mining\n        8.3 Statistics in society\n        8.4 Statistical computing\n        8.5 Statistics applied to mathematics or the arts\n    9 Specialized disciplines\n    10 See also\n    11 References\n    12 Further reading\n    13 External links\n\nScope\n\nStatistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data,[6] or as a branch of mathematics.[7] Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.[8][9]\nMathematical statistics\nMain article: Mathematical statistics\n\nMathematical statistics is the application of mathematics to statistics, which was originally conceived as the science of the state — the collection and analysis of facts about a country: its economy, land, military, population, and so forth. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[10][11]\nOverview\n\nIn applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as \"all persons living in a country\" or \"every atom composing a crystal\".\n\nIdeally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).\n\nWhen a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.\nData collection\nSampling\n\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting the use of data through statistical models. To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\n\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.\nExperimental and observational studies\n\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data – like natural experiments and observational studies[12] – for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\nExperiments\n\nThe basic steps of a statistical experiment are:\n\n    Planning the research, including finding the number of replicates of the study, using the following information: preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.\n    Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.\n    Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol.\n    Further examining the data set in secondary analyses, to suggest new hypotheses for future study.\n    Documenting and presenting the results of the study.\n\nExperiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[13]\nObservational study\n\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a case-control study, and then look for the number of cases of lung cancer in each group.\nTypes of data\nMain articles: Statistical data type and Levels of measurement\n\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\n\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\n\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977)[14] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[15] described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998),[16] van den Berg (1991).[17]\n\nThe issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer\" (Hand, 2004, p. 82).[18]\nTerminology and theory of inferential statistics\nStatistics, estimators and pivotal quantities\n\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[19] The population being examined is described by a probability distribution that may have unknown parameters.\n\nA statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.\n\nConsider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\n\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\n\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\n\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\n\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\nNull hypothesis and alternative hypothesis\n\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[20][21]\n\nThe best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\n\nWhat statisticians call an alternative hypothesis is simply an hypothesis that contradicts the null hypothesis.\nError\n\nWorking from a null hypothesis, two basic forms of error are recognized:\n\n    Type I errors where the null hypothesis is falsely rejected giving a \"false positive\".\n    Type II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\".\n\nStandard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\n\nA statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\n\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\nA least squares fit: in red the points to be fitted, in blue the fitted line.\n\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\n\nMeasurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.[22]\nInterval estimation\nMain article: Interval estimation\nConfidence intervals: the red line is true value for the mean in this example, the blue lines are random confidence intervals for 100 realizations.\n\nMost studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\n\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\nSignificance\nMain article: Statistical significance\n\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\nIn this graph the black line is probability distribution for the test statistic, the critical region is the set of values to the right of the observed data point (observed value of the test statistic) and the p-value is represented by the green area.\n\nThe standard approach[19] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\n\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\n\nWhile in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.\n\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\n\n    A difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.\n    Fallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.[23]\n    Rejecting the null hypothesis does not automatically prove the alternative hypothesis.\n    As everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.\n\nExamples\n\nSome well-known statistical tests and procedures are:\n\n    Analysis of variance (ANOVA)\n    Chi-squared test\n    Correlation\n    Factor analysis\n    Mann–Whitney U\n    Mean square weighted deviation (MSWD)\n    Pearson product-moment correlation coefficient\n    Regression analysis\n    Spearman's rank correlation coefficient\n    Student's t-test\n    Time series analysis\n    Conjoint Analysis\n\nMisuse\nMain article: Misuse of statistics\n\nMisuse of statistics can produce subtle, but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\n\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\n\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[24] A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics[24] outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[25]\n\nWays to avoid misuse of statistics include using proper diagrams and avoiding bias.[26] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[27] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[26] Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[27] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[28] According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"[29]\n\nTo assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[30]\n\n    Who says so? (Does he/she have an axe to grind?)\n    How does he/she know? (Does he/she have the resources to know the facts?)\n    What’s missing? (Does he/she give us a complete picture?)\n    Did someone change the subject? (Does he/she offer us the right answer to the wrong problem?)\n    Does it make sense? (Is his/her conclusion logical and consistent with what we already know?)\n\nThe confounding variable problem: X and Y may be correlated, not because there is causal relationship between them, but because both depend on a third variable Z. Z is called a confounding factor.\nMisinterpretation: correlation\n\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)\nHistory of statistical science\nGerolamo Cardano, the earliest pioneer on the mathematics of probability.\nMain articles: History of statistics and Founders of statistics\n\nStatistical methods date back at least to the 5th century BC.\n\nSome scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[31] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n\nIts mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel.[32] The method of least squares was first described by Adrien-Marie Legendre in 1805.\nKarl Pearson, a founder of mathematical statistics.\n\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages.[33] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics – height, weight, eyelash length among others.[34] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[35] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[36] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[37]\n\nRonald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".[38][39]\n\nThe second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance, which was the first to use the statistical term, variance, his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[40][41][42][43] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[44] In his 1930 book The Genetical Theory of Natural Selection he applied statistics to various biological concepts such as Fisher's principle[45]). Nevertheless, A. W. F. Edwards has remarked that it is \"probably the most celebrated argument in evolutionary biology\".[45] (about the sex ratio), the Fisherian runaway,[46][47][48][49][50][51] a concept in sexual selection about a positive feedback runaway affect found in evolution.\n\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[52]\n\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.[53]\nApplications\nApplied statistics, theoretical statistics and mathematical statistics\n\n\"Applied statistics\" comprises descriptive statistics and the application of inferential statistics.[54][55] Theoretical statistics concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\nMachine learning and data mining\n\nThere are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis.\nStatistics in society\n\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.\nStatistical computing\ngretl, an example of an open source statistical package\nMain article: Computational statistics\n\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\n\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available.\nStatistics applied to mathematics or the arts\n\nTraditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.\n\n    In number theory, scatter plots of data generated by a distribution function may be transformed with familiar tools used in statistics to reveal underlying patterns, which may then lead to hypotheses.\n    Methods of statistics including predictive methods in forecasting are combined with chaos theory and fractal geometry to create video works that are considered to have great beauty.[citation needed]\n    The process art of Jackson Pollock relied on artistic experiments whereby underlying distributions in nature were artistically revealed.[citation needed] With the advent of computers, statistical methods were applied to formalize such distribution-driven natural processes to make and analyze moving video art.[citation needed]\n    Methods of statistics may be used predicatively in performance art, as in a card trick based on a Markov process that only works some of the time, the occasion of which can be predicted using statistical methodology.\n    Statistics can be used to predicatively create art, as in the statistical or stochastic music invented by Iannis Xenakis, where the music is performance-specific. Though this type of artistry does not always come out as expected, it does behave in ways that are predictable and tunable using statistics.\n\nSpecialized disciplines\nMain article: List of fields of application of statistics\n\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n\n    Actuarial science (assesses risk in the insurance and finance industries)\n    Applied information economics\n    Astrostatistics (statistical evaluation of astronomical data)\n    Biostatistics\n    Business statistics\n    Chemometrics (for analysis of data from chemistry)\n    Data mining (applying statistics and pattern recognition to discover knowledge from data)\n    Data science\n    Demography\n    Econometrics (statistical analysis of economic data)\n    Energy statistics\n    Engineering statistics\n    Epidemiology (statistical analysis of disease)\n    Geography and Geographic Information Systems, specifically in Spatial analysis\n    Image processing\n    Medical Statistics\n    Psychological statistics\n    Reliability engineering\n    Social statistics\n    Statistical Mechanics\n\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\n\n    Bootstrap / Jackknife resampling\n    Multivariate statistics\n    Statistical classification\n    Structured data analysis (statistics)\n    Structural equation modelling\n    Survey methodology\n    Survival analysis\n    Statistics in various sports, particularly baseball - known as Sabermetrics - and cricket\n\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.\nSee also\nLibrary resources about\nStatistics\n\n    Resources in your library \n\nMain article: Outline of statistics\n\n    Abundance estimation\n    Data science\n    Glossary of probability and statistics\n    List of academic statistical associations\n    List of important publications in statistics\n    List of national and international statistical services\n    List of statistical packages (software)\n    List of statistics articles\n    List of university statistical consulting centers\n    Notation in probability and statistics\n\nFoundations and major areas of statistics\n\n    Foundations of statistics\n    List of statisticians\n    Official statistics\n    Multivariate analysis of variance", "skillName": "Statistics."}
{"id": 42, "category": "Statistics", "skillText": "In the preceding chapters basic elements for the proper execution of analytical work such as personnel, laboratory facilities, equipment, and reagents were discussed. Before embarking upon the actual analytical work, however, one more tool for the quality assurance of the work must be dealt with: the statistical operations necessary to control and verify the analytical procedures (Chapter 7) as well as the resulting data (Chapter 8).\n\nIt was stated before that making mistakes in analytical work is unavoidable. This is the reason why a complex system of precautions to prevent errors and traps to detect them has to be set up. An important aspect of the quality control is the detection of both random and systematic errors. This can be done by critically looking at the performance of the analysis as a whole and also of the instruments and operators involved in the job. For the detection itself as well as for the quantification of the errors, statistical treatment of data is indispensable.\n\nA multitude of different statistical tools is available, some of them simple, some complicated, and often very specific for certain purposes. In analytical work, the most important common operation is the comparison of data, or sets of data, to quantify accuracy (bias) and precision. Fortunately, with a few simple convenient statistical tools most of the information needed in regular laboratory work can be obtained: the \"t-test, the \"F-test\", and regression analysis. Therefore, examples of these will be given in the ensuing pages.\n\nClearly, statistics are a tool, not an aim. Simple inspection of data, without statistical treatment, by an experienced and dedicated analyst may be just as useful as statistical figures on the desk of the disinterested. The value of statistics lies with organizing and simplifying data, to permit some objective estimate showing that an analysis is under control or that a change has occurred. Equally important is that the results of these statistical procedures are recorded and can be retrieved.\n6.2 Definitions\n\n    6.2.1 Error\n    6.2.2 Accuracy\n    6.2.3 Precision\n    6.2.4 Bias\n\nDiscussing Quality Control implies the use of several terms and concepts with a specific (and sometimes confusing) meaning. Therefore, some of the most important concepts will be defined first.\n6.2.1 Error\n\nError is the collective noun for any departure of the result from the \"true\" value*. Analytical errors can be:\n\n    1. Random or unpredictable deviations between replicates, quantified with the \"standard deviation\".\n\n    2. Systematic or predictable regular deviation from the \"true\" value, quantified as \"mean difference\" (i.e. the difference between the true value and the mean of replicate determinations).\n\n    3. Constant, unrelated to the concentration of the substance analyzed (the analyte).\n\n    4. Proportional, i.e. related to the concentration of the analyte.\n\n        * The \"true\" value of an attribute is by nature indeterminate and often has only a very relative meaning. Particularly in soil science for several attributes there is no such thing as the true value as any value obtained is method-dependent (e.g. cation exchange capacity). Obviously, this does not mean that no adequate analysis serving a purpose is possible. It does, however, emphasize the need for the establishment of standard reference methods and the importance of external QC (see Chapter 9).\n\n6.2.2 Accuracy\n\nThe \"trueness\" or the closeness of the analytical result to the \"true\" value. It is constituted by a combination of random and systematic errors (precision and bias) and cannot be quantified directly. The test result may be a mean of several values. An accurate determination produces a \"true\" quantitative value, i.e. it is precise and free of bias.\n6.2.3 Precision\n\nThe closeness with which results of replicate analyses of a sample agree. It is a measure of dispersion or scattering around the mean value and usually expressed in terms of standard deviation, standard error or a range (difference between the highest and the lowest result).\n6.2.4 Bias\n\nThe consistent deviation of analytical results from the \"true\" value caused by systematic errors in a procedure. Bias is the opposite but most used measure for \"trueness\" which is the agreement of the mean of analytical results with the true value, i.e. excluding the contribution of randomness represented in precision. There are several components contributing to bias:\n\n1. Method bias\n\n    The difference between the (mean) test result obtained from a number of laboratories using the same method and an accepted reference value. The method bias may depend on the analyte level.\n\n2. Laboratory bias\n\n    The difference between the (mean) test result from a particular laboratory and the accepted reference value.\n\n3. Sample bias\n\n    The difference between the mean of replicate test results of a sample and the (\"true\") value of the target population from which the sample was taken. In practice, for a laboratory this refers mainly to sample preparation, subsampling and weighing techniques. Whether a sample is representative for the population in the field is an extremely important aspect but usually falls outside the responsibility of the laboratory (in some cases laboratories have their own field sampling personnel).\n\nThe relationship between these concepts can be expressed in the following equation:\n\nFigure\n\nThe types of errors are illustrated in Fig. 6-1.\n\nFig. 6-1. Accuracy and precision in laboratory measurements. (Note that the qualifications apply to the mean of results: in c the mean is accurate but some individual results are inaccurate)\n\n6.3 Basic Statistics\n\n    6.3.1 Mean\n    6.3.2 Standard deviation\n    6.3.3 Relative standard deviation. Coefficient of variation\n    6.3.4 Confidence limits of a measurement\n    6.3.5 Propagation of errors\n\nIn the discussions of Chapters 7 and 8 basic statistical treatment of data will be considered. Therefore, some understanding of these statistics is essential and they will briefly be discussed here.\n\nThe basic assumption to be made is that a set of data, obtained by repeated analysis of the same analyte in the same sample under the same conditions, has a normal or Gaussian distribution. (When the distribution is skewed statistical treatment is more complicated). The primary parameters used are the mean (or average) and the standard deviation (see Fig. 6-2) and the main tools the F-test, the t-test, and regression and correlation analysis.\n\nFig. 6-2. A Gaussian or normal distribution. The figure shows that (approx.) 68% of the data fall in the range ¯ x± s, 95% in the range ¯x ± 2s, and 99.7% in the range ¯x ± 3s.\n6.3.1 Mean\n\nThe average of a set of n data xi:\n\n¯\n\t\n\n(6.1)\n\n6.3.2 Standard deviation\n\nThis is the most commonly used measure of the spread or dispersion of data around the mean. The standard deviation is defined as the square root of the variance (V). The variance is defined as the sum of the squared deviations from the mean, divided by n-1. Operationally, there are several ways of calculation:\n\n\t\n\n(6.1)\n\nor\n\n\t\n\n(6.3)\n\nor\n\n\t\n\n(6.4)\n\nThe calculation of the mean and the standard deviation can easily be done on a calculator but most conveniently on a PC with computer programs such as dBASE, Lotus 123, Quattro-Pro, Excel, and others, which have simple ready-to-use functions. (Warning: some programs use n rather than n- 1!).\n6.3.3 Relative standard deviation. Coefficient of variation\n\nAlthough the standard deviation of analytical data may not vary much over limited ranges of such data, it usually depends on the magnitude of such data: the larger the figures, the larger s. Therefore, for comparison of variations (e.g. precision) it is often more convenient to use the relative standard deviation (RSD) than the standard deviation itself. The RSD is expressed as a fraction, but more usually as a percentage and is then called coefficient of variation (CV). Often, however, these terms are confused.\n\n\t\n\n\t\n\n(6.5; 6.6)\n\n    Note. When needed (e.g. for the F-test, see Eq. 6.11) the variance can, of course, be calculated by squaring the standard deviation:\n\nV = s2\n\t\n\n(6.7)\n\n6.3.4 Confidence limits of a measurement\n\nThe more an analysis or measurement is replicated, the closer the mean x of the results will approach the \"true\" value m, of the analyte content (assuming absence of bias).\n\nA single analysis of a test sample can be regarded as literally sampling the imaginary set of a multitude of results obtained for that test sample. The uncertainty of such subsampling is expressed by\n\n\t\n\n(6.8)\n\nwhere\n\n    m = \"true\" value (mean of large set of replicates)\n    ¯x = mean of subsamples\n    t = a statistical value which depends on the number of data and the required confidence (usually 95%).\n    s = standard deviation of mean of subsamples\n    n = number of subsamples\n\n(The term is also known as the standard error of the mean.)\n\nThe critical values for t are tabulated in Appendix 1 (they are, therefore, here referred to as ttab ). To find the applicable value, the number of degrees of freedom has to be established by: df = n -1 (see also Section 6.4.2).\n\nExample\n\nFor the determination of the clay content in the particle-size analysis, a semi-automatic pipette installation is used with a 20 mL pipette. This volume is approximate and the operation involves the opening and closing of taps. Therefore, the pipette has to be calibrated, i.e. both the accuracy (trueness) and precision have to be established.\n\nA tenfold measurement of the volume yielded the following set of data (in mL):\n\n19.941\n\t\n\n19.812\n\t\n\n19.829\n\t\n\n19.828\n\t\n\n19.742\n\n19.797\n\t\n\n19.937\n\t\n\n19.847\n\t\n\n19.885\n\t\n\n19.804\n\nThe mean is 19.842 mL and the standard deviation 0.0627 mL. According to Appendix 1 for n = 10 is ttab = 2.26 (df = 9) and using Eq. (6.8) this calibration yields:\n\npipette volume = 19.842 ± 2.26 (0.0627/) = 19.84 ± 0.04 mL\n\n(Note that the pipette has a systematic deviation from 20 mL as this is outside the found confidence interval. See also bias).\n\nIn routine analytical work, results are usually single values obtained in batches of several test samples. No laboratory will analyze a test sample 50 times to be confident that the result is reliable. Therefore, the statistical parameters have to be obtained in another way. Most usually this is done by method validation (see Chapter 7) and/or by keeping control charts, which is basically the collection of analytical results from one or more control samples in each batch (see Chapter 8). Equation (6.8) is then reduced to\n\n\t\n\n(6.9)\n\nwhere\n\n    m = \"true\" value\n    x = single measurement\n    t = applicable ttab (Appendix 1)\n    s = standard deviation of set of previous measurements.\n\nIn Appendix 1 can be seen that if the set of replicated measurements is large (say > 30), t is close to 2. Therefore, the (95%) confidence of the result x of a single test sample (n = 1 in Eq. 6.8) is approximated by the commonly used and well known expression\n\n\t\n\n(6.10)\n\nwhere S is the previously determined standard deviation of the large set of replicates (see also Fig. 6-2).\n\n    Note: This \"method-s\" or s of a control sample is not a constant and may vary for different test materials, analyte levels, and with analytical conditions.\n\nRunning duplicates will, according to Equation (6.8), increase the confidence of the (mean) result by a factor :\n\nwhere\n\n    ¯x = mean of duplicates\n    s = known standard deviation of large set\n\nSimilarly, triplicate analysis will increase the confidence by a factor , etc. Duplicates are further discussed in Section 8.3.3.\n\nThus, in summary, Equation (6.8) can be applied in various ways to determine the size of errors (confidence) in analytical work or measurements: single determinations in routine work, determinations for which no previous data exist, certain calibrations, etc.\n6.3.5 Propagation of errors\n\n    6.3.5.1. Propagation of random errors\n    6.3.5.2 Propagation of systematic errors\n\nThe final result of an analysis is often calculated from several measurements performed during the procedure (weighing, calibration, dilution, titration, instrument readings, moisture correction, etc.). As was indicated in Section 6.2, the total error in an analytical result is an adding-up of the sub-errors made in the various steps. For daily practice, the bias and precision of the whole method are usually the most relevant parameters (obtained from validation, Chapter 7; or from control charts, Chapter 8). However, sometimes it is useful to get an insight in the contributions of the subprocedures (and then these have to be determined separately). For instance if one wants to change (part of) the method.\n\nBecause the \"adding-up\" of errors is usually not a simple summation, this will be discussed. The main distinction to be made is between random errors (precision) and systematic errors (bias).\n6.3.5.1. Propagation of random errors\n\nIn estimating the total random error from factors in a final calculation, the treatment of summation or subtraction of factors is different from that of multiplication or division.\n\nI. Summation calculations\n\nIf the final result x is obtained from the sum (or difference) of (sub)measurements a, b, c, etc.:\n\nx = a + b + c +...\n\nthen the total precision is expressed by the standard deviation obtained by taking the square root of the sum of individual variances (squares of standard deviation):\n\nIf a (sub)measurement has a constant multiplication factor or coefficient (such as an extra dilution), then this is included to calculate the effect of the variance concerned, e.g. (2b)2\n\nExample\n\nThe Effective Cation Exchange Capacity of soils (ECEC) is obtained by summation of the exchangeable cations:\n\nECEC = Exch. (Ca + Mg + Na + K + H + Al)\n\nStandard deviations experimentally obtained for exchangeable Ca, Mg, Na, K and (H + Al) on a certain sample, e.g. a control sample, are: 0.30, 0.25, 0.15, 0.15, and 0.60 cmolc/kg respectively. The total precision is:\n\nIt can be seen that the total standard deviation is larger than the highest individual standard deviation, but (much) less than their sum. It is also clear that if one wants to reduce the total standard deviation, qualitatively the best result can be expected from reducing the largest individual contribution, in this case the exchangeable acidity.\n\n2. Multiplication calculations\n\nIf the final result x is obtained from multiplication (or subtraction) of (sub)measurements according to\n\nthen the total error is expressed by the standard deviation obtained by taking the square root of the sum of the individual relative standard deviations (RSD or CV, as a fraction or as percentage, see Eqs. 6.6 and 6.7):\n\nIf a (sub)measurement has a constant multiplication factor or coefficient, then this is included to calculate the effect of the RSD concerned, e.g. (2RSDb)2.\n\nExample\n\nThe calculation of Kjeldahl-nitrogen may be as follows:\n\nwhere\n\n    a = ml HCl required for titration sample\n    b = ml HCl required for titration blank\n    s = air-dry sample weight in gram\n    M = molarity of HCl\n    1.4 = 14×10-3×100% (14 = atomic weight of N)\n    mcf = moisture correction factor\n\nNote that in addition to multiplications, this calculation contains a subtraction also (often, calculations contain both summations and multiplications.)\n\nFirstly, the standard deviation of the titration (a -b) is determined as indicated in Section 7 above. This is then transformed to RSD using Equations (6.5) or (6.6). Then the RSD of the other individual parameters have to be determined experimentally. The found RSDs are, for instance:\n\n    distillation: 0.8%,\n    titration: 0.5%,\n    molarity: 0.2%,\n    sample weight: 0.2%,\n    mcf: 0.2%.\n\nThe total calculated precision is:\n\nHere again, the highest RSD (of distillation) dominates the total precision. In practice, the precision of the Kjeldahl method is usually considerably worse (» 2.5%) probably mainly as a result of the heterogeneity of the sample. The present example does not take that into account. It would imply that 2.5% - 1.0% = 1.5% or 3/5 of the total random error is due to sample heterogeneity (or other overlooked cause). This implies that painstaking efforts to improve subprocedures such as the titration or the preparation of standard solutions may not be very rewarding. It would, however, pay to improve the homogeneity of the sample, e.g. by careful grinding and mixing in the preparatory stage.\n\n    Note. Sample heterogeneity is also represented in the moisture correction factor. However, the influence of this factor on the final result is usually very small.\n\n6.3.5.2 Propagation of systematic errors\n\nSystematic errors of (sub)measurements contribute directly to the total bias of the result since the individual parameters in the calculation of the final result each carry their own bias. For instance, the systematic error in a balance will cause a systematic error in the sample weight (as well as in the moisture determination). Note that some systematic errors may cancel out, e.g. weighings by difference may not be affected by a biased balance.\n\nThe only way to detect or avoid systematic errors is by comparison (calibration) with independent standards and outside reference or control samples.\n6.4 Statistical tests\n\n    6.4.1 Two-sided vs. one-sided test\n    6.4.2 F-test for precision\n    6.4.3 t-Tests for bias\n    6.4.4 Linear correlation and regression\n    6.4.5 Analysis of variance (ANOVA)\n\nIn analytical work a frequently recurring operation is the verification of performance by comparison of data. Some examples of comparisons in practice are:\n\n    - performance of two instruments,\n\n    - performance of two methods,\n\n    - performance of a procedure in different periods,\n\n    - performance of two analysts or laboratories,\n\n    - results obtained for a reference or control sample with the \"true\", \"target\" or \"assigned\" value of this sample.\n\nSome of the most common and convenient statistical tools to quantify such comparisons are the F-test, the t-tests, and regression analysis.\n\nBecause the F-test and the t-tests are the most basic tests they will be discussed first. These tests examine if two sets of normally distributed data are similar or dissimilar (belong or not belong to the same \"population\") by comparing their standard deviations and means respectively. This is illustrated in Fig. 6-3.\n\nFig. 6-3. Three possible cases when comparing two sets of data (n1 = n2). A. Different mean (bias), same precision; B. Same mean (no bias), different precision; C. Both mean and precision are different. (The fourth case, identical sets, has not been drawn).\n\n6.4.1 Two-sided vs. one-sided test\n\nThese tests for comparison, for instance between methods A and B, are based on the assumption that there is no significant difference (the \"null hypothesis\"). In other words, when the difference is so small that a tabulated critical value of F or t is not exceeded, we can be confident (usually at 95% level) that A and B are not different. Two fundamentally different questions can be asked concerning both the comparison of the standard deviations s1 and s2 with the F-test, and of the means¯x1, and ¯x2, with the t-test:\n\n    1. are A and B different? (two-sided test)\n    2. is A higher (or lower) than B? (one-sided test).\n\nThis distinction has an important practical implication as statistically the probabilities for the two situations are different: the chance that A and B are only different (\"it can go two ways\") is twice as large as the chance that A is higher (or lower) than B (\"it can go only one way\"). The most common case is the two-sided (also called two-tailed) test: there are no particular reasons to expect that the means or the standard deviations of two data sets are different. An example is the routine comparison of a control chart with the previous one (see 8.3). However, when it is expected or suspected that the mean and/or the standard deviation will go only one way, e.g. after a change in an analytical procedure, the one-sided (or one-tailed) test is appropriate. In this case the probability that it goes the other way than expected is assumed to be zero and, therefore, the probability that it goes the expected way is doubled. Or, more correctly, the uncertainty in the two-way test of 5% (or the probability of 5% that the critical value is exceeded) is divided over the two tails of the Gaussian curve (see Fig. 6-2), i.e. 2.5% at the end of each tail beyond 2s. If we perform the one-sided test with 5% uncertainty, we actually increase this 2.5% to 5% at the end of one tail. (Note that for the whole gaussian curve, which is symmetrical, this is then equivalent to an uncertainty of 10% in two ways!)\n\nThis difference in probability in the tests is expressed in the use of two tables of critical values for both F and t. In fact, the one-sided table at 95% confidence level is equivalent to the two-sided table at 90% confidence level.\n\nIt is emphasized that the one-sided test is only appropriate when a difference in one direction is expected or aimed at. Of course it is tempting to perform this test after the results show a clear (unexpected) effect. In fact, however, then a two times higher probability level was used in retrospect. This is underscored by the observation that in this way even contradictory conclusions may arise: if in an experiment calculated values of F and t are found within the range between the two-sided and one-sided values of Ftab, and ttab, the two-sided test indicates no significant difference, whereas the one-sided test says that the result of A is significantly higher (or lower) than that of B. What actually happens is that in the first case the 2.5% boundary in the tail was just not exceeded, and then, subsequently, this 2.5% boundary is relaxed to 5% which is then obviously more easily exceeded. This illustrates that statistical tests differ in strictness and that for proper interpretation of results in reports, the statistical techniques used, including the confidence limits or probability, should always be specified.\n6.4.2 F-test for precision\n\nBecause the result of the F-test may be needed to choose between the Student's t-test and the Cochran variant (see next section), the F-test is discussed first.\n\nThe F-test (or Fisher's test) is a comparison of the spread of two sets of data to test if the sets belong to the same population, in other words if the precisions are similar or dissimilar.\n\nThe test makes use of the ratio of the two variances:\n\n\t\n\n(6.11)\n\nwhere the larger s2 must be the numerator by convention. If the performances are not very different, then the estimates s1, and s2, do not differ much and their ratio (and that of their squares) should not deviate much from unity. In practice, the calculated F is compared with the applicable F value in the F-table (also called the critical value, see Appendix 2). To read the table it is necessary to know the applicable number of degrees of freedom for s1, and s2. These are calculated by:\n\n    df1 = n1-1\n    df2 = n2-1\n\nIf Fcal £ Ftab one can conclude with 95% confidence that there is no significant difference in precision (the \"null hypothesis\" that s1, = s, is accepted). Thus, there is still a 5% chance that we draw the wrong conclusion. In certain cases more confidence may be needed, then a 99% confidence table can be used, which can be found in statistical textbooks.\n\nExample I (two-sided test)\n\nTable 6-1 gives the data sets obtained by two analysts for the cation exchange capacity (CEC) of a control sample. Using Equation (6.11) the calculated F value is 1.62. As we had no particular reason to expect that the analysts would perform differently, we use the F-table for the two-sided test and find Ftab = 4.03 (Appendix 2, df1, = df2 = 9). This exceeds the calculated value and the null hypothesis (no difference) is accepted. It can be concluded with 95% confidence that there is no significant difference in precision between the work of Analyst 1 and 2.\n\nTable 6-1. CEC values (in cmolc/kg) of a control sample determined by two analysts.\n\n1\n\t\n\n2\n\n10.2\n\t\n\n9.7\n\n10.7\n\t\n\n9.0\n\n10.5\n\t\n\n10.2\n\n9.9\n\t\n\n10.3\n\n9.0\n\t\n\n10.8\n\n11.2\n\t\n\n11.1\n\n11.5\n\t\n\n9.4\n\n10.9\n\t\n\n9.2\n\n8.9\n\t\n\n9.8\n\n10.6\n\t\n\n10.2\n\n¯x:\n\t\n\n10.34\n\t\n\n9.97\n\ns:\n\t\n\n0.819\n\t\n\n0.644\n\nn:\n\t\n\n10\n\t\n\n10\n\nFcal = 1.62\n\t\n\ntcal = 1.12\n\t\n\n\nFtab = 4.03\n\t\n\nttab = 2.10\n\t\n\n\nExample 2 (one-sided test)\n\nThe determination of the calcium carbonate content with the Scheibler standard method is compared with the simple and more rapid \"acid-neutralization\" method using one and the same sample. The results are given in Table 6-2. Because of the nature of the rapid method we suspect it to produce a lower precision then obtained with the Scheibler method and we can, therefore, perform the one sided F-test. The applicable Ftab = 3.07 (App. 2, df1, = 12, df2 = 9) which is lower than Fcal (=18.3) and the null hypothesis (no difference) is rejected. It can be concluded (with 95% confidence) that for this one sample the precision of the rapid titration method is significantly worse than that of the Scheibler method.\n\nTable 6-2. Contents of CaCO3 (in mass/mass %) in a soil sample determined with the Scheibler method (A) and the rapid titration method (B).\n\nA\n\t\n\nB\n\n2.5\n\t\n\n1.7\n\n2.4\n\t\n\n1.9\n\n2.5\n\t\n\n2.3\n\n2.6\n\t\n\n2.3\n\n2.5\n\t\n\n2.8\n\n2.5\n\t\n\n2.5\n\n2.4\n\t\n\n1.6\n\n2.6\n\t\n\n1.9\n\n2.7\n\t\n\n2.6\n\n2.4\n\t\n\n1.7\n\n-\n\t\n\n2.4\n\n-\n\t\n\n2.2\n\n\n\t\n\n2.6\n\nx:\n\t\n\n2.51\n\t\n\n2.13\n\ns:\n\t\n\n0.099\n\t\n\n0.424\n\nn:\n\t\n\n10\n\t\n\n13\n\nFcal = 18.3\n\t\n\ntcal = 3.12\n\t\n\n\nFtab = 3.07\n\t\n\nttab* = 2.18\n\t\n\n\n(ttab* = Cochran's \"alternative\" ttab)\n6.4.3 t-Tests for bias\n\n    6.4.3.1. Student's t-test\n    6.4.3.2 Cochran's t-test\n    6.4.3.3 t-Test for large data sets (n³ 30)\n    6.4.3.4 Paired t-test\n\nDepending on the nature of two sets of data (n, s, sampling nature), the means of the sets can be compared for bias by several variants of the t-test. The following most common types will be discussed:\n\n    1. Student's t-test for comparison of two independent sets of data with very similar standard deviations;\n\n    2. the Cochran variant of the t-test when the standard deviations of the independent sets differ significantly;\n\n    3. the paired t-test for comparison of strongly dependent sets of data.\n\nBasically, for the t-tests Equation (6.8) is used but written in a different way:\n\n\t\n\n(6.12)\n\nwhere\n\n    ¯x = mean of test results of a sample\n    m = \"true\" or reference value\n    s = standard deviation of test results\n    n = number of test results of the sample.\n\nTo compare the mean of a data set with a reference value normally the \"two-sided t-table of critical values\" is used (Appendix 1). The applicable number of degrees of freedom here is:\n\ndf = n-1\n\nIf a value for t calculated with Equation (6.12) does not exceed the critical value in the table, the data are taken to belong to the same population: there is no difference and the \"null hypothesis\" is accepted (with the applicable probability, usually 95%).\n\nAs with the F-test, when it is expected or suspected that the obtained results are higher or lower than that of the reference value, the one-sided t-test can be performed: if tcal > ttab, then the results are significantly higher (or lower) than the reference value.\n\nMore commonly, however, the \"true\" value of proper reference samples is accompanied by the associated standard deviation and number of replicates used to determine these parameters. We can then apply the more general case of comparing the means of two data sets: the \"true\" value in Equation (6.12) is then replaced by the mean of a second data set. As is shown in Fig. 6-3, to test if two data sets belong to the same population it is tested if the two Gauss curves do sufficiently overlap. In other words, if the difference between the means ¯x1-¯x2 is small. This is discussed next.\n\nSimilarity or non-similarity of standard deviations\n\nWhen using the t-test for two small sets of data (n1 and/or n2<30), a choice of the type of test must be made depending on the similarity (or non-similarity) of the standard deviations of the two sets. If the standard deviations are sufficiently similar they can be \"pooled\" and the Student t-test can be used. When the standard deviations are not sufficiently similar an alternative procedure for the t-test must be followed in which the standard deviations are not pooled. A convenient alternative is the Cochran variant of the t-test. The criterion for the choice is the passing or non-passing of the F-test (see 6.4.2), that is, if the variances do or do not significantly differ. Therefore, for small data sets, the F-test should precede the t-test.\n\nFor dealing with large data sets (n1, n2,³ 30) the \"normal\" t-test is used (see Section 6.4.3.3 and App. 3).\n6.4.3.1. Student's t-test\n\n(To be applied to small data sets (n1, n2 < 30) where s1, and s2 are similar according to F-test.\n\nWhen comparing two sets of data, Equation (6.12) is rewritten as:\n\n\t\n\n(6.13)\n\nwhere\n\n    ¯x1 = mean of data set 1\n    ¯x2 = mean of data set 2\n    sp = \"pooled\" standard deviation of the sets\n    n1 = number of data in set 1\n    n2 = number of data in set 2.\n\nThe pooled standard deviation sp is calculated by:\n\n\t\n\n6.14\n\nwhere\n\n    s1 = standard deviation of data set 1\n    s2 = standard deviation of data set 2\n    n1 = number of data in set 1\n    n2 = number of data in set 2.\n\nTo perform the t-test, the critical ttab has to be found in the table (Appendix 1); the applicable number of degrees of freedom df is here calculated by:\n\n    df = n1 + n2 -2\n\nExample\n\nThe two data sets of Table 6-1 can be used: With Equations (6.13) and (6.14) tcal, is calculated as 1.12 which is lower than the critical value ttab of 2.10 (App. 1, df = 18, two-sided), hence the null hypothesis (no difference) is accepted and the two data sets are assumed to belong to the same population: there is no significant difference between the mean results of the two analysts (with 95% confidence).\n\n    Note. Another illustrative way to perform this test for bias is to calculate if the difference between the means falls within or outside the range where this difference is still not significantly large. In other words, if this difference is less than the least significant difference (lsd). This can be derived from Equation (6.13):\n\n\t\n\n6.15\n\nIn the present example of Table 6-1, the calculation yields lsd = 0.69. The measured difference between the means is 10.34 -9.97 = 0.37 which is smaller than the lsd indicating that there is no significant difference between the performance of the analysts.\n\nIn addition, in this approach the 95% confidence limits of the difference between the means can be calculated (cf. Equation 6.8):\n\nconfidence limits = 0.37 ± 0.69 = -0.32 and 1.06\n\nNote that the value 0 for the difference is situated within this confidence interval which agrees with the null hypothesis of x1 = x2 (no difference) having been accepted.\n6.4.3.2 Cochran's t-test\n\nTo be applied to small data sets (n1, n2, < 30) where s1 and s2, are dissimilar according to F-test.\n\nCalculate t with:\n\n\t\n\n6.16\n\nThen determine an \"alternative\" critical t-value:\n\n\t\n\n6.17\n\nwhere\n\n    t1 = ttab at n1-1 degrees of freedom\n    t2 = ttab at n2-1 degrees of freedom\n\nNow the t-test can be performed as usual: if tcal< ttab* then the null hypothesis that the means do not significantly differ is accepted.\n\nExample\n\nThe two data sets of Table 6-2 can be used.\n\nAccording to the F-test, the standard deviations differ significantly so that the Cochran variant must be used. Furthermore, in contrast to our expectation that the precision of the rapid test would be inferior, we have no idea about the bias and therefore the two-sided test is appropriate. The calculations yield tcal = 3.12 and ttab*= 2.18 meaning that tcal exceeds ttab* which implies that the null hypothesis (no difference) is rejected and that the mean of the rapid analysis deviates significantly from that of the standard analysis (with 95% confidence, and for this sample only). Further investigation of the rapid method would have to include the use of more different samples and then comparison with the one-sided t-test would be justified (see 6.4.3.4, Example 1).\n6.4.3.3 t-Test for large data sets (n³ 30)\n\nIn the example above (6.4.3.2) the conclusion happens to have been the same if the Student's t-test with pooled standard deviations had been used. This is caused by the fact that the difference in result of the Student and Cochran variants of the t-test is largest when small sets of data are compared, and decreases with increasing number of data. Namely, with increasing number of data a better estimate of the real distribution of the population is obtained (the flatter t-distribution converges then to the standardized normal distribution). When n³ 30 for both sets, e.g. when comparing Control Charts (see 8.3), for all practical purposes the difference between the Student and Cochran variant is negligible. The procedure is then reduced to the \"normal\" t-test by simply calculating tcal with Eq. (6.16) and comparing this with ttab at df = n1 + n2-2. (Note in App. 1 that the two-sided ttab is now close to 2).\n\nThe proper choice of the t-test as discussed above is summarized in a flow diagram in Appendix 3.\n6.4.3.4 Paired t-test\n\nWhen two data sets are not independent, the paired t-test can be a better tool for comparison than the \"normal\" t-test described in the previous sections. This is for instance the case when two methods are compared by the same analyst using the same sample(s). It could, in fact, also be applied to the example of Table 6-1 if the two analysts used the same analytical method at (about) the same time.\n\nAs stated previously, comparison of two methods using different levels of analyte gives more validation information about the methods than using only one level. Comparison of results at each level could be done by the F and t-tests as described above. The paired t-test, however, allows for different levels provided the concentration range is not too wide. As a rule of fist, the range of results should be within the same magnitude. If the analysis covers a longer range, i.e. several powers of ten, regression analysis must be considered (see Section 6.4.4). In intermediate cases, either technique may be chosen.\n\nThe null hypothesis is that there is no difference between the data sets, so the test is to see if the mean of the differences between the data deviates significantly from zero or not (two-sided test). If it is expected that one set is systematically higher (or lower) than the other set, then the one-sided test is appropriate.\n\nExample 1\n\nThe \"promising\" rapid single-extraction method for the determination of the cation exchange capacity of soils using the silver thiourea complex (AgTU, buffered at pH 7) was compared with the traditional ammonium acetate method (NH4OAc, pH 7). Although for certain soil types the difference in results appeared insignificant, for other types differences seemed larger. Such a suspect group were soils with ferralic (oxic) properties (i.e. highly weathered sesquioxide-rich soils). In Table 6-3 the results often soils with these properties are grouped to test if the CEC methods give different results. The difference d within each pair and the parameters needed for the paired t-test are given also.\n\nTable 6-3. CEC values (in cmolc/kg) obtained by the NH4OAc and AgTU methods (both at pH 7) for ten soils with ferralic properties.\n\nSample\n\t\n\nNH4OAc\n\t\n\nAgTU\n\t\n\nd\n\n1\n\t\n\n7.1\n\t\n\n6.5\n\t\n\n-0.6\n\n2\n\t\n\n4.6\n\t\n\n5.6\n\t\n\n+1.0\n\n3\n\t\n\n10.6\n\t\n\n14.5\n\t\n\n+3.9\n\n4\n\t\n\n2.3\n\t\n\n5.6\n\t\n\n+3.3\n\n5\n\t\n\n25.2\n\t\n\n23.8\n\t\n\n-1.4\n\n6\n\t\n\n4.4\n\t\n\n10.4\n\t\n\n+6.0\n\n7\n\t\n\n7.8\n\t\n\n8.4\n\t\n\n+0.6\n\n8\n\t\n\n2.7\n\t\n\n5.5\n\t\n\n+2.8\n\n9\n\t\n\n14.3\n\t\n\n19.2\n\t\n\n+4.9\n\n10\n\t\n\n13.6\n\t\n\n15.0\n\t\n\n+1.4\n\n¯d = +2.19\n\t\n\ntcal = 2.89\n\nsd = 2.395\n\t\n\nttab = 2.26\n\nUsing Equation (6.12) and noting that m d = 0 (hypothesis value of the differences, i.e. no difference), the t-value can be calculated as:\n\nwhere\n\n    = mean of differences within each pair of data\n    sd = standard deviation of the mean of differences\n    n = number of pairs of data\n\nThe calculated t value (=2.89) exceeds the critical value of 1.83 (App. 1, df = n -1 = 9, one-sided), hence the null hypothesis that the methods do not differ is rejected and it is concluded that the silver thiourea method gives significantly higher results as compared with the ammonium acetate method when applied to such highly weathered soils.\n\n    Note. Since such data sets do not have a normal distribution, the \"normal\" t-test which compares means of sets cannot be used here (the means do not constitute a fair representation of the sets). For the same reason no information about the precision of the two methods can be obtained, nor can the F-test be applied. For information about precision, replicate determinations are needed.\n\nExample 2\n\nTable 6-4 shows the data of total-P in four plant tissue samples obtained by a laboratory L and the median values obtained by 123 laboratories in a proficiency (round-robin) test.\n\nTable 6-4. Total-P contents (in mmol/kg) of plant tissue as determined by 123 laboratories (Median) and Laboratory L.\n\nSample\n\t\n\nMedian\n\t\n\nLab L\n\t\n\nd\n\n1\n\t\n\n93.0\n\t\n\n85.2\n\t\n\n-7.8\n\n2\n\t\n\n201\n\t\n\n224\n\t\n\n23\n\n3\n\t\n\n78.9\n\t\n\n84.5\n\t\n\n5.6\n\n4\n\t\n\n175\n\t\n\n185\n\t\n\n10\n\n¯d = 7.70\n\t\n\ntcal =1.21\n\nsd = 12.702\n\t\n\nttab = 3.18\n\nTo verify the performance of the laboratory a paired t-test can be performed:\n\nUsing Eq. (6.12) and noting that m d=0 (hypothesis value of the differences, i.e. no difference), the t value can be calculated as:\n\nThe calculated t-value is below the critical value of 3.18 (Appendix 1, df = n - 1 = 3, two-sided), hence the null hypothesis that the laboratory does not significantly differ from the group of laboratories is accepted, and the results of Laboratory L seem to agree with those of \"the rest of the world\" (this is a so-called third-line control).\n6.4.4 Linear correlation and regression\n\n    6.4.4.1 Construction of calibration graph\n    6.4.4.2 Comparing two sets of data using many samples at different analyte levels\n\nThese also belong to the most common useful statistical tools to compare effects and performances X and Y. Although the technique is in principle the same for both, there is a fundamental difference in concept: correlation analysis is applied to independent factors: if X increases, what will Y do (increase, decrease, or perhaps not change at all)? In regression analysis a unilateral response is assumed: changes in X result in changes in Y, but changes in Y do not result in changes in X.\n\nFor example, in analytical work, correlation analysis can be used for comparing methods or laboratories, whereas regression analysis can be used to construct calibration graphs. In practice, however, comparison of laboratories or methods is usually also done by regression analysis. The calculations can be performed on a (programmed) calculator or more conveniently on a PC using a home-made program. Even more convenient are the regression programs included in statistical packages such as Statistix, Mathcad, Eureka, Genstat, Statcal, SPSS, and others. Also, most spreadsheet programs such as Lotus 123, Excel, and Quattro-Pro have functions for this.\n\nLaboratories or methods are in fact independent factors. However, for regression analysis one factor has to be the independent or \"constant\" factor (e.g. the reference method, or the factor with the smallest standard deviation). This factor is by convention designated X, whereas the other factor is then the dependent factor Y (thus, we speak of \"regression of Y on X\").\n\nAs was discussed in Section 6.4.3, such comparisons can often been done with the Student/Cochran or paired t-tests. However, correlation analysis is indicated:\n\n    1. When the concentration range is so wide that the errors, both random and systematic, are not independent (which is the assumption for the t-tests). This is often the case where concentration ranges of several magnitudes are involved.\n\n    2. When pairing is inappropriate for other reasons, notably a long time span between the two analyses (sample aging, change in laboratory conditions, etc.).\n\nThe principle is to establish a statistical linear relationship between two sets of corresponding data by fitting the data to a straight line by means of the \"least squares\" technique. Such data are, for example, analytical results of two methods applied to the same samples (correlation), or the response of an instrument to a series of standard solutions (regression).\n\n    Note: Naturally, non-linear higher-order relationships are also possible, but since these are less common in analytical work and more complex to handle mathematically, they will not be discussed here. Nevertheless, to avoid misinterpretation, always inspect the kind of relationship by plotting the data, either on paper or on the computer monitor.\n\nThe resulting line takes the general form:\n\ny = bx + a\n\t\n\n(6.18)\n\nwhere\n\n    a = intercept of the line with the y-axis\n    b = slope (tangent)\n\nIn laboratory work ideally, when there is perfect positive correlation without bias, the intercept a = 0 and the slope = 1. This is the so-called \"1:1 line\" passing through the origin (dashed line in Fig. 6-5).\n\nIf the intercept a ¹ 0 then there is a systematic discrepancy (bias, error) between X and Y; when b ¹ 1 then there is a proportional response or difference between X and Y.\n\nThe correlation between X and Y is expressed by the correlation coefficient r which can be calculated with the following equation:\n\n\t\n\n6.19\n\nwhere\n\n    xi = data X\n    ¯x = mean of data X\n    yi = data Y\n    ¯y = mean of data Y\n\nIt can be shown that r can vary from 1 to -1:\n\n    r = 1 perfect positive linear correlation\n    r = 0 no linear correlation (maybe other correlation)\n    r = -1 perfect negative linear correlation\n\nOften, the correlation coefficient r is expressed as r2: the coefficient of determination or coefficient of variance. The advantage of r2 is that, when multiplied by 100, it indicates the percentage of variation in Y associated with variation in X. Thus, for example, when r = 0.71 about 50% (r2 = 0.504) of the variation in Y is due to the variation in X.\n\nThe line parameters b and a are calculated with the following equations:\n\n\t\n\n6.20\n\nand\n\na = ¯y - b¯x\n\t\n\n6.21\n\nIt is worth to note that r is independent of the choice which factor is the independent factory and which is the dependent Y. However, the regression parameters a and do depend on this choice as the regression lines will be different (except when there is ideal 1:1 correlation).\n6.4.4.1 Construction of calibration graph\n\nAs an example, we take a standard series of P (0-1.0 mg/L) for the spectrophotometric determination of phosphate in a Bray-I extract (\"available P\"), reading in absorbance units. The data and calculated terms needed to determine the parameters of the calibration graph are given in Table 6-5. The line itself is plotted in Fig. 6-4.\n\nTable 6-5 is presented here to give an insight in the steps and terms involved. The calculation of the correlation coefficient r with Equation (6.19) yields a value of 0.997 (r2 = 0.995). Such high values are common for calibration graphs. When the value is not close to 1 (say, below 0.98) this must be taken as a warning and it might then be advisable to repeat or review the procedure. Errors may have been made (e.g. in pipetting) or the used range of the graph may not be linear. On the other hand, a high r may be misleading as it does not necessarily indicate linearity. Therefore, to verify this, the calibration graph should always be plotted, either on paper or on computer monitor.\n\nUsing Equations (6.20 and (6.21) we obtain:\n\nand\n\na = 0.350 - 0.313 = 0.037\n\nThus, the equation of the calibration line is:\n\ny = 0.626x + 0.037\n\t\n\n(6.22)\n\nTable 6-5. Parameters of calibration graph in Fig. 6-4.\n\nxi\n\t\n\nyi\n\t\n\nx1-¯x\n\t\n\n(xi-¯x)2\n\t\n\nyi-¯y\n\t\n\n(yi-¯y)2\n\t\n\n(x1-¯x)(yi-¯y)\n\n0.0\n\t\n\n0.05\n\t\n\n-0.5\n\t\n\n0.25\n\t\n\n-0.30\n\t\n\n0.090\n\t\n\n0.150\n\n0.2\n\t\n\n0.14\n\t\n\n-0.3\n\t\n\n0.09\n\t\n\n-0.21\n\t\n\n0.044\n\t\n\n0.063\n\n0.4\n\t\n\n0.29\n\t\n\n-0.1\n\t\n\n0.01\n\t\n\n-0.06\n\t\n\n0.004\n\t\n\n0.006\n\n0.6\n\t\n\n0.43\n\t\n\n0.1\n\t\n\n0.01\n\t\n\n0.08\n\t\n\n0.006\n\t\n\n0.008\n\n0.8\n\t\n\n0.52\n\t\n\n0.3\n\t\n\n0.09\n\t\n\n0.17\n\t\n\n0.029\n\t\n\n0.051\n\n1.0\n\t\n\n0.67\n\t\n\n0.5\n\t\n\n0.25\n\t\n\n0.32\n\t\n\n0.102\n\t\n\n0.160\n\n3.0\n\t\n\n2.10\n\t\n\n0\n\t\n\n0.70\n\t\n\n0\n\t\n\n0.2754\n\t\n\n0.438 S\n\n¯x=0.5\n\t\n\n¯y = 0.35\n\t\n\n\nFig. 6-4. Calibration graph plotted from data of Table 6-5. The dashed lines delineate the 95% confidence area of the graph. Note that the confidence is highest at the centroid of the graph.\n\nDuring calculation, the maximum number of decimals is used, rounding off to the last significant figure is done at the end (see instruction for rounding off in Section 8.2).\n\nOnce the calibration graph is established, its use is simple: for each y value measured the corresponding concentration x can be determined either by direct reading or by calculation using Equation (6.22). The use of calibration graphs is further discussed in Section 7.2.2.\n\n    Note. A treatise of the error or uncertainty in the regression line is given.\n\n6.4.4.2 Comparing two sets of data using many samples at different analyte levels\n\nAlthough regression analysis assumes that one factor (on the x-axis) is constant, when certain conditions are met the technique can also successfully be applied to comparing two variables such as laboratories or methods. These conditions are:\n\n    - The most precise data set is plotted on the x-axis\n    - At least 6, but preferably more than 10 different samples are analyzed\n    - The samples should rather uniformly cover the analyte level range of interest.\n\nTo decide which laboratory or method is the most precise, multi-replicate results have to be used to calculate standard deviations (see 6.4.2). If these are not available then the standard deviations of the present sets could be compared (note that we are now not dealing with normally distributed sets of replicate results). Another convenient way is to run the regression analysis on the computer, reverse the variables and run the analysis again. Observe which variable has the lowest standard deviation (or standard error of the intercept a, both given by the computer) and then use the results of the regression analysis where this variable was plotted on the x-axis.\n\nIf the analyte level range is incomplete, one might have to resort to spiking or standard additions, with the inherent drawback that the original analyte-sample combination may not adequately be reflected.\n\nExample\n\nIn the framework of a performance verification programme, a large number of soil samples were analyzed by two laboratories X and Y (a form of \"third-line control\", see Chapter 9) and the data compared by regression. (In this particular case, the paired t-test might have been considered also). The regression line of a common attribute, the pH, is shown here as an illustration. Figure 6-5 shows the so-called \"scatter plot\" of 124 soil pH-H2O determinations by the two laboratories. The correlation coefficient r is 0.97 which is very satisfactory. The slope (= 1.03) indicates that the regression line is only slightly steeper than the 1:1 ideal regression line. Very disturbing, however, is the intercept a of -1.18. This implies that laboratory Y measures the pH more than a whole unit lower than laboratory X at the low end of the pH range (the intercept -1.18 is at pHx = 0) which difference decreases to about 0.8 unit at the high end.\n\nFig. 6-5. Scatter plot of pH data of two laboratories. Drawn line: regression line; dashed line: 1:1 ideal regression line.\n\nThe t-test for significance is as follows:\n\nFor intercept a: m a = 0 (null hypothesis: no bias; ideal intercept is then zero), standard error =0.14 (calculated by the computer), and using Equation (6.12) we obtain:\n\nHere, ttab = 1.98 (App. 1, two-sided, df = n - 2 = 122 (n-2 because an extra degree of freedom is lost as the data are used for both a and b) hence, the laboratories have a significant mutual bias.\n\nFor slope: m b = 1 (ideal slope: null hypothesis is no difference), standard error = 0.02 (given by computer), and again using Equation (6.12) we obtain:\n\nAgain, ttab = 1.98 (App. 1; two-sided, df = 122), hence, the difference between the laboratories is not significantly proportional (or: the laboratories do not have a significant difference in sensitivity). These results suggest that in spite of the good correlation, the two laboratories would have to look into the cause of the bias.\n\n    Note. In the present example, the scattering of the points around the regression line does not seem to change much over the whole range. This indicates that the precision of laboratory Y does not change very much over the range with respect to laboratory X. This is not always the case. In such cases, weighted regression (not discussed here) is more appropriate than the unweighted regression as used here.\n\n    Validation of a method (see Section 7.5) may reveal that precision can change significantly with the level of analyte (and with other factors such as sample matrix).\n\n6.4.5 Analysis of variance (ANOVA)\n\nWhen results of laboratories or methods are compared where more than one factor can be of influence and must be distinguished from random effects, then ANOVA is a powerful statistical tool to be used. Examples of such factors are: different analysts, samples with different pre-treatments, different analyte levels, different methods within one of the laboratories). Most statistical packages for the PC can perform this analysis.\n\nAs a treatise of ANOVA is beyond the scope of the present Guidelines, for further discussion the reader is referred to statistical textbooks, some of which are given in the list of Literature.\n\nError or uncertainty in the regression line\n\nThe \"fitting\" of the calibration graph is necessary because the response points yi, composing the line do not fall exactly on the line. Hence, random errors are implied. This is expressed by an uncertainty about the slope and intercept b and a defining the line. A quantification can be found in the standard deviation of these parameters. Most computer programmes for regression will automatically produce figures for these. To illustrate the procedure, the example of the calibration graph in Section 6.4.3.1 is elaborated here.\n\nA practical quantification of the uncertainty is obtained by calculating the standard deviation of the points on the line; the \"residual standard deviation\" or \"standard error of the y-estimate\", which we assumed to be constant (but which is only approximately so, see Fig. 6-4):\n\n\t\n\n(6.23)\n\nwhere\n\n    = \"fitted\" y-value for each xi, (read from graph or calculated with Eq. 6.22). Thus, is the (vertical) deviation of the found y-values from the line.\n\n    n = number of calibration points.\n\n    Note: Only the y-deviations of the points from the line are considered. It is assumed that deviations in the x-direction are negligible. This is, of course, only the case if the standards are very accurately prepared.\n\nNow the standard deviations for the intercept a and slope b can be calculated with:\n\n\t\n\n6.24\n\nand\n\n\t\n\n6.25\n\nTo make this procedure clear, the parameters involved are listed in Table 6-6.\n\nThe uncertainty about the regression line is expressed by the confidence limits of a and b according to Eq. (6.9): a ± t.sa and b ± t.sb\n\nTable 6-6. Parameters for calculating errors due to calibration graph (use also figures of Table 6-5).\n\nxi\n\t\n\nyi\n\t\n\n\t\n\n\t\n\n0\n\t\n\n0.05\n\t\n\n0.037\n\t\n\n0.013\n\t\n\n0.0002\n\n0.2\n\t\n\n0.14\n\t\n\n0.162\n\t\n\n-0.022\n\t\n\n0.0005\n\n0.4\n\t\n\n0.29\n\t\n\n0.287\n\t\n\n0.003\n\t\n\n0.0000\n\n0.6\n\t\n\n0.43\n\t\n\n0.413\n\t\n\n0.017\n\t\n\n0.0003\n\n0.8\n\t\n\n0.52\n\t\n\n0.538\n\t\n\n-0.018\n\t\n\n0.0003\n\n1.0\n\t\n\n0.67\n\t\n\n0.663\n\t\n\n0.007\n\t\n\n0.0001\n\n\n\t\n\n\n\t\n\n\n\t\n\n\n\t\n\n0.001364 S\n\nIn the present example, using Eq. (6.23), we calculate\n\nand, using Eq. (6.24) and Table 6-5:\n\nand, using Eq. (6.25) and Table 6-5:\n\nThe applicable ttab is 2.78 (App. 1, two-sided, df = n -1 = 4) hence, using Eq. (6.9):\n\n    a = 0.037 ± 2.78 × 0.0132 = 0.037 ± 0.037\n    and\n    b = 0.626 ± 2.78 × 0.0219 = 0.626 ± 0.061\n\nNote that if sa is large enough, a negative value for a is possible, i.e. a negative reading for the blank or zero-standard. (For a discussion about the error in x resulting from a reading in y, which is particularly relevant for reading a calibration graph, see Section 7.2.3)\n\nThe uncertainty about the line is somewhat decreased by using more calibration points (assuming sy has not increased): one more point reduces ttab from 2.78 to 2.57 (see Appendix 1).", "skillName": "6_BASIC_STATISTICAL_TOOLS."}
{"id": 43, "category": "Statistics", "skillText": "Statistical software are specialized computer programs for analysis in statistics and econometrics.\n\nContents\n\n    1 Open-source\n    2 Public domain\n    3 Freeware\n    4 Proprietary\n        4.1 Add-ons\n    5 See also\n    6 References\n    7 External links\n\nOpen-source\ngretl is an example of an open-source statistical package\n\n    ADaMSoft – a generalized statistical software with data mining algorithms and methods for data management\n    ADMB – a software suite for non-linear statistical modeling based on C++ which uses automatic differentiation\n    Bayesian Filtering Library\n    Chronux – for neurobiological time series data\n    CBEcon – web-based econometrics and statistical software\n    DataMelt (DMelt) – Java-based statistical analysis framework for scientists and engineers. It includes an IDE\n    DAP – free replacement for SAS\n    Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI) a software framework for developing data mining algorithms in Java\n    Fityk – nonlinear regression software (GUI and command line)\n    GNU Octave – programming language very similar to MATLAB with statistical features\n    gretl – gnu regression, econometrics and time-series library\n    intrinsic Noise Analyzer (iNA) – For analyzing intrinsic fluctuations in biochemical systems\n    JASP – A free software alternative to IBM SPSS Statistics with additional option for Bayesian methods\n    Just another Gibbs sampler (JAGS) – a program for analyzing Bayesian hierarchical models using Markov chain Monte Carlo developed by Martyn Plummer. It is similar to WinBUGS\n    JMulTi\n    LDT - Automatic Time Series Analysis with Stationary VAR Models\n    LIBSVM – C++ support vector machine libraries\n    MLPACK (C++ library) – open-source library for machine learning, exploits C++ language features to provide maximum performance and flexibility while providing a simple and consistent application programming interface (API)\n    Mondrian – data analysis tool using interactive statistical graphics with a link to R\n    Neurophysiological Biomarker Toolbox - Matlab toolbox for data-mining of neurophysiological biomarkers\n    OpenBUGS\n    OpenEpi – A web-based, open-source, operating-independent series of programs for use in epidemiology and statistics based on JavaScript and HTML\n    OpenNN – A software library written in the programming language C++ which implements neural networks, a main area of deep learning research\n    OpenMx – A package for structural equation modeling running in R (programming language)\n    Orange, a data mining, machine learning, and bioinformatics software\n    Pandas – High-performance computing (HPC) data structures and data analysis tools for Python in Python and Cython (statsmodels, scikit-learn)\n    Perl Data Language – Scientific computing with Perl\n    Ploticus – software for generating a variety of graphs from raw data\n    PSPP – A free software alternative to IBM SPSS Statistics\n    R – free implementation of the S (programming language)\n        Programming with Big Data in R (pbdR) – a series of R packages enhanced by SPMD parallelism for big data analysis\n        R Commander – GUI interface for R\n        Rattle GUI – GUI interface for R\n        Revolution Analytics – production-grade software for the enterprise big data analytics\n        RStudio – GUI interface and development environment for R\n    ROOT – an open-source C++ system for data storage, processing and analysis, developed by CERN and used to find the Higgs boson\n    Salstat - menu-driven statistics software\n    Scilab – uses GPL-compatible CeCILL license\n    SciPy – Python library for scientific computing that contains the stats sub-package which is partly based on the venerable |STAT (a.k.a. PipeStat, formerly UNIX|STAT) software\n        scikit-learn - extends SciPy with a host of machine learning models (classification, clustering, regression, etc.)\n        statsmodels - extends SciPy with statistical models and tests (regression, plotting, example datasets, generalized linear model (GLM), time series analysis, autoregressive–moving-average model (ARMA), vector autoregression (VAR), non-parametric statistics, ANOVA, empirical likelihood)\n    Shogun (toolbox) – open-source, large-scale machine learning toolbox that provides several SVM (Support Vector Machine) implementations (like libSVM, SVMlight) under a common framework and interfaces to Octave, MATLAB, Python, R\n    Simfit – simulation, curve fitting, statistics, and plotting\n    SOCR\n    SOFA Statistics – desktop GUI program focused on ease of use, learn as you go, and beautiful output\n    Stan (software) – open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. It’s somewhat like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors\n    Statistical Lab – R-based and focusing on educational purposes\n    Torch (machine learning) – a deep learning software library written in Lua (programming language)\n    Weka (machine learning) – a suite of machine learning software written at the University of Waikato\n\nPublic domain\n\n    CSPro\n    Epi Info\n    X-12-ARIMA\n\nFreeware\n\n    BV4.1\n    GeoDA\n    MaxStat Lite – general statistical software\n    MINUIT\n    WinBUGS – Bayesian analysis using Markov chain Monte Carlo methods\n    Winpepi – package of statistical programs for epidemiologists\n\nProprietary\n\n    Analytica - visual analytics and statistics package\n    Angoss - products KnowledgeSEEKER and KnowledgeSTUDIO incorporate several data mining algorithms\n    ASReml – for restricted maximum likelihood analyses\n    BMDP – general statistics package\n    Data Applied – for building statistical models\n    DB Lytix - 800+ in-database models\n    EViews – for econometric analysis\n    FAME (database) – a system for managing time-series databases\n    GAUSS – programming language for statistics\n    Genedata – software solution for integration and interpretation of experimental data in the life science R&D\n    GenStat – general statistics package\n    GLIM – early package for fitting generalized linear models\n    GraphPad InStat – very simple with lots of guidance and explanations\n    GraphPad Prism – biostatistics and nonlinear regression with clear explanations\n    IMSL Numerical Libraries – software library with statistical algorithms\n    JMP – visual analysis and statistics package\n    LIMDEP – comprehensive statistics and econometrics package\n    LISREL – statistics package used in structural equation modeling\n    Maple – programming language with statistical features\n    Mathematica – a software package with statistical particularlyŋ features\n    MATLAB – programming language with statistical features\n    MaxStat Pro – general statistical software\n    MedCalc – for biomedical sciences\n    Microfit – econometrics package, time series\n    Minitab – general statistics package\n    MLwiN – multilevel models (free to UK academics)\n    NAG Numerical Library – comprehensive math and statistics library\n    Neural Designer – commercial deep learning package\n    NCSS – general statistics package\n    NLOGIT – comprehensive statistics and econometrics package\n    NMath Stats – statistical package for .NET Framework\n    O-Matrix – programming language\n    OriginPro – statistics and graphing, programming access to NAG library\n    PASS Sample Size Software (PASS) – power and sample size software from NCSS\n    Plotly – plotting library and styling interface for analyzing data and creating browser-based graphs. Available for R, Python, MATLAB, Julia, and Perl\n    Primer-E Primer – environmental and ecological specific\n    PV-WAVE – programming language comprehensive data analysis and visualization with IMSL statistical package\n    Qlucore Omics Explorer - interactive and visual data analysis software\n    Quantum Programming Language – part of the SPSS MR product line, mostly for data validation and tabulation in Marketing and Opinion Research\n    RapidMiner – machine learning toolbox\n    Regression Analysis of Time Series (RATS) – comprehensive econometric analysis package\n    SAS (software) – comprehensive statistical package\n    SHAZAM (Econometrics and Statistics Software) – comprehensive econometrics and statistics package\n    Simul - econometric tool for multidimensional (multi-sectoral, multi-regional) modeling\n    SigmaStat – package for group analysis\n    SmartPLS - statistics package used in partial least squares path modeling (PLS) and PLS-based structural equation modeling\n    SOCR – online tools for teaching statistics and probability theory\n    Speakeasy (computational environment) – numerical computational environment and programming language with many statistical and econometric analysis features\n    SPSS Modeler – comprehensive data mining and text analytics workbench\n    SPSS Statistics – comprehensive statistics package that stands for \"Statistical Package for the Social Sciences\"\n    Stata – comprehensive statistics package\n    Statgraphics – general statistics package to include cloud computing and Six Sigma for use in business development, process improvement, data visualization and statistical analysis, design of experiment, point processes, geospatial analysis, regression, and time series analysis are all included within this complete statistical package.\n    Statistica – comprehensive statistics package\n    StatsDirect – statistics package designed for biomedical, public health and general health science uses\n    StatXact – package for exact nonparametric and parametric statistics\n    Systat – general statistics package\n    SuperCROSS - comprehensive statistics package with ad-hoc, cross tabulation analysis\n    S-PLUS – general statistics package\n    Unistat – general statistics package that can also work as Excel add-in\n    The Unscrambler - free-to-try commercial multivariate analysis software for Windows\n    Wolfram Language[1] - the computer language that evolved from the program Mathematica. It has similar statistical capabilities as Mathematica.\n    World Programming System (WPS) – statistical package that supports the SAS language\n    XploRe\n\nAdd-ons\n\n    Analyse-it – add-on to Microsoft Excel for statistical analysis\n    NumXL – add-on to Microsoft Excel for statistical and time series analysis\n    SigmaXL – add-on to Microsoft Excel for statistical and graphical analysis\n    SPC XL – add-on to Microsoft Excel for general statistics\n    Statgraphics Sigma Express - add-on to Microsoft Excel for Six Sigma statistical analysis\n    SUDAAN – add-on to SAS and SPSS for statistical surveys\n    XLfit add-on to Microsoft Excel for curve fitting and statistical analysis\n\nSee also\n\n    Comparison of statistical packages\n    Econometric software\n    Free statistical software\n    List of computer algebra systems\n    List of graphing software\n    List of numerical analysis software\n    List of numerical libraries\n    Mathematical software\n    Psychometric software", "skillName": "List_of_statistical_packages."}
{"id": 44, "category": "Statistics", "skillText": "In the Information Age, data is no longer scarce – it’s overpowering. The key is to sift through the overwhelming volume of data available to organizations and businesses and correctly interpret its implications. But to sort through all this information, you need the right statistical data analysis tools.\n\nWith the current obsession over “big data,” analysts have produced a lot of fancy tools and techniques available to large organizations. However, there are a handful of basic data analysis tools that most organizations aren’t using…to their detriment.\n\nWe suggest starting your data analysis efforts with the following five fundamentals – and learn to avoid their pitfalls – before advancing to more sophisticated techniques.\n1. Mean\n\nThe arithmetic mean, more commonly known as “the average,” is the sum of a list of numbers divided by the number of items on the list. The mean is useful in determining the overall trend of a data set or providing a rapid snapshot of your data. Another advantage of the mean is that it’s very easy and quick to calculate.\n\nPitfall:\n\nTaken alone, the mean is a dangerous tool. In some data sets, the mean is also closely related to the mode and the median (two other measurements near the average). However, in a data set with a high number of outliers or a skewed distribution, the mean simply doesn’t provide the accuracy you need for a nuanced decision.\n2. Standard Deviation\n\nThe standard deviation, often represented with the Greek letter sigma, is the measure of a spread of data around the mean. A high standard deviation signifies that data is spread more widely from the mean, where a low standard deviation signals that more data align with the mean. In a portfolio of data analysis methods, the standard deviation is useful for quickly determining dispersion of data points.\n\nPitfall:\n\nJust like the mean, the standard deviation is deceptive if taken alone. For example, if the data have a very strange pattern such as a non-normal curve or a large amount of outliers, then the standard deviation won’t give you all the information you need.\n3. Regression\n\nRegression models the relationships between dependent and explanatory variables, which are usually charted on a scatterplot. The regression line also designates whether those relationships are strong or weak. Regression is commonly taught in high school or college statistics courses with applications for science or business in determining trends over time.\n\nPitfall:\n\nRegression is not very nuanced. Sometimes, the outliers on a scatterplot (and the reasons for them) matter significantly. For example, an outlying data point may represent the input from your most critical supplier or your highest selling product. The nature of a regression line, however, tempts you to ignore these outliers. As an illustration, examine a picture of Anscombe’s quartet, in which the data sets have the exact same regression line but include widely different data points.\n4. Sample Size Determination\n\nWhen measuring a large data set or population, like a workforce, you don’t always need to collect information from every member of that population – a sample does the job just as well. The trick is to determine the right size for a sample to be accurate. Using proportion and standard deviation methods, you are able to accurately determine the right sample size you need to make your data collection statistically significant.\n\nPitfall:\n\nWhen studying a new, untested variable in a population, your proportion equations might need to rely on certain assumptions. However, these assumptions might be completely inaccurate. This error is then passed along to your sample size determination and then onto the rest of your statistical data analysis\n5. Hypothesis Testing\n\nAlso commonly called t testing, hypothesis testing assesses if a certain premise is actually true for your data set or population. In data analysis and statistics, you consider the result of a hypothesis test statistically significant if the results couldn’t have happened by random chance. Hypothesis tests are used in everything from science and research to business and economic\n\nPitfall:\n\nTo be rigorous, hypothesis tests need to watch out for common errors. For example, the placebo effect occurs when participants falsely expect a certain result and then perceive (or actually attain) that result. Another common error is the Hawthorne effect (or observer effect), which happens when participants skew results because they know they are being studied.\n\nOverall, these methods of data analysis add a lot of insight to your decision-making portfolio, particularly if you’ve never analyzed a process or data set with statistics before. However, avoiding the common pitfalls associated with each method is just as important. Once you master these fundamental techniques for statistical data analysis, then you’re ready to advance to more powerful data analysis tools.", "skillName": "Important_Methods_For_Statistical_Data_Analysis."}
{"id": 45, "category": "Statistics", "skillText": "Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people. The relationship isn't perfect. People of the same height vary in weight, and you can easily think of two people you know where the shorter one is heavier than the taller one. Nonetheless, the average weight of people 5'5'' is less than the average weight of people 5'6'', and their average weight is less than that of people 5'7'', etc. Correlation can tell you just how much of the variation in peoples' weights is related to their heights.\n\nAlthough this correlation is fairly obvious your data may contain unsuspected correlations. You may also suspect there are correlations, but don't know which are the strongest. An intelligent correlation analysis can lead to a greater understanding of your data.\nTechniques in Determining Correlation\n\nThere are several different correlation techniques. The Survey System's optional Statistics Module includes the most common type, called the Pearson or product-moment correlation. The module also includes a variation on this type called partial correlation. The latter is useful when you want to look at the relationship between two variables while removing the effect of one or two other variables.\n\nLike all statistical techniques, correlation is only appropriate for certain kinds of data. Correlation works for quantifiable data in which numbers are meaningful, usually quantities of some sort. It cannot be used for purely categorical data, such as gender, brands purchased, or favorite color.\nRating Scales\n\nRating scales are a controversial middle case. The numbers in rating scales have meaning, but that meaning isn't very precise. They are not like quantities. With a quantity (such as dollars), the difference between 1 and 2 is exactly the same as between 2 and 3. With a rating scale, that isn't really the case. You can be sure that your respondents think a rating of 2 is between a rating of 1 and a rating of 3, but you cannot be sure they think it is exactly halfway between. This is especially true if you labeled the mid-points of your scale (you cannot assume \"good\" is exactly half way between \"excellent\" and \"fair\").\n\nMost statisticians say you cannot use correlations with rating scales, because the mathematics of the technique assume the differences between numbers are exactly equal. Nevertheless, many survey researchers do use correlations with rating scales, because the results usually reflect the real world. Our own position is that you can use correlations with rating scales, but you should do so with care. When working with quantities, correlations provide precise measurements. When working with rating scales, correlations provide general indications.\nCorrelation Coefficient\n\nThe main result of a correlation is called the correlation coefficient (or \"r\"). It ranges from -1.0 to +1.0. The closer r is to +1 or -1, the more closely the two variables are related.\n\nIf r is close to 0, it means there is no relationship between the variables. If r is positive, it means that as one variable gets larger the other gets larger. If r is negative it means that as one gets larger, the other gets smaller (often called an \"inverse\" correlation).\n\nWhile correlation coefficients are normally reported as r = (a value between -1 and +1), squaring them makes then easier to understand. The square of the coefficient (or r square) is equal to the percent of the variation in one variable that is related to the variation in the other. After squaring r, ignore the decimal point. An r of .5 means 25% of the variation is related (.5 squared =.25). An r value of .7 means 49% of the variance is related (.7 squared = .49).\n\nA correlation report can also show a second result of each test - statistical significance. In this case, the significance level will tell you how likely it is that the correlations reported may be due to chance in the form of random sampling error. If you are working with small sample sizes, choose a report format that includes the significance level. This format also reports the sample size.\n\nA key thing to remember when working with correlations is never to assume a correlation means that a change in one variable causes a change in another. Sales of personal computers and athletic shoes have both risen strongly in the last several years and there is a high correlation between them, but you cannot assume that buying computers causes people to buy athletic shoes (or vice versa).\n\nThe second caveat is that the Pearson correlation technique works best with linear relationships: as one variable gets larger, the other gets larger (or smaller) in direct proportion. It does not work well with curvilinear relationships (in which the relationship does not follow a straight line). An example of a curvilinear relationship is age and health care. They are related, but the relationship doesn't follow a straight line. Young children and older people both tend to use much more health care than teenagers or young adults. Multiple regression (also included in the Statistics Module) can be used to examine curvilinear relationships, but it is beyond the scope of this article.", "skillName": "Correlation."}
{"id": 46, "category": "Statistics", "skillText": "The concept of big data has been around for years; most organizations now understand that if they capture all the data that streams into their businesses, they can apply analytics and get significant value from it. But even in the 1950s, decades before anyone uttered the term “big data,” businesses were using basic analytics (essentially numbers in a spreadsheet that were manually examined) to uncover insights and trends.\n\nThe new benefits that big data analytics brings to the table, however, are speed and efficiency. Whereas a few years ago a business would have gathered information, run analytics and unearthed information that could be used for future decisions, today that business can identify insights for immediate decisions. The ability to work faster – and stay agile – gives organizations a competitive edge they didn’t have before.\n\n \nThe Importance of Big Data Analytics Graphic\nWhy is big data analytics important?\n\nBig data analytics helps organizations harness their data and use it to identify new opportunities. That, in turn, leads to smarter business moves, more efficient operations, higher profits and happier customers. In his report Big Data in Big Companies, IIA Director of Research Tom Davenport interviewed more than 50 businesses to understand how they used big data. He found they got value in the following ways:\n\n    Cost reduction. Big data technologies such as Hadoop and cloud-based analytics bring significant cost advantages when it comes to storing large amounts of data – plus they can identify more efficient ways of doing business.\n    Faster, better decision making. With the speed of Hadoop and in-memory analytics, combined with the ability to analyze new sources of data, businesses are able to analyze information immediately – and make decisions based on what they’ve learned.\n    New products and services. With the ability to gauge customer needs and satisfaction through analytics comes the power to give customers what they want. Davenport points out that with big data analytics, more companies are creating new products to meet customers’ needs.\n\n\nAnalysis of data is a process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, suggesting conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, in different business, science, and social science domains.\n\nData mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes. Business intelligence covers data analysis that relies heavily on aggregation, focusing on business information. In statistical applications, some people divide data analysis into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data and CDA on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All are varieties of data analysis.\n\nData integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination. The term data analysis is sometimes used as a synonym for data modeling.\n\nContents\n\n    1 The process of data analysis\n        1.1 Data requirements\n        1.2 Data collection\n        1.3 Data processing\n        1.4 Data cleaning\n        1.5 Exploratory data analysis\n        1.6 Modeling and algorithms\n        1.7 Data product\n        1.8 Communication\n    2 Quantitative messages\n    3 Techniques for analyzing quantitative data\n    4 Analytical activities of data users\n    5 Barriers to effective analysis\n        5.1 Confusing fact and opinion\n        5.2 Cognitive biases\n        5.3 Innumeracy\n    6 Other topics\n        6.1 Analytics and business intelligence\n        6.2 Education\n    7 Practitioner notes\n        7.1 Initial data analysis\n            7.1.1 Quality of data\n            7.1.2 Quality of measurements\n            7.1.3 Initial transformations\n            7.1.4 Did the implementation of the study fulfill the intentions of the research design?\n            7.1.5 Characteristics of data sample\n            7.1.6 Final stage of the initial data analysis\n            7.1.7 Analysis\n            7.1.8 Nonlinear analysis\n        7.2 Main data analysis\n            7.2.1 Exploratory and confirmatory approaches\n            7.2.2 Stability of results\n            7.2.3 Statistical methods\n    8 Free software for data analysis\n    9 See also\n    10 References\n        10.1 Citations\n        10.2 Bibliography\n    11 Further reading\n\nThe process of data analysis\nData science process flowchart\n\nAnalysis refers to breaking a whole into its separate components for individual examination. Data analysis is a process for obtaining raw data and converting it into information useful for decision-making by users. Data is collected and analyzed to answer questions, test hypotheses or disprove theories.[1]\n\nStatistician John Tukey defined data analysis in 1961 as: \"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"[2]\n\nThere are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases.[3]\nData requirements\n\nThe data necessary as inputs to the analysis are specified based upon the requirements of those directing the analysis or customers who will use the finished product of the analysis. The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained. Data may be numerical or categorical (i.e., a text label for numbers).[3]\nData collection\n\nData is collected from a variety of sources. The requirements may be communicated by analysts to custodians of the data, such as information technology personnel within an organization. The data may also be collected from sensors in the environment, such as traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.[3]\nData processing\nThe phases of the intelligence cycle used to convert raw information into actionable intelligence or knowledge are conceptually similar to the phases in data analysis.\n\nData initially obtained must be processed or organized for analysis. For instance, this may involve placing data into rows and columns in a table format for further analysis, such as within a spreadsheet or statistical software.[3]\nData cleaning\n\nOnce processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that data is entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, deduplication, and column segmentation.[4] Such data problems can also be identified through a variety of analytical techniques. For example, with financial information, the totals for particular variables may be compared against separately published numbers believed to be reliable.[5] Unusual amounts above or below pre-determined thresholds may also be reviewed. There are several types of data cleaning that depend on the type of data. Quantitative data methods for outlier detection can be used to get rid of likely incorrectly entered data. Textual data spellcheckers can be used to lessen the amount of mistyped words, but it is harder to tell if the words themselves are correct.[6]\nExploratory data analysis\n\nOnce the data is cleaned, it can be analyzed. Analysts may apply a variety of techniques referred to as exploratory data analysis to begin understanding the messages contained in the data.[7][8] The process of exploration may result in additional data cleaning or additional requests for data, so these activities may be iterative in nature. Descriptive statistics such as the average or median may be generated to help understand the data. Data visualization may also be used to examine the data in graphical format, to obtain additional insight regarding the messages within the data.[3]\nModeling and algorithms\n\nMathematical formulas or models called algorithms may be applied to the data to identify relationships among the variables, such as correlation or causation. In general terms, models may be developed to evaluate a particular variable in the data based on other variable(s) in the data, with some residual error depending on model accuracy (i.e., Data = Model + Error).[1]\n\nInferential statistics includes techniques to measure relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X) explains the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as Y = aX + b + error, where the model is designed such that a and b minimize the error when the model predicts Y for a given range of values of X. Analysts may attempt to build models that are descriptive of the data to simplify analysis and communicate results.[1]\nData product\n\nA data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. An example is an application that analyzes data about customer purchasing history and recommends other purchases the customer might enjoy.[3]\nCommunication\nData visualization to understand the results of a data analysis.[9]\nMain article: Data visualization\n\nOnce the data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.[3]\n\nWhen determining how to communicate the results, the analyst may consider data visualization techniques to help clearly and efficiently communicate the message to the audience. Data visualization uses information displays such as tables and charts to help communicate key messages contained in the data. Tables are helpful to a user who might lookup specific numbers, while charts (e.g., bar charts or line charts) may help explain the quantitative messages contained in the data.\nQuantitative messages\nMain article: Data visualization\nA time series illustrated with a line chart demonstrating trends in U.S. federal spending and revenue over time.\nA scatterplot illustrating correlation between two variables (inflation and unemployment) measured at points in time.\n\nAuthor Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.\n\n    Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.\n    Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons.\n    Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.\n    Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.\n    Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis.\n    Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.\n    Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.\n    Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.[10][11]\n\nTechniques for analyzing quantitative data\nSee also: Problem solving\n\nAuthor Jonathan Koomey has recommended a series of best practices for understanding quantitative data. These include:\n\n    Check raw data for anomalies prior to performing your analysis;\n    Re-perform important calculations, such as verifying columns of data that are formula driven;\n    Confirm main totals are the sum of subtotals;\n    Check relationships between numbers that should be related in a predictable way, such as ratios over time;\n    Normalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;\n    Break problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.[5]\n\nFor the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.\nAn illustration of the MECE principle used for data analysis.\n\nThe consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as \"Mutually Exclusive and Collectively Exhaustive\" or MECE. For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).\n\nAnalysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.\n\nRegression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.\n\nNecessary condition analysis \n(NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., \"To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?\"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X's can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.\nAnalytical activities of data users\n\nUsers may have particular data points of interest within a data set, as opposed to general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.[12][13][14]\n# \tTask \tGeneral\nDescription \tPro Forma\nAbstract \tExamples\n1 \tRetrieve Value \tGiven a set of specific cases, find attributes of those cases. \tWhat are the values of attributes {X, Y, Z, ...} in the data cases {A, B, C, ...}? \t- What is the mileage per gallon of the Audi TT?\n\n- How long is the movie Gone with the Wind?\n2 \tFilter \tGiven some concrete conditions on attribute values, find data cases satisfying those conditions. \tWhich data cases satisfy conditions {A, B, C...}? \t- What Kellogg's cereals have high fiber?\n\n- What comedies have won awards?\n\n- Which funds underperformed the SP-500?\n3 \tCompute Derived Value \tGiven a set of data cases, compute an aggregate numeric representation of those data cases. \tWhat is the value of aggregation function F over a given set S of data cases? \t- What is the average calorie content of Post cereals?\n\n- What is the gross income of all stores combined?\n\n- How many manufacturers of cars are there?\n4 \tFind Extremum \tFind data cases possessing an extreme value of an attribute over its range within the data set. \tWhat are the top/bottom N data cases with respect to attribute A? \t- What is the car with the highest MPG?\n\n- What director/film has won the most awards?\n\n- What Robin Williams film has the most recent release date?\n5 \tSort \tGiven a set of data cases, rank them according to some ordinal metric. \tWhat is the sorted order of a set S of data cases according to their value of attribute A? \t- Order the cars by weight.\n\n- Rank the cereals by calories.\n6 \tDetermine Range \tGiven a set of data cases and an attribute of interest, find the span of values within the set. \tWhat is the range of values of attribute A in a set S of data cases? \t- What is the range of film lengths?\n\n- What is the range of car horsepowers?\n\n- What actresses are in the data set?\n7 \tCharacterize Distribution \tGiven a set of data cases and a quantitative attribute of interest, characterize the distribution of that attribute’s values over the set. \tWhat is the distribution of values of attribute A in a set S of data cases? \t- What is the distribution of carbohydrates in cereals?\n\n- What is the age distribution of shoppers?\n8 \tFind Anomalies \tIdentify any anomalies within a given set of data cases with respect to a given relationship or expectation, e.g. statistical outliers. \tWhich data cases in a set S of data cases have unexpected/exceptional values? \t- Are there exceptions to the relationship between horsepower and acceleration?\n\n- Are there any outliers in protein?\n9 \tCluster \tGiven a set of data cases, find clusters of similar attribute values. \tWhich data cases in a set S of data cases are similar in value for attributes {X, Y, Z, ...}? \t- Are there groups of cereals w/ similar fat/calories/sugar?\n\n- Is there a cluster of typical film lengths?\n10 \tCorrelate \tGiven a set of data cases and two attributes, determine useful relationships between the values of those attributes. \tWhat is the correlation between attributes X and Y over a given set S of data cases? \t- Is there a correlation between carbohydrates and fat?\n\n- Is there a correlation between country of origin and MPG?\n\n- Do different genders have a preferred payment method?\n\n- Is there a trend of increasing film length over the years?\nBarriers to effective analysis\n\nBarriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.\nConfusing fact and opinion\n\nYou are entitled to your own opinion, but you are not entitled to your own facts.\nDaniel Patrick Moynihan\n\nEffective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011-2020 time period would add approximately $3.3 trillion to the national debt.[15] Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.\n\nAs another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects.\" This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.\nCognitive biases\n\nThere are a variety of cognitive biases that can adversely effect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one's preconceptions. In addition, individuals may discredit information that does not support their views.\n\nAnalysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.[16]\nInnumeracy\n\nEffective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate. Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.[17]\n\nFor example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization[5] or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.\n\nAnalysts may also analyze data under different assumptions or scenarios. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock. Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures.\nOther topics\nAnalytics and business intelligence\nMain article: Analytics\n\nAnalytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that use data to understand and analyze business performance.[18]\nEducation\nAnalytic activities of data visualization users\n\nIn education, most educators have access to a data system for the purpose of analyzing student data.[19] These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators’ data analyses.[20]\nPractitioner notes\n\nThis section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.\nInitial data analysis\n\nThe most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:[21]\nQuality of data\n\nThe quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms, n: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.\n\n    Test for common-method variance.\n\nThe choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.[22]\nQuality of measurements\n\nThe quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.\nThere are two ways to assess measurement\n\n    Analysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach's α of the scales, and the change in the Cronbach's alpha when an item would be deleted from a scale.[23]\n\nInitial transformations\n\nAfter assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.[24]\nPossible transformations of variables are:[25]\n\n    Square root transformation (if the distribution differs moderately from normal)\n    Log-transformation (if the distribution differs substantially from normal)\n    Inverse transformation (if the distribution differs severely from normal)\n    Make categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)\n\nDid the implementation of the study fulfill the intentions of the research design?\n\nOne should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups.\nIf the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.\nOther possible data distortions that should be checked are:\n\n    dropout (this should be identified during the initial data analysis phase)\n    Item nonresponse (whether this is random or not should be assessed during the initial data analysis phase)\n    Treatment quality (using manipulation checks).[26]\n\nCharacteristics of data sample\n\nIn any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.\nThe characteristics of the data sample can be assessed by looking at:\n\n    Basic statistics of important variables\n    Scatter plots\n    Correlations and associations\n    Cross-tabulations[27]\n\nFinal stage of the initial data analysis\n\nDuring the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.\nAlso, the original plan for the main data analyses can and should be specified in more detail or rewritten.\nIn order to do this, several decisions about the main data analyses can and should be made:\n\n    In the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?\n    In the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?\n    In the case of outliers: should one use robust analysis techniques?\n    In case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?\n    In the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?\n    In case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?[28]\n\nAnalysis\n\nSeveral analyses can be used during the initial data analysis phase:[29]\n\n    Univariate statistics (single variable)\n    Bivariate associations (correlations)\n    Graphical techniques (scatter plots)\n\nIt is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:[30]\n\n    Nominal and ordinal variables\n        Frequency counts (numbers and percentages)\n        Associations\n            circumambulations (crosstabulations)\n            hierarchical loglinear analysis (restricted to a maximum of 8 variables)\n            loglinear analysis (to identify relevant/important variables and possible confounders)\n        Exact tests or bootstrapping (in case subgroups are small)\n        Computation of new variables\n    Continuous variables\n        Distribution\n            Statistics (M, SD, variance, skewness, kurtosis)\n            Stem-and-leaf displays\n            Box plots\n\nNonlinear analysis\n\nNonlinear analysis will be necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods. Nonlinear data analysis is closely related to nonlinear system identification.[31]\nMain data analysis\n\nIn the main analysis phase analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.[32]\nExploratory and confirmatory approaches\n\nIn the main analysis phase either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.\n\nExploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.[33]\nStability of results\n\nIt is important to obtain some indication about how generalizable the results are.[34] While this is hard to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing this:\n\n    Cross-validation: By splitting the data in multiple parts we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well.\n    Sensitivity analysis: A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do this is with bootstrapping.\n\nStatistical methods\n\nMany statistical methods have been used for statistical analyses. A very brief list of four of the more popular methods is:\n\n    General linear model: A widely used model on which various methods are based (e.g. t test, ANOVA, ANCOVA, MANOVA). Usable for assessing the effect of several predictors on one or more continuous dependent variables.\n    Generalized linear model: An extension of the general linear model for discrete dependent variables.\n    Structural equation modelling: Usable for assessing latent structures from measured manifest variables.\n    Item response theory: Models for (mostly) assessing one latent variable from several binary measured variables (e.g. an exam).\n\nFree software for data analysis\n\n    NCA Calculator \n    - a simple online calculator for finding necessary but not sufficient conditions in datasets\n    NCA Software \n    - R package for finding necessary but not sufficient conditions in datasets\n    Data Applied - an online data mining and data visualization solution.\n    DataMelt - a multiplatform (Java-based) data analysis framework from the jWork.ORG \n    community of developers led by Dr. S.Chekanov\n    DevInfo - a database system endorsed by the United Nations Development Group for monitoring and analyzing human development.\n    ELKI - data mining framework in Java with data mining oriented visualization functions.\n    KNIME - the Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\n    MEPX \n    - cross platform tool for regression and classification problems.\n    PAW - FORTRAN/C data analysis framework developed at CERN\n    Orange - A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.\n    QSoas - An open source, command-driven program for analyzing y=f(x) data (noise removal, baseline corrections, global fitting the solutions of differential equations or kinetic schemes, and more). Binaries available for Mac OSX and Windows. http://www.qsoas.org \n    [35]\n    R - a programming language and software environment for statistical computing and graphics.\n    ROOT - C++ data analysis framework developed at CERN\n    dotplot - cloud based visual designer to create analytic models[36]\n    SciPy - A set of Python tools for data analysis http://scipy.org/stackspec.html \n    Statsmodels - a Python module that allows users to explore data, estimate statistical models, and perform statistical tests http://statsmodels.sourceforge.net/ \n    Pandas - A software library written for the Python programming language for data manipulation and analysis.\n    myInvenio [37]- a cloud based solution to automatically discover processes from event logs.\n\nSee also\nPortal icon \tstatistics portal\n\n    Analytics\n    Business intelligence\n    Censoring (statistics)\n    Computational physics\n    Data acquisition\n    Data governance\n    Data mining\n    Data Presentation Architecture\n    Data science\n    Digital signal processing\n    Dimension reduction\n    Early case assessment\n    Exploratory data analysis\n    Fourier analysis\n    Machine learning\n    Multilinear PCA\n    Multilinear subspace learning\n    Multiway Data Analysis\n    Nearest neighbor search\n    nonlinear system identification\n    Predictive analytics\n    Principal component analysis\n    Qualitative research\n    Scientific computing\n    Structured data analysis (statistics)\n    system identification\n    Test method\n    Text analytics\n    Unstructured data\n    Wavelet", "skillName": "DataAnalytics."}
{"id": 47, "category": "Statistics", "skillText": "Summary Table for Statistical Techniques\n\n(printable version )\nSummary Table for Statistical Techniques\n  \t\nInference\n\t\nParameter\n\t\nStatistic\n\t\nType of Data\n\t\nExamples\n\t\nAnalysis\n\t\nMinitab Command\n\t\nConditions\n1 \tEstimating a Mean \t\n\nOne Population Mean\n\n\\(\\mu\\)\n\t\n\nSample Mean\n\n\\(\\bar{x}\\)\n\tNumerical \t\n\nWhat is the average weight of adults?\n\nWhat is the average cholesterol level of adult females?\n\t\n\n1-sample t-interval\n\n\\(\\bar{x}\\pm t_{\\alpha /2}\\cdot \\frac{s}{\\sqrt{n}}\\)\n\tStat > Basic statistics > 1-sample t \t\n\ndata approximately normal\n\nOR\n\nhave a large sample size (n ≥ 30)\n2 \tTest About a Mean \t\n\nOne population Mean\n\n\\(\\mu\\)\n\t\n\nSample Mean\n\n\\(\\bar{x}\\)\n\tNumerical \t\n\nIs the average GPA of juniors at Penn State higher than 3.0?\n\nIs the average winter temperature in State College less than 42°F?\n\t\n\n\\(H_0: \\mu = \\mu_0\\)\n\n\\(H_a: \\mu \\ne \\mu_0\\)\nOR\n\\(H_a: \\mu > \\mu_0\\)\nOR\n\\(H_a: \\mu < \\mu_0\\)\n\nThe 1-sample t-test:\n\n\\(t=\\frac{\\bar{x}-\\mu_{0}}{\\frac{s}{\\sqrt{n}}}\\)\n\tStat > Basic statistics > 1-sample t \t\n\ndata approximately normal\n\nOR\n\nhave a large sample size (n ≥ 30)\n3 \tEstimating a Proportion \t\n\nOne Population Proportion\n\n\\(p\\)\n\t\n\nSample Proportion\n\n\\(\\hat{p}\\)\n\tCategorical (Binary) \t\n\nWhat is the proportion of males in the world?\n\nWhat is the proportion of students that smoke?\n\t\n\n1-proportion Z-interval\n\n\\(\\hat{p}\\pm z_{\\alpha /2}\\sqrt{\\frac{\\hat{p}\\cdot \\left ( 1-\\hat{p} \\right )}{n}}\\)\n\tStat > Basic statistics > 1-sample proportion \t\n\nhave at least 5 in each category\n4 \tTest About a Proportion \t\n\nOne Population Proportion\n\n\\(p\\)\n\tSample Proportion \\(\\hat{p}\\) \tCategorical (Binary) \t\n\nIs the proportion of females different from 0.5?\n\nIs the proportion of students who fail STAT 500 less than 0.1?\n\t\n\n\\(H_0: p = p_0\\)\n\n\\(H_a: p \\ne p_0\\)\nOR\n\\(H_a: p > p_0\\)\nOR\n\\(H_a: p < p_0\\)\n\nThe one proportion Z-test:\n\n\\(z=\\frac{\\hat{p}-p _{0}}{\\sqrt{\\frac{p _{0}\\left ( 1- p _{0}\\right )}{n}}}\\)\n\tStat > Basic statistics > 1-sample proportion \t\n\n\\(np_0 \\geq 5\\) and \\(n (1 - p_0) \\geq 5\\)\n5 \tEstimating the Difference of Two Means \t\n\nDifference in two population means\n\n\\(\\mu_1 - \\mu_2\\)\n\t\n\nDifference in two sample means\n\n\\(\\bar{x}_{1} - \\bar{x}_{2}\\)\n\tNumerical \t\n\nHow different are the mean GPAs of males and females?\n\nHow many fewer colds do vitamin C takers get, on average, than non-vitamin takers?\n\t\n\n2-sample t-interval\n\n\\(\\bar{x}_{1}-\\bar{x}_{2}\\pm t_{\\alpha /2}\\cdot s.e.\\left (\\bar{x}_{1}-\\bar{x}_{2}  \\right )\\)\n\tStat > Basic statistics > 2-sample t \t\n\nIndependent samples from the two populations\n\nData in each sample are about normal or large samples\n6 \tTest to Compare Two Means \t\n\nDifference in two population means\n\n\\(\\mu_1 - \\mu_2\\)\n\t\n\nDifference in two sample means\n\n\\(\\bar{x}_{1} - \\bar{x}_{2}\\)\n\tNumerical \t\n\nDo the mean pulse rates of exercisers and non-exercisers differ?\n\nIs the mean EDS score for dropouts greater than the mean EDS score for graduates?\n\t\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\n\\(H_a: \\mu_1 \\ne \\mu_2\\) OR \\(H_a: \\mu_1 > \\mu_2\\) OR \\(H_a: \\mu_1 < \\mu_1\\)\n\nThe 2-sample t-test:\n\n\\(t=\\frac{\\left (\\bar{x}_{1}-\\bar{x}_{2}  \\right )-0}{s.e.\\left (\\bar{x}_{1}-\\bar{x}_{2}  \\right )} \\)\n\tStat > Basic statistics > 2-sample t \t\n\nIndependent samples from the two populations\n\nData in each sample are about normal or large samples\n7 \tEstimating a Mean with Paired Data \t\n\nMean of paired difference\n\n\\(\\mu_D\\)\n\t\n\nSample mean of difference\n\n\\(\\bar{d}\\)\n\tNumerical \t\n\nWhat is the difference in pulse rates, on the average, before and after exercise?\n\t\n\npaired t-interval\n\n\\(\\bar{d}\\pm t_{\\alpha /2}\\cdot \\frac{s_{d}}{\\sqrt{n}}\\)\n\tStat > Basic statistics > Paired t \t\n\nDifferences approximately normal\n\nOR\n\nHave a large number of pairs (n ≥ 30)\n8 \tTest About a Mean with Paired Data \t\n\nMean of paired difference\n\n\\(\\mu_D\\)\n\t\n\nSample mean of difference\n\n\\(\\bar{d}\\)\n\tNumerical \t\n\nIs the difference in IQ of pairs of twins zero?\n\nAre the pulse rates of people higher after exercise?\n\t\n\n\\(H_0: \\mu_D = 0\\)\n\n\\(H_a: \\mu_D \\ne 0\\)\nOR\n\\(H_a: \\mu_D > 0\\)\nOR\n\\(H_a: \\mu_D < 0\\)\n\n\\(t=\\frac{\\bar{d}-0}{\\frac{s_{d}}{\\sqrt{n}}}\\)\n\tStat > Basic statistics > Paired t \t\n\nDifferences approximately normal\n\nOR\n\nHave a large number of pairs (n ≥ 30)\n9 \tEstimating the Difference of Two Proportions \t\n\nDifference in two population proportions\n\n\\(p_1 - p_2\\)\n\t\n\nDifference in two sample proportions\n\n\\(\\hat{p}_{1} - \\hat{p}_{2}\\)\n\tCategorical (Binary) \t\n\nHow different are the percentages of male and female smokers?\n\nHow different are the percentages of upper- and lower-class binge drinkers?\n\t\n\ntwo-proportions Z-interval\n\n\\(\\hat{p _{1}}-\\hat{p _{2}}\\pm z_{\\alpha /2}\\cdot s.e.\\left ( \\hat{p _{1}}-\\hat{p _{2}} \\right )\\)\n\tStat > Basic statistics > 2 proportions \t\n\nIndependent samples from the two populations\n\nHave at least 5 in each category for both populations\n10 \tTest to Compare Two Proportions \t\n\nDifference in two population proportions\n\n\\(p_1 - p_2\\)\n\t\n\nDifference in two sample proportions\n\n\\(\\hat{p}_{1} - \\hat{p}_{2}\\)\n\tCategorical (Binary) \t\n\nIs the percentage of males with lung cancer higher than the percentage of females with lung cancer?\n\nAre the percentages of upper- and lower- class binge drinkers different?\n\t\n\n\\(H_0: p_1 = p_2\\)\n\n\\(H_a: p_1 \\ne p_2 \\)\nOR\n\\(H_a: p_1 > p_2\\) OR\n\\(H_a: p_1 < p_2\\)\n\nThe two proportion z test:\n\n\\(z=\\frac{\\hat{p}_{1}-\\hat{p}_{2}}{\\sqrt{\\hat{p}\\left ( 1-\\hat{p} \\right )\\left ( \\frac{1}{n_{1}}+ \\frac{1}{n_{2}}\\right )}}\\)\n\n\\(\\hat{p}=\\frac{x_{1}+x_{2}}{n_{1}+n_{2}}\\)\n\tStat > Basic statistics > 2 proportions \t\n\nIndependent samples from the two populations\n\nHave at least 5 in each category for both populations\n11 \tRelationship in a 2-Way Table \tRelationship between two categorical variables or difference in two or more population proportions \tThe observed counts in a two-way table \tCategorical \t\n\nIs there a relationship between smoking and lung cancer?\n\nDo the proportions of students in each class who smoke differ?\n\t\n\nHo: The two variables are not related\n\nHa: The two variables are related\n\nThe chi-square statistic:\n\n\\(X^2=\\sum_{\\text{all cells}}\\frac{(\\text{Observed-Expected})^2}{\\text{Expected}}\\)\n\tStat > Tables > Chi square Test \t\n\nAll expected counts should be greater than 1\n\nAt least 80% of the cells should have an expected count greater than 5\n12 \tTest About a Slope \t\n\nSlope of the population regression line\n\n\\(\\beta_1\\)\n\t\n\nSample estimate of the slope\n\nb1\n\tNumerical \t\n\nIs there a linear relationship between height and weight of a person?\n\t\n\n\\(H_0: \\beta_1 = 0\\)\n\n\\(H_a: \\beta_1 \\ne 0\\) OR \\(H_a: \\beta_1 > 0\\) OR \\(H_a: \\beta_1 < 0\\)\n\nThe t-test with n - 2 degrees of freedom:\n\n\\(t=\\frac{b_{1}-0}{s.e.\\left ( b_{1} \\right )}\\)\n\tStat > Regression > Regression \t\n\nThe form of the equation that links the two variables must be correct\n\nThe error terms are normally distributed\n\nThe errors terms have equal variances\n\nThe error terms are independent of each other\n13 \tTest to Compare Several Means \t\n\nPopulation means of the t populations\n\n\\(\\mu_1, \\mu_2, \\cdots , \\mu_t\\)\n\t\n\nSample means of the t populations\n\n\\(x_1, x_2, \\cdots , x_t\\)\n\tNumerical \t\n\nIs there a difference between the mean GPA of freshman, sophomore, junior, and senior classes?\n\t\n\n\\(H_0: \\mu_1 = \\mu_2 = ... = \\mu_t\\)\n\n\\(H_a: \\text{not all the means are equal}\\)\n\nThe F-test for one-way ANOVA:\n\n\\(F=\\frac{MSTR}{MSE}\\)\n\tStat > ANOVA > Oneway \t\n\nEach population is normally distributed\n\nIndependent samples from the t populations\n\nEqual population standard deviations\n14 \tTest of Strength & Direction of Linear Relationship of 2 Quantitative Variables \t\n\nPopulation Correlation\n\n\\(\\rho\\)\n\n\"rho\"\n\t\n\nSample correlation\n\n\\(r\\)\n\tNumerical \tIs there a linear relationship between height and weight? \t\n\n\\(H_0: \\rho = 0\\)\n\n\\(H_a: \\rho \\ne 0\\)\n\n\\(t=\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\)\n\tStat > Basic Statistics > Correlation \t\n\n2 variables are continuous\n\nRelated pairs\n\nNo significant outliers\n\nNormality of both variables\n\nLinear relationship between the variables\n15 \tTest to Compare Two Population Variances \t\n\nPopulation variances of 2 populations\n\n\\(\\sigma_{1}^{2}, \\sigma_{2}^{2}\\)\n\t\n\nSample variances of 2 populations\n\n\\(s_{1}^{2}, s_{2}^{2}\\)\n\tNumerical \t\n\nAre the variances of length of lumber produced by Company A different from those produced by Company B\n\t\n\n\\(H_0: \\sigma_{1}^{2} = \\sigma_{2}^{2}\\)\n\n\\(H_2:  \\sigma_{1}^{2} \\ne \\sigma_{2}^{2}\\)\n\n\\(F=\\frac{s_{1}^{2}}{s_{2}^{2}}\\)\n\tStat > Basic statistics > 2 variances \t\n\nEach population is normally distributed\n\nIndependent samples from the 2 populations", "skillName": "Summary_Table_for_Statistical_Techniques."}
{"id": 48, "category": "Recuting", "skillText": "The following 25 pages are in this category, out of 25 total. This list may not reflect recent changes (learn more).\n \n\n    Executive search\n\nA\n\n    AIMS International\n    Antal International Ltd\n\nC\n\n    Challenger, Gray & Christmas\n\nE\n\n    Eclaro International Inc.\n    Egon Zehnder\n    Harris Search Associates\n\nH\n\n    Heidrick & Struggles\n    HigdonBraddockMatthews\n    Howard-Sloan Professional Search\n    Hudson Global, Inc.\n    Human Capital Partners\n\nI\n\n    IRC Global Executive Search Partners\n    Ivy Exec\n\nK\n\n    Korn Ferry\n\nN\n\n    Neumann International\n\nP\n\n    PageGroup\n\nR\n\n    R. William Funk & Associates\n    Randstad India\n    Reed Hamilton\n    Rosenzweig & Company\n\nS\n\n    Spencer Stuart\n    Stellar Search\n\nT\n\n    Transearch International\n\nW\n\n    Whitehead Mann", "skillName": "Executive_search_firms."}
{"id": 49, "category": "Recuting", "skillText": "Executive search (informally headhunting) is a specialized recruitment service used to source candidates for senior, executive or other highly specialised positions in organizations. The method usually involves commissioning a third-party organization, typically an executive search firm but possibly a standalone consultant, to research the availability of suitable candidates working for competitors or related businesses. Having identified possible recruits that match the client's requirements, the executive search firm may act as an intermediary to investigate whether the individual might be interested in moving to a new employer and also carry out initial screening of the candidate, negotiations on remuneration, and the employment contract.\n\nContents\n\n    1 Executive search firms\n        1.1 Retained search\n            1.1.1 Delimited or engaged search\n        1.2 Contingent search\n        1.3 Pros and cons of different executive search models\n    2 See also\n\nExecutive search firms\n\nAn executive search firm is a type of professional service firm that specializes in recruiting executive human capital for their client companies in various industries. Executive search agents/professionals typically have a wide range of personal contacts in their industry or field of specialty; detailed, specific knowledge of the area; and typically operate at the most senior level of executive positions. Executive search professionals are also involved throughout the hiring process, conducting detailed interviews and presenting candidates to clients selectively, when they feel the candidate meets all stated requirements and would fit into the culture of the hiring firm. Executive search firms typically have long-lasting relationships with clients spanning many years, and in such cases the suitability of candidates is paramount. It is also important that such firms operate with a high level of professionalism and confidentiality. When corporate entities elect to use an outside executive search firm, it is usually because they lack the internal research resources, networks, or evaluative skills to properly recruit for themselves. Using an outside firm also allows the corporate entity the freedom of recruiting from competitors without doing so directly, and the ability to choose among candidates that would not be available through internal or passive sourcing methodologies. Executive search firms are national and international. Many specialize in a particular business industry sector.\n\nThe contractual relationship between client and executive search firm falls into two broad categories: contingent and retained. Contingent recruiters are paid only upon the successful completion of the \"search assignment.\" Retained recruiters are paid for the process, typically earning a recruiting fee in three stages based on the anticipated compensation of the executive.\nRetained search\n\nHigh-end executive search firms get a retainer (up-front fee) to perform a specific search for a corporate officer or other senior executive position. Typically, retained searches tend to be for positions that pay upwards of US$150,000 and often far more.\n\nSearch fees are typically 33.33% of the annual compensation of the recruited executive. Fee payments may be made in thirds, 1/3 of fee paid on initiation of the search, 1/3 paid thirty days later, and the final 1/3 paid thirty days later or upon placement of the candidate. Alternatively, a fixed fee may be established. Retained search firms provide a guarantee to do an assignment over if the hired candidate leaves before one year as long as there has not been a material change in the position requirements or management team.\n\nIn a retained search, the fee is for the time and expertise of the search firm. The firm is employed to conduct the entire recruitment effort from startup until the candidate has started working.\n\nRetained recruiters work for the organizations who are their clients, not for job candidates seeking employment, in some countries, such as the UK, recruiters are not legally permitted to charge candidates. In the U.S. job candidates may pay an up front retainer to a consulting or career counseling firms to assist them in their job search.\n\nSearch firms generally commit to off-limits agreements. These agreements prevent a firm from approaching employees of their current clients as candidates for other clients (for instance, if a headhunter recruits the new CEO into Boeing, they will agree not to recommend Boeing executives to other companies). Since they act as management consultants working in the best interests of the clients for whom they conduct searches, it would be counterproductive to simultaneously remove talented executives from those client companies. Search firms may decline assignments from certain companies, in order to preserve their ability to recruit candidates from those companies. Some large search firms may insist on guarantees of a certain number or dollar value of searches before they will put an entire company \"off-limits\".\nDelimited or engaged search\n\nAnother form of high-end executive search, delimited or engaged search, is often improperly categorized as retained search, although there are distinct differences.\n\nSimilar to retained search firms, delimited/engaged search firms require an up-front fee before engaging the search. Unlike a conventional retainer, however, the delimited/engaged search commitment fee is refundable if the recruiter fails to achieve a hire or other deliverable specified in the contract. Moreover, the delimited/engaged search commitment fee does not follow the typical 1/3, 1/3, 1/3 model of retainers, but rather is a relatively small up-front fee which is discounted from the final placement fee of 25-35% of the successful candidate’s first year compensation.\n\nBoth retained and delimited/engaged searches involve partial payment prior to filling the job, and the contracted recruiter has the search exclusively. Therefore, the search can be customized to the client organization’s needs, with the search professional providing a consultative service throughout the process.\n\nWhile both retained and delimited/engaged searches serve client employers rather than job-seeking executives, delimited/engaged search contracts always (as opposed to sometimes) state a future date when the project must be completed or the downpayment refunded.\nContingent search\n\nAs stated, contingent search firms are remunerated only upon the successful completion of the search—typically when the candidate accepts the position. These recruiters may earn 20% to 35% of the candidate's first-year base salary or total remuneration as a hiring fee; the fee may also be calculated to include the candidate's (that is, the successful hire's) median or expected first-year bonus payout. In any case, the fee is (as always) paid by the hiring company, not the candidate/hire. Contingent firms in some of the emerging markets may quote fees in the range of 12% to 20% as well.\nPros and cons of different executive search models\n\nClients (companies seeking to hire) often tend to work with contingent search firms when filling mid-level positions. As contingent search firms generally rely heavily on their contacts, and seldom work on an exclusive basis, it is not rare for a client to work with a large number of contingent recruiters on the same search at the same time, in order to maximize the volume of candidate (job seeker) resumes they receive. Beyond the increased volume of candidates that such an approach allows, contingent firms do not get paid until the placement is made (a candidate is successfully hired), and thus the search risk is shifted almost entirely to the search firms. Moreover, contingent search firms often work with clients on higher percentage fee basis, relative to retained and delimited search firms as they shoulder more risk.\n\nFor senior level roles, clients often prefer to work with Recruiters who have performed in the past for them and usually will end up in the hands of a retained or delimited recruiter. By working exclusively with one firm on such searches, the client generally develops a much deeper relationship with the recruiter, and receives a much higher level of service. With all methods, retained, delimited, and contingency, clients rely on search professionals to provide not just resumes, but also insightful, consultative information about the market in general.\n\nA delimited search is often preferred by clients who are seeking a retainer-style service level, while not willing to accept the level of risk that retained search entails. While delimited search does entail up-front fees, they tend to be much smaller than total pre-placement fees that retained search entails. Moreover, delimited search professionals shoulder the risk of their own failure to execute the search within a specified time-frame, offering to refund the up-front fees in such an event. While delimited search is not as desirable for searches that are open-ended in nature, the “ticking clock” is often seen by clients as an incentive that motivates delimited search recruiters to stay more active and involved throughout the hiring process.\nSee also\n\n    Employment agency\n    Recruitment\n    Onboarding", "skillName": "Executive_search."}
{"id": 50, "category": "Recuting", "skillText": "The personality–job fit theory postulates that a person's personality traits will reveal insight as to adaptability within an organization. The degree of confluence between a person and the organization is expressed as their Person-Organization (P-O) fit.[1] This is also referred to as a person–environment fit.[2][3][4] A common measure of the P-O fit is workplace efficacy; the rate at which workers are able to complete tasks. These tasks are mitigated by workplace environs- for example, a worker who works more efficiently as an individual than in a team will have a higher P-O fit for a workplace that stresses individual tasks (such as accountancy).[1] By matching the right personality with the right job, company workers can achieve a better synergy and avoid pitfalls such as high turnover and low job satisfaction. Employees are more likely to stay committed to organizations if the fit is 'good'.\n\nIn practice, P-O fit would be used to gauge integration with organizational competencies. The Individual is assessed on these competencies, which reveals efficacy, motivation, influence, and co-worker respect. Competencies can be assessed using various tools like psychological tests, assessment centres competency based interview, situational analysis, etc.\n\nIf the Individual displays a high P-O fit, we can say that the Individual would most likely be able to adjust to the company environment and work culture, and would be able to perform at an optimum level.\n\nBuettner proposed a framework for recommender systems searching online social networks for future employees that covers the whole P-O fit.[5] The framework show how the candidate's personality traits and the organization's culture traits can be automatically extracted from online social networks.\nSee also\n\n    Aptitude\n    Onboarding\n    Person-environment fit\n    Social influence\n    Trait activation theory", "skillName": "Personality-job_fit_theory."}
{"id": 51, "category": "Recuting", "skillText": "M\n\n    ► Monster.com‎ (13 P)\n\nR\n\n    ► Rozee.pk‎ (2 P)\n\nPages in category \"Employment websites\"\n\nThe following 96 pages are in this category, out of 96 total. This list may not reflect recent changes (learn more).\n \n\n    Employment website\n\nA\n\n    Aap3\n    Aasaanjobs.com\n    Adzuna\n    AfterCollege\n    Akhtaboot\n    AlJazeera Jobs\n    America's Job Exchange\n    Anphabe.com\n    The Ashdown Group\n\nB\n\n    Bayt.com\n    BCGsearch.com\n\nC\n\n    Canadian Job Bank\n    CareerBuilder\n    CareerBuilder.ca\n    CareerStructure.com\n    ClearanceJobs\n    CoolAvenues.com\n    Craigslist\n    CV-Library\n    CyberCoders\n\nD\n\n    Delete Agency\n    Dice.com\n\nE\n\n    Eluta.ca\n    The Employment Guide\n    Entelo\n    EntertainmentCareers.net\n\nF\n\n    Find a Crew\n    Efinancialcareers.com\n\nG\n\n    Glassdoor\n    Gumtree\n\nH\n\n    Hispanic Business\n    Hosco\n    Hound.com\n\nI\n\n    IGrad\n    Indeed.com\n    Insidetrak\n    Internships.com\n    Internwise\n    Ivory Standard\n    Ivy Exec\n\nJ\n\n    Jobable\n    Jobberman\n    Jobbi\n    Jobindex\n    Jobing.com\n    Jobpostings.ca\n    .jobs\n    JobServe\n    Jobster\n    JobStreet.com\n    JOBTRAK\n\nK\n\n    Kijiji\n\nL\n\n    TheLadders.com\n    LawCrossing\n    LinkUp (website)\n    List of employment websites\n\nM\n\n    Monster.com\n    TheMuse\n    The Daily Muse\n    Myjobmatcher\n    MyScience\n\nN\n\n    Naukri.com\n    Naukrigulf.com\n    Naukrinama.com\n\nO\n\n    Oodle, Inc.\n\nP\n\n    Draft:PaperPk.com\n    PeoplePerHour\n    Preloved\n    Proven (company)\n\nQ\n\n    Qatarsale\n\nR\n\n    ResearchGate\n    RNDeer\n    Rozee.pk\n\nS\n\n    Scripted (company)\n    Simply Hired\n    SmartMatch\n    Snagajob\n    SpotJobs\n    StepStone\n    Studentgems\n    Swissnex\n    SXMJobs.com\n\nT\n\n    Talent Zoo\n    TalentEgg\n    Technojobs\n    Television Writers Vault\n    TimesJobs.com\n    TolMol\n    Trovit\n    TweetMyJobs\n\nU\n\n    Uloop\n\nW\n\n    Working in Canada\n    Workopolis\n    Workpop\n\nZ\n\n    ZipRecruiter", "skillName": "Employment_websites."}
{"id": 52, "category": "Recuting", "skillText": "Human resource management\nFrom Wikipedia, the free encyclopedia\n\tIt has been suggested that this article be merged with Human Resources Management. (Discuss) Proposed since July 2016.\n\nHuman resource management (HRM or simply HR) is the management of human resources. It is a function in organizations designed to maximize employee performance in service of an employer's strategic objectives.[1] HR is primarily concerned with the management of people within organizations, focusing on policies and on systems.[2] HR departments and units in organizations typically undertake a number of activities, including employee benefits design, employee recruitment, \"training and development\", performance appraisal, and rewarding (e.g., managing pay and benefit systems).[3] HR also concerns itself with organizational change and industrial relations, that is, the balancing of organizational practices with requirements arising from collective bargaining and from governmental laws.[4] According to R. Buettner, HRM covers the following core areas:[5]\n\n    job design and analysis,\n    workforce planning,\n    recruitment and selection,\n    training and development,\n    performance management,\n    [Remuneration compensation (remuneration)]\n    legal issues.\n\nHR is a product of the human relations movement of the early 20th century, when researchers began documenting ways of creating business value through the strategic management of the workforce. The function was initially dominated by transactional work, such as payroll and benefits administration, but due to globalization, company consolidation, technological advances, and further research, HR as of 2015 focuses on strategic initiatives like mergers and acquisitions, talent management, succession planning, industrial and labor relations, and diversity and inclusion.\n\nHuman Resources is a business field focused on maximizing employee productivity. Human Resources professionals manage the human capital of an organization and focus on implementing policies and processes. They can be specialists focusing in on recruiting, training, employee relations or benefits. Recruiting specialists are in charge of finding and hiring top talent. Training and development professionals ensure that employees are trained and have continuous development. This is done through training programs, performance evaluations and reward programs. Employee relations deals with concerns of employees when policies are broken, such as harassment or discrimination. Someone in benefits develops compensation structures, family leave programs, discounts and other benefits that employees can get. On the other side of the field are Human Resources Generalists or Business Partners. These human resources professionals could work in all areas or be labor relations representatives working with unionized employees.\n\nIn startup companies, trained professionals may perform HR duties. In larger companies, an entire functional group is typically dedicated to the discipline, with staff specializing in various HR tasks and functional leadership engaging in strategic decision-making across the business. To train practitioners for the profession, institutions of higher education, professional associations, and companies themselves have established programs of study dedicated explicitly to the duties of the function. Academic and practitioner organizations likewise seek to engage and further the field of HR, as evidenced by several field-specific publications. HR is also a field of research study that is popular within the fields of management and industrial/organizational psychology, with research articles appearing in a number of academic journals, including those mentioned later in this article.\n\nBusinesses are moving globally and forming more diverse teams. It is the role of human resources to make sure that these teams can function and people are able to communicate cross culturally and across borders. Due to changes in business, current topics in human resources are diversity and inclusion as well as using technology to advance employee engagement. In the current global work environment, most companies focus on lowering employee turnover and on retaining the talent and knowledge held by their workforce.[citation needed] New hiring not only entails a high cost but also increases the risk of a newcomer not being able to replace the person who worked in a position before. HR departments strive to offer benefits that will appeal to workers, thus reducing the risk of losing corporate knowledge.\n\nContents\n\n    1 Human resource management core functions\n    2 Human resources management activities\n    3 History\n        3.1 Antecedent theoretical developments\n        3.2 Birth and evolution of the discipline\n        3.3 In popular media\n    4 Practice\n        4.1 Business function\n        4.2 Careers\n        4.3 Virtual Human Resources\n    5 Education\n    6 Professional associations\n    7 Publications\n    8 See also\n    9 References\n    10 External links\n\nHuman resource management core functions\n\nAccording to Mondy and Mondy, human resource management has five core functions which are:[6]\n\n    Staffing\n    Human resource development\n    Compensation and benefits\n    Safety and health\n    Employee and labor relations\n\nHuman resources management activities\n\nA Human Resources Manager has several functions in a company:\n\n    Determine needs of the staff.\n    Determine to use temporary staff or hire employees to fill these needs.\n    Recruit and train the best employees.\n    Supervise the work.\n    Harmonize relationship between company and workers.\n    Manage employee relations, unions and collective bargaining.\n    Prepare employee records and personal policies.\n    Ensure high performance.\n    Manage employee payroll, benefits and compensation.\n    Ensure equal opportunities.\n    Deal with discrimination.\n    Deal with performance issues.\n    Ensure that human resources practices conform to various regulations.\n    Push the employee's motivation.\n    Focus on individual who possess energy and capabilities to ensure the job done through people to achieve results.\n    Managers need to develop their interpersonal skills to be effective. Organizations behavior focuses on how to improve factors that make organizations more effective.\n    Focus on individual who possess energy and capabilities to ensure the job done through people to achieve results.\n\nHistory\nAntecedent theoretical developments\n\nThe Human Resources field evolved first in 18th century Europe from a simple idea by Robert Owen and Charles Babbage during the industrial revolution. These men knew that people were crucial to the success of an organization. They expressed that the wellbeing of employees led to perfect work. Without healthy workers, the organization would not survive.[7] HR later emerged as a specific field in the early 20th century, influenced by Frederick Winslow Taylor (1856-1915). Taylor explored what he termed \"scientific management\" others later referred to \"Taylorism\", striving to improve economic efficiency in manufacturing jobs. He eventually keyed in on one of the principal inputs into the manufacturing process—labor—sparking inquiry into workforce productivity.[8]\n\nMeanwhile, in England C S Myers, inspired by unexpected problems among soldiers which had alarmed generals and politicians in the First World War, set up a National Institute of Industrial Psychology,[9] setting seeds for the human relations movement, which on both sides of the Atlantic built on the research of Elton Mayo and others to document through the Hawthorne studies (1924-1932) and others how stimuli, unrelated to financial compensation and working conditions, could yield more productive workers.[10] Work by Abraham Maslow (1908-1970), Kurt Lewin (1890-1947), Max Weber (1864-1920), Frederick Herzberg (1923-2000), and David McClelland (1917-1998), forming the basis for studies in industrial and organizational psychology, organizational behavior and organizational theory, was interpreted in such a way as to further claims of legitimacy for an applied discipline.\nBirth and evolution of the discipline\n\nBy the time enough theoretical evidence existed to make a business case for strategic workforce management, changes in the business landscape (à la Andrew Carnegie, John Rockefeller) and in public policy (à la Sidney and Beatrice Webb, Franklin D. Roosevelt and the New Deal) had transformed the employer-employee relationship, and the discipline became formalized as \"industrial and labor relations\". In 1913 one of the oldest known professional HR associations — the Chartered Institute of Personnel and Development (CIPD) — started in England as the Welfare Workers' Association; it changed its name a decade later to the Institute of Industrial Welfare Workers, and again the next decade to Institute of Labour Management before settling upon its current name in 2000.[11] Likewise in the United States, the world's first institution of higher education dedicated to workplace studies — the School of Industrial and Labor Relations — formed at Cornell University in 1945.[12] In 1948, what would later become the largest professional HR association — the Society for Human Resource Management (SHRM) — formed as the American Society for Personnel Administration (ASPA).[13]\n\nIn the Soviet Union, meanwhile, Stalin's use of patronage exercised through the \"HR Department\" equivalent in the Bolshevik Party, its Orgburo, demonstrated the effectiveness and influence of human-resource policies and practices,[14][15] and Stalin himself acknowledged the importance of the human resource.[16]\n\nDuring the latter half of the 20th century, union membership declined significantly, while workforce management continued to expand its influence within organizations.[citation needed] In the USA, the phrase \"industrial and labor relations\" came into use to refer specifically to issues concerning collective representation, and many[quantify] companies began referring to the proto-HR profession as \"personnel administration\".[citation needed] Many current HR practices originated with the needs of companies in the 1950s to develop and retain talent.[17]\n\nIn the late 20th century, advances in transportation and communications greatly facilitated workforce mobility and collaboration. Corporations began viewing employees as assets rather than as cogs in a machine. \"Human resources management\" consequently,[citation needed] became the dominant term for the function—the ASPA even changing its name to the Society for Human Resource Management (SHRM) in 1998.[13]\n\n\"Human capital management\" (HCM[18]) is sometimes used[by whom?] synonymously with HR, although \"human capital\" typically refers to a more narrow view of human resources; i.e., the knowledge the individuals embody and can contribute to an organization. Likewise, other terms sometimes used to describe the field include \"organizational management\", \"manpower management\", \"talent management\", \"personnel management\", and simply \"people management\".\nIn popular media\n\nSeveral popular media productions have depicted HR. On the U.S. television series of The Office, HR representative Toby Flenderson is sometimes seen as a nag because he constantly reminds coworkers of company policies and government regulations.[19] Long-running American comic strip Dilbert frequently portrays sadistic HR policies through character Catbert, the \"evil director of human resources\".[20] An HR manager is the title character in the 2010 Israeli film The Human Resources Manager, while an HR intern is the protagonist in 1999 French film Ressources humaines. Additionally, the main character in the BBC sitcom dinnerladies, Philippa, is an HR manager.\nPractice\nBusiness function\n\nDave Ulrich lists the functions of HR as: aligning HR and business strategy, re-engineering organization processes, listening and responding to employees, and managing transformation and change.[21]\n\nAt the macro-level, HR is in charge of overseeing organizational leadership and culture. HR also ensures compliance with employment and labor laws, which differ by geography, and often oversees health, safety, and security. In circumstances where employees desire and are legally authorized to hold a collective bargaining agreement, HR will typically also serve as the company's primary liaison with the employee's representatives (usually a labor union). Consequently, HR, usually through representatives, engages in lobbying efforts with governmental agencies (e.g., in the United States, the United States Department of Labor and the National Labor Relations Board) to further its priorities.\n\nTo look at Human Resource Management more specifically, it has four basic functions: staffing, training and development, motivation and maintenance. Staffing is the recruitment and selection of potential employees, done through interviewing, applications, networking, etc. Training and development is the next step in a continuous process of training and developing competent and adapted employees. Motivation is key to keeping employees highly productive. This function can include employee benefits, performance appraisals and rewards. The last function of maintenance involves keeping the employees' commitment and loyalty to the organization.\n\nThe discipline may also engage in mobility management, especially pertaining to expatriates; and it is frequently involved in the merger and acquisition process. HR is generally viewed as a support function to the business, helping to minimize costs and reduce risk.[22]\nCareers\n\nThere are half a million HR practitioners in the United States and millions more worldwide.[23] The Chief HR Officer or HR Director is the highest ranking HR executive in most companies and typically reports directly to the Chief Executive Officer and works with the Board of Directors on CEO succession.[24][25]\n\nWithin companies, HR positions generally fall into one of two categories: generalist and specialist. Generalists support employees directly with their questions, grievances, and work on a range of projects within the organization. They \"may handle all aspects of human resources work, and thus require an extensive range of knowledge. The responsibilities of human resources generalists can vary widely, depending on their employer's needs.\"[26] Specialists, conversely, work in a specific HR function. Some practitioners will spend an entire career as either a generalist or a specialist while others will obtain experiences from each and choose a path later. Being an HR manager consistently ranks as one of the best jobs, with a #4 ranking by CNN Money in 2006 and a #20 ranking by the same organization in 2009, due to its pay, personal satisfaction, job security, future growth, and benefit to society.[27][28]\n\nHuman resource consulting is a related career path where individuals may work as advisers to companies and complete tasks outsourced from companies. In 2007, there were 950 HR consultancies globally, constituting a USD $18.4 billion market. The top five revenue generating firms were Mercer, Ernst & Young, Deloitte, Watson Wyatt (now part of Towers Watson), Aon (now merged with Hewitt), and PwC consulting.[29] For 2010, HR consulting was ranked the #43 best job in America by CNN Money.[30]\n\nSome individuals with PhDs in HR and related fields, such as industrial and organizational psychology and management, are professors who teach HR principles at colleges and universities. They are most often found in Colleges of Business in departments of HR or Management. Many professors conduct research on topics that fall within the HR domain, such as financial compensation, recruitment, and training.\nVirtual Human Resources\n\nTechnology has had a significant impact on human resources practices. Human Resources is transitioning to a more technology based profession because utilizing technology makes information more accessible to the whole organization, eliminates time doing administrative tasks, allows businesses to function globally and cuts costs.[31] Information technology has improved HR practices in the following areas:\n\n    · E-Recruiting\n\nRecruiting has been the most influenced by information technology.[32] In the past, recruiters had relied on printing in publications and word of mouth to fill open positions. HR professionals were not able to post a job in more than one location and did not have access to millions of people, causing the lead time of new hires to be drawn out and tiresome. With the use of e-recruiting tools, HR professionals can post jobs and track applicants for thousands of jobs in various locations all in one place. Interview feedback, background and drug tests, and onboarding can all be viewed online. This helps the HR professionals keep track of all of their open jobs and applicants in a way that is faster and easier than before. E-recruiting also helps eliminate limitations of geographic location.[32] Jobs can be posted and seen by anyone with internet access. In addition to recruiting portals, HR professionals have a social media presence that allows them to attract employees through the World Wide Web. On social media they can build the company's brand by posting news about the company and photos of fun company events.\n\n    · Human Resources Information Systems (HRIS)\n\nHuman resources professionals generally process a considerable amount of paperwork on a daily basis. This paperwork could be anything from a department transfer request to an employee's confidential tax form. In addition to processing this paperwork, it has to be on file for a considerable period of time. The use of Human Resources Information Systems (HRIS) has made it possible for companies to store and retrieve files in an electronic format for people within the organization to access when needed. This eliminates thousands of files and frees up space within the office. Another benefit of HRIS is that it allows for information to be accessed in a timelier manner. Instead of HR professionals having to dig through files to gain information, it is accessible in seconds via the HRIS.[33] Having all of the information in one place also allows for professionals to analyze data quicker and across multiple locations because the information is in a centralized location. Examples of some Human Resources Information Systems are PeopleSoft, MyTime, SAP, Timeco, and JobsNavigator.\n\n    · Training\n\nTechnology makes it possible for human resources professionals to train new staff members in a more efficient manner. This gives employees the ability to access onboarding and training programs from anywhere. This eliminates the need for trainers to meet with new hires face to face when completing necessary paperwork to start. Training in virtual classrooms makes it possible for the HR professionals to train a large number of employees quickly and to assess their progress through computerized testing programs.[31] Some employers even incorporate an instructor with virtual training so that new hires are receiving the most vital training. Employees can take control of their own learning and development by engaging in training at a time and place of their choosing, helping them manage their work-life balance. Managers are able to track the training through the internet as well, which helps to reduce redundancy in training and training costs. Skype, virtual chat rooms, and interactive training sites are all resources that enable a more technological approach to training to enhance the experience for the new hire.\nEducation\nThe School of Industrial and Labor Relations at Cornell University was the world's first school for college-level study in HR.\n\nSeveral universities offer programs of study pertaining to HR and related fields. The School of Industrial and Labor Relations at Cornell University was the world's first school for college-level study in HR.[34] It continues to offer education at the undergraduate, graduate, and professional levels; and it operates a joint degree program with the Samuel Curtis Johnson Graduate School of Management. Other universities with entire colleges dedicated to the study of HR include Pennsylvania State University, Rutgers, The State University of New Jersey School of Management and Labor Relations, Michigan State University, Indiana University, Purdue University, University of Minnesota, Xavier Labour Relations Institute at Jamshedpur-India, University of Illinois at Urbana-Champaign, Renmin University of China and the London School of Economics. In Canada, the School of Human Resources Management at York University is leading education and research in the HRM field. Many colleges and universities house departments and institutes related to the field, either within a business school or in another college. Most business schools offer courses in HR, often in their departments of management.\nProfessional associations\nMain article: List of human resource management associations\n\nThere are a number of professional associations, some of which offer training and certification. The Society for Human Resource Management, which is based in the United States, is the largest professional association dedicated to HR,[23] with over 250,000 members in 140 countries.[35] It offers a suite of Professional in Human Resources (PHR) certifications through its HR Certification Institute. The Chartered Institute of Personnel and Development, based in England, is the oldest professional HR association,with its predecessor institution being founded in 1918.\n\nSeveral associations also serve niches within HR. The Institute of Recruiters (IOR) is a recruitment professional association, offering members education, support and training.[36] WorldatWork focuses on \"total rewards\" (i.e., compensation, benefits, work life, performance, recognition, and career development), offering several certifications and training programs dealing with remuneration and work-life balance. Other niche associations include the American Society for Training & Development and Recognition Professionals International.\n\nA largely academic organization that is relevant to HR is the Academy of Management that has an HR division. This division is concerned with finding ways to improve the effectiveness of HR.[37] The Academy publishes several journals devoted in part to research on HR, including Academy of Management Journal[38] and Academy of Management Review,[39] and it hosts an annual meeting.\nPublications\n\nAcademic and practitioner publications dealing exclusively with HR:\n\n    Cornell HR Review[40]\n    HR Magazine (SHRM)[41]\n    Human Resource Management[42]\n    Human Resource Management Review[43]\n    International Journal of Human Resource Management[44]\n    Perspectives on Work (LERA)[45]\n\nRelated publications:\n\n    Academy of Management Journal[38]\n    Academy of Management Review[39]\n    Administrative Science Quarterly[46]\n    International Journal of Selection and Assessment[47]\n    Journal of Applied Psychology[48]\n    Journal of Management[49]\n    Journal of Occupational and Organizational Psychology[50]\n    Journal of Personnel Psychology[51]\n    Organization Science[52]\n    Personnel Psychology[53]\n\nSee also\n\n    Human resource management system\n    Aspiration Management\n    Domestic inquiry\n    Organization development\n    Organizational theory", "skillName": "Human_resource_management."}
{"id": 53, "category": "Recuting", "skillText": "► Executive search firms‎ (25 P)\n\nH\n\n    ► High tech recruitment companies‎ (19 P)\n\nO\n\n    ► Online employment auction websites‎ (13 P)\n\nP\n\n    Public employment service‎ (1 C, 20 P)\n\nT\n\n    ► Temporary employment agencies‎ (36 P)\n\nPages in category \"Employment agencies\"\n\nThe following 23 pages are in this category, out of 23 total. This list may not reflect recent changes (learn more).\n \n\n    Employment agency\n\nA\n\n    Aquis Search\n    The Ashdown Group\n\nB\n\n    Bangladesh Association of International Recruiting Agencies\n\nC\n\n    Challenger, Gray & Christmas\n    Nick Corcodilos\n    CV-Library\n\nE\n\n    Elwood Staffing\n\nF\n\n    Fee-Charging Employment Agencies Convention (Revised), 1949\n    Fee-Charging Employment Agencies Convention, 1933 (shelved)\n\nH\n\n    Harvey Nash\n    Hays plc\n    Hiring hall\n    Hughes-Castell\n\nI\n\n    InterQuest Group plc\n    IQ Analytics\n\nM\n\n    MJ Boyd Consulting\n\nP\n\n    Private Employment Agencies Convention, 1997\n\nR\n\n    Relevante\n    Robert Half International\n    Routes To Work South\n\nU\n\n    United States Employment Service\n\nW\n\n    Robert Walters plc", "skillName": "Employment_agencies."}
{"id": 54, "category": "Recuting", "skillText": "An employment agency is an organization which matches employers to employees. In all developed countries, there is a publicly funded employment agency and multiple private businesses which act as employment agencies.\nContents\n\n    1 Public employment agencies\n    2 Private employment agency\n        2.1 Legal status\n        2.2 Executive recruitment\n        2.3 Executive agent\n    3 See also\n    4 Notes\n    5 References\n\nPublic employment agencies\nMain article: Public employment service\n\nOne of the oldest references to a public employment agency was in 1650, when Henry Robinson proposed an \"Office of Addresses and Encounters\" that would link employers to workers.[1] The British Parliament rejected the proposal, but he himself opened such a business, which was short-lived.[2]\n\nSince the beginning of the 20th century, every developed country has created a public employment agency as a way to combat unemployment and help people find work.[citation needed]\n\nIn the United Kingdom, the first labour exchange was established by social reformer and employment campaigner Alsager Hay Hill in London in 1871. This was later augmented by officially sanctioned exchanges created by the Labour Bureau (London) Act 1902, which subsequently went nationwide, a movement prompted by the Liberal government through the Labour Exchanges Act 1909. The present public provider of job search help is called Jobcentre Plus.\n\nIn the United States, a federal programme of employment services was rolled out in the New Deal. The initial legislation was called the Wagner-Peyser Act of 1933 and more recently job services happen through one-stop centers established by the Workforce Investment Act of 1998.\n\nIn Australia, the first public employment service was set up in 1946, called the Commonwealth Employment Service.\nPrivate employment agency\n\nThe first private employment agency in the United States was opened by Fred Winslow who opened Engineering Agency in 1893.[citation needed] It later became part of General Employment Enterprises who also owned Businessmen's Clearing House (est. 1902). Another of the oldest agencies was developed by Katharine Felton as a response to the problems brought on by the 1906 San Francisco earthquake and fire.\n\nMany temporary agencies specialize in a particular profession or field of business, such as accounting, health care, technical, or secretarial.\nLegal status\n\nFor most of the twentieth century, private employment agencies were considered quasi illegal entities under international law[citation needed]. The International Labour Organization instead called for the establishment of public employment agencies. To prevent the abusive practices of private agencies, they were either to be fully abolished, or tightly regulated. In most countries they are legal but regulated.\n\nProbably inspired by the dissenting judgments in a US Supreme Court case called Adams v. Tanner, the International Labour Organization's first ever Recommendation was targeted at fee charging agencies. The Unemployment Recommendation, 1919 (No.1), Art. 1 called for each member to,\n\n    \"take measures to prohibit the establishment of employment agencies which charge fees or which carry on their business for profit. Where such agencies already exist, it is further recommended that they be permitted to operate only under government licenses, and that all practicable measures be taken to abolish such agencies as soon as possible.\"\n\nThe Unemployment Convention, 1919, Art. 2 instead required the alternative of,\n\n    \"a system of free public employment agencies under the control of a central authority. Committees, which shall include representatives of employers and workers, shall be appointed to advise on matters concerning the carrying on of these agencies.\"\n\nIn 1933 the Fee-Charging Employment Agencies Convention (No.34) formally called for abolition. The exception was if the agencies were licensed and a fee scale was agreed in advance. In 1949 a new revised Convention (No.96) was produced. This kept the same scheme, but secured an ‘opt out’ (Art.2) for members that did not wish to sign up. Agencies were an increasingly entrenched part of the labor market. The United States did not sign up to the Conventions. The latest Convention, the Private Employment Agencies Convention, 1997 (No.181) takes a much softer stance and calls merely for regulation.\n\nIn most countries, agencies are regulated, for instance in the UK under the Employment Agencies Act 1973, or in Germany under the Arbeitnehmerüberlassungsgesetz (Employee Hiring Law of 1972).\nExecutive recruitment\nMain article: Executive search\n\nAn executive-search firm specializes in recruiting executive personnel for companies in various industries. This term may apply to job-search-consulting firms who charge job candidates a fee and who specialize in mid-to-upper-level executives. In the United States, some states require job-search-consulting firms to be licensed as employment agencies.\n\nSome third-party recruiters work on their own, while others operate through an agency, acting as direct contacts between client companies and the job candidates they recruit. They can specialize in client relationships only (sales or business development), in finding candidates (recruiting or sourcing), or in both areas. Most recruiters tend to specialize in either permanent, full-time, direct-hire positions or in contract positions, but occasionally in more than one. In an executive-search assignment, the employee-gaining client company – not the person being hired – pays the search firm its fee.\nExecutive agent\n\nAn executive agent is a type of agency that represents executives seeking senior executive positions which are often unadvertised. In the United Kingdom, almost all positions up to £125,000 ($199,000) a year are advertised and 50% of vacancies paying £125,000 – £150,000 are advertised. However only 5% of positions which pay more than £150,000 (with the exception of the public sector) are advertised and are often in the domain of around 4,000 executive recruiters in the United Kingdom.[3] Often such roles are unadvertised to maintain stakeholder confidence and to overcome internal uncertainties.\nSee also\n\n    Temporary work\n    UK agency worker law\n    Talent agent\n    Recruitment\n    Professional employer organization\n    Contingent workforce\n    Payrolling\n    Hiring hall\n    Executive search", "skillName": "Employment_agency."}
{"id": 55, "category": "Recuting", "skillText": "Onboarding, also known as organizational socialization, refers to the mechanism through which new employees acquire the necessary knowledge, skills, and behaviors to become effective organizational members and insiders.[1] Tactics used in this process include formal meetings, lectures, videos, printed materials, or computer-based orientations to introduce newcomers to their new jobs and organizations. Research has demonstrated that these socialization techniques lead to positive outcomes for new employees such as higher job satisfaction, better job performance, greater organizational commitment, and reduction in occupational stress and intent to quit.[2][3][4] These outcomes are particularly important to an organization looking to retain a competitive advantage in an increasingly mobile and globalized workforce. In the United States, for example, up to 25% of workers are organizational newcomers engaged in an onboarding process.[5] The term Induction is used instead in regions such as Australia, New Zealand parts of Europe and Canada.[6] This is known in some parts of the world as \"training.\"[7]\nContents\n\n    1 Antecedents of success\n        1.1 New employee characteristics\n        1.2 New employee behaviors\n        1.3 Organization socialization efforts\n            1.3.1 Socialization tactics\n            1.3.2 Jones' model (1986)\n            1.3.3 Formal orientations\n            1.3.4 Recruitment events\n            1.3.5 Mentorship\n    2 Employee adjustment\n        2.1 Role clarity\n        2.2 Self-efficacy\n        2.3 Social acceptance\n        2.4 Knowledge of organizational culture\n    3 Outcomes\n    4 Limits and criticisms of onboarding theory\n    5 Executive onboarding\n    6 Recommendations for practitioners\n    7 See also\n    8 References\n    9 Further reading\n\nAntecedents of success\n\nOnboarding is a multifaceted operation influenced by a number of factors pertaining to both the individual newcomer and the organization. Researchers have separated these factors into three broad categories: new employee characteristics, new employee behaviors, and organizational efforts.[8] New employee characteristics are individual differences across incoming workers, ranging from personality traits to previous work experiences. New employee behaviors refer to the specific actions carried out by newcomers as they take an active role in the socialization process. Finally, organizational efforts help facilitate the process of acclimating a new worker to an establishment through activities such as orientation or mentoring programs.\nNew employee characteristics\n\nResearch has shown evidence that employees with certain personality traits and experiences adjust to an organization more quickly.[9] These are a proactive personality, the \"Big Five\", curiosity, and greater experience levels.\n\n\"Proactive personality\" refers to the tendency to take charge of situations and achieve control over one's environment. This type of personality predisposes some workers to engage in behaviors such as information seeking that accelerate the socialization process, thus helping them to adapt more efficiently and become high-functioning organizational members.[1] Empirical evidence also demonstrates that a proactive personality is related to increased levels of job satisfaction and performance.[10][11]\n\nThe Big Five personality traits—openness, conscientiousness, extraversion, agreeableness, and neuroticism—have been linked to onboarding success, as well. Specifically, new employees who are proactive or particularly open to experience are more likely to seek out information, feedback, acceptance, and relationships with co-workers. They also exhibit higher levels of adjustment and tend to frame events more positively.[3]\n\nCuriosity also plays a substantial role in the newcomer adaptation process and is defined as the \"desire to acquire knowledge\" that energizes individual exploration of an organization's culture and norms.[12] Individuals with a curious disposition tend to frame challenges in a positive light and eagerly seek out information to help them make sense of their new organizational surroundings and responsibilities, leading to a smoother onboarding experience.[13]\n\nEmployee experience levels also affect the onboarding process such that more experienced members of the workforce tend to adapt to a new organization differently from, for example, a new college graduate starting his or her first job. This is because seasoned employees can draw from past experiences to help them adjust to their new work settings and therefore may be less affected by specific socialization efforts because they have (a) a better understanding of their own needs and requirements at work (2002).[14] and (b) are more familiar with what is acceptable in the work context.[15][16] Additionally, veteran workers may have used their past experiences to seek out organizations in which they will be a better fit, giving them an immediate advantage in adapting to their new jobs.[17]\nNew employee behaviors\n\nCertain behaviors enacted by incoming employees, such as building relationships and seeking information and feedback, can help facilitate the onboarding process. Newcomers can also quicken the speed of their adjustment by demonstrating behaviors that assist them in clarifying expectations, learning organizational values and norms, and gaining social acceptance.[1]\n\nInformation seeking occurs when new employees ask questions of their co-workers and superiors in an effort to learn about their new job and the company's norms, expectations, procedures, and policies. Miller and Jablin (1991) developed a typology of information sought after by new hires. These include referent information, understanding what is required to function on the job (role clarity); appraisal information, understanding how effectively the newcomer is able to function in relation to job role requirements (self-efficacy); and finally, relational information, information about the quality of relationships with current organizational employees (social acceptance). By actively seeking information, employees can effectively reduce uncertainties about their new jobs and organizations and make sense of their new working environments.[18] Newcomers can also passively seek information via monitoring their surroundings or by simply viewing the company website or handbook.[1] Research has shown that information seeking by incoming employees is associated with social integration, higher levels of organizational commitment, job performance, and job satisfaction in both individualistic and collectivist cultures.[19]\n\nFeedback seeking is similar to information seeking, but it is focused on a new employee's particular behaviors rather than on general information about the job or company. Specifically, feedback seeking refers to new employee efforts to gauge how to behave in their new organization. A new employee may ask co-workers or superiors for feedback on how well he or she is performing certain job tasks or whether certain behaviors are appropriate in the social and political context of the organization. In seeking constructive criticism about their actions, new employees learn what kinds of behaviors are expected, accepted, or frowned upon within the company or work group, and when they incorporate this feedback and adjust their behavior accordingly, they begin to blend seamlessly into the organization.[20] Instances of feedback inquiry vary across cultural contexts such that individuals high in self-assertiveness and cultures low in power distance report more feedback seeking than newcomers in cultures where self-assertiveness is low and power distance is high.[21]\n\nAlso called networking, relationship building involves an employee's efforts to develop camaraderie with co-workers and even supervisors. This can be achieved informally through simply talking to their new peers during a coffee break or through more formal means such as taking part in pre-arranged company events. Research has shown relationship building to be a key part of the onboarding process, leading to outcomes such as greater job satisfaction and better job performance,[2] as well as decreased stress.[4]\nOrganization socialization efforts\n\nOrganizations also invest a great amount of time and resources into the training and orientation of new company hires. Organizations differ in the variety of socialization activities they offer in order to integrate productive new workers. Possible activities include their socialization tactics, formal orientation programs, recruitment strategies, and mentorship opportunities.\nSocialization tactics\n\nSocialization tactics, or orientation tactics, are designed based on an organization's needs, values, and structural policies. Some organizations favor a more systematic approach to socialization, while others follow a more \"sink or swim\" approach in which new employees are challenged to figure out existing norms and company expectations without guidance.\n\nVan Maanen and Schein model (1979)\n\nJohn Van Maanen and Edgar H. Schein have identified at least six major tactical dimensions that characterize and represent all of the ways in which organizations may differ in their approaches to socialization.\n\nCollective versus Individual socialization\n\nCollective socialization refers to the process of taking a group of recruits who are facing a given boundary passage and putting them through the same set of experiences together. Examples of this include: basic training/boot camp for a military organization, pledging for fraternities/sororities, education in graduate schools, and so forth. Socialization in the Individual mode allows newcomers to accumulate unique experiences separate from other newcomers. Examples of this process include: Apprenticeship programs, specific internships, \"on-the-job\" training, etc.[22]\n\nFormal vs. Informal socialization\n\nFormal socialization refers to those tactics in which newcomers are more or less segregated from others and trained on the job. These processes can be witnessed with such socialization programs as police academies, internships, and apprenticeships. Informal socialization processes, on the other hand, involve little separation between newcomers and the existing employees, nor is there any effort made to distinguish the newcomer’s role specifically. Informal tactics provides a non-interventional environment for recruits to learn their new roles via trial and error. Examples of informal socialization include on-the-job training assignments, apprenticeship programs with no clearly defined role, and more generally, any situation in which a newcomer is placed into a work group with no recruit role.[22]\n\nSequential vs. Random socialization\n\nSequential socialization refers to the degree to which an organization or occupation specifies discrete and identifiable steps for the newcomers to know what phases they need to go through. Random socialization occurs when the sequences of steps leading to the targeted role are unknown, and the entire progression is quite ambiguous. In other words, while there are numerous steps or stages leading to specific organizational roles, there is necessarily no specific order in which the steps should be taken.[22]\n\nFixed vs. Variable socialization\n\nThis dimension refers to the extent to which the steps have a timetable developed by the organization and communicated to the recruit in order to convey when the socialization process is complete. Fixed socialization provides a recruit with the exact knowledge of the time it will take complete a given passage. For instance, some management trainees can be put on \" fast tracks\" where they are required to accept new rotational assignment on an annual basis despite their own preferences. Variable socialization processes gives a newcomer no specific timetable, but a few clues as to when to expect a given boundary passage. This type of socialization is commonly associated upwardly mobile careers within business organizations because of several uncontrolled factors such as the state of the economy or turnover rates which determine whether any given newcomer will be promoted to a higher level or not.[22]\n\nSerial vs. Disjunctive socialization\n\nA serial socialization process refers to experienced members of the organization grooming the newcomers who are about to occupy similar positions within the organization. These experience members essentially serve as role models for the inexperienced newcomers. A prime example of serial socialization would be a rookie police officer getting assigned patrol duties with an experienced veteran who has been in law enforcement for a lengthy period of time. Disjunctive socialization, in contrast, refers to when newcomers are not following the guidelines of their predecessors, and there are no role models to inform new recruits on how to fulfill their duties.[22]\n\nInvestiture vs. Divestiture socialization\n\nThis tactic refers to the degree to which a socialization process either affirms or disaffirms the identity of the newly entering recruit. Investiture socialization processes sanction and document for newcomers the viability and efficacy of the personal characteristics that they bring to the organization. When organizations use this socialization process it prefers that the recruit remains the exact way that he or she naturally behaves and the organization merely makes use of the skills, values, and attitudes that the recruit is believed to have in their possession. Divestiture socialization, on the other hand, is a process that organizations use to reject and remove the certain personal characteristics of a recruit. Many occupations and organizations require newcomers to sever previous ties, and forget old habits in order to create a new self-image based upon new assumptions.[22]\n\nThus, tactics influence the socialization process by defining the type of information newcomers receive, the source of this information, and the ease of obtaining it.[22]\nJones' model (1986)\n\nBuilding upon the work of Van Maanen and Schein, Jones (1986) proposed that the previous six dimensions could be reduced to two categories: institutionalized and individualized socialization. Companies that use institutionalized socialization tactics implement structured step-by-step programs, enter into an orchestrated orientation as a group, and receive help from an assigned role model or mentor. Examples of organizations using institutionalized tactics include the military, in which new recruits undergo extensive training and socialization activities through a participative cohort, as well as incoming freshmen at universities, who may attend orientation weekends before beginning classes.\n\nOn the opposite end of the spectrum, other organizations use individualized socialization tactics in which the new employee immediately starts working on his or her new position and figures out company norms, values, and expectations along the way. In this orientation system, individuals must play a more proactive role in seeking out information and initiating work relationships.[23]\nFormal orientations\n\nRegardless of the socialization tactics utilized, formal orientation programs can facilitate understanding of company culture, and introduces new employees to their work roles and the organizational social environment. Formal orientation programs may consist of lectures, videotapes, and written material, while other organizations may rely on more usual approaches. More recent approaches such as computer-based orientations and Internets have been used by organizations to standardize training programs across branch locations. A review of the literature indicates that orientation programs are successful in communicating the company's goals, history, and power structure.[24]\nRecruitment events\n\nRecruitment events play a key role in identifying which prospective employees are a good fit with an organization. Recruiting events allow employees to gather initial information about an organization's expectations and company culture. By providing a realistic job preview of what life inside the organization is like, companies can weed out potential employees who are clearly a misfit to an organization and individuals can identify which employment agencies are the most suitable match for their own personal values, goals, and expectations. Research has shown that new employees who receive a great amount of accurate information about the job and the company tend to adjust better.[25] Organizations can also provide realistic job previews by offering internship opportunities.\nMentorship\n\nMentorship has demonstrated importance in the socialization of new employees.[26][27] Ostroff and Kozlowski (1993) discovered that newcomers with mentors become more knowledgeable about the organization than did newcomers without mentors. Mentors can help newcomers better manage their expectations and feel comfortable with their new environment through advice-giving and social support.[28] Chatman (1991) found that newcomers are more likely to have internalized the key values of their organization's culture if they had spent time with an assigned mentor and attended company social events. Literature has also suggested the importance of demographic matching between organizational mentors and protégés.[26] Enscher & Murphy (1997) examined the effects of similarity (race and gender) on the amount of contact and quality of mentor relationships. Results indicate that liking, satisfaction, and contact were higher in conditions of perceived mentor-protégé similarity.[29] But what often separates rapid on-boarders from their slower counterparts is not the availability of a mentor but the presence of a \"buddy,\" someone of whom the newcomer can comfortably ask questions that are either trivial (\"How do I order office supplies?\") or politically sensitive (\"Whose opinion really matters here?\").[30] Like mentors, buddies can be people who are officially assigned by a manager or who simply emerge informally (a nearby co-worker, for instance) as an easily accessible resource and confidant.[31] Furthermore, buddies can help establish relationships with co-workers in ways that can't always be facilitated by a newcomer's manager or mentor.[32]\nEmployee adjustment\n\nIn order to increase the success of an onboarding program, it is important for an organization to monitor how well their new hires are adjusting to their new roles, responsibilities, peers, supervisors, and the organization at large. Researchers have noted that role clarity, self-efficacy, social acceptance, and knowledge of organizational culture are particularly good indicators of well-adjusted new employees who have benefitted from an effective onboarding system.\nRole clarity\n\nRole clarity describes a new employee's understanding of his or her job responsibilities and organizational role. One of the goals of an onboarding process is to aid newcomers in reducing ambiguity and uncertainty so that it is easier for them to get their jobs done correctly and efficiently. Because there often is a disconnect between the chief responsibilities listed in a job description and the specific, repeatable tasks that employees must complete to be successful in their roles, it's vital that managers are trained to discuss exactly what they expect from their employees.[33] A poor onboarding program, for example, may produce employees who exhibit sub-par productivity because they are unsure of their exact roles and responsibilities. On the other hand, a strong onboarding program would produce employees who are especially productive because they know exactly what is expected of them in their job tasks and their organizational role. Given this information, it is easy to see why an organization would benefit substantially from increasing role clarity for a new employee. Not only does role clarity imply greater productivity, but it has also been linked to both job satisfaction and organizational commitment.[34]\nSelf-efficacy\n\nSelf-efficacy is the degree to which new employees feel capable of successfully completing their assigned job tasks and fulfilling their responsibilities. It makes logical sense that employees who feel as though they can get the job done would fare better than those who feel overwhelmed in their new positions, and unsurprisingly, researchers have found that job satisfaction, organizational commitment, and turnover are all correlated with feelings of self-efficacy.[3]\nSocial acceptance\n\nSocial acceptance gives new employees the support needed to be successful. While role clarity and self-efficacy are important to a newcomer's ability to meet the requirements of a job, the feeling of \"fitting in\" can do a lot for one's perception of the work environment and has been demonstrated to increase commitment to an organization and decrease turnover.[3] If an employee feels well received by his or her peers, a personal investment in the organization develops, and leaving becomes less likely.\nKnowledge of organizational culture\n\nKnowledge of organizational culture refers to how well a new employee understands a company's values, goals, roles, norms, and overall organizational environment. For example, some organizations may have very strict, yet unspoken, rules of how interactions with superiors should be conducted or whether overtime hours are the norm and an expectation. Knowledge of one's organizational culture is important for the newcomer looking to adapt to a new company, as it allows for social acceptance and aids in completing work tasks in a way that meets company standards. Overall, knowledge of organizational culture has been linked to increased satisfaction and commitment, as well as decreased turnover.[35]\nOutcomes\n\nHistorically, organizations have overlooked the influence of business practices in shaping enduring work attitudes and thus have continually underestimated their impact on financial success.[36] Employees' job attitudes are particularly important from an organization's perspective because of their link to employee engagement and performance on the job. Employee engagement attitudes, such as satisfaction with one's job and organizational commitment or loyalty, have important implications for an employee's work performance and intentions to stay with or quit an organization. This translates into strong monetary gains for organizations as research has demonstrated that individuals who are highly satisfied with their jobs and who exhibit high organizational commitment are likely to perform better and remain in an organization, whereas individuals who have developed negative attitudes (are highly dissatisfied and unattached to their jobs) are characterized by low performance and high turnover rates.[36][37] Unengaged employees are very costly to organizations in terms of slowed performance and rehiring expenses. Since, attitudinal formations begin from the initial point of contact with an organization, practitioners would be wise to take advantage of positive attitudinal development during socialization periods in order to ensure a strong, productive, and dedicated workforce.\nLimits and criticisms of onboarding theory\n\nAlthough the outcomes of organizational socialization have been positively associated with the process of uncertainty reduction, they may not necessarily be desirable to all organizations. Jones (1986) as well as Allen and Meyer (1990) found that socialization tactics were related to commitment, but they were negatively correlated to role clarity.[23][38] Because formal socialization tactics insulate the newcomer from their full responsibilities while \"learning the ropes\", there is a potential for role confusion once expected to fully enter the organization. In some cases though, organizations may even desire a certain level of person-organizational misfit in order to achieve outcomes via innovative behaviors.[8] Depending on the culture of the organization, it may be more desirable to increase ambiguity despite the potentially negative connection with organizational commitment.\n\nAdditionally, socialization researchers have had major concerns over the length of time that it takes newcomers to adjust. There has been great difficulty determining the role that time plays, but once the length of the adjustment is determined, organizations can make appropriate recommendations regarding what matters most in various stages of the adjustment process.[8]\n\nFurther criticisms include the use of special orientation sessions to educate newcomers about the organization and strengthen their organizational commitment. While these sessions have been found to be often formal and ritualistic, several studies have found them unpleasant or traumatic.[39] Orientation sessions are a frequently used socialization tactic, however, employees have not found them to be helpful, nor has any research provided any evidence for their benefits.[40][41][42][43][44]\nExecutive onboarding\n\nExecutive onboarding is the application of general onboarding principles to helping new executives become productive members of an organization. Practically, executive onboarding involves acquiring, accommodating, assimilating and accelerating new executives.[45] Proponents emphasize the importance of making the most of the \"honeymoon\" stage of a hire, a period which has been described by various sources as either the first 90 to 100 days or the first full year.[46][47][48]\n\nEffective onboarding of new executives can be one of the most important contributions any hiring manager, direct supervisor or human resources professional can make to long-term organizational success, because executive onboarding done right can improve productivity and executive retention, and build shared corporate culture. A study of 20,000 searches revealed that 40 percent of executives hired at the senior level are pushed out, fail, or quit within 18 months.[49]\n\nOnboarding may be especially valuable for externally recruited executives transitioning into complex roles, because it may be difficult for those individuals to uncover personal, organizational, and role risks in complicated situations when they don't have formal onboarding assistance.[50] Onboarding is also an essential tool for executives promoted into new roles and/or transferred from one business unit to another.[51]\n\nIt is often valuable to have new executives start some onboarding activities in the \"Fuzzy Front End\" even before their first day.[52] This is one of ten steps executives can follow to accelerate their onboarding.[53]\n\n    Position yourself for success\n    Choose how to engage the context and culture\n    Embrace and leverage the Fuzzy Front End before day one\n    Take control of day one: Make a powerful first impression\n    Drive action by activating and directing ongoing communication\n    Embed a strong burning imperative\n    Exploit key milestones to drive team performance\n    Over-invest in early wins to build team confidence\n    Secure adept people in the right roles and deal with the inevitable resistance\n    Evolve people, plans, and practices to capitalize on changing circumstances.\n\nRecommendations for practitioners\n\nSome, including scholars at MIT Sloan, suggest that practitioners should seek to design an onboarding strategy that takes individual newcomer characteristics into consideration and encourages proactive behaviors, such as information seeking, that help facilitate the development of role clarity, self-efficacy, social acceptance, and knowledge of organizational culture. Research has consistently shown that doing so produces valuable outcomes such as high job satisfaction (the extent to which one enjoys the nature of his or her work), organizational commitment (the connection one feels to an organization), and job performance in employees, as well as lower turnover rates and decreased intent to quit.[54]\n\nIn terms of structure, empirical evidence indicates that formal institutionalized socialization is the most effective onboarding method.[24] New employees who complete these kinds of programs tend to experience more positive job attitudes and lower levels of turnover in comparison to those who undergo individualized tactics.[8][55] Some evidence suggests that in-person onboarding techniques are more effective than virtual ones. Though it may initially appear to be less expensive for a company to use a standard computer-based orientation program to introduce their new employees to the organization, research has demonstrated that employees learn more about their roles and company culture through face-to-face orientation.[56]\nSee also\n\n    Employment\n    Industrial and organizational psychology\n    Job performance\n    Job satisfaction\n    Mentoring\n    Business networking\n    Organizational commitment\n\n\t\n\n    Organizational culture\n    Personnel psychology\n    Recruitment\n    Socialization\n    Personality-Job Fit Theory\n    Person-environment fit", "skillName": "Onboarding."}
{"id": 56, "category": "Recuting", "skillText": "The human resource consulting industry has emerged from management consulting and addresses human resource management tasks and decisions. HR Consultants can fill two typical roles (1) Expert Resource Consultant (2) Process/People consultant. These two roles are defined by Steele F. (1975), Kubr,M. (1993, 1996); Niedereicholz (1996), Curnow-Reuvid (2003) and Kipping, K. and Clarck (2014).[1]\n\nThe Expert Resource Consultant suggests solution based on expertise and experience and assist in the implementation. The role is very typical in information benchmarking and design consulting (see examples of actual design practices in the subsequent section below).\n\nThe Process/People consultant assists in searching for solutions with methods that facilitate and raise creativity of the client company so that they will be able to implement solutions themselves. The role is traditionally demonstrated by organizational development and change consulting.\n\nContents\n\n    1 Core fields in practice\n    2 Companies in the field\n    3 Qualifications and certifications\n    4 See also\n    5 References\n\nCore fields in practice\n\nThe following are core fields around which most human resource consultancies are based:\n\nEmployee engagement: measure employee engagement levels through surveys and interviews, define and improve performance in employee engagement and retention. While this area of HR consulting is necessarily broad, encapsulating total rewards strategy, employee performance management, leadership transformation, and organisation structure design,[2] most companies do have very specialised independent practices.[3][4]\n\nCompensation: design and manage compensation programs related to basic salary, bonuses, and stock plans. Evaluation of positions and building of salary structures, bonus plans and stock plans for clients are common.[5] Specialisations are often based on employee types (eg. Executive compensation consultants[6] and sales compensation consultants[7]).\n\nEmployee benefits: optimize benefit plan design and administration (inclusive of health-related benefits) by assessing competitiveness and effectiveness of benefit plans (analytics and design), and cost-effectiveness and quality of vendors (brokerage).[8][9]\n\nActuarial and retirement: provide actuarial and administration services to manage cost and effectiveness of retirement programs, including defined benefit and defined contribution plans.[10]\n\nMergers and acquisitions: conduct human capital due diligence, coordinate and administer cross-functional activities during execution, including payroll and HR technology. Align organizational cultures and work styles during post-merger integration.[11][12][13]\n\nTalent mobility: Provides the insight and execution for full international expatriates (usually for executives) or local plus (partial-package expatriates), from pre-move informative guide, to post-move expat management program.[14]\n\nOther services may also include legal counseling, global initiatives, investments consulting, and the implementation of human resource technologies to facilitate human capital management.\nCompanies in the field\n\nHuman resource consultancies vary in their ranges of services and sizes, with many consultants and academicians breaking off to form their own practices. In 2007, there were 950 human resource consultancies globally, constituting a USD $18.4 billion market.\n\nAs of 2015, Mercer, Towers Watson, Aon Hewitt, Hay Group, and Buck Consultants were referred to as the largest HR consulting firms.[15][16] While the MBB and Big Fours may not have the largest HR Consulting practice, some of them are known to be among the best in this field. According to Vault.com, a website that provides career information by industry and by Fortune 1000 company, the top 10 HR Consulting Firms to work for in 2017 are as follows:[17]\nRank \tCompany\n1 \tMercer (consulting firm)\n2 \tTowers Watson\n3 \tAon Hewitt\n4 \tDeloitte Consulting LLP\n5 \tMcKinsey & Company\n6 \tAccenture\n7 \tThe Boston Consulting Group, Inc.\n8 \tHay Group\n9 \tPricewaterhouseCoopers Advisory Services LLC (PwC Advisory Services)\n10 \tBain & Company\nQualifications and certifications\n\nMany human resource consultants have specialized qualifications or certifications, such as:\n\n    Accountancy: ACCA, CA, CPA, CCA\n    Actuarial: EA, ASA, FSA, MAAA, FIA, FIAA, FFA\n    Educational: MS in Management/HR/Industrial Organizational psychology, MBA, Ph.D. in Management, DBA, J.D.\n    Finance: CFA\n    General consulting: CMC\n    HR consulting: Associate or Fellow Chartered Institute of Personnel & Development (Assoc. CIPD or FCIPD), Fellow Australian Human Resources Institute (FAHRI), Certified Human Resources Consultant (CHRC), Professional in Human Resources (PHR), SPHR, GPHR by HRCI, USA, SHRM-CP or SHRM-SCP by SHRM, USA\n    Health and benefits: CEBS, CBP\n    Compensation: CCP (Certified Compensation Professional)\n    Human resources: SHRM (US), PHR and SPHR (Canada), FCIPD, MCIPD, PGDHR, DHR (UK), Registered Professional Recruiter (RPR) (Canada)\n\nSee also\n\n    Consulting firm\n    Industrial and organizational psychology", "skillName": "Human_resource_consulting."}
{"id": 57, "category": "Distributed", "skillText": "Distributed computing\nDistributed computing is a field of computer science that studies distributed systems. A distributed system is a model in which components located on networked computers communicate and coordinate their actions by passing messages.[1] The components interact with each other in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.[1] Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.\n\nA computer program that runs in a distributed system is called a distributed program, and distributed programming is the process of writing such programs.[2] There are many alternatives for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.\n\nA goal and challenge pursued by some computer scientists and practitioners in distributed systems is location transparency; however, this goal has fallen out of favour in industry, as distributed systems are different from conventional non-distributed systems, and the differences, such as network partitions, partial system failures, and partial upgrades, cannot simply be \"papered over\" by attempts at \"transparency\" (see CAP theorem).\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[3] which communicate with each other by message passing.[4]\n\n1\tIntroduction\n2\tParallel and distributed computing\n3\tHistory\n4\tArchitectures\n5\tApplications\n6\tExamples\n7\tTheoretical foundations\n7.1\tModels\n7.2\tAn example\n7.3\tComplexity measures\n7.4\tOther problems\n7.5\tProperties of distributed systems\n8\tCoordinator election\n8.1\tBully algorithm\n8.2\tChang and Roberts algorithm\n9\tSee also\n10\tNotes\n11\tReferences\n12\tFurther reading\n13\tExternal links\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[5] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[4] While there is no single definition of a distributed system,[6] the following defining properties are commonly used:\n\nThere are several autonomous computational entities, each of which has its own local memory.[7]\nThe entities communicate with each other by message passing.[8]\nIn this article, the computational entities are called computers or nodes.\n\nA distributed system may have a common goal, such as solving a large computational problem.[9] Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[10]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[11]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[12]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[13]\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers, which have the same goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them.[14] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[15] Parallel computing may be seen as a particular tightly coupled form of distributed computing,[16] and distributed computing may be seen as a loosely coupled form of parallel computing.[6] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[17]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[18]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; as usual, the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.\n\nHistory\nThe use of concurrent processes that communicate by message-passing has its roots in operating system architectures studied in the 1960s.[19] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[20]\n\nARPANET, the predecessor of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[21] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET, and its successor, the Internet, other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its European counterpart International Symposium on Distributed Computing (DISC) was first held in 1985.\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.\n\nDistributed programming typically falls into one of several basic architectures: client�server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.\n\nClient�server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there is no special machines that provide a service or manage the network resources.[22]:227 Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[23]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example, it may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer. A distributed system can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[24]\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[25]\n\ntelecommunication networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld wide web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing and grid computing and various volunteer computing projects (see the list of distributed computing projects),\ndistributed rendering in computer graphics\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question�answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random access machines (PRAM) that are used.[26] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[27][28]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[29] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is a lot of interaction between the two fields. For example, the Cole�Vishkin algorithm for graph coloring[30] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[31] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[32] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits�PRAM machines can simulate Boolean circuits efficiently and vice versa.[33]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbours. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[34]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[35]\n\nOther commonly used measures are the total number of bits transmitted in the network (cf. communication complexity).\n\nOther problems\nTraditional computational problems take the perspective that we ask a question, a computer (or a distributed system) processes the question for a while, and then produces an answer and stops. However, there are also problems where we do not want the system to ever stop. Examples of such problems include the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing. The first example is challenges that are related to fault-tolerance. Examples of related problems include consensus problems,[36] Byzantine fault tolerance,[37] and self-stabilisation.[38]\n\nA lot of research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[39]\nLogical clocks provide a causal happened-before ordering of events.[40]\nClock synchronization algorithms provide globally consistent physical time stamps.[41]\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[42] i.e., it is decidable, but it is not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nCoordinator election\nCoordinator election (sometimes called leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira [43] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kind of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[44]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[45]\n\nBully algorithm\nWhen using the Bully algorithm, any process sends a message to the current coordinator. If there is no response within a given time limit, the process tries to elect itself as leader.\n\nChang and Roberts algorithm\nThe Chang and Roberts algorithm (or \"Ring Algorithm\") is a ring-based election algorithm used to find a process with the largest unique identification number.\n\nSee also\nAppScale\nBOINC\nBlock chain (database)\nCode mobility\nDecentralized computing\nDew computing\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed operating system\nEdsger W. Dijkstra Prize in Distributed Computing\nFog computing\nFolding@home\nInferno\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture - LOA\nList of distributed computing conferences\nList of distributed computing projects\nList of important publications in concurrent, parallel, and distributed computing\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs", "skillName": "Distributed Computing."}
{"id": 58, "category": "Distributed", "skillText": "Hortonworks\nHortonworks is a business computer software company based in Santa Clara, California. The company focuses on the development and support of Apache Hadoop, a framework that allows for the distributed processing of large data sets across clusters of computers.\n\nHortonworks was formed in June 2011 funded by $23 million from Yahoo! and Benchmark Capital as an independent company.[2] The company employs contributors to the open source software project Apache Hadoop.[3]\n\nEric Baldeschweiler was chief executive, and Rob Bearden chief operating officer, formerly from SpringSource.[2] Additional investors included a $25 million round led by Index Ventures in November 2011.[4]\n\nHortonworks is a sponsor of the Apache Software Foundation.[5]\n\nForrester Research named Hortonworks �a technology leader and ecosystem builder for the entire Hadoop industry.�[6]\n\nHortonworks' product named Hortonworks Data Platform (HDP) includes Apache Hadoop and is used for storing, processing, and analyzing large volumes of data. The platform is designed to deal with data from many sources and formats. The platform includes various Apache Hadoop projects including the Hadoop Distributed File System, MapReduce, Pig, Hive, HBase and Zookeeper and additional components.[7]\n\nIn October 2011 Hortonworks announced Microsoft would collaborate on a Hadoop distribution for Microsoft Azure and Windows Server.[8] On February 25, 2013, Hortonworks announced availability of a beta version of the Hortonworks Data Platform for Windows.[citation needed]\n\nIn November 2011 it announced HParser software from Informatica would be available for free download by its customers.[9]\n\nIn February 2012 Teradata announced an alliance.[10] In October 2012 Teradata's Aster Data Systems division announced an appliance supporting Hortonworks' distribution,[11][12] and Impetus Technologies announced a partnership.[13]\n\nIn June 2013 Hortonworks raised another $50 million in financing, from previous investors and adding Tenaya Capital and Dragoneer Investment Group.[14]\n\nIn September 2013 SAP AG announced it would resell the Hortonworks distribution (as well as one from Intel).[15]\n\nHortonworks hosts the Hadoop Summit community event, along with Yahoo!.[16]\n\nIn December 2014, Hortonworks went public with 6,250,000 shares listed in NASDAQ.[17]\n\nPartnerships\nHortonworks partners with a variety of software-related companies, including BMC Software for business service management and automation,[18] Attunity[19] and Cleo for data integration,[20] and SAP and VMWare for cloud, database and other virtualization infrastructure.[21] As part of its efforts to make Hadoop easier to use, in 2015 Hortonworks started partnering with ManTech Commercial Services and B23 to develop OpenSOC; ManTech brought cybersecurity expertise and B23 brought expertise in big data analytics.[22]\n\nAcquisitions\nIn August, 2015, Hortonworks announced it has signed a definitive agreement to acquire Onyara, Inc., the creator of and key contributor to Apache NiFi, a top-level open source project. Apache NiFi was made available through the NSA Technology Transfer Program in the fall of 2014. Over the past eight years, Onyara�s engineers were the key contributors to the U.S. government software project that evolved into Apache NiFi. In July 2015, NiFi became a Top-Level Project, signifying that its community and technology have been successfully governed under the Apache Software Foundation. [23] [24] [25]\n\nIn May 2014, Hortonworks acquired XA Secure, a data security company, to add a secure layer to its Hadoop Data Platform.[26]", "skillName": "Hortonworks."}
{"id": 59, "category": "Distributed", "skillText": "Cloud computing\nCloud computing metaphor: For a user, the network elements representing the provider-rendered services are invisible, as if obscured by a cloud.\nCloud computing is a kind of Internet-based computing that provides shared processing resources and data to computers and other devices on demand. It is a model for enabling ubiquitous, on-demand access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services),[1][2] which can be rapidly provisioned and released with minimal management effort. Cloud computing and storage solutions provide users and enterprises with various capabilities to store and process their data in third-party data centers.[3] It relies on sharing of resources to achieve coherence and economy of scale, similar to a utility (like the electricity grid) over a network.\n\nAdvocates claim that cloud computing allows companies to avoid upfront infrastructure costs, and focus on projects that differentiate their businesses instead of on infrastructure.[4] Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and enables IT to more rapidly adjust resources to meet fluctuating and unpredictable business demand.[4][5][6] Cloud providers typically use a \"pay as you go\" model. This will lead to unexpectedly high charges if administrators do not adapt to the cloud pricing model.[7]\n\nThe present availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture, and autonomic and utility computing have led to a growth in cloud computing.[8][9][10] Companies can scale up as computing needs increase and then scale down again as demands decrease.\n\nCloud computing has become a highly demanded service or utility due to the advantages of high computing power, cheap cost of services, high performance, scalability, accessibility as well as availability. Some cloud vendors are experiencing growth rates of 50% per year,[11] but being still in a stage of infancy, it has pitfalls that need to be addressed to make cloud computing services more reliable and user friendly.[12][13]\n\n1\tHistory of cloud computing\n1.1\tOrigin of the term\n1.2\tThe 1970s\n1.3\tThe 1990s\n1.4\t2000s\n2\tSimilar concepts\n3\tCharacteristics\n4\tService models\n4.1\tInfrastructure as a service (IaaS)\n4.2\tPlatform as a service (PaaS)\n4.3\tSoftware as a service (SaaS)\n5\tCloud clients\n6\tDeployment models\n6.1\tPrivate cloud\n6.2\tPublic cloud\n6.3\tHybrid cloud\n6.4\tOthers\n6.4.1\tCommunity cloud\n6.4.2\tDistributed cloud\n6.4.3\tIntercloud\n6.4.4\tMulticloud\n7\tArchitecture\n7.1\tCloud engineering\n8\tSecurity and privacy\n9\tLimitations\n10\tThe future\n11\tSee also\n12\tReferences\n13\tFurther reading\n14\tExternal links\nHistory of cloud computing\nOrigin of the term\nThe origin of the term cloud computing in computing is unclear. The word \"cloud\" is commonly used in science to describe a large agglomeration of objects that visually appear from a distance as a cloud and describes any set of things whose details are not further inspected in a given context.[14] Another explanation is that the old programs that drew network schematics surrounded the icons for servers with a circle, and a cluster of servers in a network diagram had several overlapping circles, which resembled a cloud.[15]\n\nIn analogy to the above usage, the word cloud was used as a metaphor for the Internet and a standardized cloud-like shape was used to denote a network on telephony schematics. Later it was used to depict the Internet in computer network diagrams. With this simplification, the implication is that the specifics of how the end points of a network are connected are not relevant for the purposes of understanding the diagram. The cloud symbol was used to represent networks of computing equipment in the original ARPANET by as early as 1977,[16] and the CSNET by 1981[17]�both predecessors to the Internet itself.\n\nThe term cloud has been used to refer to platforms for distributed computing. In Wired's April 1994 feature \"Bill and Andy's Excellent Adventure II\" on the Apple spin-off General Magic, Andy Hertzfeld commented on General Magic's distributed programming language Telescript that:\n\n\"The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create sort of a virtual service. No one had conceived that before. The example Jim White [the designer of Telescript, X.400 and ASN.1] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties.\"\n\n�?[18]\nReferences to \"cloud computing\" in its modern sense appeared as early as 1996, with the earliest known mention in a Compaq internal document.[19]\n\nThe popularization of the term can be traced to 2006 when Amazon.com introduced its Elastic Compute Cloud.[20]\n\nThe 1970s\nDuring the 1960s, the initial concepts of time-sharing became popularized via RJE (Remote Job Entry);[21] this terminology was mostly associated with large vendors such as IBM and DEC. Full time-sharing solutions were available by the early 1970s on such platforms as Multics (on GE hardware), Cambridge CTSS, and the earliest UNIX ports (on DEC hardware). Yet, the \"data center\" model where users submitted jobs to operators to run on IBM mainframes was overwhelmingly predominant.\n\nThe 1990s\nIn the 1990s, telecommunications companies, who previously offered primarily dedicated point-to-point data circuits, began offering virtual private network (VPN) services with comparable quality of service, but at a lower cost. By switching traffic as they saw fit to balance server use, they could use overall network bandwidth more effectively.[citation needed] They began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for. Cloud computing extended this boundary to cover all servers as well as the network infrastructure.[22]\n\nAs computers became more diffused, scientists and technologists explored ways to make large-scale computing power available to more users through time-sharing.[citation needed] They experimented with algorithms to optimize the infrastructure, platform, and applications to prioritize CPUs and increase efficiency for end users.[23]\n\n2000s\nSince 2000, cloud computing has come into existence. In early 2008, NASA's OpenNebula, enhanced in the RESERVOIR European Commission-funded project, became the first open-source software for deploying private and hybrid clouds, and for the federation of clouds.[24] In the same year, efforts were focused on providing quality of service guarantees (as required by real-time interactive applications) to cloud-based infrastructures, in the framework of the IRMOS European Commission-funded project, resulting in a real-time cloud environment.[25][26] By mid-2008, Gartner saw an opportunity for cloud computing \"to shape the relationship among consumers of IT services, those who use IT services and those who sell them\"[27] and observed that \"organizations are switching from company-owned hardware and software assets to per-use service-based models\" so that the \"projected shift to computing ... will result in dramatic growth in IT products in some areas and significant reductions in other areas.\"[28]\n\nIn August 2006 Amazon introduced its Elastic Compute Cloud.[20] Microsoft Azure was announced as \"Azure\" in October 2008 and was released on 1 February 2010 as Windows Azure, before being renamed to Microsoft Azure on 25 March 2014.[29] For a time, Azure was on the TOP500 supercomputer list, before it dropped off it.[30]\n\nIn July 2010, Rackspace Hosting and NASA jointly launched an open-source cloud-software initiative known as OpenStack. The OpenStack project intended to help organizations offering cloud-computing services running on standard hardware. The early code came from NASA's Nebula platform as well as from Rackspace's Cloud Files platform.\n\nOn March 1, 2011, IBM announced the IBM SmartCloud framework to support Smarter Planet.[31] Among the various components of the Smarter Computing foundation, cloud computing is a critical part.\n\nOn June 7, 2012, Oracle announced the Oracle Cloud.[32] While aspects of the Oracle Cloud are still in development, this cloud offering is poised to be the first to provide users with access to an integrated set of IT solutions, including the Applications (SaaS), Platform (PaaS), and Infrastructure (IaaS) layers.[33][34][35]\n\nSimilar concepts\nCloud computing is the result of the evolution and adoption of existing technologies and paradigms. The goal of cloud computing is to allow users to take bene?t from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs, and helps the users focus on their core business instead of being impeded by IT obstacles.[36]\n\nThe main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system�level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations, and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.[36]\n\nUsers routinely face difficult business problems. Cloud computing adopts concepts from Service-oriented Architecture (SOA) that can help the user break these problems into services that can be integrated to provide a solution. Cloud computing provides all of its resources as services, and makes use of the well-established standards and best practices gained in the domain of SOA to allow global and easy access to cloud services in a standardized way.\n\nCloud computing also leverages concepts from utility computing to provide metrics for the services used. Such metrics are at the core of the public cloud pay-per-use models. In addition, measured services are an essential part of the feedback loop in autonomic computing, allowing services to scale on-demand and to perform automatic failure recovery.\n\nCloud computing is a kind of grid computing; it has evolved by addressing the QoS (quality of service) and reliability problems. Cloud computing provides the tools and technologies to build data/compute intensive parallel applications with much more affordable prices compared to traditional parallel computing techniques.[36]\n\nCloud computing shares characteristics with:\n\nClient�server model�Client�server computing refers broadly to any distributed application that distinguishes between service providers (servers) and service requestors (clients).[37]\nGrid computing�\"A form of distributed and parallel computing, whereby a 'super and virtual computer' is composed of a cluster of networked, loosely coupled computers acting in concert to perform very large tasks.\"\nFog computing�Distributed computing paradigm that provides data, compute, storage and application services closer to client or near-user edge devices, such as network routers. Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client side (e.g. mobile devices), instead of sending data to a remote location for processing.\nDew computing�In the existing computing hierarchy, the Dew computing is positioned as the ground level for the cloud and fog computing paradigms. Compared to fog computing, which supports emerging IoT applications that demand real-time and predictable latency and the dynamic network reconfigurability, Dew computing pushes the frontiers to computing applications, data, and low level services away from centralized virtual nodes to the end users.[38]\nMainframe computer�Powerful computers used mainly by large organizations for critical applications, typically bulk data processing such as: census; industry and consumer statistics; police and secret intelligence services; enterprise resource planning; and financial transaction processing.\nUtility computing�The \"packaging of computing resources, such as computation and storage, as a metered service similar to a traditional public utility, such as electricity.\"[39][40]\nPeer-to-peer�A distributed architecture without the need for central coordination. Participants are both suppliers and consumers of resources (in contrast to the traditional client�server model).\nGreen computing\nCharacteristics\nCloud computing exhibits the following key characteristics:\n\nAgility improves with users' ability to re-provision technological infrastructure resources.\nCost reductions claimed by cloud providers. A public-cloud delivery model converts capital expenditure to operational expenditure.[41] This purportedly lowers barriers to entry, as infrastructure is typically provided by a third party and need not be purchased for one-time or infrequent intensive computing tasks. Pricing on a utility computing basis is fine-grained, with usage-based options and fewer IT skills are required for implementation (in-house).[42] The e-FISCAL project's state-of-the-art repository[43] contains several articles looking into cost aspects in more detail, most of them concluding that costs savings depend on the type of activities supported and the type of infrastructure available in-house.\nDevice and location independence[44] enable users to access systems using a web browser regardless of their location or what device they use (e.g., PC, mobile phone). As infrastructure is off-site (typically provided by a third-party) and accessed via the Internet, users can connect from anywhere.[42]\nMaintenance of cloud computing applications is easier, because they do not need to be installed on each user's computer and can be accessed from different places.\nMultitenancy enables sharing of resources and costs across a large pool of users thus allowing for:\ncentralization of infrastructure in locations with lower costs (such as real estate, electricity, etc.)\npeak-load capacity increases (users need not engineer for highest possible load-levels)\nutilisation and efficiency improvements for systems that are often only 10�20% utilised.[45][46]\nPerformance is monitored, and consistent and loosely coupled architectures are constructed using web services as the system interface.[42][47][48]\nProductivity may be increased when multiple users can work on the same data simultaneously, rather than waiting for it to be saved and emailed. Time may be saved as information does not need to be re-entered when fields are matched, nor do users need to install application software upgrades to their computer.[49]\nReliability improves with the use of multiple redundant sites, which makes well-designed cloud computing suitable for business continuity and disaster recovery.[50]\nScalability and elasticity via dynamic (\"on-demand\") provisioning of resources on a fine-grained, self-service basis in near real-time[51][52] (Note, the VM startup time varies by VM type, location, OS and cloud providers[51]), without users having to engineer for peak loads.[53][54][55] This gives the ability to scale up when the usage need increases or down if resources are not being used.[56]\nSecurity can improve due to centralization of data, increased security-focused resources, etc., but concerns can persist about loss of control over certain sensitive data, and the lack of security for stored kernels. Security is often as good as or better than other traditional systems, in part because providers are able to devote resources to solving security issues that many customers cannot afford to tackle.[57] However, the complexity of security is greatly increased when data is distributed over a wider area or over a greater number of devices, as well as in multi-tenant systems shared by unrelated users. In addition, user access to security audit logs may be difficult or impossible. Private cloud installations are in part motivated by users' desire to retain control over the infrastructure and avoid losing control of information security.\nThe National Institute of Standards and Technology's definition of cloud computing identifies \"five essential characteristics\":\n\nOn-demand self-service. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.\n\nBroad network access. Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).\n\nResource pooling. The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.\n\nRapid elasticity. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.\n\nMeasured service. Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.\n�?National Institute of Standards and Technology[58]\nService models\nThough service-oriented architecture advocates \"everything as a service\" (with the acronyms EaaS or XaaS or simply aas),[59] cloud-computing providers offer their \"services\" according to different models,[58][60][need quotation to verify] which happen to form a stack: infrastructure-, platform- and software-as-a-service.[61]\n\n\nCloud-computing layers accessible within a stack\nInfrastructure as a service (IaaS)\nSee also: Category:Cloud infrastructure\nAccording to the Internet Engineering Task Force (IETF), the most basic cloud-service model is that of providers offering computing infrastructure � virtual machines and other resources � as a service to subscribers. Infrastructure as a service (IaaS) refers to online services that abstract the user from the details of infrastructure like physical computing resources, location, data partitioning, scaling, security, backup etc. A hypervisor, such as Xen, Oracle VirtualBox, Oracle VM, KVM, VMware ESX/ESXi, or Hyper-V, runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. Containerisation offers higher performance than virtualization, because there is no hypervisor overhead. Also, container capacity auto-scales dynamically with computing load, which eliminates the problem of over-provisioning and enables usage-based billing.[62] IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.[63] IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks).\n\nTo deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure.[64][unreliable source?] In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the amount of resources allocated and consumed.[65][66][67][68]\n\nPlatform as a service (PaaS)\nMain article: Platform as a service\nSee also: Category:Cloud platforms\nPaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a computing platform, typically including operating system, programming-language execution environment, database, and web server. Application developers can develop and run their software solutions on a cloud platform without the cost and complexity of buying and managing the underlying hardware and software layers. With some PaaS offers like Microsoft Azure and Google App Engine, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually. The latter has also been proposed by an architecture aiming to facilitate real-time in cloud environments.[69][need quotation to verify] Even more specific application types can be provided via PaaS, such as media encoding as provided by services like bitcodin.com[70] or media.io.[71]\n\nSome integration and data management providers have also embraced specialized applications of PaaS as delivery models for data solutions. Examples include iPaaS and dPaaS. iPaaS (Integration Platform as a Service) enables customers to develop, execute and govern integration flows.[72] Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.[73] dPaaS (Data Platform as a Service) delivers integration�and data-management�products as a fully managed service.[74] Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of data solutions by building tailored data applications for the customer. dPaaS users retain transparency and control over data through data-visualization tools.[75]\n\nPlatform as a Service (PaaS) consumers do not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but have control over the deployed applications and possibly configuration settings for the application-hosting environment.\n\nSoftware as a service (SaaS)\nMain article: Software as a service\nIn the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as \"on-demand software\" and is usually priced on a pay-per-use basis or using a subscription fee.[citation needed]\n\nIn the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability�which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand.[76] Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.\n\nThe pricing model for SaaS applications is typically a monthly or yearly flat fee per user,[77] so prices become scalable and adjustable if users are added or removed at any point.[78]\n\nProponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result,[citation needed] there could be unauthorized access to the data. For this reason, users are increasingly[quantify] adopting intelligent third-party key-management systems to help secure their data.[citation needed]\n\nCloud clients\nSee also: Category:Cloud clients and Cloud API\nUsers access cloud computing using networked client devices, such as desktop computers, laptops, tablets and smartphones and any Ethernet enabled device such as Home Automation Gadgets. Some of these devices�cloud clients�rely on cloud computing for all or a majority of their applications so as to be essentially useless without it. Examples are thin clients and the browser-based Chromebook. Many cloud applications do not require specific software on the client and instead use a web browser to interact with the cloud application. With Ajax and HTML5 these Web user interfaces can achieve a similar, or even better, look and feel to native applications. Some cloud applications, however, support specific client software dedicated to these applications (e.g., virtual desktop clients and most email clients). Some legacy applications (line of business applications that until now have been prevalent in thin client computing) are delivered via a screen-sharing technology.\n\nDeployment models\n\nCloud computing types\nPrivate cloud\nPrivate cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third-party, and hosted either internally or externally.[58] Undertaking a private cloud project requires a significant level and degree of engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. When done right, it can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers[79] are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users \"still have to buy, build, and manage them\" and thus do not benefit from less hands-on management,[80] essentially \"[lacking] the economic model that makes cloud computing such an intriguing concept\".[81][82]\n\nPublic cloud\nA cloud is called a \"public cloud\" when the services are rendered over a network that is open for public use. Public cloud services may be free.[83] Technically there may be little or no difference between public and private cloud architecture, however, security consideration may be substantially different for services (applications, storage, and other resources) that are made available by a service provider for a public audience and when communication is effected over a non-trusted network. Generally, public cloud service providers like Amazon Web Services (AWS), Microsoft and Google own and operate the infrastructure at their data center and access is generally via the Internet. AWS and Microsoft also offer direct connect services called \"AWS Direct Connect\" and \"Azure ExpressRoute\" respectively, such connections require customers to purchase or lease a private connection to a peering point offered by the cloud provider.[42]\n\nHybrid cloud\nHybrid cloud is a composition of two or more clouds (private, community or public) that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.[58]\n\nGartner, Inc. defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.[84] A hybrid cloud service crosses isolation and provider boundaries so that it can't be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.\n\nVaried use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.[85] This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.[86]\n\nAnother example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.[87] This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.[58] Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and \"bursts\" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization only pays for extra compute resources when they are needed.[88] Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.[89]\n\nThe specialized model of hybrid cloud, which is built atop heterogeneous hardware, is called \"Cross-platform Hybrid Cloud\". A cross-platform hybrid cloud is usually powered by different CPU architectures, for example, x86-64 and ARM, underneath. Users can transparently deploy and scale applications without knowledge of the cloud's hardware diversity.[90] This kind of cloud emerges from the raise of ARM-based system-on-chip for server-class computing.\n\nOthers\nCommunity cloud\nCommunity cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party, and either hosted internally or externally. The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.[58]\n\nDistributed cloud\nA cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud.\n\nPublic-resource computing�This type of distributed cloud results from an expansive definition of cloud computing, because they are more akin to distributed computing than cloud computing. Nonetheless, it is considered a sub-class of cloud computing, and some examples include distributed computing platforms such as BOINC and Folding@Home.\nVolunteer cloud�Volunteer cloud computing is characterized as the intersection of public-resource computing and cloud computing, where a cloud computing infrastructure is built using volunteered resources. Many challenges arise from this type of infrastructure, because of the volatility of the resources used to built it and the dynamic environment it operates in. It can also be called peer-to-peer clouds, or ad-hoc clouds. An interesting effort in such direction is Cloud@Home, it aims to implement a cloud computing infrastructure using volunteered resources providing a business-model to incentivize contributions through financial restitution.[91]\nIntercloud\nMain article: Intercloud\nThe Intercloud[92] is an interconnected global \"cloud of clouds\"[93][94] and an extension of the Internet \"network of networks\" on which it is based. The focus is on direct interoperability between public cloud service providers, more so than between providers and consumers (as is the case for hybrid- and multi-cloud).[95][96][97]\n\nMulticloud\nMain article: Multicloud\nMulticloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).[98][99][100]\n\nArchitecture\n\nCloud computing sample architecture\nCloud architecture,[101] the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.\n\nCloud engineering\nCloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization, and governance in conceiving, developing, operating and maintaining cloud computing systems. It is a multidisciplinary method encompassing contributions from diverse areas such as systems, software, web, performance, information, security, platform, risk, and quality engineering.\n\nSecurity and privacy\nMain article: Cloud computing issues\nCloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time. It could accidentally or deliberately alter or even delete information.[102] Many cloud providers can share information with third parties if necessary for purposes of law and order even without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end users' choices for how data is stored.[102] Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.[3][102]\n\nAccording to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and API's, Data Loss & Leakage, and Hardware Failure�which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users there may be a possibility that information belonging to different customers resides on same data server. Therefore, Information leakage may arise by mistake when information for one customer is given to other.[103] Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. \"There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into\". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack�a process he called \"hyperjacking\". Some examples of this include the Dropbox security breach, and iCloud 2014 leak.[104] Dropbox had been breached in October 2014, having over 7 million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).[104]\n\nThere is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership.[105]\n\nPhysical control of the computer equipment (private cloud) is more secure than having the equipment off site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.[106] Some small businesses that don't have expertise in IT security could find that it's more secure for them to use a public cloud.\n\nThere is the risk that end users don't understand the issues involved when signing on to a cloud service (persons sometimes don't read the many pages of the terms of service agreement, and just click \"Accept\" without reading). This is important now that cloud computing is becoming popular and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Now).\n\nFundamentally private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.[107]\n\nLimitations\nAccording to Bruce Schneier, \"The downside is that you will have limited customization options. Cloud computing is cheaper because of economics of scale, and like any outsourced task, you tend to get what you get. A restaurant with a limited menu is cheaper than a personal chef who can cook anything you want. Fewer options at a much cheaper price: it's a feature, not a bug and the cloud provider might not meet your legal needs. As a business, you need to weigh the benefits against the risks.\"[108]\n\nThe future\nCloud computing is therefore still as much a research topic, as it is a market offering.[109] What is clear through the evolution of cloud computing services is that the chief technical officer (CTO) is a major driving force behind cloud adoption.[110] The major cloud technology developers continue to invest billions a year in cloud R&D; for example: in 2011 Microsoft committed 90% of its US$9.6bn R&D budget to its cloud.[111] Centaur Partners also predict that SaaS revenue will grow from US$13.5B in 2011 to $32.8B in 2016.[112] This expansion also includes Finance and Accounting SaaS.[113] Additionally, more industries are turning to cloud technology as an efficient way to improve quality services due to its capabilities to reduce overhead costs, downtime, and automate infrastructure deployment.[114]\n\nPortal icon\tComputer networking portal\nCategory: Cloud computing providers\nCategory: Cloud platforms\nCloud computing security\nCloud computing comparison\nCloud management\nCloud research\nCloud storage\nEdge computing\neScience\nMobile cloud computing\nPersonal cloud\nRobot as a Service\nService-Oriented Architecture\nUbiquitous computing\nWeb computing", "skillName": "Cloud_Computing."}
{"id": 60, "category": "Distributed", "skillText": "Cloudera\nCloudera Inc. is an American-based software company that provides Apache Hadoop-based software, support and services, and training to business customers.\n\nCloudera's open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology. Cloudera says that more than 50% of its engineering output is donated upstream to the various Apache-licensed open source projects (Apache Hive, Apache Avro, Apache HBase, and so on) that combine to form the Hadoop platform. Cloudera is also a sponsor of the Apache Software Foundation.[2]\n\n\nThree engineers from Google, Yahoo and Facebook (Christophe Bisciglia, Amr Awadallah and Jeff Hammerbacher, respectively) joined with a former Oracle executive (Mike Olson) to form Cloudera in 2008.[3] Olson was the CEO of Sleepycat Software, the creator of the open-source embedded database engine Berkeley DB (acquired by Oracle in 2006). Awadallah was from Yahoo, where he ran one of the first business units using Hadoop for data analysis.[4] At Facebook Hammerbacher used Hadoop for building analytic applications involving massive volumes of user data.[5]\n\nArchitect Doug Cutting, also a former chairman of the Apache Software Foundation, authored the open-source Lucene and Nutch search technologies before he wrote the initial Hadoop software in 2004. He designed and managed a Hadoop storage and analysis cluster at Yahoo! before joining Cloudera in 2009. Chief operating officer was Kirk Dunn.[6]\n\nIn March 2009, Cloudera announced the availability of Cloudera Distribution Including Apache Hadoop in conjunction with a $5 million investment led by Accel Partners.[7] In 2011, the company raised a further $40 million from Ignition Partners, Accel Partners, Greylock Partners, Meritech Capital Partners, and In-Q-Tel, a venture capital firm with open connections to the CIA.[8]\n\nIn June 2013 Tom Reilly became chief executive, although Olson remained as chairman of the board and chief strategist. Reilly was chief executive at ArcSight when it was acquired by Hewlett-Packard in 2010.[9] In March 2014 Cloudera announced a $900 million funding round, led by Intel Capital ($740 million), for that Intel received 18% share in cloudera and Intel dropped its own Hadoop distribution and dedicated 70 Intel engineers to work exclusively on cloudera projects. With additional funds coming from T Rowe Price, Google Ventures and an affiliate of MSD Capital, L.P., the private investment firm for Michael S. Dell. and others.[10]\n\nIn January 2012 - Oracle announced exclusive partnership with Cloudera to provide first Big data Appliance powered by Cloudera[11]\n\nIn January 2013 - Dell announced a strategic partnership with Cloudera[12]\n\nIn March 2013 - Intel invested $740 Million in Cloudera for an 18% investment[13]\n\nIn May 2013 - SAS announced a strategic partnership with Cloudera on how to driver predictive analytics with SAS software[14]\n\nIn June 2014 - Accenture announced - Data as a platform offering based on Cloudera[15]\n\nIn June 2014 - Cloudera acquired Gazzang - this gives Cloudera the most secure Hadoop distribution on the marketplace. This delivers enterprise-grade data encryption and key management, addressing head on the challenges associated with securing and processing sensitive and legally protected data within the Hadoop ecosystem. Thus fulfilling a requirement in myriad compliance regulations like HIPAA-HITECH, PCI-DSS, FERPA and the EU Data Protection Directive.[16]\n\nIn October 2014 - Cloudera announces the first PCI compliant hadoop distribution with MasterCard.[17]\n\nIn February 2015 - Deloitte announced a strategic alliance with Cloudera on Advanced Analytics[18]\n\nIn May 2015 - Capgemini announced SAP HANA - Cloudera Insight driven operations - get more for less with your SAP HANA installation with Cloudera Enterprise[19]\n\nIn July 9, 2015 - Cloudera announced partnership with Teradata - Integrated Enterprise ready Appliance for Hadoop[20]\n\nIn September 2015 - Cloudera announced Kudu - New Apache Hadoop Storage for fast analytics on fast data[21]\n\nIn September 2015 - Cloudera announced RecordService - Fine-grained security enforcement across the Hadoop ecosystem[22]\n\nIn September 2015 - Microsoft Azure announced full support of Cloudera Enterprise on Azure[23]\n\nIn November 2015 - Cloudera announced that it had reached 1,000 employees.\n\nIn Jan 2016 - TCS announced an IoT framework based on Cloudera for sensor data analytics[24]\n\nIn Feb 2016 - EMC announces evolution in advanced storage with DSSD support for Cloudera[25]\n\nIn March 2016 Cloudera announced onDemand Training. This allows consultants, Big data practitioners to take training at a time that suits their schedule.[26]\n\nIn 2016 - Cloudera has expanded their international teams with over 200 people employed in EMEA with staff in over 12 countries in EMEA.\n\nIn March 2016 - Cloudera announced it had over 2,100 partners consisting of 450 Software and OEM, 1400 System Integrators and Resellers, 200 Platform and Cloud and 50 Data Systems. This means that your existing investment in ISVs, Hardware and platform partners is maintained with Cloudera. Cloudera has the largest partner eco-systems of all Hadoop Distributions.[1]\n\nProducts and services\nCloudera offers software, services and support in three different bundles:\n\nCloudera Enterprise includes CDH and an annual subscription license (per node) to Cloudera Manager and technical support. It comes in three editions: Basic, Flex, and Data Hub.\nCloudera Express includes CDH and a version of Cloudera Manager lacking enterprise features such as rolling upgrades and backup/disaster recovery, LDAP and SNMP integration.\nCDH may be downloaded from Cloudera's website at no charge, but with no technical support nor Cloudera Manager.\nCloudera Navigator - is the only complete data governance solution for Hadoop, offering critical capabilities such as data discovery, continuous optimization, audit, lineage, metadata management, and policy enforcement. As part of Cloudera Enterprise, Cloudera Navigator is critical to enabling high-performance agile analytics, supporting continuous data architecture optimization, and meeting regulatory compliance requirements.[27]\nCloudera Navigator Optimizer (beta) - A SaaS based tool to provides instant insights into your workloads and recommends optimization strategies to get the best results with Hadoop.[28]\nServices\n\nCloudera University - World class training provided by Cloudera university[1]\nCloudera Professional Services - World class professionals to help with customer success.[29]\nCloudera Support - World class support team available 24x7 to help customers proactively and predictively ensure success[30]\nAll versions may be downloaded from Cloudera's website.\n\nCDH contains the main, core elements of Hadoop that provide reliable, scalable distributed data processing of large data sets (chiefly MapReduce and HDFS), as well as other enterprise-oriented components that provide security, high availability, and integration with hardware and other software.[31]\n\nIn October 2012, Cloudera announced the Cloudera Impala project, an open-source distributed query engine for Apache Hadoop.[32]\n\nAwards", "skillName": "Cloudera."}
{"id": 61, "category": "Distributed", "skillText": "Grid computing is the collection of computer resources from multiple locations to reach a common goal. The grid can be thought of as a distributed system with non-interactive workloads that involve a large number of files. Grid computing is distinguished from conventional high performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application.[1][better source needed] Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers.[2] Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large.[3]\n\nGrids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, “distributed” or “grid” computing, can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a computer network (private or public) by a conventional network interface, such as Ethernet. This is in contrast to the traditional notion of a supercomputer, which has many processors connected by a local high-speed computer bus.\n\nContents\n\n    1 Overview\n    2 Comparison of grids and conventional supercomputers\n    3 Design considerations and variations\n    4 Market segmentation of the grid computing market\n        4.1 The provider side\n        4.2 The user side\n    5 CPU scavenging\n    6 History\n        6.1 Progress in Grid computing\n    7 Fastest virtual supercomputers\n    8 Projects and applications\n        8.1 Definitions\n    9 See also\n        9.1 Related concepts\n        9.2 Alliances and organizations\n        9.3 Production grids\n        9.4 International projects\n        9.5 National projects\n        9.6 Standards and APIs\n        9.7 Software implementations and middleware\n        9.8 Monitoring frameworks\n    10 See also\n    11 References\n        11.1 Bibliography\n    12 External links\n\nOverview\n\nGrid computing combines computers from multiple administrative domains to reach a common goal,[4] to solve a single task, and may then disappear just as quickly.\n\nOne of the main strategies of grid computing is to use middleware to divide and apportion pieces of a program among several computers, sometimes up to many thousands. Grid computing involves computation in a distributed fashion, which may also involve the aggregation of large-scale clusters.\n\nThe size of a grid may vary from small—confined to a network of computer workstations within a corporation, for example—to large, public collaborations across many companies and networks. \"The notion of a confined grid may also be known as an intra-nodes cooperation whilst the notion of a larger, wider grid may thus refer to an inter-nodes cooperation\".[5]\n\nGrids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform very large tasks. This technology has been applied to computationally intensive scientific, mathematical, and academic problems through volunteer computing, and it is used in commercial enterprises for such diverse applications as drug discovery, economic forecasting, seismic analysis, and back office data processing in support for e-commerce and Web services.\n\nCoordinating applications on Grids can be a complex task, especially when coordinating the flow of information across distributed computing resources. Grid workflow systems have been developed as a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in the Grid context.\nComparison of grids and conventional supercomputers\n\n“Distributed” or “grid” computing in general is a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a network (private, public or the Internet) by a conventional network interface producing commodity hardware, compared to the lower efficiency of designing and constructing a small number of custom supercomputers. The primary performance disadvantage is that the various processors and local storage areas do not have high-speed connections. This arrangement is thus well-suited to applications in which multiple parallel computations can take place independently, without the need to communicate intermediate results between processors.[6] The high-end scalability of geographically dispersed grids is generally favorable, due to the low need for connectivity between nodes relative to the capacity of the public Internet.[citation needed]\n\nThere are also some differences in programming and deployment. It can be costly and difficult to write programs that can run in the environment of a supercomputer, which may have a custom operating system, or require the program to address concurrency issues. If a problem can be adequately parallelized, a “thin” layer of “grid” infrastructure can allow conventional, standalone programs, given a different part of the same problem, to run on multiple machines. This makes it possible to write and debug on a single conventional machine, and eliminates complications due to multiple instances of the same program running in the same shared memory and storage space at the same time.\nDesign considerations and variations\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2015) (Learn how and when to remove this template message)\n\nOne feature of distributed grids is that they can be formed from computing resources belonging to one or more multiple individuals or organizations (known as multiple administrative domains). This can facilitate commercial transactions, as in utility computing, or make it easier to assemble volunteer computing networks.\n\nOne disadvantage of this feature is that the computers which are actually performing the calculations might not be entirely trustworthy. The designers of the system must thus introduce measures to prevent malfunctions or malicious participants from producing false, misleading, or erroneous results, and from using the system as an attack vector. This often involves assigning work randomly to different nodes (presumably with different owners) and checking that at least two different nodes report the same answer for a given work unit. Discrepancies would identify malfunctioning and malicious nodes. However, due to the lack of central control over the hardware, there is no way to guarantee that nodes will not drop out of the network at random times. Some nodes (like laptops or dialup Internet customers) may also be available for computation but not network communications for unpredictable periods. These variations can be accommodated by assigning large work units (thus reducing the need for continuous network connectivity) and reassigning work units when a given node fails to report its results in expected time.\n\nThe impacts of trust and availability on performance and development difficulty can influence the choice of whether to deploy onto a dedicated cluster, to idle machines internal to the developing organization, or to an open external network of volunteers or contractors. In many cases, the participating nodes must trust the central system not to abuse the access that is being granted, by interfering with the operation of other programs, mangling stored information, transmitting private data, or creating new security holes. Other systems employ measures to reduce the amount of trust “client” nodes must place in the central system such as placing applications in virtual machines.\n\nPublic systems or those crossing administrative domains (including different departments in the same organization) often result in the need to run on heterogeneous systems, using different operating systems and hardware architectures. With many languages, there is a trade off between investment in software development and the number of platforms that can be supported (and thus the size of the resulting network). Cross-platform languages can reduce the need to make this trade off, though potentially at the expense of high performance on any given node (due to run-time interpretation or lack of optimization for the particular platform). There are diverse scientific and commercial projects to harness a particular associated grid or for the purpose of setting up new grids. BOINC is a common one for various academic projects seeking public volunteers; more are listed at the end of the article.\n\nIn fact, the middleware can be seen as a layer between the hardware and the software. On top of the middleware, a number of technical areas have to be considered, and these may or may not be middleware independent. Example areas include SLA management, Trust and Security, Virtual organization management, License Management, Portals and Data Management. These technical areas may be taken care of in a commercial solution, though the cutting edge of each area is often found within specific research projects examining the field.\nMarket segmentation of the grid computing market\n\nFor the segmentation of the grid computing market, two perspectives need to be considered: the provider side and the user side:\nThe provider side\n\nThe overall grid market comprises several specific markets. These are the grid middleware market, the market for grid-enabled applications, the utility computing market, and the software-as-a-service (SaaS) market.\n\nGrid middleware is a specific software product, which enables the sharing of heterogeneous resources, and Virtual Organizations. It is installed and integrated into the existing infrastructure of the involved company or companies, and provides a special layer placed among the heterogeneous infrastructure and the specific user applications. Major grid middlewares are Globus Toolkit, gLite, and UNICORE.\n\nUtility computing is referred to as the provision of grid computing and applications as service either as an open grid utility or as a hosting solution for one organization or a VO. Major players in the utility computing market are Sun Microsystems, IBM, and HP.\n\nGrid-enabled applications are specific software applications that can utilize grid infrastructure. This is made possible by the use of grid middleware, as pointed out above.\n\nSoftware as a service (SaaS) is “software that is owned, delivered and managed remotely by one or more providers.” (Gartner 2007) Additionally, SaaS applications are based on a single set of common code and data definitions. They are consumed in a one-to-many model, and SaaS uses a Pay As You Go (PAYG) model or a subscription model that is based on usage. Providers of SaaS do not necessarily own the computing resources themselves, which are required to run their SaaS. Therefore, SaaS providers may draw upon the utility computing market. The utility computing market provides computing resources for SaaS providers.\nThe user side\n\nFor companies on the demand or user side of the grid computing market, the different segments have significant implications for their IT deployment strategy. The IT deployment strategy as well as the type of IT investments made are relevant aspects for potential grid users and play an important role for grid adoption.\nCPU scavenging\n\nCPU-scavenging, cycle-scavenging, or shared computing creates a “grid” from the unused resources in a network of participants (whether worldwide or internal to an organization). Typically this technique uses desktop computer instruction cycles that would otherwise be wasted at night, during lunch, or even in the scattered seconds throughout the day when the computer is waiting for user input on relatively fast devices. In practice, participating computers also donate some supporting amount of disk storage space, RAM, and network bandwidth, in addition to raw CPU power.[citation needed]\n\nMany volunteer computing projects, such as BOINC, use the CPU scavenging model. Since nodes are likely to go \"offline\" from time to time, as their owners use their resources for their primary purpose, this model must be designed to handle such contingencies.\n\nCreating an Opportunistic Environment is another implementation of CPU-scavenging where special workload management system harvests the idle desktop computers for compute-intensive jobs, it also refers as Enterprise Desktop Grid (EDG). For instance, HTCondor [7] the open-source high-throughput computing software framework for coarse-grained distributed rationalization of computationally intensive tasks can be configured to only use desktop machines where the keyboard and mouse are idle to effectively harness wasted CPU power from otherwise idle desktop workstations. Like other full-featured batch systems, HTCondor provides a job queueing mechanism, scheduling policy, priority scheme, resource monitoring, and resource management. It can be used to manage workload on a dedicated cluster of computers as well or it can seamlessly integrate both dedicated resources (rack-mounted clusters) and non-dedicated desktop machines (cycle scavenging) into one computing environment.\nHistory\n\nThe term grid computing originated in the early 1990s as a metaphor for making computer power as easy to access as an electric power grid. The power grid metaphor for accessible computing quickly became canonical when Ian Foster and Carl Kesselman published their seminal work, \"The Grid: Blueprint for a new computing infrastructure\" (1999). This was preceded by decades by the metaphor of utility computing (1961): computing as a public utility, analogous to the phone system.[8][9]\n\nCPU scavenging and volunteer computing were popularized beginning in 1997 by distributed.net and later in 1999 by SETI@home to harness the power of networked PCs worldwide, in order to solve CPU-intensive research problems.[citation needed]\n\nThe ideas of the grid (including those from distributed computing, object-oriented programming, and Web services) were brought together by Ian Foster, Carl Kesselman, and Steve Tuecke, widely regarded as the \"fathers of the grid\".[10] They led the effort to create the Globus Toolkit incorporating not just computation management but also storage management, security provisioning, data movement, monitoring, and a toolkit for developing additional services based on the same infrastructure, including agreement negotiation, notification mechanisms, trigger services, and information aggregation. While the Globus Toolkit remains the de facto standard for building grid solutions, a number of other tools have been built that answer some subset of services needed to create an enterprise or global grid.[11]\n\nIn 2007 the term cloud computing came into popularity, which is conceptually similar to the canonical Foster definition of grid computing (in terms of computing resources being consumed as electricity is from the power grid) and earlier utility computing. Indeed, grid computing is often (but not always) associated with the delivery of cloud computing systems as exemplified by the AppLogic system from 3tera.[citation needed]\nProgress in Grid computing\n\nIn November 2006, Seidel received the Sidney Fernbach Award at the Supercomputing Conference in Tampa, Florida.[12]\"For outstanding contributions to the development of software for HPC and Grid computing to enable the collaborative numerical investigation of complex problems in physics; in particular, modeling black hole collisions.\"[13] This award, which is one of the highest honors in computing, was awarded for his achievements in numerical relativity.\nFastest virtual supercomputers\n\n    As of August 2015, BOINC – 139 PFLOPS.[14]\n    As of August 2015, Folding@home – 36.3 x86-equivalent PFLOPS.[15]\n    As of August 2015, Einstein@Home  0.801 PFLOPS.[16]\n    As of August 2015, SETI@Home  0.677 PFLOPS.[17]\n    As of August 2015, MilkyWay@Home  0.381 PFLOPS.[18]\n    As of August 2015, GIMPS  0.235 PFLOPS.[19]\n\nAlso, As of August 2015, the Bitcoin Network had computing power claimed to be equivalent to 4,873,841.62 PFLOPS.[20] However, the elements of that network can perform only one specific cryptographic hash computation required by the bitcoin protocol. They cannot perform general floating-point arithmetic operations, therefore their computing power cannot be measured in FLOPs.[citation needed]\nProjects and applications\nMain article: List of distributed computing projects\n\nGrid computing offers a way to solve Grand Challenge problems such as protein folding, financial modeling, earthquake simulation, and climate/weather modeling. Grids offer a way of using the information technology resources optimally inside an organization. They also provide a means for offering information technology as a utility for commercial and noncommercial clients, with those clients paying only for what they use, as with electricity or water.\n\nGrid computing is being applied by the National Science Foundation's National Technology Grid, NASA's Information Power Grid, Pratt & Whitney, Bristol-Myers Squibb Co., and American Express.[citation needed]\n\nOne cycle-scavenging network is SETI@home, which was using more than 3 million computers to achieve 23.37 sustained teraflops (979 lifetime teraflops) as of September 2001.[21]\n\nAs of August 2009 Folding@home achieves more than 4 petaflops on over 350,000 machines.\n\nThe European Union funded projects through the framework programmes of the European Commission. BEinGRID (Business Experiments in Grid) was a research project funded by the European Commission[22] as an Integrated Project under the Sixth Framework Programme (FP6) sponsorship program. Started on June 1, 2006, the project ran 42 months, until November 2009. The project was coordinated by Atos Origin. According to the project fact sheet, their mission is “to establish effective routes to foster the adoption of grid computing across the EU and to stimulate research into innovative business models using Grid technologies”. To extract best practice and common themes from the experimental implementations, two groups of consultants are analyzing a series of pilots, one technical, one business. The project is significant not only for its long duration, but also for its budget, which at 24.8 million Euros, is the largest of any FP6 integrated project. Of this, 15.7 million is provided by the European commission and the remainder by its 98 contributing partner companies. Since the end of the project, the results of BEinGRID have been taken up and carried forward by IT-Tude.com \n.\n\nThe Enabling Grids for E-sciencE project, based in the European Union and included sites in Asia and the United States, was a follow-up project to the European DataGrid (EDG) and evolved into the European Grid Infrastructure. This, along with the LHC Computing Grid[23] (LCG), was developed to support experiments using the CERN Large Hadron Collider. A list of active sites participating within LCG can be found online[24] as can real time monitoring of the EGEE infrastructure.[25] The relevant software and documentation is also publicly accessible.[26] There is speculation that dedicated fiber optic links, such as those installed by CERN to address the LCG's data-intensive needs, may one day be available to home users thereby providing internet services at speeds up to 10,000 times faster than a traditional broadband connection.[27] The European Grid Infrastructure has been also used for other research activities and experiments such as the simulation of oncological clinical trials.[28]\n\nThe distributed.net project was started in 1997. The NASA Advanced Supercomputing facility (NAS) ran genetic algorithms using the Condor cycle scavenger running on about 350 Sun Microsystems and SGI workstations.\n\nIn 2001, United Devices operated the United Devices Cancer Research Project based on its Grid MP product, which cycle-scavenges on volunteer PCs connected to the Internet. The project ran on about 3.1 million machines before its close in 2007.[29]\n\nAs of 2011, over 6.2 million machines running the open-source Berkeley Open Infrastructure for Network Computing (BOINC) platform are members of the World Community Grid, which tops the processing power of the current fastest supercomputer system (China's Tianhe-I).[30]\nDefinitions\n\nToday there are many definitions of grid computing:\n\n    In his article “What is the Grid? A Three Point Checklist”,[4] Ian Foster lists these primary attributes:\n        Computing resources are not administered centrally.\n        Open standards are used.\n        Nontrivial quality of service is achieved.\n    Plaszczak/Wellner[31] define grid technology as \"the technology that enables resource virtualization, on-demand provisioning, and service (resource) sharing between organizations.\"\n    IBM defines grid computing as “the ability, using a set of open standards and protocols, to gain access to applications and data, processing power, storage capacity and a vast array of other computing resources over the Internet. A grid is a type of parallel and distributed system that enables the sharing, selection, and aggregation of resources distributed across ‘multiple’ administrative domains based on their (resources) availability, capacity, performance, cost and users' quality-of-service requirements”.[32]\n    An earlier example of the notion of computing as utility was in 1965 by MIT's Fernando Corbató. Corbató and the other designers of the Multics operating system envisioned a computer facility operating “like a power company or water company”.[33]\n    Buyya/Venugopal[34] define grid as \"a type of parallel and distributed system that enables the sharing, selection, and aggregation of geographically distributed autonomous resources dynamically at runtime depending on their availability, capability, performance, cost, and users' quality-of-service requirements\".\n    CERN, one of the largest users of grid technology, talk of The Grid: “a service for sharing computer power and data storage capacity over the Internet.”[35]\n\nSee also\nRelated concepts\n\n    Cloud computing\n    Code mobility\n    Jungle computing\n    Sensor grid\n    Utility computing\n\nAlliances and organizations\n\n    Open Grid Forum (Formerly Global Grid Forum)\n    Object Management Group\n\nProduction grids\n\n    European Grid Infrastructure\n    Enabling Grids for E-sciencE\n    INFN Production Grid\n    NorduGrid\n    OurGrid\n    Sun Grid\n    Techila\n    Xgrid\n\nInternational projects\nName \tRegion \tStart \tEnd\nEuropean Grid Infrastructure (EGI) \tEurope \tMay 2010 \tDec 2014\nOpen Middleware Infrastructure Institute Europe (OMII-Europe) \tEurope \tMay 2006 \tMay 2008\nEnabling Grids for E-sciencE (EGEE, EGEE II and EGEE III) \tEurope \tMarch 2004 \tApril 2010\nGrid enabled Remote Instrumentation with Distributed Control and Computation (GridCC) \tEurope \tSeptember 2005 \tSeptember 2008\nEuropean Middleware Initiative (EMI) \tEurope \tMay 2010 \tactive\nKnowARC \tEurope \tJune 2006 \tNovember 2009\nNordic Data Grid Facility \tScandinavia and Finland \tJune 2006 \tDecember 2012\nWorld Community Grid \tGlobal \tNovember 2004 \tactive\nXtreemOS \tEurope \tJune 2006 \t(May 2010) ext. to September 2010\nOurGrid \tBrazil \tDecember 2004 \tactive\nNational projects\n\n    GridPP (UK)\n    CNGrid (China)\n    D-Grid (Germany)\n    GARUDA (India)\n    VECC (Calcutta, India)\n    IsraGrid (Israel)\n    INFN Grid (Italy)\n    PL-Grid (Poland)\n    National Grid Service (UK)\n    Open Science Grid (USA)\n    TeraGrid (USA)\n    Grid5000 (France)\n\nStandards and APIs\n\n    Distributed Resource Management Application API (DRMAA)\n    A technology-agnostic information model for a uniform representation of Grid resources (GLUE)\n    Grid Remote Procedure Call (GridRPC)\n    Grid Security Infrastructure (GSI)\n    Open Grid Services Architecture (OGSA)\n    Open Grid Services Infrastructure (OGSI)\n    A Simple API for Grid Applications (SAGA)\n    Web Services Resource Framework (WSRF)\n\nSoftware implementations and middleware\n\n    Advanced Resource Connector (NorduGrid's ARC)\n    Altair PBS GridWorks\n    Berkeley Open Infrastructure for Network Computing (BOINC)\n    DIET\n    Discovery Net\n    European Middleware Initiative\n    gLite\n    Globus Toolkit\n    GridWay\n    OurGrid\n    Portable Batch System (PBS)\n    Platform LSF\n    LinuxPMI\n    ProActive\n    Platform Symphony\n    SDSC Storage resource broker (data grid)\n    Simple Grid Protocol\n    Sun Grid Engine\n    Techila Grid\n    UNICORE\n    Univa Grid Engine\n    Xgrid\n    ZeroC ICE IceGrid\n\nMonitoring frameworks\n\n    GStat", "skillName": "Grid_computing."}
{"id": 62, "category": "Distributed", "skillText": "Grid computing is the collection of computer resources from multiple locations to reach a common goal. The grid can be thought of as a distributed system with non-interactive workloads that involve a large number of files. Grid computing is distinguished from conventional high performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application.[1][better source needed] Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers.[2] Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large.[3]\n\nGrids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, “distributed” or “grid” computing, can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a computer network (private or public) by a conventional network interface, such as Ethernet. This is in contrast to the traditional notion of a supercomputer, which has many processors connected by a local high-speed computer bus.\n\nContents\n\n    1 Overview\n    2 Comparison of grids and conventional supercomputers\n    3 Design considerations and variations\n    4 Market segmentation of the grid computing market\n        4.1 The provider side\n        4.2 The user side\n    5 CPU scavenging\n    6 History\n        6.1 Progress in Grid computing\n    7 Fastest virtual supercomputers\n    8 Projects and applications\n        8.1 Definitions\n    9 See also\n        9.1 Related concepts\n        9.2 Alliances and organizations\n        9.3 Production grids\n        9.4 International projects\n        9.5 National projects\n        9.6 Standards and APIs\n        9.7 Software implementations and middleware\n        9.8 Monitoring frameworks\n    10 See also\n    11 References\n        11.1 Bibliography\n    12 External links\n\nOverview\n\nGrid computing combines computers from multiple administrative domains to reach a common goal,[4] to solve a single task, and may then disappear just as quickly.\n\nOne of the main strategies of grid computing is to use middleware to divide and apportion pieces of a program among several computers, sometimes up to many thousands. Grid computing involves computation in a distributed fashion, which may also involve the aggregation of large-scale clusters.\n\nThe size of a grid may vary from small—confined to a network of computer workstations within a corporation, for example—to large, public collaborations across many companies and networks. \"The notion of a confined grid may also be known as an intra-nodes cooperation whilst the notion of a larger, wider grid may thus refer to an inter-nodes cooperation\".[5]\n\nGrids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform very large tasks. This technology has been applied to computationally intensive scientific, mathematical, and academic problems through volunteer computing, and it is used in commercial enterprises for such diverse applications as drug discovery, economic forecasting, seismic analysis, and back office data processing in support for e-commerce and Web services.\n\nCoordinating applications on Grids can be a complex task, especially when coordinating the flow of information across distributed computing resources. Grid workflow systems have been developed as a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in the Grid context.\nComparison of grids and conventional supercomputers\n\n“Distributed” or “grid” computing in general is a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a network (private, public or the Internet) by a conventional network interface producing commodity hardware, compared to the lower efficiency of designing and constructing a small number of custom supercomputers. The primary performance disadvantage is that the various processors and local storage areas do not have high-speed connections. This arrangement is thus well-suited to applications in which multiple parallel computations can take place independently, without the need to communicate intermediate results between processors.[6] The high-end scalability of geographically dispersed grids is generally favorable, due to the low need for connectivity between nodes relative to the capacity of the public Internet.[citation needed]\n\nThere are also some differences in programming and deployment. It can be costly and difficult to write programs that can run in the environment of a supercomputer, which may have a custom operating system, or require the program to address concurrency issues. If a problem can be adequately parallelized, a “thin” layer of “grid” infrastructure can allow conventional, standalone programs, given a different part of the same problem, to run on multiple machines. This makes it possible to write and debug on a single conventional machine, and eliminates complications due to multiple instances of the same program running in the same shared memory and storage space at the same time.\nDesign considerations and variations\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2015) (Learn how and when to remove this template message)\n\nOne feature of distributed grids is that they can be formed from computing resources belonging to one or more multiple individuals or organizations (known as multiple administrative domains). This can facilitate commercial transactions, as in utility computing, or make it easier to assemble volunteer computing networks.\n\nOne disadvantage of this feature is that the computers which are actually performing the calculations might not be entirely trustworthy. The designers of the system must thus introduce measures to prevent malfunctions or malicious participants from producing false, misleading, or erroneous results, and from using the system as an attack vector. This often involves assigning work randomly to different nodes (presumably with different owners) and checking that at least two different nodes report the same answer for a given work unit. Discrepancies would identify malfunctioning and malicious nodes. However, due to the lack of central control over the hardware, there is no way to guarantee that nodes will not drop out of the network at random times. Some nodes (like laptops or dialup Internet customers) may also be available for computation but not network communications for unpredictable periods. These variations can be accommodated by assigning large work units (thus reducing the need for continuous network connectivity) and reassigning work units when a given node fails to report its results in expected time.\n\nThe impacts of trust and availability on performance and development difficulty can influence the choice of whether to deploy onto a dedicated cluster, to idle machines internal to the developing organization, or to an open external network of volunteers or contractors. In many cases, the participating nodes must trust the central system not to abuse the access that is being granted, by interfering with the operation of other programs, mangling stored information, transmitting private data, or creating new security holes. Other systems employ measures to reduce the amount of trust “client” nodes must place in the central system such as placing applications in virtual machines.\n\nPublic systems or those crossing administrative domains (including different departments in the same organization) often result in the need to run on heterogeneous systems, using different operating systems and hardware architectures. With many languages, there is a trade off between investment in software development and the number of platforms that can be supported (and thus the size of the resulting network). Cross-platform languages can reduce the need to make this trade off, though potentially at the expense of high performance on any given node (due to run-time interpretation or lack of optimization for the particular platform). There are diverse scientific and commercial projects to harness a particular associated grid or for the purpose of setting up new grids. BOINC is a common one for various academic projects seeking public volunteers; more are listed at the end of the article.\n\nIn fact, the middleware can be seen as a layer between the hardware and the software. On top of the middleware, a number of technical areas have to be considered, and these may or may not be middleware independent. Example areas include SLA management, Trust and Security, Virtual organization management, License Management, Portals and Data Management. These technical areas may be taken care of in a commercial solution, though the cutting edge of each area is often found within specific research projects examining the field.\nMarket segmentation of the grid computing market\n\nFor the segmentation of the grid computing market, two perspectives need to be considered: the provider side and the user side:\nThe provider side\n\nThe overall grid market comprises several specific markets. These are the grid middleware market, the market for grid-enabled applications, the utility computing market, and the software-as-a-service (SaaS) market.\n\nGrid middleware is a specific software product, which enables the sharing of heterogeneous resources, and Virtual Organizations. It is installed and integrated into the existing infrastructure of the involved company or companies, and provides a special layer placed among the heterogeneous infrastructure and the specific user applications. Major grid middlewares are Globus Toolkit, gLite, and UNICORE.\n\nUtility computing is referred to as the provision of grid computing and applications as service either as an open grid utility or as a hosting solution for one organization or a VO. Major players in the utility computing market are Sun Microsystems, IBM, and HP.\n\nGrid-enabled applications are specific software applications that can utilize grid infrastructure. This is made possible by the use of grid middleware, as pointed out above.\n\nSoftware as a service (SaaS) is “software that is owned, delivered and managed remotely by one or more providers.” (Gartner 2007) Additionally, SaaS applications are based on a single set of common code and data definitions. They are consumed in a one-to-many model, and SaaS uses a Pay As You Go (PAYG) model or a subscription model that is based on usage. Providers of SaaS do not necessarily own the computing resources themselves, which are required to run their SaaS. Therefore, SaaS providers may draw upon the utility computing market. The utility computing market provides computing resources for SaaS providers.\nThe user side\n\nFor companies on the demand or user side of the grid computing market, the different segments have significant implications for their IT deployment strategy. The IT deployment strategy as well as the type of IT investments made are relevant aspects for potential grid users and play an important role for grid adoption.\nCPU scavenging\n\nCPU-scavenging, cycle-scavenging, or shared computing creates a “grid” from the unused resources in a network of participants (whether worldwide or internal to an organization). Typically this technique uses desktop computer instruction cycles that would otherwise be wasted at night, during lunch, or even in the scattered seconds throughout the day when the computer is waiting for user input on relatively fast devices. In practice, participating computers also donate some supporting amount of disk storage space, RAM, and network bandwidth, in addition to raw CPU power.[citation needed]\n\nMany volunteer computing projects, such as BOINC, use the CPU scavenging model. Since nodes are likely to go \"offline\" from time to time, as their owners use their resources for their primary purpose, this model must be designed to handle such contingencies.\n\nCreating an Opportunistic Environment is another implementation of CPU-scavenging where special workload management system harvests the idle desktop computers for compute-intensive jobs, it also refers as Enterprise Desktop Grid (EDG). For instance, HTCondor [7] the open-source high-throughput computing software framework for coarse-grained distributed rationalization of computationally intensive tasks can be configured to only use desktop machines where the keyboard and mouse are idle to effectively harness wasted CPU power from otherwise idle desktop workstations. Like other full-featured batch systems, HTCondor provides a job queueing mechanism, scheduling policy, priority scheme, resource monitoring, and resource management. It can be used to manage workload on a dedicated cluster of computers as well or it can seamlessly integrate both dedicated resources (rack-mounted clusters) and non-dedicated desktop machines (cycle scavenging) into one computing environment.\nHistory\n\nThe term grid computing originated in the early 1990s as a metaphor for making computer power as easy to access as an electric power grid. The power grid metaphor for accessible computing quickly became canonical when Ian Foster and Carl Kesselman published their seminal work, \"The Grid: Blueprint for a new computing infrastructure\" (1999). This was preceded by decades by the metaphor of utility computing (1961): computing as a public utility, analogous to the phone system.[8][9]\n\nCPU scavenging and volunteer computing were popularized beginning in 1997 by distributed.net and later in 1999 by SETI@home to harness the power of networked PCs worldwide, in order to solve CPU-intensive research problems.[citation needed]\n\nThe ideas of the grid (including those from distributed computing, object-oriented programming, and Web services) were brought together by Ian Foster, Carl Kesselman, and Steve Tuecke, widely regarded as the \"fathers of the grid\".[10] They led the effort to create the Globus Toolkit incorporating not just computation management but also storage management, security provisioning, data movement, monitoring, and a toolkit for developing additional services based on the same infrastructure, including agreement negotiation, notification mechanisms, trigger services, and information aggregation. While the Globus Toolkit remains the de facto standard for building grid solutions, a number of other tools have been built that answer some subset of services needed to create an enterprise or global grid.[11]\n\nIn 2007 the term cloud computing came into popularity, which is conceptually similar to the canonical Foster definition of grid computing (in terms of computing resources being consumed as electricity is from the power grid) and earlier utility computing. Indeed, grid computing is often (but not always) associated with the delivery of cloud computing systems as exemplified by the AppLogic system from 3tera.[citation needed]\nProgress in Grid computing\n\nIn November 2006, Seidel received the Sidney Fernbach Award at the Supercomputing Conference in Tampa, Florida.[12]\"For outstanding contributions to the development of software for HPC and Grid computing to enable the collaborative numerical investigation of complex problems in physics; in particular, modeling black hole collisions.\"[13] This award, which is one of the highest honors in computing, was awarded for his achievements in numerical relativity.\nFastest virtual supercomputers\n\n    As of August 2015, BOINC – 139 PFLOPS.[14]\n    As of August 2015, Folding@home – 36.3 x86-equivalent PFLOPS.[15]\n    As of August 2015, Einstein@Home  0.801 PFLOPS.[16]\n    As of August 2015, SETI@Home  0.677 PFLOPS.[17]\n    As of August 2015, MilkyWay@Home  0.381 PFLOPS.[18]\n    As of August 2015, GIMPS  0.235 PFLOPS.[19]\n\nAlso, As of August 2015, the Bitcoin Network had computing power claimed to be equivalent to 4,873,841.62 PFLOPS.[20] However, the elements of that network can perform only one specific cryptographic hash computation required by the bitcoin protocol. They cannot perform general floating-point arithmetic operations, therefore their computing power cannot be measured in FLOPs.[citation needed]\nProjects and applications\nMain article: List of distributed computing projects\n\nGrid computing offers a way to solve Grand Challenge problems such as protein folding, financial modeling, earthquake simulation, and climate/weather modeling. Grids offer a way of using the information technology resources optimally inside an organization. They also provide a means for offering information technology as a utility for commercial and noncommercial clients, with those clients paying only for what they use, as with electricity or water.\n\nGrid computing is being applied by the National Science Foundation's National Technology Grid, NASA's Information Power Grid, Pratt & Whitney, Bristol-Myers Squibb Co., and American Express.[citation needed]\n\nOne cycle-scavenging network is SETI@home, which was using more than 3 million computers to achieve 23.37 sustained teraflops (979 lifetime teraflops) as of September 2001.[21]\n\nAs of August 2009 Folding@home achieves more than 4 petaflops on over 350,000 machines.\n\nThe European Union funded projects through the framework programmes of the European Commission. BEinGRID (Business Experiments in Grid) was a research project funded by the European Commission[22] as an Integrated Project under the Sixth Framework Programme (FP6) sponsorship program. Started on June 1, 2006, the project ran 42 months, until November 2009. The project was coordinated by Atos Origin. According to the project fact sheet, their mission is “to establish effective routes to foster the adoption of grid computing across the EU and to stimulate research into innovative business models using Grid technologies”. To extract best practice and common themes from the experimental implementations, two groups of consultants are analyzing a series of pilots, one technical, one business. The project is significant not only for its long duration, but also for its budget, which at 24.8 million Euros, is the largest of any FP6 integrated project. Of this, 15.7 million is provided by the European commission and the remainder by its 98 contributing partner companies. Since the end of the project, the results of BEinGRID have been taken up and carried forward by IT-Tude.com \n.\n\nThe Enabling Grids for E-sciencE project, based in the European Union and included sites in Asia and the United States, was a follow-up project to the European DataGrid (EDG) and evolved into the European Grid Infrastructure. This, along with the LHC Computing Grid[23] (LCG), was developed to support experiments using the CERN Large Hadron Collider. A list of active sites participating within LCG can be found online[24] as can real time monitoring of the EGEE infrastructure.[25] The relevant software and documentation is also publicly accessible.[26] There is speculation that dedicated fiber optic links, such as those installed by CERN to address the LCG's data-intensive needs, may one day be available to home users thereby providing internet services at speeds up to 10,000 times faster than a traditional broadband connection.[27] The European Grid Infrastructure has been also used for other research activities and experiments such as the simulation of oncological clinical trials.[28]\n\nThe distributed.net project was started in 1997. The NASA Advanced Supercomputing facility (NAS) ran genetic algorithms using the Condor cycle scavenger running on about 350 Sun Microsystems and SGI workstations.\n\nIn 2001, United Devices operated the United Devices Cancer Research Project based on its Grid MP product, which cycle-scavenges on volunteer PCs connected to the Internet. The project ran on about 3.1 million machines before its close in 2007.[29]\n\nAs of 2011, over 6.2 million machines running the open-source Berkeley Open Infrastructure for Network Computing (BOINC) platform are members of the World Community Grid, which tops the processing power of the current fastest supercomputer system (China's Tianhe-I).[30]\nDefinitions\n\nToday there are many definitions of grid computing:\n\n    In his article “What is the Grid? A Three Point Checklist”,[4] Ian Foster lists these primary attributes:\n        Computing resources are not administered centrally.\n        Open standards are used.\n        Nontrivial quality of service is achieved.\n    Plaszczak/Wellner[31] define grid technology as \"the technology that enables resource virtualization, on-demand provisioning, and service (resource) sharing between organizations.\"\n    IBM defines grid computing as “the ability, using a set of open standards and protocols, to gain access to applications and data, processing power, storage capacity and a vast array of other computing resources over the Internet. A grid is a type of parallel and distributed system that enables the sharing, selection, and aggregation of resources distributed across ‘multiple’ administrative domains based on their (resources) availability, capacity, performance, cost and users' quality-of-service requirements”.[32]\n    An earlier example of the notion of computing as utility was in 1965 by MIT's Fernando Corbató. Corbató and the other designers of the Multics operating system envisioned a computer facility operating “like a power company or water company”.[33]\n    Buyya/Venugopal[34] define grid as \"a type of parallel and distributed system that enables the sharing, selection, and aggregation of geographically distributed autonomous resources dynamically at runtime depending on their availability, capability, performance, cost, and users' quality-of-service requirements\".\n    CERN, one of the largest users of grid technology, talk of The Grid: “a service for sharing computer power and data storage capacity over the Internet.”[35]\n\nSee also\nRelated concepts\n\n    Cloud computing\n    Code mobility\n    Jungle computing\n    Sensor grid\n    Utility computing\n\nAlliances and organizations\n\n    Open Grid Forum (Formerly Global Grid Forum)\n    Object Management Group\n\nProduction grids\n\n    European Grid Infrastructure\n    Enabling Grids for E-sciencE\n    INFN Production Grid\n    NorduGrid\n    OurGrid\n    Sun Grid\n    Techila\n    Xgrid\n\nInternational projects\nName \tRegion \tStart \tEnd\nEuropean Grid Infrastructure (EGI) \tEurope \tMay 2010 \tDec 2014\nOpen Middleware Infrastructure Institute Europe (OMII-Europe) \tEurope \tMay 2006 \tMay 2008\nEnabling Grids for E-sciencE (EGEE, EGEE II and EGEE III) \tEurope \tMarch 2004 \tApril 2010\nGrid enabled Remote Instrumentation with Distributed Control and Computation (GridCC) \tEurope \tSeptember 2005 \tSeptember 2008\nEuropean Middleware Initiative (EMI) \tEurope \tMay 2010 \tactive\nKnowARC \tEurope \tJune 2006 \tNovember 2009\nNordic Data Grid Facility \tScandinavia and Finland \tJune 2006 \tDecember 2012\nWorld Community Grid \tGlobal \tNovember 2004 \tactive\nXtreemOS \tEurope \tJune 2006 \t(May 2010) ext. to September 2010\nOurGrid \tBrazil \tDecember 2004 \tactive\nNational projects\n\n    GridPP (UK)\n    CNGrid (China)\n    D-Grid (Germany)\n    GARUDA (India)\n    VECC (Calcutta, India)\n    IsraGrid (Israel)\n    INFN Grid (Italy)\n    PL-Grid (Poland)\n    National Grid Service (UK)\n    Open Science Grid (USA)\n    TeraGrid (USA)\n    Grid5000 (France)\n\nStandards and APIs\n\n    Distributed Resource Management Application API (DRMAA)\n    A technology-agnostic information model for a uniform representation of Grid resources (GLUE)\n    Grid Remote Procedure Call (GridRPC)\n    Grid Security Infrastructure (GSI)\n    Open Grid Services Architecture (OGSA)\n    Open Grid Services Infrastructure (OGSI)\n    A Simple API for Grid Applications (SAGA)\n    Web Services Resource Framework (WSRF)\n\nSoftware implementations and middleware\n\n    Advanced Resource Connector (NorduGrid's ARC)\n    Altair PBS GridWorks\n    Berkeley Open Infrastructure for Network Computing (BOINC)\n    DIET\n    Discovery Net\n    European Middleware Initiative\n    gLite\n    Globus Toolkit\n    GridWay\n    OurGrid\n    Portable Batch System (PBS)\n    Platform LSF\n    LinuxPMI\n    ProActive\n    Platform Symphony\n    SDSC Storage resource broker (data grid)\n    Simple Grid Protocol\n    Sun Grid Engine\n    Techila Grid\n    UNICORE\n    Univa Grid Engine\n    Xgrid\n    ZeroC ICE IceGrid\n\nMonitoring frameworks\n\n    GStat", "skillName": "Grid computing."}
{"id": 63, "category": "Distributed", "skillText": "Distributed computing\nDistributed computing is a field of computer science that studies distributed systems. A distributed system is a model in which components located on networked computers communicate and coordinate their actions by passing messages.[1] The components interact with each other in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components.[1] Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.\n\nA computer program that runs in a distributed system is called a distributed program, and distributed programming is the process of writing such programs.[2] There are many alternatives for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.\n\nA goal and challenge pursued by some computer scientists and practitioners in distributed systems is location transparency; however, this goal has fallen out of favour in industry, as distributed systems are different from conventional non-distributed systems, and the differences, such as network partitions, partial system failures, and partial upgrades, cannot simply be \"papered over\" by attempts at \"transparency\" (see CAP theorem).\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[3] which communicate with each other by message passing.[4]\n\n1\tIntroduction\n2\tParallel and distributed computing\n3\tHistory\n4\tArchitectures\n5\tApplications\n6\tExamples\n7\tTheoretical foundations\n7.1\tModels\n7.2\tAn example\n7.3\tComplexity measures\n7.4\tOther problems\n7.5\tProperties of distributed systems\n8\tCoordinator election\n8.1\tBully algorithm\n8.2\tChang and Roberts algorithm\n9\tSee also\n10\tNotes\n11\tReferences\n12\tFurther reading\n13\tExternal links\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[5] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[4] While there is no single definition of a distributed system,[6] the following defining properties are commonly used:\n\nThere are several autonomous computational entities, each of which has its own local memory.[7]\nThe entities communicate with each other by message passing.[8]\nIn this article, the computational entities are called computers or nodes.\n\nA distributed system may have a common goal, such as solving a large computational problem.[9] Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[10]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[11]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[12]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[13]\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers, which have the same goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them.[14] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[15] Parallel computing may be seen as a particular tightly coupled form of distributed computing,[16] and distributed computing may be seen as a loosely coupled form of parallel computing.[6] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[17]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[18]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; as usual, the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.\n\nHistory\nThe use of concurrent processes that communicate by message-passing has its roots in operating system architectures studied in the 1960s.[19] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[20]\n\nARPANET, the predecessor of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[21] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET, and its successor, the Internet, other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its European counterpart International Symposium on Distributed Computing (DISC) was first held in 1985.\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.\n\nDistributed programming typically falls into one of several basic architectures: client�server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.\n\nClient�server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there is no special machines that provide a service or manage the network resources.[22]:227 Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[23]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example, it may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer. A distributed system can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[24]\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[25]\n\ntelecommunication networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld wide web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing and grid computing and various volunteer computing projects (see the list of distributed computing projects),\ndistributed rendering in computer graphics\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question�answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random access machines (PRAM) that are used.[26] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[27][28]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[29] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is a lot of interaction between the two fields. For example, the Cole�Vishkin algorithm for graph coloring[30] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[31] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[32] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits�PRAM machines can simulate Boolean circuits efficiently and vice versa.[33]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbours. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[34]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[35]\n\nOther commonly used measures are the total number of bits transmitted in the network (cf. communication complexity).\n\nOther problems\nTraditional computational problems take the perspective that we ask a question, a computer (or a distributed system) processes the question for a while, and then produces an answer and stops. However, there are also problems where we do not want the system to ever stop. Examples of such problems include the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing. The first example is challenges that are related to fault-tolerance. Examples of related problems include consensus problems,[36] Byzantine fault tolerance,[37] and self-stabilisation.[38]\n\nA lot of research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[39]\nLogical clocks provide a causal happened-before ordering of events.[40]\nClock synchronization algorithms provide globally consistent physical time stamps.[41]\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[42] i.e., it is decidable, but it is not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nCoordinator election\nCoordinator election (sometimes called leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira [43] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kind of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[44]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[45]\n\nBully algorithm\nWhen using the Bully algorithm, any process sends a message to the current coordinator. If there is no response within a given time limit, the process tries to elect itself as leader.\n\nChang and Roberts algorithm\nThe Chang and Roberts algorithm (or \"Ring Algorithm\") is a ring-based election algorithm used to find a process with the largest unique identification number.\n\nSee also\nAppScale\nBOINC\nBlock chain (database)\nCode mobility\nDecentralized computing\nDew computing\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed operating system\nEdsger W. Dijkstra Prize in Distributed Computing\nFog computing\nFolding@home\nInferno\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture - LOA\nList of distributed computing conferences\nList of distributed computing projects\nList of important publications in concurrent, parallel, and distributed computing\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs", "skillName": "Distributed_Computing."}
{"id": 64, "category": "Distributed", "skillText": "Apache Hadoop\nApache Hadoop (pronunciation: /h?'du?p/) is an open-source software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework.[3]\n\nThe core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for nodes to process in parallel based on the data that needs to be processed. This approach takes advantage of data locality[4]� nodes manipulating the data they have access to� to allow the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking.[5]\n\nThe base Apache Hadoop framework is composed of the following modules:\n\nHadoop Common � contains libraries and utilities needed by other Hadoop modules;\nHadoop Distributed File System (HDFS) � a distributed file-system that stores data on commodity machines, providing very high aggregate bandwidth across the cluster;\nHadoop YARN � a resource-management platform responsible for managing computing resources in clusters and using them for scheduling of users' applications;[6][7] and\nHadoop MapReduce � an implementation of the MapReduce programming model for large scale data processing.\nThe term Hadoop has come to refer not just to the base modules above, but also to the ecosystem,[8] or collection of additional software packages that can be installed on top of or alongside Hadoop, such as Apache Pig, Apache Hive, Apache HBase, Apache Phoenix, Apache Spark, Apache ZooKeeper, Cloudera Impala, Apache Flume, Apache Sqoop, Apache Oozie, Apache Storm.[9]\n\nApache Hadoop's MapReduce and HDFS components were inspired by Google papers on their MapReduce and Google File System.[10]\n\nThe Hadoop framework itself is mostly written in the Java programming language, with some native code in C and command line utilities written as shell scripts. Though MapReduce Java code is common, any programming language can be used with \"Hadoop Streaming\" to implement the \"map\" and \"reduce\" parts of the user's program.[11] Other projects in the Hadoop ecosystem expose richer user interfaces.\n\n1\tHistory\n2\tTimeline\n3\tArchitecture\n3.1\tFile systems\n3.1.1\tHadoop distributed file system\n3.1.2\tOther file systems\n3.2\tJobTracker and TaskTracker: the MapReduce engine\n3.2.1\tScheduling\n3.2.1.1\tFair scheduler\n3.2.1.2\tCapacity scheduler\n3.3\tOther applications\n4\tProminent users\n5\tHadoop hosting in the Cloud\n5.1\tOn Microsoft Azure\n5.2\tOn Amazon EC2/S3 services\n5.3\tAmazon Elastic MapReduce\n5.4\tOn CenturyLink Cloud (CLC)\n5.5\tGoogle Cloud Platform\n6\tCommercial support\n6.1\tASF's view on the use of \"Hadoop\" in product names\n7\tPapers\n8\tSee also\n9\tReferences\n10\tBibliography\n11\tExternal links\nHistory\nThe genesis of Hadoop came from the Google File System paper[12] that was published in October 2003. This paper spawned another research paper from Google - MapReduce: Simplified Data Processing on Large Clusters.[13] Development started in the Apache Nutch project, but was moved to the new Hadoop subproject in January 2006.[14] Doug Cutting, who was working at Yahoo! at the time,[15] named it after his son's toy elephant.[16] The initial code that was factored out of Nutch consisted of 5k lines of code for NDFS and 6k lines of code for MapReduce.\n\nThe first committer added to the Hadoop project was Owen O�Malley in March 2006.[17] Hadoop 0.1.0 was released in April 2006 [18] and continues to evolve by the many contributors[19] to the Apache Hadoop project.\n\nTimeline\nYear\tMonth\tEvent\tRef.\n2003\tOctober\tGoogle File System paper released\t[20]\n2004\tDecember\tMapReduce: Simplified Data Processing on Large Clusters\t[21]\n2006\tJanuary\tHadoop subproject created with mailing lists, jira, and wiki\t[22]\n2006\tJanuary\tHadoop is born from Nutch 197\t[23]\n2006\tFebruary\tNDFS+ MapReduce moved out of Apache Nutch to create Hadoop\t[24]\n2006\tFebruary\tOwen Omalley's first patch goes into Hadoop\t[25]\n2006\tFebruary\tHadoop is named after Cutting's son's yellow plush toy\t[26]\n2006\tApril\tHadoop 0.1.0 released\t[27]\n2006\tApril\tHadoop sorts 1.8TB on 188 nodes in 47.9 hours\t[24]\n2006\tMay\tYahoo deploys 300 machine Hadoop cluster\t[24]\n2006\tOctober\tYahoo Hadoop cluster reaches 600 machines\t[24]\n2007\tApril\tYahoo runs 2 clusters of 1,000 machines\t[24]\n2007\tJune\tOnly 3 companies on \"Powered by Hadoop Page\"\t[28]\n2007\tOctober\tFirst release of Hadoop that includes HBase\t[29]\n2007\tOctober\tYahoo Labs creates Pig, and donates it to the ASF\t[30]\n2008\tJanuary\tYARN JIRA opened\tYarn Jira (Mapreduce 279)\n2008\tJanuary\t20 companies on \"Powered by Hadoop Page\"\t[28]\n2008\tFebruary\tYahoo moves its web index onto Hadoop\t[31]\n2008\tFebruary\tYahoo! production search index generated by a 10,000-core Hadoop cluster\t[24]\n2008\tMarch\tFirst Hadoop Summit\t[32]\n2008\tApril\tHadoop world record fastest system to sort a terabyte of data. Running on a 910-node cluster, Hadoop sorted one terabyte in 209 seconds\t[24]\n2008\tMay\tHadoop wins TeraByte Sort (World Record sortbenchmark.org)\t[33]\n2008\tJuly\tHadoop wins Terabyte Sort Benchmark\t[34]\n2008\tOctober\tLoading 10TB/day in Yahoo clusters\t[24]\n2008\tOctober\tCloudera, Hadoop distributor is founded\t[35]\n2008\tNovember\tGoogle MapReduce implementation sorted one terabyte in 68 seconds\t[24]\n2009\tMarch\tYahoo runs 17 clusters with 24,000 machines\t[24]\n2009\tApril\tHadoop sorts a petabyte\t[36]\n2009\tMay\tYahoo! used Hadoop to sort one terabyte in 62 seconds\t[24]\n2009\tJune\tSecond Hadoop Summit\t[37]\n2009\tJune\tSecond Hadoop Summit\t[38]\n2009\tJuly\tHadoop Core is renamed Hadoop Common\t[39]\n2009\tJuly\tMapR, Hadoop distributor founded\t[40]\n2009\tJuly\tHDFS now a separate subproject\t[39]\n2009\tJuly\tMapReduce now a separate subproject\t[39]\n2010\tJanuary\tKerberos support added to Hadoop\t[41]\n2010\tMay\tApache HBase Graduates\t[42]\n2010\tJune\tThird Hadoop Summit\t[43]\n2010\tJune\tYahoo 4,000 nodes/70 petabytes\t[44]\n2010\tJune\tFacebook 2,300 clusters/40 petabytes\t[44]\n2010\tSeptember\tApache Hive Graduates\t[45]\n2010\tSeptember\tApache Pig Graduates\t[46]\n2011\tJanuary\tApache Zookeeper Graduates\t[47]\n2011\tJanuary\tFacebook, LinkedIn, eBay and IBM collectively contribute 200,000 lines of code\t[48]\n2011\tMarch\tApache Hadoop takes top prize at Media Guardian Innovation Awards\t[49]\n2011\tJune\tRob Beardon and Eric Badleschieler spin out Hortonworks out of Yahoo.\t[50]\n2011\tJune\tYahoo has 42K Hadoop nodes and hundreds of petabytes of storage\t[50]\n2011\tJune\tThird Annual Hadoop Summit (1,700 attendees)\t[51]\n2011\tOctober\tDebate over which company had contributed more to Hadoop.\t[48]\n2012\tJanuary\tHadoop community moves to separate from MapReduce and replace with YARN\t[26]\n2012\tJune\tSan Jose Hadoop Summit (2,100 attendees)\t[52]\n2012\tNovember\tApache Hadoop 1.0 Available\t[39]\n2013\tMarch\tHadoop Summit - Amsterdam (500 attendees)\t[53]\n2013\tMarch\tYARN deployed in production at Yahoo\t[54]\n2013\tJune\tSan Jose Hadoop Summit (2,700 attendees)\t[55]\n2013\tOctober\tApache Hadoop 2.2 Available\t[39]\n2014\tFebruary\tApache Hadoop 2.3 Available\t[39]\n2014\tFebruary\tApache Spark top Level Apache Project\t[56]\n2014\tApril\tHadoop summit Amsterdam (750 attendees)\t[57]\n2014\tJune\tApache Hadoop 2.4 Available\t[39]\n2014\tJune\tSan Jose Hadoop Summit (3,200 attendees)\t[58]\n2014\tAugust\tApache Hadoop 2.5 Available\t[39]\n2014\tNovember\tApache Hadoop 2.6 Available\t[39]\n2015\tApril\tHadoop Summit Europe\t[59]\n2015\tJune\tApache Hadoop 2.7 Available\t[39]\nArchitecture\nSee also: Hadoop Distributed File System, Apache HBase, and MapReduce\nHadoop consists of the Hadoop Common package, which provides filesystem and OS level abstractions, a MapReduce engine (either MapReduce/MR1 or YARN/MR2)[60] and the Hadoop Distributed File System (HDFS). The Hadoop Common package contains the necessary Java ARchive (JAR) files and scripts needed to start Hadoop.\n\nFor effective scheduling of work, every Hadoop-compatible file system should provide location awareness: the name of the rack (more precisely, of the network switch) where a worker node is. Hadoop applications can use this information to execute code on the node where the data is, and, failing that, on the same rack/switch to reduce backbone traffic. HDFS uses this method when replicating data for data redundancy across multiple racks. This approach reduces the impact of a rack power outage or switch failure; if one of these hardware failures occurs, the data will remain available.[61]\n\nHadoop cluster\nA multi-node Hadoop cluster\nA small Hadoop cluster includes a single master and multiple worker nodes. The master node consists of a Job Tracker, Task Tracker, NameNode, and DataNode. A slave or worker node acts as both a DataNode and TaskTracker, though it is possible to have data-only worker nodes and compute-only worker nodes. These are normally used only in nonstandard applications.[62]\n\nHadoop requires Java Runtime Environment (JRE) 1.6 or higher. The standard startup and shutdown scripts require that Secure Shell (ssh) be set up between nodes in the cluster.[63]\n\nIn a larger cluster, HDFS nodes are managed through a dedicated NameNode server to host the file system index, and a secondary NameNode that can generate snapshots of the namenode's memory structures, thereby preventing file-system corruption and loss of data. Similarly, a standalone JobTracker server can manage job scheduling across nodes. When Hadoop MapReduce is used with an alternate file system, the NameNode, secondary NameNode, and DataNode architecture of HDFS are replaced by the file-system-specific equivalents.\n\nFile systems\nHadoop distributed file system\nThe Hadoop distributed file system (HDFS) is a distributed, scalable, and portable file system written in Java for the Hadoop framework. Some consider HDFS to instead be a data store due to its lack of POSIX compliance and inability to be mounted,[64] but it does provide shell commands and Java API methods that are similar to other file systems.[65] A Hadoop cluster has nominally a single namenode plus a cluster of datanodes, although redundancy options are available for the namenode due to its criticality. Each datanode serves up blocks of data over the network using a block protocol specific to HDFS. The file system uses TCP/IP sockets for communication. Clients use remote procedure call (RPC) to communicate between each other.\n\nHDFS stores large files (typically in the range of gigabytes to terabytes[66]) across multiple machines. It achieves reliability by replicating the data across multiple hosts, and hence theoretically does not require RAID storage on hosts (but to increase I/O performance some RAID configurations are still useful). With the default replication value, 3, data is stored on three nodes: two on the same rack, and one on a different rack. Data nodes can talk to each other to rebalance data, to move copies around, and to keep the replication of data high. HDFS is not fully POSIX-compliant, because the requirements for a POSIX file-system differ from the target goals for a Hadoop application. The trade-off of not having a fully POSIX-compliant file-system is increased performance for data throughput and support for non-POSIX operations such as Append.[67]\n\nHDFS added the high-availability capabilities, as announced for release 2.0 in May 2012,[68] letting the main metadata server (the NameNode) fail over manually to a backup. The project has also started developing automatic fail-over.\n\nThe HDFS file system includes a so-called secondary namenode, a misleading name that some might incorrectly interpret as a backup namenode for when the primary namenode goes offline. In fact, the secondary namenode regularly connects with the primary namenode and builds snapshots of the primary namenode's directory information, which the system then saves to local or remote directories. These checkpointed images can be used to restart a failed primary namenode without having to replay the entire journal of file-system actions, then to edit the log to create an up-to-date directory structure. Because the namenode is the single point for storage and management of metadata, it can become a bottleneck for supporting a huge number of files, especially a large number of small files. HDFS Federation, a new addition, aims to tackle this problem to a certain extent by allowing multiple namespaces served by separate namenodes. Moreover, there are some issues in HDFS, namely, small file issue, scalability problem, Single Point of Failure (SPoF), and bottleneck in huge metadata request. An advantage of using HDFS is data awareness between the job tracker and task tracker. The job tracker schedules map or reduce jobs to task trackers with an awareness of the data location. For example: if node A contains data (x,y,z) and node B contains data (a,b,c), the job tracker schedules node B to perform map or reduce tasks on (a,b,c) and node A would be scheduled to perform map or reduce tasks on (x,y,z). This reduces the amount of traffic that goes over the network and prevents unnecessary data transfer. When Hadoop is used with other file systems, this advantage is not always available. This can have a significant impact on job-completion times, which has been demonstrated when running data-intensive jobs.[69]\n\nHDFS was designed for mostly immutable files[67] and may not be suitable for systems requiring concurrent write-operations.\n\nHDFS can be mounted directly with a Filesystem in Userspace (FUSE) virtual file system on Linux and some other Unix systems.\n\nFile access can be achieved through the native Java application programming interface (API), the Thrift API to generate a client in the language of the users' choosing (C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, Smalltalk, and OCaml), the command-line interface, browsed through the HDFS-UI Web application (webapp) over HTTP, or via 3rd-party network client libraries.[70]\n\nOther file systems\nHadoop works directly with any distributed file system that can be mounted by the underlying operating system simply by using a file:// URL; however, this comes at a price: the loss of locality. To reduce network traffic, Hadoop needs to know which servers are closest to the data; this is information that Hadoop-specific file system bridges can provide.\n\nIn May 2011, the list of supported file systems bundled with Apache Hadoop were:\n\nHDFS: Hadoop's own rack-aware file system.[71] This is designed to scale to tens of petabytes of storage and runs on top of the file systems of the underlying operating systems.\nFTP File system: this stores all its data on remotely accessible FTP servers.\nAmazon S3 (Simple Storage Service) file system. This is targeted at clusters hosted on the Amazon Elastic Compute Cloud server-on-demand infrastructure. There is no rack-awareness in this file system, as it is all remote.\nWindows Azure Storage Blobs (WASB) file system. WASB, an extension on top of HDFS, allows distributions of Hadoop to access data in Azure blob stores without moving the data permanently into the cluster.\nA number of third-party file system bridges have also been written, none of which are currently in Hadoop distributions. However, some commercial distributions of Hadoop ship with an alternative filesystem as the default�specifically IBM and MapR.\n\nIn 2009, IBM discussed running Hadoop over the IBM General Parallel File System.[72] The source code was published in October 2009.[73]\nIn April 2010, Parascale published the source code to run Hadoop against the Parascale file system.[74]\nIn April 2010, Appistry released a Hadoop file system driver for use with its own CloudIQ Storage product.[75]\nIn June 2010, HP discussed a location-aware IBRIX Fusion file system driver.[76]\nIn May 2011, MapR Technologies, Inc. announced the availability of an alternative file system for Hadoop, MapR FS, which replaced the HDFS file system with a full random-access read/write file system.\nJobTracker and TaskTracker: the MapReduce engine\nMain article: MapReduce\nAbove the file systems comes the MapReduce Engine, which consists of one JobTracker, to which client applications submit MapReduce jobs. The JobTracker pushes work out to available TaskTracker nodes in the cluster, striving to keep the work as close to the data as possible. With a rack-aware file system, the JobTracker knows which node contains the data, and which other machines are nearby. If the work cannot be hosted on the actual node where the data resides, priority is given to nodes in the same rack. This reduces network traffic on the main backbone network. If a TaskTracker fails or times out, that part of the job is rescheduled. The TaskTracker on each node spawns a separate Java Virtual Machine process to prevent the TaskTracker itself from failing if the running job crashes its JVM. A heartbeat is sent from the TaskTracker to the JobTracker every few minutes to check its status. The Job Tracker and TaskTracker status and information is exposed by Jetty and can be viewed from a web browser.\n\nKnown limitations of this approach are:\n\nThe allocation of work to TaskTrackers is very simple. Every TaskTracker has a number of available slots (such as \"4 slots\"). Every active map or reduce task takes up one slot. The Job Tracker allocates work to the tracker nearest to the data with an available slot. There is no consideration of the current system load of the allocated machine, and hence its actual availability.\nIf one TaskTracker is very slow, it can delay the entire MapReduce job�especially towards the end of a job, where everything can end up waiting for the slowest task. With speculative execution enabled, however, a single task can be executed on multiple slave nodes.\nScheduling\nBy default Hadoop uses FIFO scheduling, and optionally 5 scheduling priorities to schedule jobs from a work queue.[77] In version 0.19 the job scheduler was refactored out of the JobTracker, while adding the ability to use an alternate scheduler (such as the Fair scheduler or the Capacity scheduler, described next).[78]\n\nFair scheduler\nThe fair scheduler was developed by Facebook.[79] The goal of the fair scheduler is to provide fast response times for small jobs and QoS for production jobs. The fair scheduler has three basic concepts.[80]\n\nJobs are grouped into pools.\nEach pool is assigned a guaranteed minimum share.\nExcess capacity is split between jobs.\nBy default, jobs that are uncategorized go into a default pool. Pools have to specify the minimum number of map slots, reduce slots, and a limit on the number of running jobs.\n\nCapacity scheduler\nThe capacity scheduler was developed by Yahoo. The capacity scheduler supports several features that are similar to the fair scheduler.[81]\n\nQueues are allocated a fraction of the total resource capacity.\nFree resources are allocated to queues beyond their total capacity.\nWithin a queue a job with a high level of priority has access to the queue's resources.\nThere is no preemption once a job is running.\n\nOther applications\nThe HDFS file system is not restricted to MapReduce jobs. It can be used for other applications, many of which are under development at Apache. The list includes the HBase database, the Apache Mahout machine learning system, and the Apache Hive Data Warehouse system. Hadoop can in theory be used for any sort of work that is batch-oriented rather than real-time, is very data-intensive, and benefits from parallel processing of data. It can also be used to complement a real-time system, such as lambda architecture.\n\nAs of October 2009, commercial applications of Hadoop[82] included:\n\nLog and/or clickstream analysis of various kinds\nMarketing analytics\nMachine learning and/or sophisticated data mining\nImage processing\nProcessing of XML messages\nWeb crawling and/or text processing\nGeneral archiving, including of relational/tabular data, e.g. for compliance\nProminent users\nOn February 19, 2008, Yahoo! Inc. launched what it claimed was the world's largest Hadoop production application. The Yahoo! Search Webmap is a Hadoop application that runs on a Linux cluster with more than 10,000 cores and produced data that was used in every Yahoo! web search query.[83] There are multiple Hadoop clusters at Yahoo! and no HDFS file systems or MapReduce jobs are split across multiple datacenters. Every Hadoop cluster node bootstraps the Linux image, including the Hadoop distribution. Work that the clusters perform is known to include the index calculations for the Yahoo! search engine. In June 2009, Yahoo! made the source code of the Hadoop version it runs available to the public via the open-source community.[84]\n\nIn 2010, Facebook claimed that they had the largest Hadoop cluster in the world with 21 PB of storage.[85] In June 2012, they announced the data had grown to 100 PB[86] and later that year they announced that the data was growing by roughly half a PB per day.[87]\n\nAs of 2013, Hadoop adoption had become widespread: more than half of the Fortune 50 used Hadoop.[88]\n\nHadoop hosting in the Cloud\nHadoop can be deployed in a traditional onsite datacenter as well as in the cloud.[89] The cloud allows organizations to deploy Hadoop without hardware to acquire or specific setup expertise.[90] Vendors who currently have an offer for the cloud include Microsoft, Amazon, IBM,[91] Google and Oracle.[92]\n\nOn Microsoft Azure\nAzure HDInsight[93] is a service that deploys Hadoop on Microsoft Azure. HDInsight uses Hortonworks HDP and was jointly developed for HDI with Hortonworks. HDI allows programming extensions with .NET (in addition to Java). HDInsight also supports creation of Hadoop clusters using Linux with Ubuntu.[93] By deploying HDInsight in the cloud, organizations can spin up the number of nodes they want and only get charged for the compute and storage that is used.[93] Hortonworks implementations can also move data from the on-premises datacenter to the cloud for backup, development/test, and bursting scenarios.[93] It is also possible to run Cloudera or Hortonworks Hadoop clusters on Azure Virtual Machines.\n\nOn Amazon EC2/S3 services\nIt is possible to run Hadoop on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3).[94] As an example, The New York Times used 100 Amazon EC2 instances and a Hadoop application to process 4 TB of raw image TIFF data (stored in S3) into 11 million finished PDFs in the space of 24 hours at a computation cost of about $240 (not including bandwidth).[95]\n\nThere is support for the S3 object store in the Apache Hadoop releases, though this is below what one expects from a traditional POSIX filesystem. Specifically, operations such as rename() and delete() on directories are not atomic, and can take time proportional to the number of entries and the amount of data in them.\n\nAmazon Elastic MapReduce\nElastic MapReduce (EMR)[96] was introduced by Amazon.com in April 2009. Provisioning of the Hadoop cluster, running and terminating jobs, and handling data transfer between EC2(VM) and S3(Object Storage) are automated by Elastic MapReduce. Apache Hive, which is built on top of Hadoop for providing data warehouse services, is also offered in Elastic MapReduce.[97]\n\nSupport for using Spot Instances[98] was later added in August 2011.[99] Elastic MapReduce is fault-tolerant for slave failures,[100] and it is recommended to only run the Task Instance Group on spot instances to take advantage of the lower cost while maintaining availability.[101]\n\nOn CenturyLink Cloud (CLC)\nCenturyLink Cloud [102] offers Hadoop via both a managed and un-managed model via their Hadoop[103] offering. CLC also offers customers several managed Cloudera Blueprints, the newest managed service in the CenturyLink Cloud big data Blueprints portfolio, which also includes Cassandra and MongoDB solutions.[104]\n\nGoogle Cloud Platform\nThere are multiple ways to run the Hadoop ecosystem on Google Cloud Platform ranging from self-managed to Google-managed.[105]\n\nGoogle Cloud Dataproc � A managed Spark and Hadoop service[106]\nCommand line tools (bdutil) � A collection of shell scripts to manually create and manage Spark and Hadoop clusters[107]\nThird party Hadoop distributions:\nCloudera � Using the Cloudera Director Plugin for Google Cloud Platform[108]\nHortonworks � Using bdutil support for Hortonworks HDP[109]\nMapR � Using bdutil support for MapR[110]\nGoogle also offers connectors for using other Google Cloud Platform products with Hadoop, such as a Google Cloud Storage connector for using Google Cloud Storage and a Google BigQuery connector for using Google BigQuery.\n\nCommercial support\nA number of companies offer commercial implementations or support for Hadoop.[111]\n\nASF's view on the use of \"Hadoop\" in product names\nThe Apache Software Foundation has stated that only software officially released by the Apache Hadoop Project can be called Apache Hadoop or Distributions of Apache Hadoop.[112] The naming of products and derivative works from other vendors and the term \"compatible\" are somewhat controversial within the Hadoop developer community.[113]\n\nPapers\nSome papers influenced the birth and growth of Hadoop and big data processing. Here is a partial list:\n\nJeffrey Dean, Sanjay Ghemawat (2004) MapReduce: Simplified Data Processing on Large Clusters, Google. This paper inspired Doug Cutting to develop an open-source implementation of the Map-Reduce framework. He named it Hadoop, after his son's toy elephant.\nMichael Franklin, Alon Halevy, David Maier (2005) From Databases to Dataspaces: A New Abstraction for Information Management. The authors highlight the need for storage systems to accept all data formats and to provide APIs for data access that evolve based on the storage system�s understanding of the data.\nFay Chang et al. (2006) Bigtable: A Distributed Storage System for Structured Data, Google.\nRobert Kallman et al. (2008) H-store: a high-performance, distributed main memory transaction processing system\nSee also\nPortal icon\tFree software portal\nApache Accumulo � Secure BigTable[114]\nApache Cassandra � A column-oriented database that supports access from Hadoop\nApache CouchDB is a database that uses JSON for documents, JavaScript for MapReduce queries, and regular HTTP for an API\nBig data\nCloud computing\nData Intensive Computing\nHPCC � LexisNexis Risk Solutions High Performance Computing Cluster\nHypertable � HBase alternative\nSector/Sphere � Open source distributed storage and processing\nSimple Linux Utility for Resource Management", "skillName": "Hadoop."}
{"id": 65, "category": "Distributed", "skillText": "Cloud computing\nCloud computing metaphor: For a user, the network elements representing the provider-rendered services are invisible, as if obscured by a cloud.\nCloud computing is a kind of Internet-based computing that provides shared processing resources and data to computers and other devices on demand. It is a model for enabling ubiquitous, on-demand access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services),[1][2] which can be rapidly provisioned and released with minimal management effort. Cloud computing and storage solutions provide users and enterprises with various capabilities to store and process their data in third-party data centers.[3] It relies on sharing of resources to achieve coherence and economy of scale, similar to a utility (like the electricity grid) over a network.\n\nAdvocates claim that cloud computing allows companies to avoid upfront infrastructure costs, and focus on projects that differentiate their businesses instead of on infrastructure.[4] Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and enables IT to more rapidly adjust resources to meet fluctuating and unpredictable business demand.[4][5][6] Cloud providers typically use a \"pay as you go\" model. This will lead to unexpectedly high charges if administrators do not adapt to the cloud pricing model.[7]\n\nThe present availability of high-capacity networks, low-cost computers and storage devices as well as the widespread adoption of hardware virtualization, service-oriented architecture, and autonomic and utility computing have led to a growth in cloud computing.[8][9][10] Companies can scale up as computing needs increase and then scale down again as demands decrease.\n\nCloud computing has become a highly demanded service or utility due to the advantages of high computing power, cheap cost of services, high performance, scalability, accessibility as well as availability. Some cloud vendors are experiencing growth rates of 50% per year,[11] but being still in a stage of infancy, it has pitfalls that need to be addressed to make cloud computing services more reliable and user friendly.[12][13]\n\n1\tHistory of cloud computing\n1.1\tOrigin of the term\n1.2\tThe 1970s\n1.3\tThe 1990s\n1.4\t2000s\n2\tSimilar concepts\n3\tCharacteristics\n4\tService models\n4.1\tInfrastructure as a service (IaaS)\n4.2\tPlatform as a service (PaaS)\n4.3\tSoftware as a service (SaaS)\n5\tCloud clients\n6\tDeployment models\n6.1\tPrivate cloud\n6.2\tPublic cloud\n6.3\tHybrid cloud\n6.4\tOthers\n6.4.1\tCommunity cloud\n6.4.2\tDistributed cloud\n6.4.3\tIntercloud\n6.4.4\tMulticloud\n7\tArchitecture\n7.1\tCloud engineering\n8\tSecurity and privacy\n9\tLimitations\n10\tThe future\n11\tSee also\n12\tReferences\n13\tFurther reading\n14\tExternal links\nHistory of cloud computing\nOrigin of the term\nThe origin of the term cloud computing in computing is unclear. The word \"cloud\" is commonly used in science to describe a large agglomeration of objects that visually appear from a distance as a cloud and describes any set of things whose details are not further inspected in a given context.[14] Another explanation is that the old programs that drew network schematics surrounded the icons for servers with a circle, and a cluster of servers in a network diagram had several overlapping circles, which resembled a cloud.[15]\n\nIn analogy to the above usage, the word cloud was used as a metaphor for the Internet and a standardized cloud-like shape was used to denote a network on telephony schematics. Later it was used to depict the Internet in computer network diagrams. With this simplification, the implication is that the specifics of how the end points of a network are connected are not relevant for the purposes of understanding the diagram. The cloud symbol was used to represent networks of computing equipment in the original ARPANET by as early as 1977,[16] and the CSNET by 1981[17]�both predecessors to the Internet itself.\n\nThe term cloud has been used to refer to platforms for distributed computing. In Wired's April 1994 feature \"Bill and Andy's Excellent Adventure II\" on the Apple spin-off General Magic, Andy Hertzfeld commented on General Magic's distributed programming language Telescript that:\n\n\"The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create sort of a virtual service. No one had conceived that before. The example Jim White [the designer of Telescript, X.400 and ASN.1] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties.\"\n\n�?[18]\nReferences to \"cloud computing\" in its modern sense appeared as early as 1996, with the earliest known mention in a Compaq internal document.[19]\n\nThe popularization of the term can be traced to 2006 when Amazon.com introduced its Elastic Compute Cloud.[20]\n\nThe 1970s\nDuring the 1960s, the initial concepts of time-sharing became popularized via RJE (Remote Job Entry);[21] this terminology was mostly associated with large vendors such as IBM and DEC. Full time-sharing solutions were available by the early 1970s on such platforms as Multics (on GE hardware), Cambridge CTSS, and the earliest UNIX ports (on DEC hardware). Yet, the \"data center\" model where users submitted jobs to operators to run on IBM mainframes was overwhelmingly predominant.\n\nThe 1990s\nIn the 1990s, telecommunications companies, who previously offered primarily dedicated point-to-point data circuits, began offering virtual private network (VPN) services with comparable quality of service, but at a lower cost. By switching traffic as they saw fit to balance server use, they could use overall network bandwidth more effectively.[citation needed] They began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for. Cloud computing extended this boundary to cover all servers as well as the network infrastructure.[22]\n\nAs computers became more diffused, scientists and technologists explored ways to make large-scale computing power available to more users through time-sharing.[citation needed] They experimented with algorithms to optimize the infrastructure, platform, and applications to prioritize CPUs and increase efficiency for end users.[23]\n\n2000s\nSince 2000, cloud computing has come into existence. In early 2008, NASA's OpenNebula, enhanced in the RESERVOIR European Commission-funded project, became the first open-source software for deploying private and hybrid clouds, and for the federation of clouds.[24] In the same year, efforts were focused on providing quality of service guarantees (as required by real-time interactive applications) to cloud-based infrastructures, in the framework of the IRMOS European Commission-funded project, resulting in a real-time cloud environment.[25][26] By mid-2008, Gartner saw an opportunity for cloud computing \"to shape the relationship among consumers of IT services, those who use IT services and those who sell them\"[27] and observed that \"organizations are switching from company-owned hardware and software assets to per-use service-based models\" so that the \"projected shift to computing ... will result in dramatic growth in IT products in some areas and significant reductions in other areas.\"[28]\n\nIn August 2006 Amazon introduced its Elastic Compute Cloud.[20] Microsoft Azure was announced as \"Azure\" in October 2008 and was released on 1 February 2010 as Windows Azure, before being renamed to Microsoft Azure on 25 March 2014.[29] For a time, Azure was on the TOP500 supercomputer list, before it dropped off it.[30]\n\nIn July 2010, Rackspace Hosting and NASA jointly launched an open-source cloud-software initiative known as OpenStack. The OpenStack project intended to help organizations offering cloud-computing services running on standard hardware. The early code came from NASA's Nebula platform as well as from Rackspace's Cloud Files platform.\n\nOn March 1, 2011, IBM announced the IBM SmartCloud framework to support Smarter Planet.[31] Among the various components of the Smarter Computing foundation, cloud computing is a critical part.\n\nOn June 7, 2012, Oracle announced the Oracle Cloud.[32] While aspects of the Oracle Cloud are still in development, this cloud offering is poised to be the first to provide users with access to an integrated set of IT solutions, including the Applications (SaaS), Platform (PaaS), and Infrastructure (IaaS) layers.[33][34][35]\n\nSimilar concepts\nCloud computing is the result of the evolution and adoption of existing technologies and paradigms. The goal of cloud computing is to allow users to take bene?t from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs, and helps the users focus on their core business instead of being impeded by IT obstacles.[36]\n\nThe main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system�level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations, and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.[36]\n\nUsers routinely face difficult business problems. Cloud computing adopts concepts from Service-oriented Architecture (SOA) that can help the user break these problems into services that can be integrated to provide a solution. Cloud computing provides all of its resources as services, and makes use of the well-established standards and best practices gained in the domain of SOA to allow global and easy access to cloud services in a standardized way.\n\nCloud computing also leverages concepts from utility computing to provide metrics for the services used. Such metrics are at the core of the public cloud pay-per-use models. In addition, measured services are an essential part of the feedback loop in autonomic computing, allowing services to scale on-demand and to perform automatic failure recovery.\n\nCloud computing is a kind of grid computing; it has evolved by addressing the QoS (quality of service) and reliability problems. Cloud computing provides the tools and technologies to build data/compute intensive parallel applications with much more affordable prices compared to traditional parallel computing techniques.[36]\n\nCloud computing shares characteristics with:\n\nClient�server model�Client�server computing refers broadly to any distributed application that distinguishes between service providers (servers) and service requestors (clients).[37]\nGrid computing�\"A form of distributed and parallel computing, whereby a 'super and virtual computer' is composed of a cluster of networked, loosely coupled computers acting in concert to perform very large tasks.\"\nFog computing�Distributed computing paradigm that provides data, compute, storage and application services closer to client or near-user edge devices, such as network routers. Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client side (e.g. mobile devices), instead of sending data to a remote location for processing.\nDew computing�In the existing computing hierarchy, the Dew computing is positioned as the ground level for the cloud and fog computing paradigms. Compared to fog computing, which supports emerging IoT applications that demand real-time and predictable latency and the dynamic network reconfigurability, Dew computing pushes the frontiers to computing applications, data, and low level services away from centralized virtual nodes to the end users.[38]\nMainframe computer�Powerful computers used mainly by large organizations for critical applications, typically bulk data processing such as: census; industry and consumer statistics; police and secret intelligence services; enterprise resource planning; and financial transaction processing.\nUtility computing�The \"packaging of computing resources, such as computation and storage, as a metered service similar to a traditional public utility, such as electricity.\"[39][40]\nPeer-to-peer�A distributed architecture without the need for central coordination. Participants are both suppliers and consumers of resources (in contrast to the traditional client�server model).\nGreen computing\nCharacteristics\nCloud computing exhibits the following key characteristics:\n\nAgility improves with users' ability to re-provision technological infrastructure resources.\nCost reductions claimed by cloud providers. A public-cloud delivery model converts capital expenditure to operational expenditure.[41] This purportedly lowers barriers to entry, as infrastructure is typically provided by a third party and need not be purchased for one-time or infrequent intensive computing tasks. Pricing on a utility computing basis is fine-grained, with usage-based options and fewer IT skills are required for implementation (in-house).[42] The e-FISCAL project's state-of-the-art repository[43] contains several articles looking into cost aspects in more detail, most of them concluding that costs savings depend on the type of activities supported and the type of infrastructure available in-house.\nDevice and location independence[44] enable users to access systems using a web browser regardless of their location or what device they use (e.g., PC, mobile phone). As infrastructure is off-site (typically provided by a third-party) and accessed via the Internet, users can connect from anywhere.[42]\nMaintenance of cloud computing applications is easier, because they do not need to be installed on each user's computer and can be accessed from different places.\nMultitenancy enables sharing of resources and costs across a large pool of users thus allowing for:\ncentralization of infrastructure in locations with lower costs (such as real estate, electricity, etc.)\npeak-load capacity increases (users need not engineer for highest possible load-levels)\nutilisation and efficiency improvements for systems that are often only 10�20% utilised.[45][46]\nPerformance is monitored, and consistent and loosely coupled architectures are constructed using web services as the system interface.[42][47][48]\nProductivity may be increased when multiple users can work on the same data simultaneously, rather than waiting for it to be saved and emailed. Time may be saved as information does not need to be re-entered when fields are matched, nor do users need to install application software upgrades to their computer.[49]\nReliability improves with the use of multiple redundant sites, which makes well-designed cloud computing suitable for business continuity and disaster recovery.[50]\nScalability and elasticity via dynamic (\"on-demand\") provisioning of resources on a fine-grained, self-service basis in near real-time[51][52] (Note, the VM startup time varies by VM type, location, OS and cloud providers[51]), without users having to engineer for peak loads.[53][54][55] This gives the ability to scale up when the usage need increases or down if resources are not being used.[56]\nSecurity can improve due to centralization of data, increased security-focused resources, etc., but concerns can persist about loss of control over certain sensitive data, and the lack of security for stored kernels. Security is often as good as or better than other traditional systems, in part because providers are able to devote resources to solving security issues that many customers cannot afford to tackle.[57] However, the complexity of security is greatly increased when data is distributed over a wider area or over a greater number of devices, as well as in multi-tenant systems shared by unrelated users. In addition, user access to security audit logs may be difficult or impossible. Private cloud installations are in part motivated by users' desire to retain control over the infrastructure and avoid losing control of information security.\nThe National Institute of Standards and Technology's definition of cloud computing identifies \"five essential characteristics\":\n\nOn-demand self-service. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.\n\nBroad network access. Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).\n\nResource pooling. The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.\n\nRapid elasticity. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.\n\nMeasured service. Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.\n�?National Institute of Standards and Technology[58]\nService models\nThough service-oriented architecture advocates \"everything as a service\" (with the acronyms EaaS or XaaS or simply aas),[59] cloud-computing providers offer their \"services\" according to different models,[58][60][need quotation to verify] which happen to form a stack: infrastructure-, platform- and software-as-a-service.[61]\n\n\nCloud-computing layers accessible within a stack\nInfrastructure as a service (IaaS)\nSee also: Category:Cloud infrastructure\nAccording to the Internet Engineering Task Force (IETF), the most basic cloud-service model is that of providers offering computing infrastructure � virtual machines and other resources � as a service to subscribers. Infrastructure as a service (IaaS) refers to online services that abstract the user from the details of infrastructure like physical computing resources, location, data partitioning, scaling, security, backup etc. A hypervisor, such as Xen, Oracle VirtualBox, Oracle VM, KVM, VMware ESX/ESXi, or Hyper-V, runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. Containerisation offers higher performance than virtualization, because there is no hypervisor overhead. Also, container capacity auto-scales dynamically with computing load, which eliminates the problem of over-provisioning and enables usage-based billing.[62] IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.[63] IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks).\n\nTo deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure.[64][unreliable source?] In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the amount of resources allocated and consumed.[65][66][67][68]\n\nPlatform as a service (PaaS)\nMain article: Platform as a service\nSee also: Category:Cloud platforms\nPaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a computing platform, typically including operating system, programming-language execution environment, database, and web server. Application developers can develop and run their software solutions on a cloud platform without the cost and complexity of buying and managing the underlying hardware and software layers. With some PaaS offers like Microsoft Azure and Google App Engine, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually. The latter has also been proposed by an architecture aiming to facilitate real-time in cloud environments.[69][need quotation to verify] Even more specific application types can be provided via PaaS, such as media encoding as provided by services like bitcodin.com[70] or media.io.[71]\n\nSome integration and data management providers have also embraced specialized applications of PaaS as delivery models for data solutions. Examples include iPaaS and dPaaS. iPaaS (Integration Platform as a Service) enables customers to develop, execute and govern integration flows.[72] Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.[73] dPaaS (Data Platform as a Service) delivers integration�and data-management�products as a fully managed service.[74] Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of data solutions by building tailored data applications for the customer. dPaaS users retain transparency and control over data through data-visualization tools.[75]\n\nPlatform as a Service (PaaS) consumers do not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but have control over the deployed applications and possibly configuration settings for the application-hosting environment.\n\nSoftware as a service (SaaS)\nMain article: Software as a service\nIn the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as \"on-demand software\" and is usually priced on a pay-per-use basis or using a subscription fee.[citation needed]\n\nIn the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability�which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand.[76] Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.\n\nThe pricing model for SaaS applications is typically a monthly or yearly flat fee per user,[77] so prices become scalable and adjustable if users are added or removed at any point.[78]\n\nProponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result,[citation needed] there could be unauthorized access to the data. For this reason, users are increasingly[quantify] adopting intelligent third-party key-management systems to help secure their data.[citation needed]\n\nCloud clients\nSee also: Category:Cloud clients and Cloud API\nUsers access cloud computing using networked client devices, such as desktop computers, laptops, tablets and smartphones and any Ethernet enabled device such as Home Automation Gadgets. Some of these devices�cloud clients�rely on cloud computing for all or a majority of their applications so as to be essentially useless without it. Examples are thin clients and the browser-based Chromebook. Many cloud applications do not require specific software on the client and instead use a web browser to interact with the cloud application. With Ajax and HTML5 these Web user interfaces can achieve a similar, or even better, look and feel to native applications. Some cloud applications, however, support specific client software dedicated to these applications (e.g., virtual desktop clients and most email clients). Some legacy applications (line of business applications that until now have been prevalent in thin client computing) are delivered via a screen-sharing technology.\n\nDeployment models\n\nCloud computing types\nPrivate cloud\nPrivate cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third-party, and hosted either internally or externally.[58] Undertaking a private cloud project requires a significant level and degree of engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. When done right, it can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers[79] are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users \"still have to buy, build, and manage them\" and thus do not benefit from less hands-on management,[80] essentially \"[lacking] the economic model that makes cloud computing such an intriguing concept\".[81][82]\n\nPublic cloud\nA cloud is called a \"public cloud\" when the services are rendered over a network that is open for public use. Public cloud services may be free.[83] Technically there may be little or no difference between public and private cloud architecture, however, security consideration may be substantially different for services (applications, storage, and other resources) that are made available by a service provider for a public audience and when communication is effected over a non-trusted network. Generally, public cloud service providers like Amazon Web Services (AWS), Microsoft and Google own and operate the infrastructure at their data center and access is generally via the Internet. AWS and Microsoft also offer direct connect services called \"AWS Direct Connect\" and \"Azure ExpressRoute\" respectively, such connections require customers to purchase or lease a private connection to a peering point offered by the cloud provider.[42]\n\nHybrid cloud\nHybrid cloud is a composition of two or more clouds (private, community or public) that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.[58]\n\nGartner, Inc. defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.[84] A hybrid cloud service crosses isolation and provider boundaries so that it can't be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.\n\nVaried use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.[85] This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.[86]\n\nAnother example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.[87] This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.[58] Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and \"bursts\" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization only pays for extra compute resources when they are needed.[88] Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.[89]\n\nThe specialized model of hybrid cloud, which is built atop heterogeneous hardware, is called \"Cross-platform Hybrid Cloud\". A cross-platform hybrid cloud is usually powered by different CPU architectures, for example, x86-64 and ARM, underneath. Users can transparently deploy and scale applications without knowledge of the cloud's hardware diversity.[90] This kind of cloud emerges from the raise of ARM-based system-on-chip for server-class computing.\n\nOthers\nCommunity cloud\nCommunity cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party, and either hosted internally or externally. The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.[58]\n\nDistributed cloud\nA cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud.\n\nPublic-resource computing�This type of distributed cloud results from an expansive definition of cloud computing, because they are more akin to distributed computing than cloud computing. Nonetheless, it is considered a sub-class of cloud computing, and some examples include distributed computing platforms such as BOINC and Folding@Home.\nVolunteer cloud�Volunteer cloud computing is characterized as the intersection of public-resource computing and cloud computing, where a cloud computing infrastructure is built using volunteered resources. Many challenges arise from this type of infrastructure, because of the volatility of the resources used to built it and the dynamic environment it operates in. It can also be called peer-to-peer clouds, or ad-hoc clouds. An interesting effort in such direction is Cloud@Home, it aims to implement a cloud computing infrastructure using volunteered resources providing a business-model to incentivize contributions through financial restitution.[91]\nIntercloud\nMain article: Intercloud\nThe Intercloud[92] is an interconnected global \"cloud of clouds\"[93][94] and an extension of the Internet \"network of networks\" on which it is based. The focus is on direct interoperability between public cloud service providers, more so than between providers and consumers (as is the case for hybrid- and multi-cloud).[95][96][97]\n\nMulticloud\nMain article: Multicloud\nMulticloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).[98][99][100]\n\nArchitecture\n\nCloud computing sample architecture\nCloud architecture,[101] the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.\n\nCloud engineering\nCloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization, and governance in conceiving, developing, operating and maintaining cloud computing systems. It is a multidisciplinary method encompassing contributions from diverse areas such as systems, software, web, performance, information, security, platform, risk, and quality engineering.\n\nSecurity and privacy\nMain article: Cloud computing issues\nCloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time. It could accidentally or deliberately alter or even delete information.[102] Many cloud providers can share information with third parties if necessary for purposes of law and order even without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end users' choices for how data is stored.[102] Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.[3][102]\n\nAccording to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and API's, Data Loss & Leakage, and Hardware Failure�which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users there may be a possibility that information belonging to different customers resides on same data server. Therefore, Information leakage may arise by mistake when information for one customer is given to other.[103] Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. \"There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into\". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack�a process he called \"hyperjacking\". Some examples of this include the Dropbox security breach, and iCloud 2014 leak.[104] Dropbox had been breached in October 2014, having over 7 million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).[104]\n\nThere is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership.[105]\n\nPhysical control of the computer equipment (private cloud) is more secure than having the equipment off site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.[106] Some small businesses that don't have expertise in IT security could find that it's more secure for them to use a public cloud.\n\nThere is the risk that end users don't understand the issues involved when signing on to a cloud service (persons sometimes don't read the many pages of the terms of service agreement, and just click \"Accept\" without reading). This is important now that cloud computing is becoming popular and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Now).\n\nFundamentally private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.[107]\n\nLimitations\nAccording to Bruce Schneier, \"The downside is that you will have limited customization options. Cloud computing is cheaper because of economics of scale, and like any outsourced task, you tend to get what you get. A restaurant with a limited menu is cheaper than a personal chef who can cook anything you want. Fewer options at a much cheaper price: it's a feature, not a bug and the cloud provider might not meet your legal needs. As a business, you need to weigh the benefits against the risks.\"[108]\n\nThe future\nCloud computing is therefore still as much a research topic, as it is a market offering.[109] What is clear through the evolution of cloud computing services is that the chief technical officer (CTO) is a major driving force behind cloud adoption.[110] The major cloud technology developers continue to invest billions a year in cloud R&D; for example: in 2011 Microsoft committed 90% of its US$9.6bn R&D budget to its cloud.[111] Centaur Partners also predict that SaaS revenue will grow from US$13.5B in 2011 to $32.8B in 2016.[112] This expansion also includes Finance and Accounting SaaS.[113] Additionally, more industries are turning to cloud technology as an efficient way to improve quality services due to its capabilities to reduce overhead costs, downtime, and automate infrastructure deployment.[114]\n\nPortal icon\tComputer networking portal\nCategory: Cloud computing providers\nCategory: Cloud platforms\nCloud computing security\nCloud computing comparison\nCloud management\nCloud research\nCloud storage\nEdge computing\neScience\nMobile cloud computing\nPersonal cloud\nRobot as a Service\nService-Oriented Architecture\nUbiquitous computing\nWeb computing", "skillName": "Cloud Computing."}
{"id": 66, "category": "statistical_software", "skillText": "SAS (Statistical Analysis System)[1] is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.\n\nSAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.\n\nContents\n\n    1 Technical overview and terminology\n    2 History\n        2.1 Origins\n        2.2 Development\n        2.3 Recent history\n    3 Software products\n        3.1 Comparison to other products\n    4 Adoption\n    5 See also\n    6 References\n    7 Further reading\n    8 External links\n\nTechnical overview and terminology\n\nSAS is a software suite that can mine, alter, manage and retrieve data from a variety of sources and perform statistical analysis on it.[2] SAS provides a graphical point-and-click user interface for non-technical users and more advanced options through the SAS programming language.[2] In order to use Statistical Analysis System, Data should be in an Excel table format or SAS format.[3] SAS programs have a DATA step, which retrieves and manipulates data, usually creating a SAS data set, and a PROC step, which analyzes the data.[4]\n\nEach step consists of a series of statements.[5] The DATA step has executable statements that result in the software taking an action, and declarative statements that provide instructions to read a data set or alter the data's appearance.[4] The DATA step has two phases, compilation and execution. In the compilation phase, declarative statements are processed and syntax errors are identified. Afterwards, the execution phase processes each executable statement sequentially.[6] Data sets are organized into tables with rows called \"observations\" and columns called \"variables\". Additionally, each piece of data has a descriptor and a value.[4][7]\n\nThe PROC step consists of PROC statements that call upon named procedures. Procedures perform analysis and reporting on data sets to produce statistics, analyses and graphics. There are more than 300 procedures and each one contains a substantial body of programming and statistical work.[4] PROC statements can also display results, sort data or perform other operations.[5] SAS Macros are pieces of code or variables that are coded once and referenced to perform repetitive tasks.[8]\n\nSAS data can be published in HTML, PDF, Excel and other formats using the Output Delivery System, which was first introduced in 2007.[9] The SAS Enterprise Guide is SAS' point-and-click interface. It generates code to manipulate data or perform analysis automatically and does not require SAS programming experience to use.[10]\n\nThe SAS software suite has more than 200[11] components[12][13] Some of the SAS components include:[2][12][14]\n\n    Base SAS – Basic procedures and data management\n    SAS/STAT – Statistical analysis\n    SAS/GRAPH – Graphics and presentation\n    SAS/OR – Operations research\n    SAS/ETS – Econometrics and Time Series Analysis\n    SAS/IML – Interactive matrix language\n    SAS/AF – Applications facility\n    SAS/QC – Quality control\n    SAS/INSIGHT – Data mining\n    SAS/PH – Clinical trial analysis\n    Enterprise Miner – data mining\n    Enterprise Guide - GUI based code editor & project manager\n    SAS EBI - Suite of Business Intelligence Applications\n    SAS Grid Manager - Manager of SAS grid computing environment\n\nHistory\nOrigins\n\nThe development of SAS began in 1966 after North Carolina State University re-hired Anthony Barr[15] to program his analysis of variance and regression software so that it would run on IBM System/360 computers.[16] The project was funded by the National Institute of Health[17] and was originally intended to analyze agricultural data[12][18] to improve crop yields.[19] Barr was joined by student James Goodnight, who developed the software's statistical routines, and the two became project-leaders.[15][16][20] In 1968, Barr and Goodnight integrated new multiple regression and analysis of variance routines.[21][22] In 1972, after issuing the first release of SAS, the project lost its funding.[17] According to Goodnight, this was because NIH only wanted to fund projects with medical applications.[23] Goodnight continued teaching at the university for a salary of $1 and access to mainframe computers for use with the project,[17] until it was funded by the University Statisticians of the Southern Experiment Stations the following year.[16][23] John Sall joined the project in 1973 and contributed to the software's econometrics, time series, and matrix algebra. Another early participant, Caroll G. Perkins, contributed to SAS' early programming. Jolayne W. Service and Jane T. Helwig created SAS' first documentation.[21]\n\nThe first versions of SAS were named after the year in which they were released.[24] In 1971, SAS 71 was published as a limited release.[2][25] It was used only on IBM mainframes and had the main elements of SAS programming, such as the DATA step and the most common procedures in the PROC step.[24] The following year a full version was released as SAS 72, which introduced the MERGE statement and added features for handling missing data or combining data sets.[26] In 1976, Barr, Goodnight, Sall, and Helwig removed the project from North Carolina State and incorporated it into SAS Institute, Inc.[27]\nDevelopment\n\nSAS was re-designed in SAS 76 with an open architecture that allowed for compilers and procedures. The INPUT and INFILE statements were improved so they could read most data formats used by IBM mainframes. Generating reports was also added through the PUT and FILE statements. The ability to analyze general linear models was also added[28] as was the FORMAT procedure, which allowed developers to customize the appearance of data.[24] In 1979, SAS 79 added support for the CMS operating system and introduced the DATASETS procedure. Three years later, SAS 82 introduced an early macro language and the APPEND procedure.[24]\n\nSAS version 4 had limited features, but made SAS more accessible. Version 5 introduced a complete macro language, array subscripts, and a full-screen interactive user interface called Display Manager.[24] In 1985, SAS was rewritten in the C programming language. This allowed for the SAS' Multivendor Architecture that allows the software to run on UNIX, MS-DOS, and Windows. It was previously written in PL/I, Fortran, and assembly language.[20][24]\n\nIn the 1980s and 1990s, SAS released a number of components to complement Base SAS. SAS/GRAPH, which produces graphics, was released in 1980, as well as the SAS/ETS component, which supports econometric and time series analysis. A component intended for pharmaceutical users, SAS/PH-Clinical, was released in the 1990s. The Food and Drug Administration standardized on SAS/PH-Clinical for new drug applications in 2002.[20] Vertical products like SAS Financial Management and SAS Human Capital Management (then called CFO Vision and HR Vision respectively) were also introduced.[29] JMP was developed by SAS co-founder John Sall and a team of developers to take advantage of the graphical user interface introduced in the 1984 Apple Macintosh[30] and shipped for the first time in 1989.[30] Updated versions of JMP were released continuously after 2002 with the most recent release being from 2012.[31][32][33][34][35][36][37]\n\nSAS version 6 was used throughout the 1990s and was available on a wider range of operating systems, including Macintosh, OS/2, Silicon Graphics, and Primos. SAS introduced new features through dot-releases. From 6.06 to 6.09, a user interface based on the windows paradigm was introduced and support for SQL was added.[24] Version 7 introduced the Output Delivery System (ODS) and an improved text editor. ODS was improved upon in successive releases. For example, more output options were added in version 8. The number of operating systems that were supported was reduced to UNIX, Windows and z/OS, and Linux was added.[24][38] SAS version 8 and SAS Enterprise Miner were released in 1999.[20]\nRecent history\n\nIn 2002, the Text Miner software was introduced. Text Miner analyzes text data like emails for patterns in Business Intelligence applications.[39] In 2004, SAS Version 9.0 was released, which was dubbed \"Project Mercury\" and was designed to make SAS accessible to a broader range of business users.[40][41] Version 9.0 added custom user interfaces based on the user’s role and established the point-and-click user interface of SAS Enterprise Guide as the software's primary graphical user interface (GUI).[40] The Customer Relationship Management (CRM) features were improved in 2004 with SAS Interaction Management.[42] In 2008 SAS announced Project Unity, designed to integrate data quality, data integration and master data management.[43]\n\nSAS sued World Programming, the developers of a competing implementation, World Programming System, alleging that they had infringed SAS's copyright in part by implementing the same functionality. This case was referred from the United Kingdom's High Court of Justice to the European Court of Justice on 11 August 2010.[44] In May 2012, the European Court of Justice ruled in favor of World Programming, finding that \"the functionality of a computer program and the programming language cannot be protected by copyright.\"[45]\n\nA free version was introduced for students in 2010.[46] SAS Social Media Analytics, a tool for social media monitoring, engagement and sentiment analysis, was also released that year.[47][48] SAS Rapid Predictive Modeler (RPM), which creates basic analytical models using Microsoft Excel, was introduced that same year.[48][49] JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel.[50][51] The following year, a High Performance Computing appliance was made available in a partnership with Teradata and EMC Greenplum.[52][53] In 2011, the company released Enterprise Miner 7.1.[54] The company introduced 27 data management products from October 2013 to October 2014 and updates to 160 others.[55] At the 2015 SAS Global Forum, it announced several new products that were specialized for different industries, as well as new training software.[56]\nSoftware products\n\nAs of 2011 SAS's largest set of products is its line for customer intelligence. Numerous SAS modules for web, social media and marketing analytics may be used to profile customers and prospects, predict their behaviors and manage and optimize communications.[37][57] SAS also provides the SAS Fraud Framework. The framework's primary functionality is to monitor transactions across different applications, networks and partners and use analytics to identify anomalies that are indicative of fraud.[58][59][60][61] SAS Enterprise GRC (Governance, Risk and Compliance) provides risk modeling, scenario analysis and other functions[62][63] in order to manage and visualize risk, compliance and corporate policies.[64] There is also a SAS Enterprise Risk Management product-set designed primarily for banks and financial services organizations.[65]\n\nSAS' products for monitoring and managing the operations of IT systems are collectively referred to as SAS IT Management Solutions.[66] SAS collects data from various IT assets on performance and utilization, then creates reports and analyses.[67] SAS' Performance Management products consolidate and provide graphical displays for key performance indicators (KPIs) at the employee, department and organizational level.[68][69] The SAS Supply Chain Intelligence product suite is offered for supply chain needs, such as forecasting product demand, managing distribution and inventory and optimizing pricing.[70] There is also a \"SAS for Sustainability Management\" set of software to forecast environmental, social and economic effects and identify causal relationships between operations and an impact on the environmental or ecosystem.[71]\n\nSAS has product sets for specific industries, such as government, retail, telecommunications and aerospace and for marketing optimization or high-performance computing.[72]\nComparison to other products\nSee also: Comparison of statistical packages\n\nIn a 2005 article for the Journal of Marriage and Family comparing statistical packages from SAS and its competitors Stata and SPSS, Alan C. Acock wrote that SAS programs provide \"extraordinary range of data analysis and data management tasks,\" but were difficult to use and learn.[73] SPSS and Stata, meanwhile, were both easier to learn (with better documentation) but had less capable analytic abilities, though these could be expanded with paid (in SPSS) or free (in Stata) add-ons. Acock concluded that SAS was best for power users, while occasional users would benefit most from SPSS and Stata.[73] A comparison by the University of California, Los Angeles, gave similar results.[74]\n\nCompetitors such as Revolution Analytics and Alpine Data Labs advertise their products as considerably cheaper than SAS'. In a 2011 comparison, Doug Henschen of InformationWeek found that start-up fees for the three are similar, though he admitted that the starting fees were not necessarily the best basis for comparison.[75] SAS' business model is not weighted as heavily on initial fees for its programs, instead focusing on revenue from annual subscription fees.[76][77]\nAdoption\n\nAccording to IDC, SAS is the largest market-share holder in \"advanced analytics\" with 35.4 percent of the market as of 2013.[78] It is the fifth largest market-share holder for business intelligence (BI) software with a 6.9% share[79] and the largest independent vendor. It competes in the BI market against conglomerates, such as SAP BusinessObjects, IBM Cognos, SPSS Modeler, Oracle Hyperion, and Microsoft BI.[80] SAS has been named in the Gartner Leader's Quadrant for Data Integration Tools[81] and for Business Intelligence and Analytical Platforms.[82] A study published in 2011 in BMC Health Services Research found that SAS was used in 42.6 percent of data analyses in health service research, based on a sample of 1,139 articles drawn from three journals.[83]", "skillName": "SAS."}
{"id": 67, "category": "statistical_software", "skillText": "JMP (pronounced \"jump\") is a computer program for statistics developed by the JMP business unit of SAS Institute. It was launched in 1989[1] to take advantage of the graphical user interface introduced by the Macintosh. It has since been improved and made available for the Windows operating system. JMP is used in applications such as Six Sigma, quality control and engineering, design of experiments and scientific research.\n\nThe software consists of five products: JMP, JMP Pro, JMP Clinical, JMP Genomics and the JMP Graph Builder App for the iPad. A scripting language is also available. The software is focused on exploratory analytics, whereby users investigate and explore data, rather than testing a hypothesis.\n\nContents\n\n    1 History\n    2 Software\n    3 JMP Scripting Language (JSL)\n    4 Notable applications\n    5 See also\n    6 References\n    7 Further reading\n    8 External links\n\nHistory\nVersion 1.0 of JMP from 1989\n\nJMP was developed in the 1980s by John Sall and a team of developers to make use of the graphical user interface introduced by the 1984 Apple Macintosh.[2] It originally stood for \"John's Macintosh Project\"[3] or “John’s Macintosh Product”[4] and was first released in October 1989.[2] It was used mostly by scientists and engineers for design of experiments (DOE), quality and productivity support (Six Sigma), and reliability modeling.[5] Semiconductor manufacturers were also among JMP’s early adopters.[6]\n\nInteractive graphics and other features were added in 1991[7][8] with version 2.0. Version 2 was twice the size as the original, though it was still delivered on a floppy disk. It required 2 MB of memory and came with 700 pages of documentation.[9] Support for Microsoft Windows was added in 1994.[4][10] JMP was re-written[11] with version 3 in 1999.[12][13] Version 4, released in 2002, could import data from a wider variety of data sources[14] and added support for surface plots.[8] Version 4 also added time series forecasting and new smoothing models, such as the seasonal smoothing method, called Winter's Method, and ARIMA (Autoregressive Integrated Moving Average). It was also the first version to support JSL, JMP Scripting Language.[15]\n\nIn 2005, data mining tools like a decision tree and neural net were added with version 5[16] as well as Linux support, which was later withdrawn in JMP 9.[5] Later in 2005, JMP 6 was introduced.[6][17] JMP began integrating with SAS in version 7.0 in 2007 and has strengthened this integration ever since. Users can write SAS code in JMP, connect to SAS servers, and retrieve and use data from SAS. Support for bubble plots was added in version 7.[5][18] JMP 7 also improved data visualization and diagnostics.[19]\n\nJMP 8 was released in 2009 with new drag-and-drop features and a 64-bit version to take advantage of advances in the Mac operating system.[20] It also added a new user interface for building graphs, tools for choice experiments and support for Life Distributions.[21] According to Scientific Computing, the software had improvements in \"graphics, QA, ease-of-use, SAS integration and data management areas.\"[22] JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel.[23][24] The main screen was rebuilt and enhancements were made to simulations, graphics and a new Degradation platform.[25] In March 2012, version 10 made improvements in data mining, predictive analytics, and automated model building.[26][27]\nSoftware\nScreenshot of different data displays in JMP\n\nJMP consists of JMP, JMP Pro, JMP Clinical and JMP Genomics,[27] as well as the Graph Builder iPad App.[28] JMP Clinical and JMP Genomics combine JMP with SAS software.[27]\n\nJMP software is partly focused on exploratory data analysis and visualization. It is designed for users to investigate data to learn something unexpected, as opposed to confirming a hypothesis.[4][27][29] JMP links statistical data to graphics representing them, so users can drill down or up to explore the data and various visual representations of it.[14][30][31] Its primary applications are for designed experiments and analyzing statistical data from industrial processes.[6]\n\nJMP is a desktop application with a wizard-based user interface, while SAS can be installed on servers. It runs in-memory, instead of on disk storage.[27] According to a review in Pharmaceutical Statistics, JMP is often used as a graphical front-end for a SAS system, which performs the statistical analysis and tabulations.[32] JMP Genomics, used for analyzing and visualizing genomics data,[33] requires a SAS component to operate and can access SAS/Genetics and SAS/STAT procedures or invoke SAS macros.[32] JMP Clinical, used for analyzing clinical trial data, can package SAS code within the JSL scripting language and convert SAS code to JMP.[18]\n\nJMP is also the name of the SAS Institute business unit that develops JMP. As of 2011 it had 180 employees and 250,000 users.[27]\nJMP Scripting Language (JSL)\n\nThe JMP Scripting Language (JSL) is an interpreted language for recreating analytic results and for automating or extending the functionality of JMP software.[34]:29 JSL was first introduced in JMP version 4 in 2000.[35]:1 JSL has a LISP-like syntax, structured as a series of expressions. All [rpgramming elements, including if-then statemenst and loops, are implemented as JSL functions. Data tables, display elements and analyses are represented by objects in JSL that are manipulated with named messages. Users may write JSL scripts to perform analyses and visualizations not available in the point-and-click interface or to automate a series of commands, such as weekly reports.[34] SAS, R, and Matlab code can also be executed using JSL.[36]\nNotable applications\nJMP being used in the WildTrack FIT system\n\nIn 2007, a wildlife monitoring organization, WildTrack, started using JMP with the Footprint Identification Technology (FIT) system to identify individual endangered animals by their footprints.[37] In 2009, the Chicago Botanic Garden used JMP to analyze DNA data from tropical breadfruit. Researchers determined that the seedless, starchy fruit was created by the deliberate hybridization of two fruits, the breadnut and the dugdug.[38] The Herzenberg Laboratory at Stanford has integrated JMP with the Fluorescence Activated Cell Sorter (FACS). The FACS system is used to study HIV, cancer, stem-cells and oceanography.[39]", "skillName": "JMP."}
{"id": 68, "category": "statistical_software", "skillText": "Git (/ɡɪt/[6]) is a version control system that is used for software development[7] and other version control tasks. As a distributed revision control system it is aimed at speed,[8] data integrity,[9] and support for distributed, non-linear workflows.[10] Git was created by Linus Torvalds in 2005 for development of the Linux kernel, with other kernel developers contributing to its initial development.[11]\n\nAs with most other distributed version control systems, and unlike most client–server systems, every Git directory on every computer is a full-fledged repository with complete history and full version-tracking capabilities, independent of network access or a central server.[12] Like the Linux kernel, Git is free software distributed under the terms of the GNU General Public License version 2.\n\nContents\n\n    1 History\n    2 Design\n        2.1 Characteristics\n        2.2 Data structures\n        2.3 References\n    3 Implementations\n    4 Git server\n    5 Adoption\n    6 Security\n    7 See also\n    8 References\n    9 External links\n\nHistory\n\nGit development began in April 2005, after many developers of the Linux kernel gave up access to BitKeeper, a proprietary source control management (SCM) system that they had previously used to maintain the project.[13] The copyright holder of BitKeeper, Larry McVoy, had withdrawn free use of the product after claiming that Andrew Tridgell had reverse-engineered the BitKeeper protocols.[14]\n\nTorvalds wanted a distributed system that he could use like BitKeeper, but none of the available free systems met his needs, particularly in terms of performance. Torvalds cited an example of a source-control management system requiring 30 seconds to apply a patch and update all associated metadata, and noted that this would not scale to the needs of Linux kernel development, where syncing with fellow maintainers could require 250 such actions at a time. For his design criteria, he specified that patching should take no more than three seconds,[8] and added three additional points:\n\n    Take Concurrent Versions System (CVS) as an example of what not to do; if in doubt, make the exact opposite decision[10]\n    Support a distributed, BitKeeper-like workflow[10]\n    Include very strong safeguards against corruption, either accidental or malicious[9]\n\nThese criteria eliminated every then-existing version-control system except Monotone. Performance considerations excluded this, too.[10] So immediately after the 2.6.12-rc2 Linux kernel development release, Torvalds set out to write his own system.[10]\n\nTorvalds quipped about the name git (which means \"unpleasant person\" in British English slang): \"I'm an egotistical bastard, and I name all my projects after myself. First 'Linux', now 'git'.\"[15][16] The man page describes Git as \"the stupid content tracker\".[17] The readme file of the source code elaborates further:[18]\n\nThe name \"git\" was given by Linus Torvalds when he wrote the very\nfirst version. He described the tool as \"the stupid content tracker\"\nand the name as (depending on your mood):\n\n - random three-letter combination that is pronounceable, and not\n   actually used by any common UNIX command.  The fact that it is a\n   mispronunciation of \"get\" may or may not be relevant.\n - stupid. contemptible and despicable. simple. Take your pick from the\n   dictionary of slang.\n - \"global information tracker\": you're in a good mood, and it actually\n   works for you. Angels sing, and a light suddenly fills the room.\n - \"goddamn idiotic truckload of sh*t\": when it breaks\n\nThe development of Git began on 3 April 2005.[19] Torvalds announced the project on 6 April;[20] it became self-hosting as of 7 April.[19] The first merge of multiple branches took place on 18 April.[21] Torvalds achieved his performance goals; on 29 April, the nascent Git was benchmarked recording patches to the Linux kernel tree at the rate of 6.7 per second.[22] On 16 June Git managed the kernel 2.6.12 release.[23]\n\nTorvalds turned over maintenance on 26 July 2005 to Junio Hamano, a major contributor to the project.[24] Hamano was responsible for the 1.0 release on 21 December 2005, and remains the project's maintainer.[25]\nVersion \tOriginal release date \tLatest version \tRelease date\n0.99 \t2005-07-11 \t0.99.9n \t2005-12-15\n1.0 \t2005-12-21 \t1.0.13 \t2006-01-27\n1.1 \t2006-01-08 \t1.1.6 \t2006-01-30\n1.2 \t2006-02-12 \t1.2.6 \t2006-04-08\n1.3 \t2006-04-18 \t1.3.3 \t2006-05-16\n1.4 \t2006-06-10 \t1.4.4.5 \t2008-07-16\n1.5 \t2007-02-14 \t1.5.6.6 \t2008-12-17\n1.6 \t2008-08-17 \t1.6.6.3 \t2010-12-15\n1.7 \t2010-02-13 \t1.7.12.4 \t2012-10-17\n1.8 \t2012-10-21 \t1.8.5.6 \t2014-12-17\n1.9 \t2014-02-14 \t1.9.5 \t2014-12-17\n2.0 \t2014-05-28 \t2.0.5 \t2014-12-17\n2.1 \t2014-08-16 \t2.1.4 \t2014-12-17\n2.2 \t2014-11-26 \t2.2.3 \t2015-09-04\n2.3 \t2015-02-05 \t2.3.10 \t2015-09-29\n2.4 \t2015-04-30 \t2.4.11 \t2016-03-17\n2.5 \t2015-07-27 \t2.5.5 \t2016-03-17\n2.6 \t2015-09-28 \t2.6.6 \t2016-03-17\n2.7 \t2015-10-04 \t2.7.4 \t2016-03-17\n2.8 \t2016-03-28 \t2.8.4 \t2016-06-06\n2.9 \t2016-06-13 \t2.9.0 \t2016-06-13\nLegend:\nOld version\nOlder version, still supported\nLatest version\nLatest preview version\nDesign\n\nGit's design was inspired by BitKeeper and Monotone.[26][27] Git was originally designed as a low-level version control system engine on top of which others could write front ends, such as Cogito or StGIT.[27] The core Git project has since become a complete version control system that is usable directly.[28] While strongly influenced by BitKeeper, Torvalds deliberately avoided conventional approaches, leading to a unique design.[29]\nCharacteristics\n\nGit's design is a synthesis of Torvalds's experience with Linux in maintaining a large distributed development project, along with his intimate knowledge of file system performance gained from the same project and the urgent need to produce a working system in short order. These influences led to the following implementation choices:\n\nStrong support for non-linear development\n    Git supports rapid branching and merging, and includes specific tools for visualizing and navigating a non-linear development history. A core assumption in Git is that a change will be merged more often than it is written, as it is passed around various reviewers. Branches in Git are very lightweight: A branch in Git is only a reference to a single commit. With its parental commits, the full branch structure can be constructed.\nDistributed development\n    Like Darcs, BitKeeper, Mercurial, SVK, Bazaar and Monotone, Git gives each developer a local copy of the entire development history, and changes are copied from one such repository to another. These changes are imported as additional development branches, and can be merged in the same way as a locally developed branch.\nCompatibility with existing systems/protocols\n    Repositories can be published via HTTP, FTP, rsync (until Git 2.8.0[30]), or a Git protocol over either a plain socket, or ssh. Git also has a CVS server emulation, which enables the use of existing CVS clients and IDE plugins to access Git repositories. Subversion and svk repositories can be used directly with git-svn.\nEfficient handling of large projects\n    Torvalds has described Git as being very fast and scalable,[31] and performance tests done by Mozilla[32] showed it was an order of magnitude faster than some version-control systems, and fetching version history from a locally stored repository can be one hundred times faster than fetching it from the remote server.[33]\nCryptographic authentication of history\n    The Git history is stored in such a way that the ID of a particular version (a commit in Git terms) depends upon the complete development history leading up to that commit. Once it is published, it is not possible to change the old versions without it being noticed. The structure is similar to a Merkle tree, but with additional data at the nodes as well as the leaves.[34] (Mercurial and Monotone also have this property.)\nToolkit-based design\n    Git was designed as a set of programs written in C, and a number of shell scripts that provide wrappers around those programs.[35] Although most of those scripts have since been rewritten in C for speed and portability, the design remains, and it is easy to chain the components together.[36]\nPluggable merge strategies\n    As part of its toolkit design, Git has a well-defined model of an incomplete merge, and it has multiple algorithms for completing it, culminating in telling the user that it is unable to complete the merge automatically and that manual editing is required.\nGarbage accumulates unless collected\n    Aborting operations or backing out changes will leave useless dangling objects in the database. These are generally a small fraction of the continuously growing history of wanted objects. Git will automatically perform garbage collection when enough loose objects have been created in the repository. Garbage collection can be called explicitly using git gc --prune.[37]\nPeriodic explicit object packing\n    Git stores each newly created object as a separate file. Although individually compressed, this takes a great deal of space and is inefficient. This is solved by the use of packs that store a large number of objects in a single file (or network byte stream) called packfile, delta-compressed among themselves. Packs are compressed using the heuristic that files with the same name are probably similar, but do not depend on it for correctness. A corresponding index file is created for each packfile, telling the offset of each object in the packfile. Newly created objects (newly added history) are still stored singly, and periodic repacking is required to maintain space efficiency. The process of packing the repository can be very computationally expensive. By allowing objects to exist in the repository in a loose, but quickly generated format, Git allows the expensive pack operation to be deferred until later when time does not matter (e.g. the end of the work day). Git does periodic repacking automatically but manual repacking is also possible with the git gc command. For data integrity, both packfile and its index have SHA-1 checksum inside, and also the file name of packfile contains a SHA-1 checksum. To check integrity, run the git fsck command.\n\nAnother property of Git is that it snapshots directory trees of files. The earliest systems for tracking versions of source code, SCCS and RCS, worked on individual files and emphasized the space savings to be gained from interleaved deltas (SCCS) or delta encoding (RCS) the (mostly similar) versions. Later revision control systems maintained this notion of a file having an identity across multiple revisions of a project. However, Torvalds rejected this concept.[38] Consequently, Git does not explicitly record file revision relationships at any level below the source code tree.\n\nImplicit revision relationships have some significant consequences:\n\n    It is slightly more expensive to examine the change history of a single file than the whole project.[39] To obtain a history of changes affecting a given file, Git must walk the global history and then determine whether each change modified that file. This method of examining history does, however, let Git produce with equal efficiency a single history showing the changes to an arbitrary set of files. For example, a subdirectory of the source tree plus an associated global header file is a very common case.\n    Renames are handled implicitly rather than explicitly. A common complaint with CVS is that it uses the name of a file to identify its revision history, so moving or renaming a file is not possible without either interrupting its history, or renaming the history and thereby making the history inaccurate. Most post-CVS revision control systems solve this by giving a file a unique long-lived name (analogous to an inode number) that survives renaming. Git does not record such an identifier, and this is claimed as an advantage.[40][41] Source code files are sometimes split or merged as well as simply renamed,[42] and recording this as a simple rename would freeze an inaccurate description of what happened in the (immutable) history. Git addresses the issue by detecting renames while browsing the history of snapshots rather than recording it when making the snapshot.[43] (Briefly, given a file in revision N, a file of the same name in revision N−1 is its default ancestor. However, when there is no like-named file in revision N−1, Git searches for a file that existed only in revision N−1 and is very similar to the new file.) However, it does require more CPU-intensive work every time history is reviewed, and a number of options to adjust the heuristics. This mechanism does not always work; sometimes a file that is renamed with changes in the same commit is read as a deletion of the old file and the creation of a new file. Developers can work around this limitation by committing the rename and changes separately.\n\nGit implements several merging strategies; a non-default can be selected at merge time:[44]\n\n    resolve: the traditional three-way merge algorithm.\n    recursive: This is the default when pulling or merging one branch, and is a variant of the three-way merge algorithm.\n\n        When there are more than one common ancestors that can be used for three-way merge, it creates a merged tree of the common ancestors and uses that as the reference tree for the three-way merge. This has been reported to result in fewer merge conflicts without causing mis-merges by tests done on actual merge commits taken from Linux 2.6 kernel development history. Additionally this can detect and handle merges involving renames.\n        — Linus Torvalds[45]\n\n    octopus: This is the default when merging more than two heads.\n\nData structures\n\nGit's primitives are not inherently a source code management (SCM) system. Torvalds explains,[46]\n\n    In many ways you can just see git as a filesystem – it's content-addressable, and it has a notion of versioning, but I really really designed it coming at the problem from the viewpoint of a filesystem person (hey, kernels is what I do), and I actually have absolutely zero interest in creating a traditional SCM system.\n\nFrom this initial design approach, Git has developed the full set of features expected of a traditional SCM,[28] with features mostly being created as needed, then refined and extended over time.\nSome data flows and storage levels in the Git revision control system.\n\nGit has two data structures: a mutable index (also called stage or cache) that caches information about the working directory and the next revision to be committed; and an immutable, append-only object database.\n\nThe object database contains four types of objects:\n\n    A blob (binary large object) is the content of a file. Blobs have no file name, time stamps, or other metadata.\n    A tree object is the equivalent of a directory. It contains a list of file names, each with some type bits and the name of a blob or tree object that is that file, symbolic link, or directory's contents. This object describes a snapshot of the source tree.\n    A commit object links tree objects together into a history. It contains the name of a tree object (of the top-level source directory), a time stamp, a log message, and the names of zero or more parent commit objects.\n    A tag object is a container that contains reference to another object and can hold additional meta-data related to another object. Most commonly, it is used to store a digital signature of a commit object corresponding to a particular release of the data being tracked by Git.\n\nThe index serves as connection point between the object database and the working tree.\n\nEach object is identified by a SHA-1 hash of its contents. Git computes the hash, and uses this value for the object's name. The object is put into a directory matching the first two characters of its hash. The rest of the hash is used as the file name for that object.\n\nGit stores each revision of a file as a unique blob. The relationships between the blobs can be found through examining the tree and commit objects. Newly added objects are stored in their entirety using zlib compression. This can consume a large amount of disk space quickly, so objects can be combined into packs, which use delta compression to save space, storing blobs as their changes relative to other blobs.\n\nGit servers typically listen on TCP port 9418.[47]\nReferences\n\nEvery object in the Git database which is not referred to may be cleaned up by using a garbage collection command, or automatically. An object may be referenced by another object, or an explicit reference. Git knows different types of references. The commands to create, move, and delete references vary. \"git show-ref\" lists all references. Some types are:\n\n    heads: refers to an object locally\n    remotes: refers to an object which exists in a remote repository\n    stash: refers to an object not yet committed\n    meta: e.g. a configuration in a bare repository, user rights; the refs/meta/config namespace was introduced resp gets used by Gerrit[clarification needed][48]\n    tags: see above\n\nImplementations\ngitg is a graphical front-end using GTK+\n\nGit is primarily developed on Linux, although it also supports most major operating systems including BSD, Solaris, OS X, and Microsoft Windows.[49]\n\nThe first Microsoft Windows \"port\" of Git was primarily a Linux emulation framework that hosts the Linux version. Installing Git under Windows creates a similarly named Program Files directory containing 5,236 files in 580 directories. These include the MinGW port of the GNU Compiler Collection, Perl 5, msys2.0, itself a fork of Cygwin, a Unix-like emulation environment for Windows, and various other Windows ports or emulations of Linux utilities and libraries. Currently native Windows builds of Git are distributed as 32 and 64-bit installers.\n\nThe JGit implementation of Git is a pure Java software library, designed to be embedded in any Java application. JGit is used in the Gerrit code review tool and in EGit, a Git client for the Eclipse IDE.[50]\n\nThe Dulwich implementation of Git is a pure Python software component for Python 2.[51]\n\nThe libgit2 implementation of Git is an ANSI C software library with no other dependencies, which can be built on multiple platforms including Microsoft Windows, Linux, Mac OS X, and BSD.[52] It has bindings for many programming languages, including Ruby, Python and Haskell.[53][54][55]\n\nJS-Git is a JavaScript implementation of a subset of Git.[56]\nGit server\n\nAs Git is a distributed version control system, it can be used as a server out of the box. Dedicated Git server software helps, amongst other features, to add access control, display the contents of a Git repository via the web, and help managing multiple repositories. Remote file store and shell access: A Git repository can be cloned to a shared file system, and accessed by other persons. It can also be accessed via remote shell just by having the Git software installed and allowing a user to log in.[57]\n\nGit daemon, instaweb\n    Git daemon allows users to share their own repository to colleagues quickly. Git instaweb allows users to provide web view to the repository. As of 2014-04 instaweb does not work on Windows. Both can be seen in the line of Mercurial's \"hg serve\".[58][59]\nGitolite\n    Gitolite is an access control layer on top of Git, providing fine access control to Git repositories. It relies on other software to remotely view the repositories on the server.[60][61]\nApache Allura\n    Apache Allura is an open-source forge software for managing source code repositories, bug reports, discussions, wiki pages, blogs and more for any number of individual projects.\nGerrit\n    Gerrit provides two out of three functionalities: access control and managing repositories. It uses jGit. To view repositories it is combined e.g. with Gitiles or GitBlit.\nGitblit\n    Gitblit can provide all three functions, but is in larger installations used as repository browser installed with gerrit for access control and management of repositories.[62][63] Gitblit can also provide the sync option for other repository.\nGitiles\n    Gitiles is a simple repository browser, usually used together with gerrit.[64][65]\nBonobo Git Server\n    Bonobo Git Server is a simple Git server for Windows implemented as an ASP.NET gateway.[66] It relies on the authentication mechanisms provided by Windows Internet Information Services, thus it does not support SSH access but can be easily integrated with Active Directory.\nGitorious\n    Gitorious is the free software behind the Git repository hosting service of the same name. In March 2015, Gitorious was acquired by GitLab.[67]\nGitLab\n    GitLab provides a software repository service. It offers a web interface like GitHub, and is written in Ruby.\n\nDjacket \n    Djacket is a free and open source Git server like GitHub, meant for personal and small business usages. It is written in Python and Django framework.\nGogs\n    Gogs is another self-hosted Git service written in Go. It provides similar features to GitLab as a web interface.[68]\nRhodeCode\n    RhodeCode is an open source self-hosted platform for behind-the-firewall source code management. It provides centralized control over all Git, Mercurial and Subversion repositories, with common authentication and permission management. RhodeCode allows forking, pull requests distributed revision control, and code reviews via a web interface. RhodeCode is written in Python and Pyramid framework. [69]\nGitHub\n    GitHub is a website where copies of Git repositories can be uploaded. It is a Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. Unlike Git, which is strictly a command-line tool, GitHub provides a web-based graphical interface and desktop as well as mobile integration. It also provides access control and several collaboration features such as wikis, task management, bug tracking and other features that can be helpful for projects. It allows collaboration with other people on projects. It does that by providing a centralized location to share the repository, a web-based interface to view it, and features like forking, pull requests distributed revision control, issues, and wikis.\nBitbucket\n    Bitbucket is a web-based hosting service for projects that use either the Git (since October 2011) or the Mercurial (since launch) revision control systems. Bitbucket offers both commercial plans and free accounts.\nSourcegraph\n    Sourcegraph is a self-hosted Git service, written in Go. It also provides jump-to-definition, cross-reference, and semantic search features (by analyzing code in several languages).[70]\nCommercial products\n    Commercial programs are also available to be installed on premises, amongst them GitHub (using native Git, available as a vm), RhodeCode (custom front-end web interface, native Git in the backend), Stash (using a custom front-end and native Git in the backend), Team Foundation Server (using libgit2).[71]\n\nRelated article: Comparison of source code hosting facilities\nAdoption\n\nThe Eclipse Foundation reported in its annual community survey that as of May 2014, Git is now the most widely used source code management tool, with 42.9% of professional software developers reporting that they use Git as their primary source control system[72] compared with 36.3% in 2013, 32% in 2012; or for Git responses excluding use of GitHub: 33.3% in 2014, 30.3% in 2013, 27.6% in 2012 and 12.8% in 2011.[73] Open source directory Black Duck Open Hub reports a similar uptake among open source projects.[74]\n\nThe UK IT jobs website itjobswatch.co.uk reports that as of late December 2014, 23.58% of UK permanent software development job openings have cited Git,[75] ahead of 16.34% for Subversion,[76] 11.58% for Microsoft Team Foundation Server,[77] 1.62% for Mercurial,[78] and 1.13% for Visual SourceSafe.[79]\nSecurity\n\nOn 17 December 2014, an exploit was found affecting the Windows and Mac versions of the Git client. An attacker could perform arbitrary code execution on a Windows or Mac computer with Git installed by creating a malicious Git tree (directory) named .git (a directory in Git repositories that stores all the data of the repository) in a different case (such as .GIT or .Git, needed because Git doesn't allow the all-lowercase version of .git to be created manually) with malicious files in the .git/hooks subdirectory (a folder with executable files that Git runs) on a repository that the attacker made or on a repository that the attacker can modify. If a Windows or Mac user \"pulls\" (downloads) a version of the repository with the malicious directory, then switches to that directory, the .git directory will be overwritten (due to the case-insensitive nature of the Windows and Mac filesystems) and the malicious executable files in .git/hooks may be run, which results in the attacker's commands being executed. An attacker could also modify the .git/config configuration file, which allows the attacker to create malicious Git aliases (aliases for Git commands or external commands) or modify existing aliases to execute malicious commands when run. The vulnerability was patched in version 2.2.1 of Git, released on 17 December 2014, and announced on the next day.[80][81]\n\nGit version 2.6.1, released on 29 September 2015, contained a patch for a security vulnerability (CVE-2015-7545)[82] which allowed arbitrary code execution.[83] The vulnerability was exploitable if an attacker could convince a victim to clone a specific URL, as the arbitrary commands were embedded in the URL itself.[84] An attacker could use the exploit via a man-in-the-middle attack if the connection was unencrypted,[84] as they could redirect the user to a URL of their choice. Recursive clones were also vulnerable, since they allowed the controller of a repository to specify arbitrary URLs via the gitmodules file.[84]\n\nGit uses SHA-1 hashes internally, even though SHA-1 is looking cryptographically weak compared to newer cryptographic hash functions. Linus has responded that the hash was mostly to guard against accidental corruption, and the security a cryptographically secure hash gives was just an accidental side effect, with the main security being signing elsewhere.[85][86] Though Linus has also referenced an attack on kernel.org where a malicious attacker modified data in a bitkeeper version control system, something which may not be prevented without a secure hash.[87]", "skillName": "Git."}
{"id": 69, "category": "statistical_software", "skillText": "Statistical software are specialized computer programs for analysis in statistics and econometrics.\n\nContents\n\n    1 Open-source\n    2 Public domain\n    3 Freeware\n    4 Proprietary\n        4.1 Add-ons\n    5 See also\n    6 References\n    7 External links\n\nOpen-source\ngretl is an example of an open-source statistical package\n\n    ADaMSoft – a generalized statistical software with data mining algorithms and methods for data management\n    ADMB – a software suite for non-linear statistical modeling based on C++ which uses automatic differentiation\n    Bayesian Filtering Library\n    Chronux – for neurobiological time series data\n    CBEcon – web-based econometrics and statistical software\n    DataMelt (DMelt) – Java-based statistical analysis framework for scientists and engineers. It includes an IDE\n    DAP – free replacement for SAS\n    DynStats – a on-line statistical laboratory\n    Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI) a software framework for developing data mining algorithms in Java\n    Fityk – nonlinear regression software (GUI and command line)\n    GNU Octave – programming language very similar to MATLAB with statistical features\n    gretl – gnu regression, econometrics and time-series library\n    intrinsic Noise Analyzer (iNA) – For analyzing intrinsic fluctuations in biochemical systems\n    JASP – A free software alternative to IBM SPSS Statistics with additional option for Bayesian methods\n    Just another Gibbs sampler (JAGS) – a program for analyzing Bayesian hierarchical models using Markov chain Monte Carlo developed by Martyn Plummer. It is similar to WinBUGS\n    JMulTi\n    LDT - Automatic Time Series Analysis with Stationary VAR Models\n    LIBSVM – C++ support vector machine libraries\n    MLPACK (C++ library) – open-source library for machine learning, exploits C++ language features to provide maximum performance and flexibility while providing a simple and consistent application programming interface (API)\n    Mondrian – data analysis tool using interactive statistical graphics with a link to R\n    Neurophysiological Biomarker Toolbox - Matlab toolbox for data-mining of neurophysiological biomarkers\n    OpenBUGS\n    OpenEpi – A web-based, open-source, operating-independent series of programs for use in epidemiology and statistics based on JavaScript and HTML\n    OpenNN – A software library written in the programming language C++ which implements neural networks, a main area of deep learning research\n    OpenMx – A package for structural equation modeling running in R (programming language)\n    Orange, a data mining, machine learning, and bioinformatics software\n    Pandas – High-performance computing (HPC) data structures and data analysis tools for Python in Python and Cython (statsmodels, scikit-learn)\n    Perl Data Language – Scientific computing with Perl\n    Ploticus – software for generating a variety of graphs from raw data\n    PSPP – A free software alternative to IBM SPSS Statistics\n    R – free implementation of the S (programming language)\n        Programming with Big Data in R (pbdR) – a series of R packages enhanced by SPMD parallelism for big data analysis\n        R Commander – GUI interface for R\n        Rattle GUI – GUI interface for R\n        Revolution Analytics – production-grade software for the enterprise big data analytics\n        RStudio – GUI interface and development environment for R\n    ROOT – an open-source C++ system for data storage, processing and analysis, developed by CERN and used to find the Higgs boson\n    Salstat - menu-driven statistics software\n    Scilab – uses GPL-compatible CeCILL license\n    SciPy – Python library for scientific computing that contains the stats sub-package which is partly based on the venerable |STAT (a.k.a. PipeStat, formerly UNIX|STAT) software\n        scikit-learn - extends SciPy with a host of machine learning models (classification, clustering, regression, etc.)\n        statsmodels - extends SciPy with statistical models and tests (regression, plotting, example datasets, generalized linear model (GLM), time series analysis, autoregressive–moving-average model (ARMA), vector autoregression (VAR), non-parametric statistics, ANOVA, empirical likelihood)\n    Shogun (toolbox) – open-source, large-scale machine learning toolbox that provides several SVM (Support Vector Machine) implementations (like libSVM, SVMlight) under a common framework and interfaces to Octave, MATLAB, Python, R\n    Simfit – simulation, curve fitting, statistics, and plotting\n    SOCR\n    SOFA Statistics – desktop GUI program focused on ease of use, learn as you go, and beautiful output\n    Stan (software) – open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. It’s somewhat like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors\n    Statistical Lab – R-based and focusing on educational purposes\n    Torch (machine learning) – a deep learning software library written in Lua (programming language)\n    Weka (machine learning) – a suite of machine learning software written at the University of Waikato\n\nPublic domain\n\n    CSPro\n    Epi Info\n    X-12-ARIMA\n\nFreeware\n\n    BV4.1\n    GeoDA\n    MaxStat Lite – general statistical software\n    MINUIT\n    WinBUGS – Bayesian analysis using Markov chain Monte Carlo methods\n    Winpepi – package of statistical programs for epidemiologists\n\nProprietary\n\n    Analytica - visual analytics and statistics package\n    Angoss - products KnowledgeSEEKER and KnowledgeSTUDIO incorporate several data mining algorithms\n    ASReml – for restricted maximum likelihood analyses\n    BMDP – general statistics package\n    Data Applied – for building statistical models\n    DB Lytix - 800+ in-database models\n    EViews – for econometric analysis\n    FAME (database) – a system for managing time-series databases\n    GAUSS – programming language for statistics\n    Genedata – software solution for integration and interpretation of experimental data in the life science R&D\n    GenStat – general statistics package\n    GLIM – early package for fitting generalized linear models\n    GraphPad InStat – very simple with lots of guidance and explanations\n    GraphPad Prism – biostatistics and nonlinear regression with clear explanations\n    IMSL Numerical Libraries – software library with statistical algorithms\n    JMP – visual analysis and statistics package\n    LIMDEP – comprehensive statistics and econometrics package\n    LISREL – statistics package used in structural equation modeling\n    Maple – programming language with statistical features\n    Mathematica – a software package with statistical particularlyŋ features\n    MATLAB – programming language with statistical features\n    MaxStat Pro – general statistical software\n    MedCalc – for biomedical sciences\n    Microfit – econometrics package, time series\n    Minitab – general statistics package\n    MLwiN – multilevel models (free to UK academics)\n    NAG Numerical Library – comprehensive math and statistics library\n    Neural Designer – commercial deep learning package\n    NCSS – general statistics package\n    NLOGIT – comprehensive statistics and econometrics package\n    NMath Stats – statistical package for .NET Framework\n    O-Matrix – programming language\n    OriginPro – statistics and graphing, programming access to NAG library\n    PASS Sample Size Software (PASS) – power and sample size software from NCSS\n    Plotly – plotting library and styling interface for analyzing data and creating browser-based graphs. Available for R, Python, MATLAB, Julia, and Perl\n    Primer-E Primer – environmental and ecological specific\n    PV-WAVE – programming language comprehensive data analysis and visualization with IMSL statistical package\n    Qlucore Omics Explorer - interactive and visual data analysis software\n    Quantum Programming Language – part of the SPSS MR product line, mostly for data validation and tabulation in Marketing and Opinion Research\n    RapidMiner – machine learning toolbox\n    Regression Analysis of Time Series (RATS) – comprehensive econometric analysis package\n    SAS (software) – comprehensive statistical package\n    SHAZAM (Econometrics and Statistics Software) – comprehensive econometrics and statistics package\n    Simul - econometric tool for multidimensional (multi-sectoral, multi-regional) modeling\n    SigmaStat – package for group analysis\n    SmartPLS - statistics package used in partial least squares path modeling (PLS) and PLS-based structural equation modeling\n    SOCR – online tools for teaching statistics and probability theory\n    Speakeasy (computational environment) – numerical computational environment and programming language with many statistical and econometric analysis features\n    SPSS Modeler – comprehensive data mining and text analytics workbench\n    SPSS Statistics – comprehensive statistics package that stands for \"Statistical Package for the Social Sciences\"\n    Stata – comprehensive statistics package\n    Statgraphics – general statistics package to include cloud computing and Six Sigma for use in business development, process improvement, data visualization and statistical analysis, design of experiment, point processes, geospatial analysis, regression, and time series analysis are all included within this complete statistical package.\n    Statistica – comprehensive statistics package\n    StatsDirect – statistics package designed for biomedical, public health and general health science uses\n    StatXact – package for exact nonparametric and parametric statistics\n    Systat – general statistics package\n    SuperCROSS - comprehensive statistics package with ad-hoc, cross tabulation analysis\n    S-PLUS – general statistics package\n    Unistat – general statistics package that can also work as Excel add-in\n    The Unscrambler - free-to-try commercial multivariate analysis software for Windows\n    Wolfram Language[1] - the computer language that evolved from the program Mathematica. It has similar statistical capabilities as Mathematica.\n    World Programming System (WPS) – statistical package that supports the SAS language\n    XploRe\n\nAdd-ons\n\n    Analyse-it – add-on to Microsoft Excel for statistical analysis\n    NumXL – add-on to Microsoft Excel for statistical and time series analysis\n    SigmaXL – add-on to Microsoft Excel for statistical and graphical analysis\n    SPC XL – add-on to Microsoft Excel for general statistics\n    Statgraphics Sigma Express - add-on to Microsoft Excel for Six Sigma statistical analysis\n    SUDAAN – add-on to SAS and SPSS for statistical surveys\n    XLfit add-on to Microsoft Excel for curve fitting and statistical analysis\n\nSee also\n\n    Comparison of statistical packages\n    Econometric software\n    Free statistical software\n    List of computer algebra systems\n    List of graphing software\n    List of numerical analysis software\n    List of numerical libraries\n    Mathematical software\n    Psychometric software", "skillName": "List_of_statistical_packages."}
{"id": 70, "category": "statistical_software", "skillText": "SPSS Statistics is a software package used for statistical analysis. Long produced by SPSS Inc., it was acquired by IBM in 2009. The current versions (2015) are officially named IBM SPSS Statistics. Companion products in the same family are used for survey authoring and deployment (IBM SPSS Data Collection), data mining (IBM SPSS Modeler), text analytics, and collaboration and deployment (batch and automated scoring services).\n\nThe software name originally stood for Statistical Package for the Social Sciences (SPSS),[2] reflecting the original market, although the software is now popular in other fields as well, including the health sciences and marketing.\n\nContents\n\n    1 Overview\n    2 Versions and ownership history\n    3 See also\n    4 Notes\n    5 References\n    6 External links\n\nOverview\n\nSPSS is a widely used program for statistical analysis in social science. It is also used by market researchers, health researchers, survey companies, government, education researchers, marketing organizations, data miners,[3] and others. The original SPSS manual (Nie, Bent & Hull, 1970) has been described as one of \"sociology's most influential books\" for allowing ordinary researchers to do their own statistical analysis.[4] In addition to statistical analysis, data management (case selection, file reshaping, creating derived data) and data documentation (a metadata dictionary was stored in the datafile) are features of the base software.\n\nStatistics included in the base software:\n\n    Descriptive statistics: Cross tabulation, Frequencies, Descriptives, Explore, Descriptive Ratio Statistics\n    Bivariate statistics: Means, t-test, ANOVA, Correlation (bivariate, partial, distances), Nonparametric tests\n    Prediction for numerical outcomes: Linear regression\n    Prediction for identifying groups: Factor analysis, cluster analysis (two-step, K-means, hierarchical), Discriminant\n\nThe many features of SPSS Statistics are accessible via pull-down menus or can be programmed with a proprietary 4GL command syntax language. Command syntax programming has the benefits of reproducibility, simplifying repetitive tasks, and handling complex data manipulations and analyses. Additionally, some complex applications can only be programmed in syntax and are not accessible through the menu structure. The pull-down menu interface also generates command syntax: this can be displayed in the output, although the default settings have to be changed to make the syntax visible to the user. They can also be pasted into a syntax file using the \"paste\" button present in each menu. Programs can be run interactively or unattended, using the supplied Production Job Facility.\n\nAdditionally a \"macro\" language can be used to write command language subroutines. A Python programmability extension can access the information in the data dictionary and data and dynamically build command syntax programs. The Python programmability extension, introduced in SPSS 14, replaced the less functional SAX Basic \"scripts\" for most purposes, although SaxBasic remains available. In addition, the Python extension allows SPSS to run any of the statistics in the free software package R. From version 14 onwards, SPSS can be driven externally by a Python or a VB.NET program using supplied \"plug-ins\". (From Version 20 onwards, these two scripting facilities, as well as many scripts, are included on the installation media and are normally installed by default.)\n\nSPSS Statistics places constraints on internal file structure, data types, data processing, and matching files, which together considerably simplify programming. SPSS datasets have a two-dimensional table structure, where the rows typically represent cases (such as individuals or households) and the columns represent measurements (such as age, sex, or household income). Only two data types are defined: numeric and text (or \"string\"). All data processing occurs sequentially case-by-case through the file. Files can be matched one-to-one and one-to-many, but not many-to-many.\n\nThe graphical user interface has two views which can be toggled by clicking on one of the two tabs in the bottom left of the SPSS Statistics window. The 'Data View' shows a spreadsheet view of the cases (rows) and variables (columns). Unlike spreadsheets, the data cells can only contain numbers or text, and formulas cannot be stored in these cells. The 'Variable View' displays the metadata dictionary where each row represents a variable and shows the variable name, variable label, value label(s), print width, measurement type, and a variety of other characteristics. Cells in both views can be manually edited, defining the file structure and allowing data entry without using command syntax. This may be sufficient for small datasets. Larger datasets such as statistical surveys are more often created in data entry software, or entered during computer-assisted personal interviewing, by scanning and using optical character recognition and optical mark recognition software, or by direct capture from online questionnaires. These datasets are then read into SPSS.\n\nSPSS Statistics can read and write data from ASCII text files (including hierarchical files), other statistics packages, spreadsheets and databases. SPSS Statistics can read and write to external relational database tables via ODBC and SQL.\n\nStatistical output is to a proprietary file format (*.spv file, supporting pivot tables) for which, in addition to the in-package viewer, a stand-alone reader can be downloaded. The proprietary output can be exported to text or Microsoft Word, PDF, Excel, and other formats. Alternatively, output can be captured as data (using the OMS command), as text, tab-delimited text, PDF, XLS, HTML, XML, SPSS dataset or a variety of graphic image formats (JPEG, PNG, BMP and EMF).\nThe SPSS logo used prior to the renaming in January 2010.\n\nSPSS Statistics Server is a version of SPSS Statistics with a client/server architecture. It had some features not available in the desktop version, such as scoring functions. (Scoring functions are included in the desktop version from version 19.)\nVersions and ownership history\n\nThe software was released in its first version in 1968 as the Statistical Package for the Social Sciences (SPSS) after being developed by Norman H. Nie, Dale H. Bent, and C. Hadlai Hull. Those principals incorporated as SPSS Inc. in 1975. Early versions of SPSS Statistics were designed for batch processing on mainframes, including for example IBM and ICL versions, originally using punched cards for input. A processing run read a command file of SPSS commands and either a raw input file of fixed format data with a single record type, or a 'getfile' of data saved by a previous run. To save precious computer time an 'edit' run could be done to check command syntax without analysing the data. From version 10 (SPSS-X) in 1983, data files could contain multiple record types.\n\nSPSS Statistics versions 16.0 and later run under Windows, Mac, and Linux. The graphical user interface is written in Java. The Mac OS version is provided as a Universal binary, making it fully compatible with both PowerPC and Intel-based Mac hardware.\n\nPrior to SPSS 16.0, different versions of SPSS were available for Windows, Mac OS X and Unix. The Windows version was updated more frequently and had more features than the versions for other operating systems.[citation needed]\n\nSPSS Statistics version 13.0 for Mac OS X was not compatible with Intel-based Macintosh computers, due to the Rosetta emulation software causing errors in calculations. SPSS Statistics 15.0 for Windows needed a downloadable hotfix to be installed in order to be compatible with Windows Vista.\n\nSPSS Inc announced on July 28, 2009 that it was being acquired by IBM for US$1.2 billion.[5] Because of a dispute about ownership of the name \"SPSS\", between 2009 and 2010, the product was referred to as PASW (Predictive Analytics SoftWare).[6] As of January 2010, it became \"SPSS: An IBM Company\". Complete transfer of business to IBM was done by October 1, 2010. By that date, SPSS: An IBM Company ceased to exist. IBM SPSS is now fully integrated into the IBM Corporation, and is one of the brands under IBM Software Group's Business Analytics Portfolio, together with IBM Algorithmics, IBM Cognos and IBM OpenPages.", "skillName": "SPSS."}
{"id": 71, "category": "statistical_software", "skillText": "Waikato Environment for Knowledge Analysis (Weka) is a popular suite of machine learning software written in Java, developed at the University of Waikato, New Zealand. It is free software licensed under the GNU General Public License.\n\nContents\n\n    1 Description\n    2 User interfaces\n    3 Extension packages\n    4 History\n    5 Related tools\n    6 See also\n    7 References\n    8 External links\n\nDescription\n\nWeka (pronounced to rhyme with Mecca) is a workbench[1] that contains a collection of visualization tools and algorithms for data analysis and predictive modeling, together with graphical user interfaces for easy access to these functions. The original non-Java version of Weka was a Tcl/Tk front-end to (mostly third-party) modeling algorithms implemented in other programming languages, plus data preprocessing utilities in C, and a Makefile-based system for running machine learning experiments. This original version was primarily designed as a tool for analyzing data from agricultural domains,[2][3] but the more recent fully Java-based version (Weka 3), for which development started in 1997, is now used in many different application areas, in particular for educational purposes and research. Advantages of Weka include:\n\n    Free availability under the GNU General Public License.\n    Portability, since it is fully implemented in the Java programming language and thus runs on almost any modern computing platform.\n    A comprehensive collection of data preprocessing and modeling techniques.\n    Ease of use due to its graphical user interfaces.\n\nWeka supports several standard data mining tasks, more specifically, data preprocessing, clustering, classification, regression, visualization, and feature selection. All of Weka's techniques are predicated on the assumption that the data is available as one flat file or relation, where each data point is described by a fixed number of attributes (normally, numeric or nominal attributes, but some other attribute types are also supported). Weka provides access to SQL databases using Java Database Connectivity and can process the result returned by a database query. It is not capable of multi-relational data mining, but there is separate software for converting a collection of linked database tables into a single table that is suitable for processing using Weka.[4] Another important area that is currently not covered by the algorithms included in the Weka distribution is sequence modeling..\nUser interfaces\n\nWeka's main user interface is the Explorer, but essentially the same functionality can be accessed through the component-based Knowledge Flow interface and from the command line. There is also the Experimenter, which allows the systematic comparison of the predictive performance of Weka's machine learning algorithms on a collection of datasets.\n\nThe Explorer interface features several panels providing access to the main components of the workbench:\n\n    The Preprocess panel has facilities for importing data from a database, a comma-separated values (CSV) file, etc., and for preprocessing this data using a so-called filtering algorithm. These filters can be used to transform the data (e.g., turning numeric attributes into discrete ones) and make it possible to delete instances and attributes according to specific criteria.\n    The Classify panel enables applying classification and regression algorithms (indiscriminately called classifiers in Weka) to the resulting dataset, to estimate the accuracy of the resulting predictive model, and to visualize erroneous predictions, receiver operating characteristic (ROC) curves, etc., or the model itself (if the model is amenable to visualization like, e.g., a decision tree).\n    The Associate panel provides access to association rule learners that attempt to identify all important interrelationships between attributes in the data.\n    The Cluster panel gives access to the clustering techniques in Weka, e.g., the simple k-means algorithm. There is also an implementation of the expectation maximization algorithm for learning a mixture of normal distributions.\n    The Select attributes panel provides algorithms for identifying the most predictive attributes in a dataset.\n    The Visualize panel shows a scatter plot matrix, where individual scatter plots can be selected and enlarged, and analyzed further using various selection operators.\n\nExtension packages\n\nIn version 3.7.2 (thus not available in the stable \"book\" version of Weka), a package manager was added to allow the easier installation of extension packages.[5] Some functionality that used to be included with Weka prior to this version has since been moved into such extension packages, but this change also makes it easier for other to contribute extensions to Weka and to maintain the software, as this modular architecture allows independent updates of the Weka core and individual extensions.\nHistory\n\n    In 1993, the University of Waikato in New Zealand began development of the original version of Weka, which became a mix of Tcl/Tk, C, and Makefiles.\n    In 1997, the decision was made to redevelop Weka from scratch in Java, including implementations of modeling algorithms.[6]\n    In 2005, Weka received the SIGKDD Data Mining and Knowledge Discovery Service Award.[7][8]\n    In 2006, Pentaho Corporation acquired an exclusive licence to use Weka for business intelligence.[citation needed] It forms the data mining and predictive analytics component of the Pentaho business intelligence suite.\n    All-time ranking[9] on Sourceforge.net as of 2011-08-26, 243 (with 2,487,213 downloads)\n\nRelated tools\n\n    Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI) is a similar project to Weka with a focus on cluster analysis, i.e., unsupervised methods.\n    KNIME is a machine learning and data mining software implemented in Java.\n    Massive Online Analysis (MOA) is an open-source project for large scale mining of data streams, also developed at the University of Waikato in New Zealand.\n    Neural Designer is a data mining software based on deep learning techniques written in C++.\n    Orange is a similar open-source project for data mining, machine learning and visualization written in Python and C++.\n    RapidMiner is a commercial machine learning framework implemented in Java which integrates Weka.", "skillName": "Weka."}
{"id": 72, "category": "statistical_software", "skillText": "Oracle Data Mining (ODM) is an option of Oracle Corporation's Relational Database Management System (RDBMS) Enterprise Edition (EE). It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\nOracle Data Mining Developer(s) \tOracle Corporation\nStable release \t11gR2 / September, 2009\nType \tdata mining and analytics\nLicense \tproprietary\nWebsite \tOracle Data Mining \n\nContents\n\n    1 Overview\n    2 History\n    3 Functionality\n    4 Input sources and data preparation\n    5 Graphical user interface: Oracle Data Miner\n    6 PL/SQL and Java interfaces\n    7 SQL scoring functions\n    8 PMML\n    9 Predictive Analytics MS Excel Add-In\n    10 References and further reading\n    11 See also\n    12 References\n    13 External links\n\nOverview\n\nOracle implements a variety of data mining algorithms inside the Oracle relational database. These implementations are integrated right into the Oracle database kernel, and operate natively on data stored in the relational database tables. This eliminates the need for extraction or transfer of data into standalone mining/analytic servers. The relational database platform is leveraged to securely manage models and efficiently execute SQL queries on large volumes of data. The system is organized around a few generic operations providing a general unified interface for data mining functions. These operations include functions to create, apply, test, and manipulate data mining models. Models are created and stored as database objects, and their management is done within the database - similar to tables, views, indexes and other database objects.\n\nIn data mining, the process of using a model to derive predictions or descriptions of behavior that is yet to occur is called \"scoring\". In traditional analytic workbenches, a model built in the analytic engine has to be deployed in a mission-critical system to score new data, or the data is moved from relational tables into the analytical workbench - most workbenches offer proprietary scoring interfaces. ODM simplifies model deployment by offering Oracle SQL functions to score data stored right in the database. This way, the user/application developer can leverage the full power of Oracle SQL - in terms of the ability to pipeline and manipulate the results over several levels, and in terms of parallelizing and partitioning data access for performance.\n\nModels can be created and managed by one of several means. (Oracle Data Miner) is a graphical user interface that steps the user through the process of creating, testing, and applying models (e.g. along the lines of the CRISP-DM methodology). Application and tools developers can embed predictive and descriptive mining capabilities using PL/SQL or Java APIs. Business analysts can quickly experiment with, or demonstrate the power of, predictive analytics using Oracle Spreadsheet Add-In for Predictive Analytics, a dedicated Microsoft Excel adaptor interface. ODM offers a choice of well known machine learning approaches such as Decision Trees, Naive Bayes, Support vector machines, Generalized linear model (GLM) for predictive mining, Association rules, K-means and Orthogonal Partitioning[1][2] Clustering, and Non-negative matrix factorization for descriptive mining. A minimum description length based technique to grade the relative importance of an input mining attributes for a given problem is also provided. Most Oracle Data Mining functions also allow text mining by accepting Text (unstructured data) attributes as input. Users do not need to configure text mining options, this is handled behind the scenes by the Database_options database option.\nHistory\n\nOracle Data Mining was first introduced in 2002 and its releases are named according to the corresponding Oracle database release:\n\n    Oracle Data Mining 9iR2 (9.2.0.1.0 - May 2002)\n    Oracle Data Mining 10gR1 (10.1.0.2.0 - February 2004)\n    Oracle Data Mining 10gR2 (10.2.0.1.0 - July 2005)\n    Oracle Data Mining 11gR1 (11.1 - September 2007)\n    Oracle Data Mining 11gR2 (11.2 - September 2009)\n\nOracle Data Mining is a logical successor of the Darwin data mining toolset developed by Thinking Machines Corporation in the mid-1990s and later distributed by Oracle after its acquisition of Thinking Machines in 1999. However, the product itself is a complete redesign and rewrite from ground-up - while Darwin was a classic GUI-based analytical workbench, ODM offers a data mining development/deployment platform integrated into the Oracle database, along with the Oracle Data Miner GUI.\n\nThe Oracle Data Miner 11gR2 New Workflow GUI was previewed at Oracle Open World 2009. An updated Oracle Data Miner GUI was released in 2012. It is free, and is available as an extension to Oracle SQL Developer 3.1 .\nFunctionality\n\nAs of release 11gR1 Oracle Data Mining contains the following data mining functions:\n\n    Data transformation and model analysis:\n        Data sampling, binning, discretization, and other data transformations.\n        Model exploration, evaluation and analysis.\n    Feature selection (Attribute Importance).\n        Minimum description length (MDL).\n    Classification.\n        Naive Bayes (NB).\n        Generalized linear model (GLM) for Logistic regression.\n        Support Vector Machine (SVM).\n        Decision Trees (DT).\n    Anomaly detection.\n        One-class Support Vector Machine (SVM).\n    Regression\n        Support Vector Machine (SVM).\n        Generalized linear model (GLM) for Multiple regression\n    Clustering:\n        Enhanced k-means (EKM).\n        Orthogonal Partitioning Clustering (O-Cluster).[1][2]\n    Association rule learning:\n        Itemsets and association rules (AM).\n    Feature extraction.\n        Non-negative matrix factorization (NMF).\n    Text and spatial mining:\n        Combined text and non-text columns of input data.\n        Spatial/GIS data.\n\nInput sources and data preparation\n\nMost Oracle Data Mining functions accept as input one relational table or view. Flat data can be combined with transactional data through the use of nested columns, enabling mining of data involving one-to-many relationships (e.g. a star schema). The full functionality of SQL can be used when preparing data for data mining, including dates and spatial data.\n\nOracle Data Mining distinguishes numerical, categorical, and unstructured (text) attributes. The product also provides utilities for data preparation steps prior to model building such as outlier treatment, discretization, normalization and binning (sorting in general speak)\nGraphical user interface: Oracle Data Miner\n\nUsers can access Oracle Data Mining through Oracle Data Miner, a GUI client application that provides access to the data mining functions and structured templates (called Mining Activities) that automatically prescribe the order of operations, perform required data transformations, and set model parameters. The user interface also allows the automated generation of Java and/or SQL code associated with the data-mining activities. The Java Code Generator is an extension to Oracle JDeveloper. An independent interface also exists: the Spreadsheet Add-In for Predictive Analytics which enables access to the Oracle Data Mining Predictive Analytics PL/SQL package from Microsoft Excel.\n\nFrom version 11.2 of the Oracle database, Oracle Data Miner integrates with Oracle SQL Developer.[3]\nPL/SQL and Java interfaces\n\nOracle Data Mining provides a native PL/SQL package (DBMS_DATA_MINING) to create, destroy, describe, apply, test, export and import models. The code below illustrates a typical call to build a classification model:\n\nBEGIN\n  DBMS_DATA_MINING.CREATE_MODEL (\n    model_name          => 'credit_risk_model', \n    function            => DBMS_DATA_MINING.classification, \n    data_table_name     => 'credit_card_data', \n    case_id_column_name => 'customer_id', \n    target_column_name  => 'credit_risk',\n    settings_table_name => 'credit_risk_model_settings');\nEND;\n\nwhere 'credit_risk_model' is the model name, built for the express purpose of classifying future customers' 'credit_risk', based on training data provided in the table 'credit_card_data', each case distinguished by a unique 'customer_id', with the rest of the model parameters specified through the table 'credit_risk_model_settings'.\n\nOracle Data Mining also supports a Java API consistent with the Java Data Mining (JDM) standard for data mining (JSR-73) for enabling integration with web and Java EE applications and to facilitate portability across platforms.\nSQL scoring functions\n\nAs of release 10gR2, Oracle Data Mining contains built-in SQL functions for scoring data mining models. These single-row functions support classification, regression, anomaly detection, clustering, and feature extraction. The code below illustrates a typical usage of a classification model:\n\nSELECT customer_name\n  FROM credit_card_data\n WHERE PREDICTION (credit_risk_model USING *) = 'LOW' AND customer_value = 'HIGH';\n\nPMML\n\nIn Release 11gR2 (11.2.0.2), ODM supports the import of externally created PMML for some of the data mining models. PMML is an XML-based standard for representing data mining models.\nPredictive Analytics MS Excel Add-In\n\nThe PL/SQL package DBMS_PREDICTIVE_ANALYTICS automates the data mining process including data preprocessing, model building and evaluation, and scoring of new data. The PREDICT operation is used for predicting target values classification or regression while EXPLAIN ranks attributes in order of influence in explaining a target column feature selection. The new 11g feature PROFILE finds customer segments and their profiles, given a target attribute. These operations can be used as part of an operational pipeline providing actionable results or displayed for interpretation by end users.\nReferences and further reading\n\n    T. H. Davenport, Competing on Analytics \n    , Harvard Business Review, January 2006.\n    I. Ben-Gal,Outlier detection \n    , In: Maimon O. and Rockach L. (Eds.) Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers,\" Kluwer Academic Publishers, 2005, ISBN 0-387-24435-2.\n    M. M. Campos, P. J. Stengard, and B. L. Milenova, Data-centric Automated Data Mining. In proceedings of the Fourth International Conference on Machine Learning and Applications 2005, 15–17 December 2005. pp8, ISBN 0-7695-2495-8\n    M. F. Hornick, Erik Marcade, and Sunil Venkayala. Java Data Mining: Strategy, Standard, and Practice. Morgan-Kaufmann, 2006, ISBN 0-12-370452-9.\n    B. L. Milenova, J. S. Yarmus, and M. M. Campos. SVM in Oracle database 10g: removing the barriers to widespread adoption of support vector machines. In Proceedings of the 31st international Conference on Very Large Data Bases (Trondheim, Norway, August 30 - September 2, 2005). pp1152–1163, ISBN 1-59593-154-6.\n    B. L. Milenova and M. M. Campos. O-Cluster: scalable clustering of large high dimensional data sets. In proceedings of the 2002 IEEE International Conference on Data Mining: ICDM 2002. pp290–297, ISBN 0-7695-1754-4.\n    P. Tamayo, C. Berger, M. M. Campos, J. S. Yarmus, B. L.Milenova, A. Mozes, M. Taft, M. Hornick, R. Krishnan, S.Thomas, M. Kelly, D. Mukhin, R. Haberstroh, S. Stephens and J. Myczkowski. Oracle Data Mining - Data Mining in the Database Environment. In Part VII of Data Mining and Knowledge Discovery Handbook, Maimon, O.; Rokach, L. (Eds.) 2005, p315-1329, ISBN 0-387-24435-2.\n    Brendan Tierney, Predictive Analytics using Oracle Data Miner: for the data scientist, oracle analyst, oracle developer & DBA, Oracle Press, McGraw Hill, Spring 2014.", "skillName": "Oracle Data Mining."}
{"id": 73, "category": "statistical_software", "skillText": "GNU Octave is software featuring a high-level programming language, primarily intended for numerical computations. Octave helps in solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with MATLAB. It may also be used as a batch-oriented language. Since it is part of the GNU Project, it is free software under the terms of the GNU General Public License.\n\nOctave is one of the major free alternatives to MATLAB, another being Scilab.[4][5][6][7] Scilab however puts less emphasis on (bidirectional) syntactic compatibility with MATLAB than Octave does.[4][8][9]\n\nContents\n\n    1 History\n    2 Developments\n    3 Technical details\n    4 Octave, the language\n    5 Notable features\n        5.1 Command and variable name completion\n        5.2 Command history\n        5.3 Data structures\n        5.4 Short-circuit boolean operators\n        5.5 Increment and decrement operators\n        5.6 Unwind-protect\n        5.7 Variable-length argument lists\n        5.8 Variable-length return lists\n        5.9 C++ integration\n    6 MATLAB compatibility\n        6.1 Syntax compatibility\n        6.2 Function compatibility\n    7 User interfaces\n    8 See also\n    9 References\n    10 Further reading\n    11 External links\n        11.1 Documentation\n        11.2 Numerical packages and libraries interfacing with GNU Octave\n            11.2.1 Numerical tools\n            11.2.2 Plotting tools\n                11.2.2.1 MATLAB-like IDEs\n                11.2.2.2 Other GUIs\n                11.2.2.3 Web-based user interfaces (WUI)\n\nHistory\n\nThe project was conceived around 1988. At first it was intended to be a companion to a chemical reactor design course. Real development was started by John W. Eaton in 1992. The first alpha release dates back to January 4, 1993 and on February 17, 1994 version 1.0 was released. Version 4.0.0 was released on May 29, 2015.\n\nThe program is named after Octave Levenspiel, a former professor of the principal author. Levenspiel is known for his ability to perform quick back-of-the-envelope calculations.[10]\nDevelopments\n\nIn addition to use on desktops for personal scientific computing, Octave is used in academia and industry. For example, Octave was used on a massive parallel computer at Pittsburgh supercomputing center to find vulnerabilities related to guessing social security numbers.[11]\nTechnical details\n\n    Octave is written in C++ using the C++ standard library.\n    Octave uses an interpreter to execute the Octave scripting language.\n    Octave is extensible using dynamically loadable modules.\n    Octave interpreter has an OpenGL-based graphics engine to create plots, graphs and charts and to save or print them. Alternatively, gnuplot can be used for the same purpose.\n    Octave versions 3.8.0 and later include a Graphical User Interface (GUI) in addition to the traditional Command Line Interface (CLI).\n\nOctave, the language\n\nThe Octave language is an interpreted programming language. It is a structured programming language (similar to C) and supports many common C standard library functions, and also certain UNIX system calls and functions.[12] However, it does not support passing arguments by reference.[13]\n\nOctave programs consist of a list of function calls or a script. The syntax is matrix-based and provides various functions for matrix operations. It supports various data structures and allows object-oriented programming.[14]\n\nIts syntax is very similar to MATLAB, and careful programming of a script will allow it to run on both Octave and MATLAB.[15]\n\nBecause Octave is made available under the GNU General Public License, it may be freely changed, copied and used.[10] The program runs on Microsoft Windows and most Unix and Unix-like operating systems, including OS X.[16]\nNotable features\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2013) (Learn how and when to remove this template message)\nCommand and variable name completion\n\nTyping a TAB character on the command line causes Octave to attempt to complete variable, function, and file names (similar to Bash's tab completion). Octave uses the text before the cursor as the initial portion of the name to complete. [17]\nCommand history\n\nWhen running interactively, Octave saves the commands typed in an internal buffer so that they can be recalled and edited.\nData structures\n\nOctave includes a limited amount of support for organizing data in structures. In this example, we see a structure \"x\" with elements \"a\", \"b\", and \"c\", (an integer, an array, and a string, respectively):\n\noctave:1> x.a = 1; x.b = [1, 2; 3, 4]; x.c = \"string\";\noctave:2> x.a\nans =  1\noctave:3> x.b\nans =\n\n   1   2\n   3   4\n\noctave:4> x.c\nans = string\noctave:5> x\nx =\n{\n  a =  1\n  b =\n\n     1   2\n     3   4\n\n  c = string\n}\n\nShort-circuit boolean operators\n\nOctave's '&&' and '||' logical operators are evaluated in a short-circuit fashion (like the corresponding operators in the C language), in contrast to the element-by-element operators '&' and '|'.\nIncrement and decrement operators\nMain article: Increment and decrement operators\n\nOctave includes the C-like increment and decrement operators '++' and '--' in both their prefix and postfix forms. Octave also does augmented assignment, e.g. 'x += 5'.\nUnwind-protect\n\nOctave supports a limited form of exception handling modelled after the 'unwind_protect' \nof Lisp. The general form of an unwind_protect block looks like this:\n\nunwind_protect\n   body\nunwind_protect_cleanup\n   cleanup\nend_unwind_protect\n\nAs a general rule, GNU Octave recognizes as termination of a given 'block' either the keyword 'end' (which is compatible with the MATLAB language) or a more specific keyword 'end_block'. As a consequence, an 'unwind_protect' block can be terminated either with the keyword 'end_unwind_protect' as in the example, or with the more portable keyword 'end'.\n\nThe cleanup part of the block is always executed. In case an exception is raised by the body part, cleanup is executed immediately before propagating the exception outside the block 'unwind_protect'.\n\nGNU Octave also supports another form of exception handling (compatible with the MATLAB language):\n\ntry\n   body\ncatch\n   exception_handling\nend\n\nThis latter form differs from an 'unwind_protect' block in two ways. First, exception_handling is only executed when an exception is raised by body. Second, after the execution of exception_handling the exception is not propagated outside the block (unless a 'rethrow( lasterror )' statement is purposely inserted within the exception_handling code).\nVariable-length argument lists\n\nOctave has a mechanism for handling functions that take an unspecified number of arguments without explicit upper limit. To specify a list of zero or more arguments, use the special argument varargin as the last (or only) argument in the list.\n\nfunction s = plus (varargin)\n   if (nargin==0)\n      s = 0;\n   else\n      s = varargin{1} + plus (varargin{2:nargin});\n   end\nend\n\nVariable-length return lists\n\nA function can be set up to return any number of values by using the special return value varargout. For example:\n\nfunction varargout = multiassign (data)\n   for k=1:nargout\n      varargout{k} = data(:,k);\n   end\nend\n\nC++ integration\n\nIt is also possible to execute Octave code directly in a C++ program. For example, here is a code snippet for calling rand([10,1]):\n\n#include <octave/oct.h>\n...\nColumnVector NumRands(2);\nNumRands(0) = 10;\nNumRands(1) = 1;\noctave_value_list f_arg, f_ret;\nf_arg(0) = octave_value(NumRands);\nf_ret = feval(\"rand\", f_arg, 1);\nMatrix unis(f_ret(0).matrix_value());\n\nC and C++ code can be integrated into GNU Octave by creating oct files, or using the Matlab compatible MEX files.\nMATLAB compatibility\n\nOctave has been built with MATLAB compatibility in mind, and shares many features with MATLAB:\n\n    Matrices as fundamental data type.\n    Built-in support for complex numbers.\n    Powerful built-in math functions and extensive function libraries.\n    Extensibility in the form of user-defined functions.\n\nIn fact, Octave treats incompatibility with MATLAB as a bug;[18] therefore, it can be considered a software clone, which doesn't infringe software copyright as per Lotus v. Borland court case.\nSyntax compatibility\n\nThere are a few purposeful, albeit minor, syntax additions \n:\n\n    Comment lines can be prefixed with the # character as well as the % character;\n    Various C-based operators ++, --, +=, *=, /= are supported;\n    Elements can be referenced without creating a new variable by cascaded indexing, e.g. [1:10](3);\n    Strings can be defined with the \" character as well as the ' character;\n    When the variable type is single, Octave calculates the \"mean\" in the single-domain (Matlab in double-domain) which is faster but gives less accurate results;\n    Blocks can also be terminated with more specific Control structure keywords, i.e., endif, endfor, endwhile, etc.;\n    Functions can be defined within scripts and at the Octave prompt;\n    All operators perform automatic broadcasting or singleton expansion.\n    Presence of a do-until loop (similar to do-while in C).\n\nFunction compatibility\n\nMany of the numerous MATLAB functions are available in GNU Octave, some of them are accessible through packages via Octave-forge, but not all of MATLAB functions are available in GNU Octave. List of unavailable functions exists in Octave, and developers are seeking for help to implement them. Looking for function __unimplemented.m__, leads to the list of unimplemented functions \n.\n\nUnimplemented functions are also categorized in Image \n, Mapping \n, Optimization \n, Signal \n, and Statistics \npackages.\n\nWhen an unimplemented function is called the following error message is shown:\n\n  octave:1> quad2d\n  warning: quad2d is not implemented. Consider using dblquad.\n\n  Please read <http://www.octave.org/missing.html> to learn how you can\n  contribute missing functionality.\n  warning: called from\n      __unimplemented__ at line 523 column 5\n  error: 'quad2d' undefined near line 1 column 1\n\nUser interfaces\nUntil version 3.8, Octave did not come with a graphical user interface (GUI)/integrated development environment (IDE) by default. However, an official graphical interface based on Qt has now been migrated to the main source repository and is available with Octave 3.8, but not as the default interface.[19] It has become the default interface with the release of Octave 4.0.[20] Several 3rd-party graphical front-ends have been developed.", "skillName": "Octave."}
{"id": 74, "category": "statistical_software", "skillText": "MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and fourth-generation programming language. A proprietary programming language developed by MathWorks, MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, Java, Fortran and Python.\n\nAlthough MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.\n\nIn 2004, MATLAB had around one million users across industry and academia.[3] MATLAB users come from various backgrounds of engineering, science, and economics.\n\nContents\n\n    1 History\n    2 Syntax\n        2.1 Variables\n        2.2 Vectors and matrices\n        2.3 Structures\n        2.4 Functions\n        2.5 Function handles\n        2.6 Classes and object-oriented programming\n    3 Graphics and graphical user interface programming\n    4 Interfacing with other languages\n    5 License\n    6 Alternatives\n    7 Release history\n    8 File extensions\n        8.1 MATLAB\n        8.2 Simulink\n        8.3 Simscape\n        8.4 MuPAD\n        8.5 Third-party\n    9 Easter eggs\n    10 See also\n    11 Notes\n    12 References\n    13 External links\n\nHistory\n\nCleve Moler, the chairman of the computer science department at the University of New Mexico, started developing MATLAB in the late 1970s.[4] He designed it to give his students access to LINPACK and EISPACK without them having to learn Fortran. It soon spread to other universities and found a strong audience within the applied mathematics community. Jack Little, an engineer, was exposed to it during a visit Moler made to Stanford University in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in C and founded MathWorks in 1984 to continue its development. These rewritten libraries were known as JACKPAC.[5] In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, LAPACK.[6]\n\nMATLAB was first adopted by researchers and practitioners in control engineering, Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of linear algebra, numerical analysis, and is popular amongst scientists involved in image processing.[4]\nSyntax\n\nThe MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical shell or executing text files containing MATLAB code.[7]\nVariables\n\nVariables are defined using the assignment operator, =. MATLAB is a weakly typed programming language because types are implicitly converted.[8] It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects,[9] and that their type can change. Values can come from constants, from computation involving values of other variables, or from the output of a function. For example:\n\n>> x = 17\nx =\n 17\n\n>> x = 'hat'\nx =\nhat\n\n>> y = x + 0\ny =\n       104        97       116\n\n>> x = [3*4, pi/2]\nx =\n   12.0000    1.5708\n\n>> y = 3*sin(x)\ny =\n   -1.6097    3.0000\n\nVectors and matrices\n\nA simple array is defined using the colon syntax: init:increment:terminator. For instance:\n\n>> array = 1:2:9\narray =\n 1 3 5 7 9\n\ndefines a variable named array (or assigns a new value to an existing variable with the name array) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the init value), increments with each step from the previous value by 2 (the increment value), and stops once it reaches (or to avoid exceeding) 9 (the terminator value).\n\n>> array = 1:3:9\narray =\n 1 4 7\n\nthe increment value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.\n\n>> ari = 1:5\nari =\n 1 2 3 4 5\n\nassigns to the variable named ari an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.\n\nIndexing is one-based,[10] which is the usual convention for matrices in mathematics, although not for some programming languages such as C, C++, and Java.\n\nMatrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).\n\n>> A = [16 3 2 13; 5 10 11 8; 9 6 7 12; 4 15 14 1]\nA =\n 16  3  2 13\n  5 10 11  8\n  9  6  7 12\n  4 15 14  1\n\n>> A(2,3)\nans =\n 11\n\nSets of indices can be specified by expressions such as \"2:4\", which evaluates to [2, 3, 4]. For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:\n\n>> A(2:4,3:4)\nans =\n 11 8\n 7 12\n 14 1\n\nA square identity matrix of size n can be generated using the function eye, and matrices of any size with zeros or ones can be generated with the functions zeros and ones, respectively.\n\n>> eye(3,3)\nans =\n 1 0 0\n 0 1 0\n 0 0 1\n\n>> zeros(2,3)\nans =\n 0 0 0\n 0 0 0\n\n>> ones(2,3)\nans =\n 1 1 1\n 1 1 1\n\nMost MATLAB functions can accept matrices and will apply themselves to each element. For example, mod(2*J,n) will multiply every element in \"J\" by 2, and then reduce each element modulo \"n\". MATLAB does include standard \"for\" and \"while\" loops, but (as in other similar applications such as R), using the vectorized notation often produces code that is faster to execute. This code, excerpted from the function magic.m, creates a magic square M for odd values of n (MATLAB function meshgrid is used here to generate square matrices I and J containing 1:n).\n\n[J,I] = meshgrid(1:n);\nA = mod(I + J - (n + 3) / 2, n);\nB = mod(I + 2 * J - 2, n);\nM = n * A + B + 1;\n\nStructures\n\nMATLAB has structure data types.[11] Since all variables in MATLAB are arrays, a more adequate name is \"structure array\", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names[12] (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.[13]\nFunctions\n\nWhen creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores. Functions are also often case sensitive.\nFunction handles\n\nMATLAB supports elements of lambda calculus by introducing function handles,[14] or function references, which are implemented either in .m files or anonymous[15]/nested functions.[16]\nClasses and object-oriented programming\n\nMATLAB's support for object-oriented programming includes classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics.[17] However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has handle as a super-class (for reference classes) or not (for value classes).[18]\n\nMethod call behavior is different between value and reference classes. For example, a call to a method\n\nobject.method();\n\ncan alter any member of object only if object is an instance of a reference class.\n\nAn example of a simple class is provided below.\n\nclassdef hello\n    methods\n        function greet(this)\n            disp('Hello!')\n        end\n    end\nend\n\nWhen put into a file named hello.m, this can be executed with the following commands:\n\n>> x = hello;\n>> x.greet();\nHello!\n\nGraphics and graphical user interface programming\n\nMATLAB supports developing applications with graphical user interface (GUI) features. MATLAB includes GUIDE[19] (GUI development environment) for graphically designing GUIs.[20] It also has tightly integrated graph-plotting features. For example, the function plot can be used to produce a graph from two vectors x and y. The code:\n\nx = 0:pi/100:2*pi;\ny = sin(x);\nplot(x,y)\n\nproduces the following figure of the sine function:\n\nMatlab plot sin.svg\n\nA MATLAB program can produce three-dimensional graphics using the functions surf, plot3 or mesh.\n\n[X,Y] = meshgrid(-10:0.25:10,-10:0.25:10);\nf = sinc(sqrt((X/pi).^2+(Y/pi).^2));\nmesh(X,Y,f);\naxis([-10 10 -10 10 -0.3 1])\nxlabel('{\\bfx}')\nylabel('{\\bfy}')\nzlabel('{\\bfsinc} ({\\bfR})')\nhidden off\n\n\t    \t\n\n[X,Y] = meshgrid(-10:0.25:10,-10:0.25:10);\nf = sinc(sqrt((X/pi).^2+(Y/pi).^2));\nsurf(X,Y,f);\naxis([-10 10 -10 10 -0.3 1])\nxlabel('{\\bfx}')\nylabel('{\\bfy}')\nzlabel('{\\bfsinc} ({\\bfR})')\n\nThis code produces a wireframe 3D plot of the two-dimensional unnormalized sinc function: \t    \tThis code produces a surface 3D plot of the two-dimensional unnormalized sinc function:\nMATLAB mesh sinc3D.svg \t    \tMATLAB surf sinc3D.svg\n\nIn MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.[21]\nInterfacing with other languages\n\nMATLAB can call functions and subroutines written in the programming languages C or Fortran.[22] A wrapper function is created allowing MATLAB data types to be passed and returned. The dynamically loadable object files created by compiling such functions are termed \"MEX-files\" (for MATLAB executable).[23][24] Since 2014 increasing two-way interfacing with Python is being added.[25][26]\n\nLibraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB,[27][28] and many MATLAB libraries (for example XML or SQL support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox[29] which is sold separately by MathWorks, or using an undocumented mechanism called JMI (Java-to-MATLAB Interface),[30][31] (which should not be confused with the unrelated Java Metadata Interface that is also called JMI).\n\nAs alternatives to the MuPAD based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to Maple or Mathematica.[32][33]\n\nLibraries also exist to import and export MathML.[34]\nLicense\n\nMATLAB is a proprietary product of MathWorks, so users are subject to vendor lock-in.[3][35] Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with .NET[36] or Java[37] application building environment, future development will still be tied to the MATLAB language.\n\nEach toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.\n\nIt has been reported that European Union (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor.[38] The regulators dropped the investigation after the complainant withdrew their accusation and no evidence of wrongdoing was found.[39]\nAlternatives\nSee also: list of numerical analysis software and comparison of numerical analysis software\n\nMATLAB has a number of competitors.[40] Commercial competitors include Mathematica, TK Solver, Maple, and IDL. There are also free open source alternatives to MATLAB, in particular GNU Octave, Scilab, FreeMat, Julia, and Sage which are intended to be mostly compatible with the MATLAB language. Among other languages that treat arrays as basic entities (array programming languages) are APL, Fortran 90 and higher, S-Lang, as well as the statistical languages R and S. There are also libraries to add similar functionality to existing languages, such as IT++ for C++, Perl Data Language for Perl, ILNumerics for .NET, NumPy/SciPy/matplotlib for Python, SciLua \n/Torch for Lua, SciRuby \nfor Ruby, and Numeric.js \nfor JavaScript.\n\nGNU Octave is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see MATLAB Compatibility of GNU Octave). Therefore, GNU Octave attempts to provide a software clone of MATLAB.\nRelease history\nVersion[41] \tRelease name \tNumber \tBundled JVM \tYear \tRelease date \tNotes\nMATLAB 1.0 \t\t\t\t1984 \t\t\nMATLAB 2 \t\t\t\t1986 \t\t\nMATLAB 3 \t\t\t\t1987 \t\t\nMATLAB 3.5 \t\t\t\t1990 \t\tRan on DOS but needed at least a 386 processor; version 3.5m needed math coprocessor\nMATLAB 4 \t\t\t\t1992 \t\t\nMATLAB 4.2c \t\t\t\t1994 \t\tRan on Windows 3.1x, needed a math coprocessor\nMATLAB 5.0 \tVolume 8 \t\t\t1996 \tDecember, 1996 \tUnified releases across all platforms\nMATLAB 5.1 \tVolume 9 \t\t\t1997 \tMay, 1997 \t\nMATLAB 5.1.1 \tR9.1 \t\t\t\t\nMATLAB 5.2 \tR10 \t\t\t1998 \tMarch, 1998 \t\nMATLAB 5.2.1 \tR10.1 \t\t\t\t\nMATLAB 5.3 \tR11 \t\t\t1999 \tJanuary, 1999 \t\nMATLAB 5.3.1 \tR11.1 \t\t\tNovember, 1999 \t\nMATLAB 6.0 \tR12 \t12 \t1.1.8 \t2000 \tNovember, 2000 \tFirst release with bundled Java virtual machine (JVM)\nMATLAB 6.1 \tR12.1 \t1.3.0 \t2001 \tJune, 2001 \t\nMATLAB 6.5 \tR13 \t13 \t1.3.1 \t2002 \tJuly, 2002 \t\nMATLAB 6.5.1 \tR13SP1 \t\t2003 \t\t\nMATLAB 6.5.2 \tR13SP2 \t\t\tLast release for IBM/AIX, Alpha/TRU64, and SGI/IRIX[42]\nMATLAB 7 \tR14 \t14 \t1.4.2 \t2004 \tJune, 2004 \tIntroduced anonymous and nested functions[43]\nMATLAB 7.0.1 \tR14SP1 \t\tOctober, 2004 \t\nMATLAB 7.0.4 \tR14SP2 \t1.5.0 \t2005 \tMarch 7, 2005 \tSupport for memory-mapped files[44]\nMATLAB 7.1 \tR14SP3 \t1.5.0 \tSeptember 1, 2005 \t\nMATLAB 7.2 \tR2006a \t15 \t1.5.0 \t2006 \tMarch 1, 2006 \t\nMATLAB 7.3 \tR2006b \t16 \t1.5.0 \tSeptember 1, 2006 \tHDF5-based MAT-file support\nMATLAB 7.4 \tR2007a \t17 \t1.5.0_07 \t2007 \tMarch 1, 2007 \tNew bsxfun function to apply element-by-element binary operation with singleton expansion enabled[45]\nMATLAB 7.5 \tR2007b \t18 \t1.6.0 \tSeptember 1, 2007 \tLast release for Windows 2000 and PowerPC Mac; License Server support for Windows Vista;[46] new internal format for P-code\nMATLAB 7.6 \tR2008a \t19 \t1.6.0 \t2008 \tMarch 1, 2008 \tMajor enhancements to object-oriented programming abilities with a new class definition syntax,[47] and ability to manage namespaces with packages[48]\nMATLAB 7.7 \tR2008b \t20 \t1.6.0_04 \tOctober 9, 2008 \tNew Map data structure:[49] upgrades to random number generators[50]\nMATLAB 7.8 \tR2009a \t21 \t1.6.0_04 \t2009 \tMarch 6, 2009 \tFirst release for Microsoft 32-bit & 64-bit Windows 7, new external interface to .NET Framework[51]\nMATLAB 7.9 \tR2009b \t22 \t1.6.0_12 \tSeptember 4, 2009 \tFirst release for Intel 64-bit Mac, and last for Solaris SPARC; new use for the tilde operator (~) to ignore arguments in function calls[52][53]\nMATLAB 7.9.1 \tR2009bSP1 \t1.6.0_12 \t2010 \tApril 1, 2010 \tbug fixes.\nMATLAB 7.10 \tR2010a \t23 \t1.6.0_12 \tMarch 5, 2010 \tLast release for Intel 32-bit Mac\nMATLAB 7.11 \tR2010b \t24 \t1.6.0_17 \tSeptember 3, 2010 \tAdd support for enumerations[54]\nMATLAB 7.11.1 \tR2010bSP1 \t1.6.0_17 \t2011 \tMarch 17, 2011 \tbug fixes and updates\nMATLAB 7.11.2 \tR2010bSP2 \t1.6.0_17 \tApril 5, 2012[55] \tbug fixes\nMATLAB 7.12 \tR2011a \t25 \t1.6.0_17 \tApril 8, 2011 \tNew rng function to control random number generation[56][57][58]\nMATLAB 7.13 \tR2011b \t26 \t1.6.0_17 \tSeptember 1, 2011 \tAccess-change parts of variables directly in MAT-files, without loading into memory;[59] increased maximum local workers with Parallel Computing Toolbox from 8 to 12[60]\nMATLAB 7.14 \tR2012a \t27 \t1.6.0_17 \t2012 \tMarch 1, 2012 \t\nMATLAB 8 \tR2012b \t28 \t1.6.0_17 \tSeptember 11, 2012 \tFirst release with Toolstrip interface;[61] MATLAB Apps.[62] redesigned documentation system\nMATLAB 8.1 \tR2013a \t29 \t1.6.0_17 \t2013 \tMarch 7, 2013 \tNew unit testing framework[63]\nMATLAB 8.2 \tR2013b \t30 \t1.7.0_11 \tSeptember 6, 2013[64] \tNew table data type[65]\nMATLAB 8.3 \tR2014a \t31 \t1.7.0_11 \t2014 \tMarch 7, 2014[66] \tSimplified compiler setup for building MEX-files; USB Webcams support in core MATLAB; number of local workers no longer limited to 12 with Parallel Computing Toolbox\nMATLAB 8.4 \tR2014b \t32 \t1.7.0_11 \tOctober 3, 2014 \tNew class-based graphics engine (a.k.a. HG2);[67] tabbing function in GUI;[68] improved user toolbox packaging and help files;[69] new objects for time-date manipulations;[70] Git-Subversion integration in IDE;[71] big data abilities with MapReduce (scalable to Hadoop);[72] new py package for using Python from inside MATLAB, new engine interface to call MATLAB from Python;[73][74] several new and improved functions: webread (RESTful web services with JSON/XML support), tcpclient (socket-based connections), histcounts, histogram, animatedline, and others\nMATLAB 8.5 \tR2015a \t33 \t1.7.0_60 \t2015 \tMarch 5, 2015 \t\nMATLAB 8.6 \tR2015b \t34 \t1.7.0_60 \tSeptember 3, 2015 \t\nMATLAB 9.0 \tR2016a \t35 \t1.7.0_60 \t2016 \tMarch 3, 2016 \t\n\nThe number (or release number) is the version reported by Concurrent License Manager program FLEXlm.\n\nFor a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.[75]\nFile extensions\nMATLAB\n\n.fig \n    MATLAB figure\n.m \n    MATLAB code (function, script, or class)\n.mat \n    MATLAB data (binary file for storing variables)\n.mex... (.mexw32, .mexw64, .mexglx, ...) \n    MATLAB executable MEX-files[76] (platform specific, e.g. \".mexmac\" for the Mac, \".mexglx\" for Linux, etc.[77])\n.p \n    MATLAB content-obscured .m file (P-code[78])\n.mlappinstall \n    MATLAB packaged App Installer[79]\n.mlpkginstall\n    support package installer (add-on for third-party hardware)[80]\n.mltx\n    packaged custom toolbox[81]\n.prj\n    project file used by various solutions (packaged app/toolbox projects, MATLAB Compiler/Coder projects, Simulink projects)\n.rpt\n    report setup file created by MATLAB Report Generator[82]\n\nSimulink\n\n.mdl \n    Simulink Model\n.mdlp \n    Simulink Protected Model\n.slx \n    Simulink Model (SLX format)\n.slxp \n    Simulink Protected Model (SLX format)\n\nSimscape\n\n.ssc \n    Simscape[83] Model\n\nMuPAD\n\n.mn \n    MuPAD Notebook\n.mu \n    MuPAD Code\n.xvc, .xvz \n    MuPAD Graphics\n\nThird-party\n\n.jkt \n    GPU Cache file generated by Jacket for MATLAB (AccelerEyes)\n.mum \n    MATLAB CAPE-OPEN Unit Operation Model File (AmsterCHEM)\n\nEaster eggs\nScreen capture of two easter eggs in MATLAB 3.5.\n\nSeveral easter eggs exist in MATLAB.[84] These include hidden pictures,[85] and jokes. For example, typing in \"spy\" will generate a picture of the spies from Spy vs Spy. \"Spy\" was changed to an image of a dog in recent releases (R2011B). Typing in \"why\" randomly outputs a philosophical answer. Other commands include \"penny\", \"toilet\", \"image\", and \"life\". Not every Easter egg appears in every version of MATLAB.", "skillName": "MATLAB."}
{"id": 75, "category": "statistical_software", "skillText": "General Architecture for Text Engineering or GATE is a Java suite of tools originally developed at the University of Sheffield beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many natural language processing tasks, including information extraction in many languages.[1]\n\nGATE has been compared to NLTK, R and RapidMiner.[2] As well as being widely used in its own right, it forms the basis of the KIM semantic platform.[3]\n\nGATE community and research has been involved in several European research projects including TAO, SEKT, NeOn, Media-Campaign, Musing, Service-Finder, LIRICS and KnowledgeWeb, as well as many other projects.\n\nAs of May 28, 2011, 881 people are on the gate-users mailing list at SourceForge.net, and 111,932 downloads from SourceForge are recorded since the project moved to SourceForge in 2005.[4] The paper \"GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications\"[5] has received over 800 citations in the seven years since publication (according to Google Scholar). Books covering the use of GATE, in addition to the GATE User Guide,[6] include \"Building Search Applications: Lucene, LingPipe, and Gate\", by Manu Konchady,[7] and \"Introduction to Linguistic Annotation and Text Analytics\", by Graham Wilcock.[8]\n\nContents\n\n    1 Features\n    2 GATE Developer\n    3 GATE Mímir\n    4 See also\n    5 References\n\nFeatures\n\nGATE includes an information extraction system called ANNIE (A Nearly-New Information Extraction System) which is a set of modules comprising a tokenizer, a gazetteer, a sentence splitter, a part of speech tagger, a named entities transducer and a coreference tagger. ANNIE can be used as-is to provide basic information extraction functionality, or provide a starting point for more specific tasks.\n\nLanguages currently handled in GATE include English, Chinese, Arabic, Bulgarian, French, German, Hindi, Italian, Cebuano, Romanian, Russian, Danish.\n\nPlugins are included for machine learning with Weka, RASP, MAXENT, SVM Light, as well as a LIBSVM integration and an in-house perceptron implementation, for managing ontologies like WordNet, for querying search engines like Google or Yahoo, for part of speech tagging with Brill or TreeTagger, and many more. Many external plugins are also available, for handling e.g. tweets.[9]\n\nGATE accepts input in various formats, such as TXT, HTML, XML, Doc, PDF documents, and Java Serial, PostgreSQL, Lucene, Oracle Databases with help of RDBMS storage over JDBC.\n\nJAPE transducers are used within GATE to manipulate annotations on text. Documentation is provided in the GATE User Guide.[10] A tutorial has also been written by Press Association Images.[11]\nGATE Developer\nGATE 5 main window.\n\nThe screenshot shows the document viewer used to display a document and its annotations. In pink are <A> hyperlink annotations from an HTML file. The right list is the annotation sets list, and the bottom table is the annotation list. In the center is the annotation editor window.\nGATE Mímir\n\nGenerate vast quantities of information including; natural language text, semantic annotations, and ontological information. Sometimes the data itself is the end product of an application but often the information would be more useful if it could be efficiently searched. GATE Mimir provides support for indexing and searching the linguistic and semantic information generated by such applications and allows for querying the information using arbitrary combinations of text, structural information, and SPARQL.", "skillName": "GATE."}
{"id": 76, "category": "statistical_software", "skillText": "KNIME (pronounced /naɪm/), the Konstanz Information Miner, is an open source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining concept. A graphical user interface allows assembly of nodes for data preprocessing (ETL: Extraction, Transformation, Loading), for modeling and data analysis and visualization.\n\nSince 2006, KNIME has been used in pharmaceutical research,[2] but is also used in other areas like CRM customer data analysis, business intelligence and financial data analysis.\n\nContents\n\n    1 History\n    2 Internals\n    3 License\n    4 See also\n    5 References\n    6 External links\n\nHistory\n\nThe Development of KNIME was started January 2004 by a team of software engineers at University of Konstanz as a proprietary product. The original developer team headed by Michael Berthold came from a company in the Silicon Valley providing software for the pharmaceutical industry. KNIME has been developed from day one using rigorous professional software engineering processes since it was clear from the beginning that it was to be used in large scale enterprises[citation needed]. The initial goal was to create a modular, highly scalable and open data processing platform which allowed for the easy integration of different data loading, processing, transformation, analysis and visual exploration modules without the focus on any particular application area. The platform was intended to be a collaboration and research platform and should also serve as an integration platform for various other data analysis projects[citation needed].\n\nIn 2006 the first version of KNIME was released and several pharmaceutical companies started using KNIME and a number of life science software vendors began integrating their tools into KNIME.[3][4][5][6][7] Later that year, after an article in the German magazine c't,[8] users from a number of other areas[9][10] joined ship. As of 2012, KNIME is in use by over 15,000 actual users (i.e. not counting downloads but users regularly retrieving updates when they become available) not only in the life sciences but also at banks, publishers, car manufacturer, telcos, consulting firms, and various other industries but also at a large number of research groups worldwide.\nA screenshot of KNIME.\nInternals\n\nKNIME allows users to visually create data flows (or pipelines), selectively execute some or all analysis steps, and later inspect the results, models, and interactive views. KNIME is written in Java and based on Eclipse and makes use of its extension mechanism to add plugins providing additional functionality. The core version already includes hundreds of modules for data integration (file I/O, database nodes supporting all common database management systems), data transformation (filter, converter, combiner) as well as the commonly used methods for data analysis and visualization. With the free Report Designer extension, KNIME workflows can be used as data sets to create report templates that can be exported to document formats like doc, ppt, xls, pdf and others. Other capabilities of KNIME are:\n\n    KNIMEs core-architecture allows processing of large data volumes that are only limited by the available hard disk space (most other open source data analysis tools are working in main memory and are therefore limited to the available RAM). E.g. KNIME allows analysis of 300 million customer addresses, 20 million cell images and 10 million molecular structures.\n    Additional plugins allows the integration of methods for Text mining, Image mining, as well as time series analysis.\n    KNIME integrates various other open-source projects, e.g. machine learning algorithms from Weka, the statistics package R project, as well as LIBSVM, JFreeChart, ImageJ, and the Chemistry Development Kit.[11]\n\nKNIME is implemented in Java but also allows for wrappers calling other code in addition to providing nodes that allow to run Java, Python, Perl and other code fragments.\nLicense\nAs of version 2.1, KNIME is released under GPLv3 with an exception that allows others to use the well defined node API to add proprietary extensions.[12] This allows also commercial SW vendors to add wrappers calling their tools from KNIME.", "skillName": "KNIME."}
{"id": 77, "category": "statistical_software", "skillText": "Waffles is a collection of command-line tools for performing machine learning operations developed at Brigham Young University. These tools are written in C++, and are available under the GNU Lesser General Public License.\n\nContents\n\n    1 Description\n    2 Advantages\n    3 Disadvantages\n    4 See also\n    5 References\n\nDescription\n\nThe Waffles machine learning toolkit[1] contains command-line tools for performing various operations related to machine learning, data mining, and predictive modeling. The primary focus of Waffles is to provide tools that are simple to use in scripted experiments or processes. For example, the supervised learning algorithms included in Waffles are all designed to support multi-dimensional labels, classification and regression, automatically impute missing values, and automatically apply necessary filters to transform the data to a type that the algorithm can support, such that arbitrary learning algorithms can be used with arbitrary data sets. Many other machine learning toolkits provide similar functionality, but require the user to explicitly configure data filters and transformations to make it compatible with a particular learning algorithm. The algorithms provided in Waffles also have the ability to automatically tune their own parameters (with the cost of additional computational overhead).\n\nBecause Waffles is designed for script-ability, it deliberately avoids presenting its tools in a graphical environment. It does, however, include a graphical \"wizard\" tool that guides the user to generate a command that will perform a desired task. This wizard does not actually perform the operation, but requires the user to paste the command that it generates into a command terminal or a script. The idea motivating this design is to prevent the user from becoming \"locked in\" to a graphical interface.\n\nAll of the Waffles tools are implemented as thin wrappers around functionality in a C++ class library. This makes it possible to convert scripted processes into native applications with minimal effort.\n\nWaffles was first released as an open source project in 2005. Since that time, it has been developed at Brigham Young University, with a new version having been released approximately every 6–9 months. Waffles is not an acronym—the toolkit was named after the food for historical reasons.\nAdvantages\n\nSome of the advantages of Waffles in contrast with other popular open source machine learning toolkits include:\n\n    Waffles automatically takes care of many issues related to data format in order to simplify its tools.\n    Because it is implemented in C++, many of its algorithms are particularly fast. Also, the lack of dependency on any virtual machine makes it easier to deploy in conjunction with other applications.\n    The functionality included in Waffles is very broad, including algorithms for dimensionality reduction, collaborative filtering, visualization, clustering, supervised learning, optimization, linear algebra, data transformation, image and signal processing, policy learning, and sparse matrix operations.\n\nDisadvantages\n\n    Although Waffles provides significant breadth, it lacks the depth of many toolkits that focus on a particular area of machine learning. The Weka (machine learning) toolkit, for example, provides many more classification algorithms than Waffles provides.\n    Waffles only has a limited graphical interface.", "skillName": "Waffles."}
{"id": 78, "category": "statistical_software", "skillText": "Wolfram Mathematica (sometimes referred to as Mathematica) is a symbolic mathematical computation program, sometimes called a computer algebra program, used in many scientific, engineering, mathematical, and computing fields. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois.[6][7] The Wolfram Language is the programming language used in Mathematica.[8]\n\nContents\n\n    1 Features\n    2 Interface\n    3 High-performance computing\n    4 Deployment\n    5 Connections with other applications\n    6 Computable data\n    7 Version history\n    8 See also\n    9 References\n    10 External links\n\nFeatures\nDini's surface plotted with adjustable parameters\n\nFeatures of Wolfram Mathematica include:[9]\n\n    Elementary and Special mathematical function libraries\n    Support for complex number, arbitrary precision, interval arithmetic and symbolic computation\n    Matrix and data manipulation tools including support for sparse arrays\n    2D and 3D data, function and geo visualization and animation tools\n    Solvers for systems of equations, diophantine equations, ODEs, PDEs, DAEs, DDEs, SDEs and recurrence relations\n    Numeric and symbolic tools for discrete and continuous calculus including continuous and discrete integral transforms\n    Constrained and unconstrained local and global optimization\n    Multivariate statistics libraries including fitting, hypothesis testing, and probability and expectation calculations on over 140 distributions.\n    Support for censored data, temporal data, time-series and unit based data\n    Calculations and simulations on random processes and queues\n    Supervised and unsupervised machine learning tools for data, images and sounds\n    Tools for text mining including regular expressions and semantic analysis\n    Data mining tools such as cluster analysis, sequence alignment and pattern matching\n    Computational geometry in 2D, 3D and higher dimensions\n    Finite element analysis including 2D and 3D adaptive mesh generation\n    Libraries for signal processing including wavelet analysis on sounds, images and data\n    Linear and non-linear Control systems libraries\n    Tools for 2D and 3D image processing[10] and morphological image processing including image recognition\n    Tools for visualizing and analysing directed and undirected graphs\n    Tools for combinatoric problems\n    Number theory function library\n    Tools for financial calculations including bonds, annuities, derivatives, options etc.\n    Group theory and symbolic tensor functions\n    Import and export filters for data, images, video, sound, CAD, GIS,[11] document and biomedical formats\n    Database collection for mathematical, scientific, and socio-economic information and access to WolframAlpha data and computations\n    Technical word processing including formula editing and automated report generating\n    Programming language supporting procedural, functional and object oriented constructs\n    Toolkit for adding user interfaces to calculations and applications\n    Tools for connecting to DLL, SQL, Java, .NET, C++, Fortran, CUDA, OpenCL, and http based systems\n    Tools for parallel programming\n    Using both \"free-form linguistic input\" (a natural language user interface)[12][13] and Wolfram Language in notebook when connected to the Internet\n\nInterface\n\nWolfram Mathematica is split into two parts, the kernel and the front end. The kernel interprets expressions (Wolfram Language code) and returns result expressions.\n\nMathematica is a programming language that has evolved over several years of development\n\nThe front end, designed by Theodore Gray,[14] provides a GUI, which allows the creation and editing of Notebook documents containing program code with prettyprinting, formatted text together with results including typeset mathematics, graphics, GUI components, tables, and sounds. All content and formatting can be generated algorithmically or edited interactively. Most standard word processing capabilities are supported. It includes a spell-checker but does not spell check automatically as you type.\n\nDocuments can be structured using a hierarchy of cells, which allow for outlining and sectioning of a document and support automatic numbering index creation. Documents can be presented in a slideshow environment for presentations. Notebooks and their contents are represented as Mathematica expressions that can be created, modified or analyzed by Mathematica programs. This allows conversion to other formats such as TeX or XML.\n\nThe front end includes development tools such as a debugger, input completion and automatic syntax coloring.\n\nAmong the alternative front ends is the Wolfram Workbench, an Eclipse based IDE, introduced in 2006. It provides project-based code development tools for Mathematica, including revision management, debugging, profiling, and testing.[15] The Mathematica Kernel also includes a command line front end.[16] Other interfaces include JMath,[17] based on GNU readline and MASH[18] which runs self-contained Mathematica programs (with arguments) from the UNIX command line.\n\nWolfram Research has published a series of hands-on starter webcasts that introduce the user interface and the engine.[19]\nHigh-performance computing\n\nIn recent years, the capabilities for high-performance computing have been extended with the introduction of packed arrays (version 4, 1999)[20] and sparse matrices (version 5, 2003),[21] and by adopting the GNU Multi-Precision Library to evaluate high-precision arithmetic.\n\nVersion 5.2 (2005) added automatic multi-threading when computations are performed on multi-core computers.[22] This release included CPU specific optimized libraries. In addition Mathematica is supported by third party specialist acceleration hardware such as ClearSpeed.[23]\n\nIn 2002, gridMathematica was introduced to allow user level parallel programming on heterogeneous clusters and multiprocessor systems[24] and in 2008 parallel computing technology was included in all Mathematica licenses including support for grid technology such as Windows HPC Server 2008, Microsoft Compute Cluster Server and Sun Grid.\n\nSupport for CUDA and OpenCL GPU hardware was added in 2010. Also, since version 8 it can generate C code, which is automatically compiled by a system C compiler, such as GCC or Microsoft Visual Studio.\nDeployment\n\nThere are several ways to deploy applications written in Wolfram Mathematica:\n\n    Mathematica Player Pro is a runtime version of Mathematica that will run any Mathematica application but does not allow editing or creation of the code.[25]\n    A free-of-charge version, Wolfram CDF Player, is provided for running Mathematica programs that have been saved in the Computable Document Format (CDF).[26] It can also view standard Mathematica files, but not run them. It includes plugins for common web browsers on Windows and Macintosh.\n    webMathematica allows a web browser to act as a front end to a remote Mathematica server. It is designed to allow a user written application to be remotely accessed via a browser on any platform. It may not be used to give full access to Mathematica. Due to bandwidth limitations interactive 3D graphics ist not fully supported within a web browser.\n    Wolfram Language code can be converted to C code or to an automatically generated DLL.\n    Wolfram Language code can be run on a Wolfram cloud service as a web-app or as an API\n\nConnections with other applications\n\nCommunication with other applications occurs through a protocol called WSTP \n. It allows communication between the Wolfram Mathematica kernel and front-end, and also provides a general interface between the kernel and other applications. Wolfram Research freely distributes a developer kit for linking applications written in the C programming language to the Mathematica kernel through WSTP. Using J/Link.,[27] a Java program can ask Mathematica to perform computations; likewise, a Mathematica program can load Java classes, manipulate Java objects and perform method calls. Similar functionality is achieved with .NET /Link,[28] but with .NET programs instead of Java programs. Other languages that connect to Mathematica include Haskell,[29] AppleScript,[30] Racket,[31] Visual Basic,[32] Python[33][34] and Clojure.[35]\n\nLinks are available to many mathematical software packages including OpenOffice.org Calc,[36] Microsoft Excel,[37] MATLAB,[38][39][40] R,[41] Sage,[42][43] SINGULAR,[44] Wolfram SystemModeler, and Origin.[45] Mathematical equations can be exchanged with other computational or typesetting software via MathML.\n\nCommunication with SQL databases is achieved through built-in support for JDBC.[46] Mathematica can also install web services from a WSDL description.[47][48] It can access HDFS data via Hadoop.[49]\n\nMathematica can capture real-time data via a link to LabVIEW,[50] from financial data feeds[51] and directly from hardware devices via GPIB (IEEE 488),[52] USB[53] and serial interfaces.[54] It automatically detects and reads from HID devices.\nComputable data\nA stream plot of live weather data\n\nWolfram Mathematica includes collections of curated data provided for use in computations. Mathematica is also integrated with Wolfram Alpha, an online service which provides additional data, some of which is kept updated in real time. Some of the data sets include astronomical, chemical, geopolitical, language, biomedical and weather data, in addition to mathematical data (such as knots and polyhedra).[55]\nVersion history\n\nWolfram Mathematica built on the ideas in Cole and Wolfram's earlier Symbolic Manipulation Program (SMP).[56][57] The name of the program \"Mathematica\" was suggested to Stephen Wolfram by Apple co-founder Steve Jobs although Stephen Wolfram had thought about it earlier and rejected it.[58]\n\nWolfram Research has released the following versions of Mathematica:[59]\n\n    Mathematica 1.0 (June 23, 1988)[60][61][62][63]\n    Mathematica 1.1 (October 31, 1988)\n    Mathematica 1.2 (August 1, 1989)[64][63]\n    Mathematica 2.0 (January 15, 1991)[65][63]\n    Mathematica 2.1 (June 15, 1992)[63]\n    Mathematica 2.2 (June 1, 1993)[63][66]\n    Mathematica 3.0 (September 3, 1996)[67]\n    Mathematica 4.0 (May 19, 1999)[63][68]\n    Mathematica 4.1 (November 2, 2000)[63]\n    Mathematica 4.2 (November 1, 2002)[63]\n    Mathematica 5.0 (June 12, 2003)[63][69]\n    Mathematica 5.1 (October 25, 2004)[63][70]\n    Mathematica 5.2 (June 20, 2005)[63][71]\n    Mathematica 6.0 (May 1, 2007)[72][73]\n    Mathematica 7.0 (November 18, 2008)[74]\n    Mathematica 8.0 (November 15, 2010)\n    Mathematica 9.0 (November 28, 2012)\n    Mathematica 10.0 (July 9, 2014)\n    Mathematica 10.0.2 (December 10, 2014)\n    Mathematica 10.1 (March 30, 2015)[75]\n    Mathematica 10.2 (July 14, 2015)[76]\n    Mathematica 10.3 (October 15, 2015)\n    Mathematica 10.3.1 (December 16, 2015)\n    Mathematica 10.4 (March 2, 2016)\n    Mathematica 10.4.1 (April 18, 2016)", "skillName": "Mathematica."}
{"id": 79, "category": "statistical_software", "skillText": "The Apache Software Foundation /əˈpætʃiː/ (ASF) is an American non-profit corporation (classified as 501(c)(3) in the United States) to support Apache software projects, including the Apache HTTP Server. The ASF was formed from the Apache Group and incorporated in Delaware, U.S., in June 1999.[1][2]\n\nThe Apache Software Foundation is a decentralized community of developers. The software they produce is distributed under the terms of the Apache License and is free and open source software (FOSS). The Apache projects are characterized by a collaborative, consensus-based development process and an open and pragmatic software license. Each project is managed by a self-selected team of technical experts who are active contributors to the project. The ASF is a meritocracy, implying that membership of the foundation is granted only to volunteers who have actively contributed to Apache projects. The ASF is considered a second generation[3] open-source organization, in that commercial support is provided without the risk of platform lock-in.\n\nAmong the ASF's objectives are: to provide legal protection[4] to volunteers working on Apache projects; to prevent the Apache brand name from being used by other organizations without permission.\n\nThe ASF also holds several ApacheCon[5] conferences each year, highlighting Apache projects, related technology.\n\nContents\n\n    1 History\n    2 Projects\n    3 Board of directors\n    4 Financials\n    5 See also\n    6 Notes\n    7 Further reading\n    8 External links\n\nHistory\n\nThe history of the Apache Software Foundation is linked to the Apache HTTP Server, development beginning in February 1995. A group of eight developers started working on enhancing the NCSA HTTPd daemon. They came to be known as the Apache Group. On March 25, 1999, the Apache Software Foundation was formed.[1] The first official meeting of the Apache Software Foundation was held on April 13, 1999, and by general consent that the initial membership list of the Apache Software Foundation, would be: Brian Behlendorf, Ken Coar, Miguel Gonzales, Mark Cox, Lars Eilebrecht, Ralf S. Engelschall, Roy T. Fielding, Dean Gaudet, Ben Hyde, Jim Jagielski, Alexei Kosut, Martin Kraemer, Ben Laurie, Doug MacEachern, Aram Mirzadeh, Sameer Parekh, Cliff Skolnick, Marc Slemko, William (Bill) Stoddard, Paul Sutton, Randy Terbush and Dirk-Willem van Gulik.[6] After a series of additional meetings to elect board members and resolve other legal matters regarding incorporation, the effective incorporation date of the Apache Software Foundation was set to June 1, 1999.[2]\n\nThe name 'Apache' was chosen from respect for the Native American Apache Nation, well known for their superior skills in warfare strategy and their inexhaustible endurance. It also makes a pun on \"a patchy web server\"—a server made from a series of patches—but this was not its origin. The group of developers who released this new software soon started to call themselves the \"Apache Group\".[citation needed]\nProjects\nSee also: List of Apache Software Foundation projects\n\nApache divides its software development activities into separate semi-autonomous areas called \"top-level projects\" (formally known as a \"Project Management Committee\" in the bylaws[7]), some of which have a number of sub-projects. Unlike some other organizations that host FOSS projects, before a project is hosted at Apache it has to be licensed to the ASF with a grant or contributor agreement.[8] In this way, the ASF gains the necessary intellectual property rights for the development and distribution of all its projects.[9]\nBoard of directors\n\nThe ASF board of directors has responsibility for overseeing the ASF's activities and acting as a central point of contact and communication for its projects. The board assigns corporate issues, assigning resources to projects, and manages corporate services, including funds and legal issues. It does not make technical decisions about individual projects; these are made by the individual Project Management Committees. The board is elected annually by members of the foundation and, after the March 2016 Annual Members Meeting, it consists of:[10][11][12]\n\n    Shane Curcuru\n    Bertrand Delacretaz\n    Isabel Drost-Fromm\n    Marvin Humphrey\n    Jim Jagielski\n    Chris Mattmann (Treasurer)\n    Brett Porter (Chairman)\n    Greg Stein (Vice Chairman)\n    Mark Thomas\n\nFinancials\n\nIn the 2010–11 fiscal year, the Foundation took in $539,410, almost entirely from grants and contributions with $12,349 from two ApacheCons. With no employees and 2,663 volunteers, it spent $270,846 on infrastructure, $92,364 on public relations, and $17,891 on two ApacheCons.[13]\nSee also\n\n    Apache Attic\n    Apache Incubator\n\n\n\nApache Incubator is the gateway for open-source projects intended to become fully fledged Apache Software Foundation projects.\n\nThe Incubator project was created in October 2002 to provide an entry path to the Apache Software Foundation for projects and codebases wishing to become part of the Foundation's efforts. All code donations from external organizations and existing external projects wishing to move to Apache must enter through the Incubator.\n\nThe Apache Incubator project serves on the one hand as a temporary container project until the incubating project is accepted and becomes a top-level project of the Apache Software Foundation or becomes subproject of a proper project such as the Jakarta Project or Apache XML. On the other hand, the Incubator project documents how the Foundation works, and how to get things done within its framework. This means documenting process, roles and policies within the Apache Software Foundation and its member projects.\n\n\n\n\nThe mission of the Apache Software Foundation (ASF) is to provide software for the public good. We do this by providing services and support for many like-minded software project communities of individuals who choose to join the ASF.\nWhat is the ASF?\n\nEstablished in 1999, the ASF is a US 501(c)(3) charitable organization, funded by individual donations and corporate sponsors. Our all-volunteer board oversees more than 350 leading Open Source projects, including Apache HTTP Server -- the world's most popular Web server software.\n\nThe ASF provides an established framework for intellectual property and financial contributions that simultaneously limits potential legal exposure for our project committers. Through the ASF's meritocratic process known as \"The Apache Way,\" more than 500 individual Members and 4,500 Committers successfully collaborate to develop freely available enterprise-grade software, benefiting millions of users worldwide: thousands of software solutions are distributed under the Apache License; and the community actively participates in ASF mailing lists, mentoring initiatives, and ApacheCon, the Foundation's official user conference, trainings, and expo.\nHow did the ASF and Apache® projects grow?\n\nFormerly known as the Apache Group, the ASF was incorporated in 1999 as a membership-based, not-for-profit corporation in order to ensure that the Apache projects continue to exist beyond the participation of individual volunteers. Individuals who have demonstrated a commitment to collaborative open-source software development, through sustained participation and contributions within the Foundation's projects, are eligible for membership in the ASF. An individual is awarded membership after nomination and approval by a majority of the existing ASF members. Thus, the ASF is governed by the community it most directly serves -- the people collaborating within its projects.\nHow are the ASF and Apache projects governed?\n\nThe ASF members periodically elect a Board of Directors to manage the organizational affairs of the Foundation, as accorded by the ASF Bylaws. The Board, in turn, appoints a number of officers to oversee the day-to-day operations of the Foundation. A number of public records of our operation are made available to the community. A more detailed explanation of How the ASF works in terms of day to day operations is available, and the Apache Community Development project's goal is to help newcomers learn more about the Apache Software Foundation.\n\nIndividual Apache projects are in turn governed directly by Project Management Committees (PMC) made up of individuals who have shown merit and leadership within those projects. There are detailed descriptions of ASF and project governance models.", "skillName": "Software."}
{"id": 80, "category": "statistical_software", "skillText": "Oracle Data Mining (ODM) is an option of Oracle Corporation's Relational Database Management System (RDBMS) Enterprise Edition (EE). It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\nOracle Data Mining Developer(s) \tOracle Corporation\nStable release \t11gR2 / September, 2009\nType \tdata mining and analytics\nLicense \tproprietary\nWebsite \tOracle Data Mining \n\nContents\n\n    1 Overview\n    2 History\n    3 Functionality\n    4 Input sources and data preparation\n    5 Graphical user interface: Oracle Data Miner\n    6 PL/SQL and Java interfaces\n    7 SQL scoring functions\n    8 PMML\n    9 Predictive Analytics MS Excel Add-In\n    10 References and further reading\n    11 See also\n    12 References\n    13 External links\n\nOverview\n\nOracle implements a variety of data mining algorithms inside the Oracle relational database. These implementations are integrated right into the Oracle database kernel, and operate natively on data stored in the relational database tables. This eliminates the need for extraction or transfer of data into standalone mining/analytic servers. The relational database platform is leveraged to securely manage models and efficiently execute SQL queries on large volumes of data. The system is organized around a few generic operations providing a general unified interface for data mining functions. These operations include functions to create, apply, test, and manipulate data mining models. Models are created and stored as database objects, and their management is done within the database - similar to tables, views, indexes and other database objects.\n\nIn data mining, the process of using a model to derive predictions or descriptions of behavior that is yet to occur is called \"scoring\". In traditional analytic workbenches, a model built in the analytic engine has to be deployed in a mission-critical system to score new data, or the data is moved from relational tables into the analytical workbench - most workbenches offer proprietary scoring interfaces. ODM simplifies model deployment by offering Oracle SQL functions to score data stored right in the database. This way, the user/application developer can leverage the full power of Oracle SQL - in terms of the ability to pipeline and manipulate the results over several levels, and in terms of parallelizing and partitioning data access for performance.\n\nModels can be created and managed by one of several means. (Oracle Data Miner) is a graphical user interface that steps the user through the process of creating, testing, and applying models (e.g. along the lines of the CRISP-DM methodology). Application and tools developers can embed predictive and descriptive mining capabilities using PL/SQL or Java APIs. Business analysts can quickly experiment with, or demonstrate the power of, predictive analytics using Oracle Spreadsheet Add-In for Predictive Analytics, a dedicated Microsoft Excel adaptor interface. ODM offers a choice of well known machine learning approaches such as Decision Trees, Naive Bayes, Support vector machines, Generalized linear model (GLM) for predictive mining, Association rules, K-means and Orthogonal Partitioning[1][2] Clustering, and Non-negative matrix factorization for descriptive mining. A minimum description length based technique to grade the relative importance of an input mining attributes for a given problem is also provided. Most Oracle Data Mining functions also allow text mining by accepting Text (unstructured data) attributes as input. Users do not need to configure text mining options, this is handled behind the scenes by the Database_options database option.\nHistory\n\nOracle Data Mining was first introduced in 2002 and its releases are named according to the corresponding Oracle database release:\n\n    Oracle Data Mining 9iR2 (9.2.0.1.0 - May 2002)\n    Oracle Data Mining 10gR1 (10.1.0.2.0 - February 2004)\n    Oracle Data Mining 10gR2 (10.2.0.1.0 - July 2005)\n    Oracle Data Mining 11gR1 (11.1 - September 2007)\n    Oracle Data Mining 11gR2 (11.2 - September 2009)\n\nOracle Data Mining is a logical successor of the Darwin data mining toolset developed by Thinking Machines Corporation in the mid-1990s and later distributed by Oracle after its acquisition of Thinking Machines in 1999. However, the product itself is a complete redesign and rewrite from ground-up - while Darwin was a classic GUI-based analytical workbench, ODM offers a data mining development/deployment platform integrated into the Oracle database, along with the Oracle Data Miner GUI.\n\nThe Oracle Data Miner 11gR2 New Workflow GUI was previewed at Oracle Open World 2009. An updated Oracle Data Miner GUI was released in 2012. It is free, and is available as an extension to Oracle SQL Developer 3.1 .\nFunctionality\n\nAs of release 11gR1 Oracle Data Mining contains the following data mining functions:\n\n    Data transformation and model analysis:\n        Data sampling, binning, discretization, and other data transformations.\n        Model exploration, evaluation and analysis.\n    Feature selection (Attribute Importance).\n        Minimum description length (MDL).\n    Classification.\n        Naive Bayes (NB).\n        Generalized linear model (GLM) for Logistic regression.\n        Support Vector Machine (SVM).\n        Decision Trees (DT).\n    Anomaly detection.\n        One-class Support Vector Machine (SVM).\n    Regression\n        Support Vector Machine (SVM).\n        Generalized linear model (GLM) for Multiple regression\n    Clustering:\n        Enhanced k-means (EKM).\n        Orthogonal Partitioning Clustering (O-Cluster).[1][2]\n    Association rule learning:\n        Itemsets and association rules (AM).\n    Feature extraction.\n        Non-negative matrix factorization (NMF).\n    Text and spatial mining:\n        Combined text and non-text columns of input data.\n        Spatial/GIS data.\n\nInput sources and data preparation\n\nMost Oracle Data Mining functions accept as input one relational table or view. Flat data can be combined with transactional data through the use of nested columns, enabling mining of data involving one-to-many relationships (e.g. a star schema). The full functionality of SQL can be used when preparing data for data mining, including dates and spatial data.\n\nOracle Data Mining distinguishes numerical, categorical, and unstructured (text) attributes. The product also provides utilities for data preparation steps prior to model building such as outlier treatment, discretization, normalization and binning (sorting in general speak)\nGraphical user interface: Oracle Data Miner\n\nUsers can access Oracle Data Mining through Oracle Data Miner, a GUI client application that provides access to the data mining functions and structured templates (called Mining Activities) that automatically prescribe the order of operations, perform required data transformations, and set model parameters. The user interface also allows the automated generation of Java and/or SQL code associated with the data-mining activities. The Java Code Generator is an extension to Oracle JDeveloper. An independent interface also exists: the Spreadsheet Add-In for Predictive Analytics which enables access to the Oracle Data Mining Predictive Analytics PL/SQL package from Microsoft Excel.\n\nFrom version 11.2 of the Oracle database, Oracle Data Miner integrates with Oracle SQL Developer.[3]\nPL/SQL and Java interfaces\n\nOracle Data Mining provides a native PL/SQL package (DBMS_DATA_MINING) to create, destroy, describe, apply, test, export and import models. The code below illustrates a typical call to build a classification model:\n\nBEGIN\n  DBMS_DATA_MINING.CREATE_MODEL (\n    model_name          => 'credit_risk_model', \n    function            => DBMS_DATA_MINING.classification, \n    data_table_name     => 'credit_card_data', \n    case_id_column_name => 'customer_id', \n    target_column_name  => 'credit_risk',\n    settings_table_name => 'credit_risk_model_settings');\nEND;\n\nwhere 'credit_risk_model' is the model name, built for the express purpose of classifying future customers' 'credit_risk', based on training data provided in the table 'credit_card_data', each case distinguished by a unique 'customer_id', with the rest of the model parameters specified through the table 'credit_risk_model_settings'.\n\nOracle Data Mining also supports a Java API consistent with the Java Data Mining (JDM) standard for data mining (JSR-73) for enabling integration with web and Java EE applications and to facilitate portability across platforms.\nSQL scoring functions\n\nAs of release 10gR2, Oracle Data Mining contains built-in SQL functions for scoring data mining models. These single-row functions support classification, regression, anomaly detection, clustering, and feature extraction. The code below illustrates a typical usage of a classification model:\n\nSELECT customer_name\n  FROM credit_card_data\n WHERE PREDICTION (credit_risk_model USING *) = 'LOW' AND customer_value = 'HIGH';\n\nPMML\n\nIn Release 11gR2 (11.2.0.2), ODM supports the import of externally created PMML for some of the data mining models. PMML is an XML-based standard for representing data mining models.\nPredictive Analytics MS Excel Add-In\n\nThe PL/SQL package DBMS_PREDICTIVE_ANALYTICS automates the data mining process including data preprocessing, model building and evaluation, and scoring of new data. The PREDICT operation is used for predicting target values classification or regression while EXPLAIN ranks attributes in order of influence in explaining a target column feature selection. The new 11g feature PROFILE finds customer segments and their profiles, given a target attribute. These operations can be used as part of an operational pipeline providing actionable results or displayed for interpretation by end users.\nReferences and further reading\n\n    T. H. Davenport, Competing on Analytics \n    , Harvard Business Review, January 2006.\n    I. Ben-Gal,Outlier detection \n    , In: Maimon O. and Rockach L. (Eds.) Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers,\" Kluwer Academic Publishers, 2005, ISBN 0-387-24435-2.\n    M. M. Campos, P. J. Stengard, and B. L. Milenova, Data-centric Automated Data Mining. In proceedings of the Fourth International Conference on Machine Learning and Applications 2005, 15–17 December 2005. pp8, ISBN 0-7695-2495-8\n    M. F. Hornick, Erik Marcade, and Sunil Venkayala. Java Data Mining: Strategy, Standard, and Practice. Morgan-Kaufmann, 2006, ISBN 0-12-370452-9.\n    B. L. Milenova, J. S. Yarmus, and M. M. Campos. SVM in Oracle database 10g: removing the barriers to widespread adoption of support vector machines. In Proceedings of the 31st international Conference on Very Large Data Bases (Trondheim, Norway, August 30 - September 2, 2005). pp1152–1163, ISBN 1-59593-154-6.\n    B. L. Milenova and M. M. Campos. O-Cluster: scalable clustering of large high dimensional data sets. In proceedings of the 2002 IEEE International Conference on Data Mining: ICDM 2002. pp290–297, ISBN 0-7695-1754-4.\n    P. Tamayo, C. Berger, M. M. Campos, J. S. Yarmus, B. L.Milenova, A. Mozes, M. Taft, M. Hornick, R. Krishnan, S.Thomas, M. Kelly, D. Mukhin, R. Haberstroh, S. Stephens and J. Myczkowski. Oracle Data Mining - Data Mining in the Database Environment. In Part VII of Data Mining and Knowledge Discovery Handbook, Maimon, O.; Rokach, L. (Eds.) 2005, p315-1329, ISBN 0-387-24435-2.\n    Brendan Tierney, Predictive Analytics using Oracle Data Miner: for the data scientist, oracle analyst, oracle developer & DBA, Oracle Press, McGraw Hill, Spring 2014.", "skillName": "Oracle_Data_Mining."}
{"id": 81, "category": "e-commerce", "skillText": "Electronic commerce, commonly written as e-commerce or eCommerce, is the trading or facilitation of trading in products or services using computer networks, such as the Internet or online social networks.[1] Electronic commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. Modern electronic commerce typically uses the World Wide Web for at least one part of the transaction's life cycle although it may also use other technologies such as e-mail.\n\nE-commerce businesses may employ some or all of the following:\n\n    Online shopping web sites for retail sales direct to consumers\n    Providing or participating in online marketplaces, which process third-party business-to-consumer or consumer-to-consumer sales\n    Business-to-business buying and selling\n    Gathering and using demographic data through web contacts and social media\n    Business-to-business electronic data interchange\n    Marketing to prospective and established customers by e-mail or fax (for example, with newsletters)\n    Engaging in pretail for launching new products and services\n    Online financial exchanges for currency exchanges or trading purposes\n\nContents\n\n    1 Timeline\n    2 Business application\n    3 Governmental regulation\n    4 Forms\n    5 Global trends\n    6 Impact on markets and retailers\n    7 Impact on supply chain management\n    8 The social impact of e-commerce\n    9 Distribution channels\n    10 Examples of new e-commerce systems\n    11 See also\n    12 References\n    13 Further reading\n    14 External links\n\nTimeline\n\nA timeline for the development of e-commerce:\n\n    1971 or 1972: The ARPANET is used to arrange a cannabis sale between students at the Stanford Artificial Intelligence Laboratory and the Massachusetts Institute of Technology, later described as \"the seminal act of e-commerce\" in John Markoff's book What the Dormouse Said.[2]\n    1979: Michael Aldrich demonstrates the first online shopping system.[3]\n    1981: Thomson Holidays UK is the first business-to-business online shopping system to be installed.[4]\n    1982: Minitel was introduced nationwide in France by France Télécom and used for online ordering.\n    1983: California State Assembly holds first hearing on \"electronic commerce\" in Volcano, California.[5] Testifying are CPUC, MCI Mail, Prodigy, CompuServe, Volcano Telephone, and Pacific Telesis. (Not permitted to testify is Quantum Technology, later to become AOL.)\n    1984: Gateshead SIS/Tesco is first B2C online shopping system [6] and Mrs Snowball, 72, is the first online home shopper[7]\n    1984: In April 1984, CompuServe launches the Electronic Mall in the USA and Canada. It is the first comprehensive electronic commerce service.[8]\n    1989: In May 1989, Sequoia Data Corp. Introduced Compumarket The first internet based system for e-commerce. Sellers and buyers could post items for sale and buyers could search the database and make purchases with a credit card.\n    1990: Tim Berners-Lee writes the first web browser, WorldWideWeb, using a NeXT computer.[9]\n    1992: Book Stacks Unlimited in Cleveland opens a commercial sales website (www.books.com) selling books online with credit card processing.\n    1993: Paget Press releases edition No. 3 [10] of the first[citation needed] app store, The Electronic AppWrapper [11]\n    1994: Netscape releases the Navigator browser in October under the code name Mozilla. Netscape 1.0 is introduced in late 1994 with SSL encryption that made transactions secure.\n    1994: Ipswitch IMail Server becomes the first software available online for sale and immediate download via a partnership between Ipswitch, Inc. and OpenMarket.\n    1994: \"Ten Summoner's Tales\" by Sting becomes the first secure online purchase through NetMarket.[12]\n    1995: The US National Science Foundation lifts its former strict prohibition of commercial enterprise on the Internet.[13]\n    1995: Thursday 27 April 1995, the purchase of a book by Paul Stanfield, Product Manager for CompuServe UK, from W H Smith's shop within CompuServe's UK Shopping Centre is the UK's first national online shopping service secure transaction. The shopping service at launch featured W H Smith, Tesco, Virgin Megastores/Our Price, Great Universal Stores (GUS), Interflora, Dixons Retail, Past Times, PC World (retailer) and Innovations.\n    1995: Jeff Bezos launches Amazon.com and the first commercial-free 24-hour, internet-only radio stations, Radio HK and NetRadio start broadcasting. eBay is founded by computer programmer Pierre Omidyar as AuctionWeb.\n    1996: IndiaMART B2B marketplace established in India.\n    1996: ECPlaza B2B marketplace established in Korea.\n    1998: Electronic postal stamps can be purchased and downloaded for printing from the Web.[14]\n    1999: Alibaba Group is established in China. Business.com sold for US $7.5 million to eCompanies, which was purchased in 1997 for US $149,000. The peer-to-peer filesharing software Napster launches. ATG Stores launches to sell decorative items for the home online.\n    2000: Complete Idiot's Guide to E-commerce released on Amazon\n    2000: The dot-com bust.\n    2001: Alibaba.com achieved profitability in December 2001.\n    2002: eBay acquires PayPal for $1.5 billion.[15] Niche retail companies Wayfair and NetShops are founded with the concept of selling products through several targeted domains, rather than a central portal.\n    2003: Amazon.com posts first yearly profit.\n    2003: Bossgoo B2B marketplace established in China.\n    2004: DHgate.com, China's first online b2b transaction platform, is established, forcing other b2b sites to move away from the \"yellow pages\" model.[16]\n    2007: Business.com acquired by R.H. Donnelley for $345 million.[17]\n    2009: Zappos.com acquired by Amazon.com for $928 million.[18] Retail Convergence, operator of private sale website RueLaLa.com, acquired by GSI Commerce for $180 million, plus up to $170 million in earn-out payments based on performance through 2012.[19]\n    2010: Groupon reportedly rejects a $6 billion offer from Google. Instead, the group buying websites went ahead with an IPO on 4 November 2011. It was the largest IPO since Google.[20][21]\n    2014: Overstock.com processes over $1 million in Bitcoin sales.[22] India’s e-commerce industry is estimated to have grown more than 30% from 2012 to $12.6 billion in 2013.[23] US eCommerce and Online Retail sales projected to reach $294 billion, an increase of 12 percent over 2013 and 9% of all retail sales.[24] Alibaba Group has the largest Initial public offering ever, worth $25 billion.\n    2015: Amazon.com accounts for more than half of all ecommerce growth,[25] selling almost 500 Million SKU's in the US.[26]\n\nBusiness application\nAn example of an automated online assistant on a merchandising website.\n\nSome common applications related to electronic commerce are:\n\n    Document automation in supply chain and logistics\n    Domestic and international payment systems\n    Enterprise content management\n    Group buying\n    Print on demand\n    Automated online assistant\n    Newsgroups\n    Online shopping and order tracking\n    Online banking\n    Online office suites\n    Shopping cart software\n    Teleconferencing\n    Electronic tickets\n    Social networking\n    Instant messaging\n    Pretail\n    Digital Wallet\n\nGovernmental regulation\n\nIn the United States, certain electronic commerce activities are regulated by the Federal Trade Commission (FTC). These activities include but not limit to the use of commercial e-mails, online advertising and consumer privacy. The CAN-SPAM Act of 2003 establishes national standards for direct marketing over e-mail. The Federal Trade Commission Act regulates all forms of advertising, including online advertising, and states that advertising must be truthful and non-deceptive.[27] Using its authority under Section 5 of the FTC Act, which prohibits unfair or deceptive practices, the FTC has brought a number of cases to enforce the promises in corporate privacy statements, including promises about the security of consumers' personal information.[28] As result, any corporate privacy policy related to e-commerce activity may be subject to enforcement by the FTC.\n\nThe Ryan Haight Online Pharmacy Consumer Protection Act of 2008, which came into law in 2008, amends the Controlled Substances Act to address online pharmacies.[29]\n\nConflict of laws in cyberspace is a major hurdle for harmonization of legal framework for e-commerce around the world. In order to give a uniformity to e-commerce law around the world, many countries adopted the UNCITRAL Model Law on Electronic Commerce (1996) [30]\n\nInternationally there is the International Consumer Protection and Enforcement Network (ICPEN), which was formed in 1991 from an informal network of government customer fair trade organisations. The purpose was stated as being to find ways of co-operating on tackling consumer problems connected with cross-border transactions in both goods and services, and to help ensure exchanges of information among the participants for mutual benefit and understanding. From this came Econsumer.gov, an ICPEN initiative since April 2001. It is a portal to report complaints about online and related transactions with foreign companies.\n\nThere is also Asia Pacific Economic Cooperation (APEC) was established in 1989 with the vision of achieving stability, security and prosperity for the region through free and open trade and investment. APEC has an Electronic Commerce Steering Group as well as working on common privacy regulations throughout the APEC region.\n\nIn Australia, Trade is covered under Australian Treasury Guidelines for electronic commerce,[31] and the Australian Competition and Consumer Commission[32] regulates and offers advice on how to deal with businesses online,[33][34] and offers specific advice on what happens if things go wrong.[35]\n\nIn the United Kingdom, The Financial Services Authority (FSA)[36] was formerly the regulating authority for most aspects of the EU's Payment Services Directive (PSD), until its replacement in 2013 by the Prudential Regulation Authority and the Financial Conduct Authority.[37] The UK implemented the PSD through the Payment Services Regulations 2009 (PSRs), which came into effect on 1 November 2009. The PSR affects firms providing payment services and their customers. These firms include banks, non-bank credit card issuers and non-bank merchant acquirers, e-money issuers, etc. The PSRs created a new class of regulated firms known as payment institutions (PIs), who are subject to prudential requirements. Article 87 of the PSD requires the European Commission to report on the implementation and impact of the PSD by 1 November 2012.[38]\n\nIn India, the Information Technology Act 2000 governs the basic applicability of e-commerce.\n\nIn China, the Telecommunications Regulations of the People's Republic of China (promulgated on 25 September 2000), stipulated the Ministry of Industry and Information Technology (MIIT) as the government department regulating all telecommunications related activities, including electronic commerce.[39] On the same day, The Administrative Measures on Internet Information Services released, is the first administrative regulation to address profit-generating activities conducted through the Internet, and lay the foundation for future regulations governing e-commerce in China.[40] On 28 August 2004, the eleventh session of the tenth NPC Standing Committee adopted The Electronic Signature Law, which regulates data message, electronic signature authentication and legal liability issues. It is considered the first law in China’s e-commerce legislation. It was a milestone in the course of improving China’s electronic commerce legislation, and also marks the entering of China’s rapid development stage for electronic commerce legislation.[41]\nForms\n\nContemporary electronic commerce involves everything from ordering \"digital\" content for immediate online consumption, to ordering conventional goods and services, to \"meta\" services to facilitate other types of electronic commerce.\n\nOn the institutional level, big corporations and financial institutions use the internet to exchange financial data to facilitate domestic and international business. Data integrity and security are pressing issues for electronic commerce.\n\nAside from traditional e-Commerce, the terms m-Commerce (mobile commerce) as well (around 2013) t-Commerce[42] have also been used.\nGlobal trends\n\nIn 2010, the United Kingdom had the biggest e-commerce market in the world when measured by the amount spent per capita.[43] As of 2013, the Czech Republic was the European country where ecommerce delivers the biggest contribution to the enterprises´ total revenue. Almost a quarter (24%) of the country’s total turnover is generated via the online channel.[44]\n\nAmong emerging economies, China's e-commerce presence continues to expand every year. With 668 million internet users, China's online shopping sales reached $253 billion in the first half of 2015, accounting for 10% of total Chinese consumer retail sales in the same period.[45] The Chinese retailers have been able to help consumers feel more comfortable shopping online.[46] E-commerce transactions between China and other countries increased 32% to 2.3 trillion yuan ($375.8 billion) in 2012 and accounted for 9.6% of China's total international trade [47] In 2013, Alibaba had an e-commerce market share of 80% in China.[48] In 2014, there were 600 million Internet users in China (twice as many than in the US), making it the world's biggest online market.[49] China is also the largest e-commerce market in the world by value of sales, with an estimated US$899 billion in 2016.[50]\n\nIn 2013, Brazil's eCommerce was growing quickly with retail eCommerce sales expected to grow at a healthy double-digit pace through 2014. By 2016, eMarketer expected retail ecommerce sales in Brazil to reach $17.3 billion.[51] India has an internet user base of about 243.2 million as of January 2014.[citation needed] Despite being third largest user base in world, the penetration of Internet is low compared to markets like the United States, United Kingdom or France but is growing at a much faster rate, adding around 6 million new entrants every month.[citation needed] In India, cash on delivery is the most preferred payment method, accumulating 75% of the e-retail activities.[citation needed]\n\nE-Commerce has become an important tool for small and large businesses worldwide, not only to sell to customers, but also to engage them.[52][53]\n\nIn 2012, ecommerce sales topped $1 trillion for the first time in history.[54]\n\nMobile devices are playing an increasing role in the mix of eCommerce, this is also commonly called mobile commerce, or m-commerce. In 2014, one estimate saw purchases made on mobile devices making up 25% of the market by 2017.[55]\n\nFor traditional businesses, one research stated that information technology and cross-border e-commerce is a good opportunity for the rapid development and growth of enterprises. Many companies have invested enormous volume of investment in mobile applications.The DeLone and McLean Model stated that 3 perspectives are contributed to a successful e-business, including information system quality, service quality and users satisfaction.[56] There is no limit of time and space, there are more opportunities to reach out to customers around the world, and to cut down unnecessary intermediate links, thereby reducing the cost price, and can benefit from one on one large customer data analysis, to achieve a high degree of personal customization strategic plan, in order to fully enhance the core competitiveness of the products in company[57]\nImpact on markets and retailers\n\nEconomists have theorized that e-commerce ought to lead to intensified price competition, as it increases consumers' ability to gather information about products and prices. Research by four economists at the University of Chicago has found that the growth of online shopping has also affected industry structure in two areas that have seen significant growth in e-commerce, bookshops and travel agencies. Generally, larger firms are able to use economies of scale and offer lower prices. The lone exception to this pattern has been the very smallest category of bookseller, shops with between one and four employees, which appear to have withstood the trend.[58] Depending on the category, e-commerce may shift the switching costs—procedural, relational, and financial—experienced by customers.[59]\n\nIndividual or business involved in e-commerce whether buyers or sellers rely on Internet-based technology in order to accomplish their transactions. E-commerce is recognized for its ability to allow business to communicate and to form transaction anytime and anyplace. Whether an individual is in the US or overseas, business can be conducted through the internet. The power of e-commerce allows geophysical barriers to disappear, making all consumers and businesses on earth potential customers and suppliers. Thus, switching barriers and switching costs may shift.[59] eBay is a good example of e-commerce business individuals and businesses are able to post their items and sell them around the Globe.[60]\n\nIn e-commerce activities, supply chain and logistics are two most crucial factors need to be considered. Typically, cross-border logistics need about few weeks time round. Based on this low efficiency of the supply chain service, customer satisfaction will be greatly reduced.[61] Some researcher stated that combining e-commerce competence and IT setup could well enhance company’s overall business worth.[62] Other researcher stated that e-commerce need to consider the establishment of warehouse centers in foreign countries, to create high efficiency of the logistics system, not only improve customers’ satisfaction, but also can improve customers’ loyalty.[weasel words].\nImpact on supply chain management\n\nFor a long time, companies had been troubled by the gap between the benefits which supply chain technology has and the solutions to deliver those benefits. However, the emergence of e-commerce has provided a more practical and effective way of delivering the benefits of the new supply chain technologies.[63]\n\nE-commerce has the capability to integrate all inter-company and intra-company functions, meaning that the three flows (physical flow, financial flow and information flow) of the supply chain could be also affected by e-commerce. The affections on physical flows improved the way of product and inventory movement level for companies. For the information flows, e-commerce optimised the capacity of information processing than companies used to have, and for the financial flows, e-commerce allows companies to have more efficient payment and settlement solutions.[63]\n\nIn addition, e-commerce has a more sophisticated level of impact on supply chains: Firstly, the performance gap will be eliminated since companies can identify gaps between different levels of supply chains by electronic means of solutions; Secondly, as a result of e-commerce emergence, new capabilities such implementing ERP systems have helped companies to manage operations with customers and suppliers. Yet these new capabilities are still not fully exploited. Thirdly, technology companies would keep investing on new e-commerce software solutions as they are expecting investment return. Fourthly, e-commerce would help to solve many aspects of issues that companies may feel difficult to cope with, such as political barriers or cross-country changes. Finally, e-commerce provides companies a more efficient and effective way to collaborate with each other within the supply chain.[63]\nThe social impact of e-commerce\n\nAlong with the e-commerce and its unique charm that has appeared gradually, virtual enterprise, virtual bank, network marketing, online shopping, payment and advertising, such this new vocabulary which is unheard-of and now has become as familiar to people. This reflects that the e-commerce has huge impact on the economy and society from the other side.[64] For instance, B2B is a rapidly growing business in the world that leads to lower cost and then improves the economic efficiency and also bring along the growth of employment.[65]\n\nTo understand how the e-commerce has affected the society and economy, this article will mention three issues below:\n\n1. The e-commerce has changed the relative importance of time, but as the pillars of indicator of the country’s economic state that the importance of time should not be ignored.\n\n2. The e-commerce offers the consumer or enterprise various information they need, making information into total transparency, will force enterprise no longer is able to use the mode of space or advertisement to raise their competitive edge.[66] Moreover, in theory, perfect competition between the consumer sovereignty and industry will maximize social welfare.[67]\n\n3. In fact, during the economic activity in the past, large enterprise frequently has advantage of information resource, and thus at the expense of consumers. Nowadays, the transparent and real-time information protects the rights of consumers, because the consumers can use internet to pick out the portfolio to the benefit of themselves. The competitiveness of enterprises will be much more obvious than before, consequently, social welfare would be improved by the development of the e-commerce.\n\n4. The new economy led by the e-commerce change humanistic spirit as well, but above all, is the employee loyalty.[68] Due to the market with competition, the employee’s level of professionalism becomes the crucial for enterprise in the niche market. The enterprises must pay attention to how to build up the enterprises inner culture and a set of interactive mechanisms and it is the prime problem for them. Furthermore, though the mode of e-commerce decrease the information cost and transaction cost, however, its development also makes human being are overly computer literate. In hence, emphasized more humanistic attitude to work is another project for enterprise to development. Life is the root of all and high technology are merely an assistive tool to support our quality of life.\n\n5. Online merchants gather purchase activity and interests of their customers. This information is being used by the online marketers to promote relevant products and services. This creates an extra convenience for the online shoppers.\n\n6. Online merchandise is searchable, which makes it more accessible to the shoppers. Many online retailers offer a review mechanism, which helps shoppers decide on the product to purchase. This is another convenience and a satisfaction improvement factor.\n\nThe e-commerce is not a kind of new industry, but it is creating a new economic model. Most of people agree that the e-commerce indeed to be important and significant for economic society in the future, but actually that is a bit of clueless feeling at the beginning, this problem is exactly prove the e-commerce is a sort of incorporeal revolution.[69] This is due to the fact that the cost of running an e-commerce business is very low when compared with running a physical store. There is no rent to pay on expensive premises, business processes are simplified and less man-hours are required to run it smoothly. Generally speaking, as a type of business active procedure, the e-commerce is going to leading an unprecedented revolution in the world, the influence of this model far exceeded the commercial affair itself.[70] Except the mentioned above, in the area of law, education, culture and also policy, the e-commerce will continue that rise in impact. The e-commerce is truly to take human beings into the information society.\nDistribution channels\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2013) (Learn how and when to remove this template message)\n\nE-commerce has grown in importance as companies have adopted pure-click and brick-and-click channel systems. We can distinguish pure-click and brick-and-click channel system adopted by companies.\n\n    Pure-click or pure-play companies are those that have launched a website without any previous existence as a firm.\n    Bricks-and-clicks companies are those existing companies that have added an online site for e-commerce.\n    Click-to-brick online retailers that later open physical locations to supplement their online efforts.[71]\n\nExamples of new e-commerce systems\n\nAccording to eMarketer research company, \"by 2017, 65.8 per cent of Britons will use smartphones\".[72]\nSee also\n\n    Alternative payments\n    Comparison of shopping cart software\n    Comparison of Payment Systems\n    Digital economy\n    Electronic bill payment\n    Electronic money\n    E-commerce credit card payment system\n    Comparison of free software e-commerce web application frameworks\n    Non-store retailing\n    Online marketplace\n    Paid content\n    Payments as a service\n    Types of e-commerce\n    Timeline of e-commerce", "skillName": "E-commerce."}
{"id": 82, "category": "e-commerce", "skillText": "An online marketplace (or online e-commerce marketplace) is a type of e-commerce site where product or service information is provided by multiple third parties, whereas transactions are processed by the marketplace operator. Online marketplaces are the primary type of multichannel ecommerce and can be described as a \"simple and convenient portal\" to streamline the production process.\n\nIn an online marketplace, consumer transactions are processed by the marketplace operator and then delivered and fulfilled by the participating retailers or wholesalers (often called drop shipping). Other capabilities might include auctioning (forward or reverse), catalogs, ordering, wanted advertisement, trading exchange functionality and capabilities like RFQ, RFI or RFP. These type of sites allow users to register and sell single items to a large number of items for a \"post-selling\" fee.\n\nIn general, because marketplaces aggregate products from a wide array of providers, selection is usually more wide, and availability is higher than in vendor-specific online retail stores.[1] Also prices may be more competetive.[citation needed]\n\nSince 2014, online marketplaces are abundant since organized marketplaces are sought after.[2] Some have a wide variety of general interest products that cater to almost all the needs of the consumers, however, some are consumer specific and cater to a particular segment only. Not only is the platform for selling online, but the user interface and user experience matters. People tend to log on to online marketplaces that are organized and products are much more accessible to them.\n\nContents\n\n    1 For services and outsourcing\n    2 Criticism\n    3 See also\n    4 References\n\nFor services and outsourcing\n\nThere are marketplaces for the online outsourcing of professional services like IT services,[3] search engine optimization, marketing,[4] crowdsourcing,[5] and skilled crafts & trades work.[6]\nCriticism\n\nMany service related online marketplaces have been criticized for taking jobs that would go to local industries that can't compete on price against outsourced providers.\n\nAnother criticism is that the laws and regulations surrounding online marketplaces are quite underdeveloped. As of consequence, there is a discrepancy between the responsibility, accountability and liability of the marketplace and third parties. Online marketplaces and platforms have faced much criticism in recent years for their lack of consumer protections.[7]\nSee also\n\n    Comparison of payment systems\n    Darknet market\n    Digital distribution\n    E-marketplace\n    List of online marketplaces", "skillName": "Online_marketplace."}
{"id": 83, "category": "e-commerce", "skillText": "Various types of e-commerce platforms fall into several industry classifications based upon their licensing model, sales scenario and data exchange.\n\nContents\n\n    1 Types of licensing models\n        1.1 On-premises e-commerce\n        1.2 Software as a service (SaaS) E-commerce\n        1.3 Fully Managed (FM) E-commerce\n        1.4 Open source E-commerce\n    2 Types of sales scenario\n        2.1 Business-to-Consumer (B2C)\n        2.2 Business-to-Business (B2B)\n        2.3 Consumer-to-Business (C2B)\n        2.4 Consumer-to-Consumer (C2C) also known as Peer-to-Peer or (P2P)\n    3 Types of data exchange\n        3.1 Integrated E-commerce\n        3.2 Interfaced E-commerce\n    4 See also\n    5 References\n\nTypes of licensing models\nOn-premises e-commerce\n\nOn-premises e-commerce software usually requires initial one time purchase investment in terms of licensing fees. Also, it implies extra costs related to hardware and installation services as well as data migration and on-going maintenance fees that are usually charged on a yearly basis for software updates and support. Some examples of typical on premises E-commerce platforms are Oracle Web Commerce (formerly ATG) \nHybris (company),[1] Intershop Communications,[2] Sana Commerce.,[3] and IBM WebSphere.[4]\n\nAdvantages:\n\n    Easily customizable;\n    Data security;\n    High performance;\n\nDisadvantages:\n\n    Large initial investment;\n    Self-maintenance;\n    Technical knowledge\n\nSoftware as a service (SaaS) E-commerce\n\nSoftware as a Service (SaaS)- is a cloud based delivery model in which applications are hosted and managed in a service provider's datacenter, paid for on a subscription basis and accessed via a browser over an internet connection. Two examples of typical SaaS E-commerce solutions are Shopify[5] and Demandware.[6]\n\nAdvantages:\n\n    Affordable low-cost solution;\n    Hosted/upgraded by E-commerce provider;\n    Easily scalable\n\nDisadvantages:\n\n    Limited integration with back-end systems;\n    Lack of data security;\n    Limited control over the system;\n\nFully Managed (FM) E-commerce\n\nFully Managed (FM) E-commerce - is the next step of Platform as a Service (PaaS). As a basis, PaaS consists of e-commerce software and hardware hosting. In addition to this, fully managed e-commerce solutions provide services like product picture taking, image editing, data management, customer support, marketing consulting.[7] FM E-Commerce is offered to brick-and-mortar stores as a B2B solution to help them start selling online quickly and at low cost. The licensing model is usually based on the sales volume.\nOpen source E-commerce\n\nOpen source e-Commerce is a free of charge platform that doesn’t imply licenses fee. Furthermore, open source users are also responsible for installing, maintaining, securing and configuring the software on their own servers. In order to set up an open source platform, basic technical expertise is required in the areas of web design and development. Software products that are distributed as open source are generally free, and users can access and modify the source code.\n\nAdvantages:\n\n    Free of charge system;\n    Wide variety of available addons/plugins/extensions;\n    Better flexibility with a customizable source code;\n\nDisadvantages:\n\n    More technical knowledge required;\n    Performance depends on hosting costs\n    No standard integration with back-end system;\n\nTypes of sales scenario\n\nThere are multiple types of sales scenario some of it are as follows:-\nBusiness-to-Consumer (B2C)\n\nIn a Business-to-Consumer E-commerce environment, companies sell their online goods to consumers who are the end users of their products or services. Usually, B2C E-commerce web shops have an open access for any visitor and user.\nBusiness-to-Business (B2B)\n\nIn a Business-to-Business E-commerce environment, companies sell their online goods to other companies without being engaged in sales to consumers. In most B2B E-commerce environments entering the web shop will require a log in. B2B web shop usually contains customer-specific pricing, customer-specific assortments and customer-specific discounts. There are several SaaS B2B eCommerce platforms available.\nConsumer-to-Business (C2B)\n\nIn a Consumer-to-Business E-commerce environment, consumers usually post their products or services online on which companies can post their bids. A consumer reviews the bids and selects the company that meets his price expectations.\nConsumer-to-Consumer (C2C) also known as Peer-to-Peer or (P2P)\n\nIn a Consumer-to-Consumer E-commerce environment consumers sell their goods to other consumers. A well-known example is eBay. New mobile version gaining a lot of traction are OfferUP and Close5.\nTypes of data exchange\n\nMany E-commerce software retrieve the information from a certain back-end system, such as an Enterprise resource planning (ERP) or Customer relationship management (CRM) system. This information is stored in the database of that system. The Business logic contains all of the business rules that define the way data is stored, created, displayed, calculated and is being recreated inside an ERP or CRM system. For example: different product discounts are applied for different customer accounts. In order to retrieve the information, two types of data exchange are taken into account.\nIntegrated E-commerce\n\nIn integrated E-commerce, a part of the software solution is installed inside the back-end system. This means that the connection between the business logic and database of a back-end system is configured automatically. Information that is available in the back-end system is being re-used and displayed in the front/back end of the E-commerce system. An integrated E-commerce software product thus doesn’t require to invest in recreating a separate database or business logic. Instead, it re-uses those of the back-end system. Integrated software is mostly used in B2B e-commerce, where there is a strong need for personalized business data to be available on the web store real-time. A typical example of an integrated E-commerce software is Sana Commerce.\nInterfaced E-commerce\n\nIn interfaced E-commerce, the software solution is installed on top of the back-end system. This means that the connection between the business logic and database of a back-end system is set up manually. Information that is available in the back-end system is being duplicated into the E-commerce software. An interfaced E-commerce software product thus has their own database and business logic that are being synchronized constantly through a connection to a certain back-end system. Interfaced E-commerce is mostly used in a B2C scenario. An example of an interfacing E-commerce software is Magento.\nSee also\n\n    Comparison of free software e-commerce web application frameworks\n    Comparison of shopping cart software\n    E-commerce\n    Mobile commerce", "skillName": "Types_of_e-commerce."}
{"id": 84, "category": "e-commerce", "skillText": "Electronic commerce, commonly known as e-commerce or eCommerce, or e-business consists of the buying and selling of products or services over electronic systems such as the Internet and other computer networks. The amount of trade conducted electronically has grown extraordinarily with widespread Internet usage. The use of commerce is conducted in this way, spurring and drawing on innovations in electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. Modern electronic commerce typically uses the World Wide Web at least at some point in the transaction's lifecycle, although it can encompass a wider range of technologies such as e-mail as well.\n\nA large percentage of electronic commerce is conducted entirely electronically for virtual items such as access to premium content on a website, but most electronic commerce involves the transportation of physical items in some way. Online retailers are sometimes known as e-tailers and online retail is sometimes known as e-tail. Almost all big retailers have electronic commerce presence on the World Wide Web.\n\nElectronic commerce that is conducted between businesses is referred to as business-to-business or B2B. B2B can be open to all interested parties (e.g. commodity market) or limited to specific, pre-qualified participants (private electronic market). Electronic commerce that is conducted between businesses and consumers, on the other hand, is referred to as business-to-consumer or B2C. This is the type of electronic commerce conducted by companies such as Amazon.com. Online shopping is a form of electronic commerce where the buyer is connected directly online to the seller's computer usually via the Internet. There is no specific intermediary service. The sale and purchase transaction is completed electronically and interactively in real-time, such as when buying a new book on Amazon.com. If an intermediary is present, then the sale and purchase transaction is called consumer-to-consumer, such as an online auction conducted on eBay.com.\n\nThis payment system has been widely accepted by consumers and merchants throughout the world, and is by far the most popular method of payments especially in the retail markets.[1] Some of the most important advantages over the traditional modes of payment are: privacy, integrity, compatibility, good transaction efficiency, acceptability, convenience, mobility, low financial risk and anonymity.[1]\n\nThis flow of ecommerce payment system can be better understood from the flow of the system below.\nCredit card 20101225.png\nFigure: Online Credit Card (VISA) Transaction Process\nSee also\n\n    Payment service provider (PSP)\n    Electronic money\n    List of free and open source eCommerce software\n    Multichannel ecommerce\n    Non-store retailing\n    Online marketplace\n    Paid content\n    Payments as a platform\n    Virtual economy\n    Comparison of Payment Systems", "skillName": "E-commerce_credit_card_payment_system."}
{"id": 85, "category": "e-commerce", "skillText": "Non-store retailing is the selling of goods and services outside the confines of a retail facility. It is a generic term describing retailing taking place outside of shops and stores (that is, off the premises of fixed retail locations and of markets stands). The non-store distribution channel can be divided into direct selling (off-premises sales) and distance selling, the latter including all forms of electronic commerce. Distance selling includes mail order, catalogue sales, telephone solicitations and automated vending. Electronic commerce includes online shopping, internet trading platforms, travel portals, global distribution systems and teleshopping. Direct selling includes party sales and all forms of selling in consumers’ homes and offices, including even garage sales.\n\nNon-store retailing, sometimes also labelled ‘home shopping’, is consistently achieving double-digit growth, and slowly taking a bigger share of overall retailing. In the first quarter of 2014 online sales in the US represented over 6% of all sales.[1] However, in product niches such as travel, books, and media, the share is significantly higher. As of March 2014, 19.5% of all book sales made by Amazon are for their Kindle e-book reader.[2] Fashion and lifestyle brands have entered the non-store retailing space including Everlane, Dollar Shave Club, 800razors.com and Tieks. According to Eurostat, 38 per cent of European consumers consider the internet as the most important source of information about travel [3] and 42 per cent of consumers purchased travel services over the internet in 2008.[4]\n\nThe non-store distribution channel is marked by low entry thresholds. Compared to store retailing that requires a retail outlet, inventory, cash flow to hire staff and advertising, non-store retail start-ups usually have to invest little to reach out to potential buyers of the goods and services they offer. Non-store retailing is therefore not only used by established brick and mortar business retailers who develop an online bricks and clicks business model presence, but also by the individual pure play, often him- or herself a consumer, to create an EShop or to run sales parties. The rise of social media helps to connect sellers to potential buyers.\n\nUnder European Union law, non-store retailing is heavily regulated.[citation needed] The Distance Selling Directive 97/7/EC (incorporated into UK law as Consumer Protection (Distance Selling) Regulations 2000), the Doorstep Selling Directive 85/577/EEC, the E-Commerce Directive 2000/31/EC and Electronic Commerce Regulations 2002 and the Audiovisual Services Directive 2010/13/EU are the principal regulatory tools to deal with the most technologically intensive but also innovative distribution methods.\nSee also\n\n    Brick and mortar", "skillName": "Non-store_retailing."}
{"id": 86, "category": "e-commerce", "skillText": "Digital economy refers to an economy that is based on digital computing technologies. The digital economy is also sometimes called the Internet Economy, the New Economy, or Web Economy. Increasingly, the \"digital economy\" is intertwined with the traditional economy making a clear delineation harder.\n\nContents\n\n    1 Definition\n    2 Impact\n    3 Response\n    4 See also\n    5 References\n    6 Further reading\n\nDefinition\n\nThe term 'Digital Economy' was coined in Don Tapscott's 1995 best-seller The Digital Economy: Promise and Peril in the Age of Networked Intelligence.[1] The Digital Economy was among the first books to show how the Internet would change the way we did business. It became an international best-seller within one month of its release, appearing on a number of best-seller lists, including the New York Times Business Book list and a seven-month run on the BusinessWeek best sellers list. BusinessWeek also named The Digital Economy the top selling business book for 1996.[2]\n\nAccording to Thomas Mesenbourg (2001),[3] three main components of the 'Digital Economy' concept can be identified:\n\n    supporting infrastructure (hardware, software, telecoms, networks, etc.),\n    e-business (how business is conducted, any process that an organization conducts over computer-mediated networks),\n    e-commerce (transfer of goods, for example when a book is sold online).\n\nBut, as Bill Imlah[4] comments, new applications are blurring these boundaries and adding complexity – for example, social media, and Internet search.\n\nIn the last decade of the 20th century. Nicholas Negroponte (1995) used a metaphor of shifting from processing atoms to processing bits.[5] He discussed the disadvantages of the former (e.g., mass, materials, transport) and advantages of the latter (e.g., weightlessness, virtual, instant global movement). In this new economy, digital networking and communication infrastructures provide a global platform over which people and organizations devise strategies, interact, communicate, collaborate and search for information. More recently,[6] Digital Economy has been defined as the branch of economics studying zero marginal cost intangible goods over the Net.\nImpact\n\nIt is widely accepted that the growth of the digital economy has widespread impact on the whole economy. Various attempts at categorising the size of the impact on traditional sectors have been made.[7][8] The Boston Consulting Group discussed “four waves of change sweeping over consumer goods and retail”, for instance.[9] Deloitte ranked six industry sectors as having a “short fuse” and to experience a “big bang” as a result of the digital economy.[10] Telstra, a leading Australian telecommunications provider, describes how competition will become more global and more intense as a result of the digital economy.[8]\nResponse\n\nGiven its expected broad impact, traditional firms are actively assessing how to respond to the changes brought about by the digital economy.[11][12][13] For corporations, timing of their response is of the essence.[14] Banks are trying to innovate and use digital tools to improve their traditional business.[15] Governments are investing in infrastructure. In 2013, the Australian National Broadband Network, for instance, aimed to provide a 1 GB/sec download speed fibre based broadband to 93% of the population over ten years.[16]\nSee also\n\n    National Broadband Network\n    Canada 3.0\n    Electronic business\n    Electronic commerce\n    Information economy\n    information society\n    Knowledge economy\n    Knowledge management\n    Knowledge market\n    Network economy\n    Virtual economy\n    Digitization economics", "skillName": "Digital_economy."}
{"id": 87, "category": "Business", "skillText": "Business communication is the sharing of information between people within and outside the organization that is performed for the commercial benefit of the organization. It can also be defined as relaying of information within a business by its people.\n\nContents\n\n    1 Overview\n    2 Effective business communication\n        2.1 Face-to-face\n        2.2 Email\n        2.3 Telephone\n        2.4 Listening\n    3 Choosing Communication Media\n    4 Business Writing Process\n        4.1 STEP1: Planning\n        4.2 STEP2: Drafting\n        4.3 STEP3: Revising\n    5 Organizations\n    6 References\n\nOverview\n\nBusiness communication (or simply \"communication\", in a business context) encompasses topics such as marketing, brand management, customer relations, consumer behavior, advertising, public relations, corporate communication, community engagement, reputation management, interpersonal communication, employee engagement, and event management. It is closely related to the fields of professional communication and technical communication.\n\nMedia channels for business communication include the Internet, print media, radio, television, ambient media, and word of mouth.\n\nBusiness communication can also refer to internal communication that takes place within an organization.\n\nBusiness communication is a common topic included in the curricula of Undergraduate and Master programs of many colleges and universities.\n\nThere are several methods of business communication, including:\n\n    Web-based communication - for better and improved communication, anytime anywhere ...\n    video conferencing which allow people in different locations to hold interactive meetings;\n    Reports - important in documenting the activities of any department;\n    Presentations - very popular method of communication in all types of organizations, usually involving audiovisual material, like copies of reports, or material prepared in Microsoft PowerPoint or Adobe Flash;\n    telephone meetings, which allow for long distance speech;\n    forum boards, which allow people to instantly post information at a centralized location; and\n    face-to-face meetings, which are personal and should be succeeded by a written followup.\n    suggestion box: It is primarily used for upward communication, because some people may hesitate to communicate with management directly, so they opt to give suggestions by drafting one and putting it in the suggestion box.\n\nEffective business communication\n\nA two way information sharing process which involves one party sending a message that is easily understood by the receiving party. Effective communication by business managers facilitates information sharing between company employees and can substantially contribute to its commercial success.[1]\n\nFor business communication to be effective these qualities are essential :\n\n    Establish clear hierarchy\n    Use visual communication\n    Conflict Management\n    Consider Cultural Issues\n    Good Written communication\n\nFace-to-face\n\nFace-to-face communication helps to establish a personal connection and will help sell the product or service to the customer.[2] These interactions can portray a whole different message than written communication as tone, pitch, and body language is observed.[3] Information is easier to access and delivered immediately with interactions rather than waiting for an email or phone call. Conflicts are also easily resolved this way, as verbal and non-verbal cues are observed and acted upon. Communicating professionally is very important as one is representing the company. Speak clearly and ask questions to understand the needs and wants, let the recipient respond as one resolves the issue. Decisions are made more confidently during a face-to-face interaction as the recipient asks questions to understand and move forward with their decision.\nEmail\n\nWhen using email to communicate in the business world, it is important to be careful with the choice of words. Miscommunication is very frequent as the reader doesn’t know what non-verbal cues one is giving off, such as the pitch, tone, or expressions. Before beginning an email, make sure the email address one is using is appropriate and professional as well as the message one is going to send. Again, make sure the information is clear and to the point so the recipient isn’t confused. Make sure one includes their signature, title, and other contact information at the end.[citation needed].\nTelephone\n\nWhen making a business call, make it clear who is on the line and where one is from as well as one's message when on the phone. Smile and have a positive attitude as the recipient will be able to read the caller and that will affect how they react. When leaving a message, make sure one is clear and brief. One should state their name and who they are and the purpose for contacting them. If replying to a voicemail, try to respond as soon as possible and take into consideration the time of day. Don't call too early or too late, as it is important to respect other's time. Also be mindful of where one is and the noise level as well as the people one is around when trying to reach someone by phone.[4]\n\nWhen making a sales call, hope for the person one are trying to connect to does not answer the phone. Leave up to five enticing messages and one's target audience will be ready to speak when one either gets a call back or one calls and reaches the person. The enticing message prepares the person to speak to the representative. It may be that the person is not interested based on what one had said in each voice message. Always be polite and accept that one may have many more to call. If the individual is reached, one might ask if there might be someone better suited for the advertised program.\n\nIf one is calling and leaving voice messages, include time of availability for callbacks. There is nothing worse than a callback coming to one when one is not available. Use the telephone as a great communication tool. Be polite and always put oneself in the other person's position. For more tips on making business calls and leaving enticing messages see Harlan J Brown's book on Telephone Participation.\nListening\n\nWhen listening to another employee or customer speak it is very important to be an avid listener. Here are some obstacles that you might have to overcome:\n\n    Filters and Assumptions\n    Biases and Prejudices\n    Inattention and Impatience\n    Surrounding Environment\n\nA good way to overcome these factors is by using LOTS Better Communication method. This method includes four steps in order to produce good listening skills and the ability to respond with an educated statement. The four steps to this method are:\n\n    Listen\n    Observe\n    Think\n    Speak\n\nDoing all of these things while showing good eye contact and body posture will assure the speaker that he/she is getting full attention from the listeners.\n\nChoice of Means and Mode of Communication - Choosing the right means and mode of communication plays a vital role in the effectiveness of the message being communicated and such choice depends on various factors such as:\n\nOrganization Size and Policy - If the organisation is small, probably more communication will be oral, than in larger organizations where it may organizations where it may be in writing. The policy for communication also would play a major role in influencing one's choice of mode of communication.\n\nCost Factor - The main point to be considered here would be to evaluate wheather the cost involved in sending the message would be commensurate with the results expected.\n\nNature of Message - Whether the message is confidential in nature, urgent or important etc. and whether a matter would require hand delivery or be set by registered post etc. also influences the choice of mode and means of communication.\n\nDistance Involved - Whether the message to be sent is also another vital factor which could influence the choice of means and modes of communication. For example, if a letter is to be sent to a partner in a joint venture in Japan and it is urgent, you would not think of sending someone to personally deliver it.\n\nResources - The resources available to both the sender and receiver would also influence your choice. You can only send a fax if the other person/organization has a fax machine. Therefore we can see that the choice of a particular mode and means of communication will depend on a case to case basis and is influenced by various factors.\nChoosing Communication Media\n\nWhen choosing a media of communication, it is important to consider who are the respective audience and the objective of the message itself. Rich media are more interactive than lean media and provide the opportunity for two-way communication: the receiver can ask questions and express opinions easily in person.[5] To help such decision, one may roughly refer to the continuum shown below.\n\nFrom Richer to Leaner[6]\n\n1.Face-to-Face Meeting\n2.In-Person Oral Presentation\n3.Online Meeting\n4.Videoconferencing\n5.Teleconferencing\n6.Phone Call\n7.Voice Message\n8.Video\n9.Blog\n10.Report\n11.Brochure\n12.Newsletter\n13.Flier\n14.Email\n\n15. Memo\n\nSubliminal method of communication\n\nSubliminal perception refers to the individual ability to perceive and respond to stimuli that are below the threshold or level of consciousness, which proved to influence thoughts, feelings or actions altogether or separately. There are four distinct methods of communicating subliminally. These are visual stimuli in movies, accelerated speech, embedded images in a print advertisement, and suggestiveness which is not normally seen at first glance.Focussing on Subliminal Communication through visual stimuli, Marketing people have adopted this method even incorporating it films and television shows.Subliminal method of communication first made its debut in a 1957 advertisement, during which a brief message flashed, telling viewers to eat popcorn and drink Coca-Cola. Since that time, subliminal communication has occupied a controversial role in the advertising landscape, with some people claiming it's omnipresent, while others emphasize it's not real. As of publication, there is still an ongoing scientific debate about whether subliminal advertising works. Subliminal messaging is a form of advertising in which a subtle message is inserted into a standard ad. This subtle message affects the consumer's behavior, but the consumer does not know she's seen the message. For example, a marketer might incorporate a single frame telling consumers to drink tea in a movie. In print media, advertisers might put hidden images or coded messages into ad text.\n\nArguments for Effectiveness\n\nA 2009 study at the University College of London found that people were especially likely to be affected by negative subliminal communication . For example, a cosmetic advertisement conveying to a consumer that she is ugly might be more effective. Subliminal ads \"prime\" the brain to seek out stimuli that match the message in the advertisement, according to a 1992 study published in \"Personality and Social Psychology Bulletin.\" This can affect behavior, particularly when a message addresses an individual's insecurities or behavioral tendencies and when a consumer is in a context that allows her to act on the ad's message.\nBusiness Writing Process\n\n[7]\n\nThe challenge of the communication process is for the sender and receiver to gain a mutual understanding about the meaning of the message. A writer can put his or her words on paper, but the reader may not react to the words as the writer intended. Most writers are much more effective, successful, and productive if they spend time thinking about the communication situation before beginning to write. Successful writers approach writing as a three- step process that involves planning before starting to write, drafting with the audience (the reader) in mind, and revising the document to determine if it meets the audience’s needs and if it represents the organization well.\nSTEP1: Planning\n\nYou should spend more time planning and revising your document than you spend writing. Dr. Ken Davis suggests effective writers spend as much as 40 percent of writing time on planning the document.\nSTEP2: Drafting\n\nOnce you have planned the purpose of your message, considered how your audience might react to the message, gathered your information, decided on an order for your information, and selected your medium for delivery, you are ready to compose your document. About 20 percent of your writing time should be spent drafting the document.\n\nDo not be concerned with perfection as you draft your message. Write in a conversational tone, without using slang; write as you would speak in a workplace environment. One guideline that helps in the drafting stage is to write as though you are presenting the information to a friend. Rather than thinking of the audience as just “someone out there,” think of the audience as a specific person with whom you are building or maintaining a relationship. Thinking of a friend helps you choose effective words and tone, helps you be clear, and helps you include information helpful to the reader.\nSTEP3: Revising\n\nRevising is more than checking your spelling and punctuation. Revising requires you to check every part of your message to see if it is clear, concise, and correct and will take approximately 40 percent of your writing time. You want to look at every word to see if you selected the most appropriate one, at every sentence to see whether the structure is the best it can be, and at every paragraph to see whether it includes a well-developed argument. Finally review the document design to look for an attractive, professional appearance that meets your employer’s and your reader’s expectations.\nOrganizations\n\n    Founded in 1936 by Shankar is the Association for Business Communication (ABC),[8] originally called the Association of College Teachers of Business Writing, is \"an international organization committed to fostering excellence in business communication scholarship,research ,education, and practice.\"\n    The IEEE Professional Communication Society (PCS) [2] \n    is dedicated to understanding and promoting effective communication in engineering, scientific, and other environments, including business environments. PCS's academic journal,[9] is one of the premier journals in Europe communication. The journal’s readers are engineers, writers, information designers, managers, and others working as scholars, educators, and practitioners who share an interest in the effective communication of technical and business information.\n    The Society for Technical Communication is a professional association dedicated to the advancement of the theory and practice of technical communication. With membership of more than 6,000 technical communicators, it's the largest organization of its type in North America.", "skillName": "Business communication."}
{"id": 88, "category": "Business", "skillText": "Business process management\nBusiness process management (BPM) is a field in operations management that focuses on improving corporate performance by managing and optimising a company's business processes.[1] It can therefore be described as a \"process optimization process.\" It is argued that BPM enables organizations to be more efficient, more effective and more capable of change than a functionally focused, traditional hierarchical management approach.[2] These processes can impact the cost and revenue generation of an organization.\n\nAs a policy-making approach, BPM sees processes as important assets of an organization that must be understood, managed, and developed to announce value-added products and services to clients or customers. This approach closely resembles other total quality management or continual improvement process methodologies and BPM proponents also claim that this approach can be supported, or enabled, through technology.[3] As such, many BPM articles and scholars frequently discuss BPM from one of two viewpoints: people and/or technology.\n\tDefinitions\n2\tChanges in business process management\n3\tBPM life-cycle\n3.1\tDesign\n3.2\tModeling\n3.3\tExecution\n3.4\tMonitoring\n3.5\tOptimization\n3.6\tRe-engineering\n4\tBPM suites\n5\tPractice\n5.1\tBPM technology\n5.2\tCloud computing BPM\n5.2.1\tMarket\n5.2.2\tBenefits\n5.3\tInternet of Things\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nDefinitions\nBPMInstitute.org[4] defines Business Process Management as:\n\nthe definition, improvement and management of a firm's end-to-end enterprise business processes in order to achieve three outcomes crucial to a performance-based, customer-driven firm: 1) clarity on strategic direction, 2) alignment of the firm's resources, and 3) increased discipline in daily operations. Read the article What is BPM Anyway?[5]\nThe Workflow Management Coalition,[6] BPM.com[7] and several other sources[8] have come to agreement on the following definition:\n\nBusiness Process Management (BPM) is a discipline involving any combination of modeling, automation, execution, control, measurement and optimization of business activity flows, in support of enterprise goals, spanning systems, employees, customers and partners within and beyond the enterprise boundaries.\nThe Association Of Business Process Management Professionals[9] defines BPM as:\n\nBusiness Process Management (BPM) is a disciplined approach to identify, design, execute, document, measure, monitor, and control both automated and non-automated business processes to achieve consistent, targeted results aligned with an organization�s strategic goals. BPM involves the deliberate, collaborative and increasingly technology-aided definition, improvement, innovation, and management of end-to-end business processes that drive business results, create value, and enable an organization to meet its business objectives with more agility. BPM enables an enterprise to align its business processes to its business strategy, leading to effective overall company performance through improvements of specific work activities either within a specific department, across the enterprise, or between organizations.\nGartner defines Business process management (BPM) as:\n\n\"the discipline of managing processes (rather than tasks) as the means for improving business performance outcomes and operational agility. Processes span organizational boundaries, linking together people, information flows, systems and other assets to create and deliver value to customers and constituents.\"[10]\nIt is common to confuse BPM with a BPM Suite (BPMS). BPM is a professional discipline done by people, while a BPMS is a technological suite of tool designed to help the BPM professional accomplish their goals. BPM should also not be confused with an application or solution that was developed to support a particular process. Suites and solutions represent ways of automating business processes, but automation is only one aspect of BPM.\n\nChanges in business process management\nThe concept of business process may be as traditional as concepts of tasks, department, production, and outputs, arising from job shop scheduling problems in the early 20th Century.[11] The management and improvement approach as of 2010, with formal definitions and technical modeling, has been around since the early 1990s (see business process modeling). Note that the term \"business process\" is sometimes used by IT practitioners as synonymous with the management of middleware processes or with integrating application software tasks.[citation needed]\n\nAlthough BPM initially focused on the automation of business processes with the use of information technology, it has since been extended[by whom?] to integrate human-driven processes in which human interaction takes place in series or parallel with the use of technology. For example, workflow management systems can assign individual steps requiring deploying human intuition or judgment to relevant humans and other tasks in a workflow to a relevant automated system.[12]\n\nMore recent variations such as \"human interaction management\"[13][14] are concerned with the interaction between human workers performing a task.[citation needed]\n\nAs of 2010 technology has allowed the coupling of BPM with other methodologies, such as Six Sigma.[citation needed] Some BPM tools such as SIPOCs, process flows, RACIs, CTQs and histograms allow users to:\n\nvisualize - functions and processes\nmeasure - determine the appropriate measure to determine success\nanalyze - compare the various simulations to determine an optimal improvement\nimprove - select and implement the improvement\ncontrol - deploy this implementation and by use of user-defined dashboards monitor the improvement in real time and feed the performance information back into the simulation model in preparation for the next improvement iteration\nre-engineer - revamp the processes from scratch for better results\nThis brings with it the benefit of being able to simulate changes to business processes based on real-world data (not just on assumed knowledge). Also, the coupling of BPM to industry methodologies allows users to continually streamline and optimize the process to ensure that it is tuned to its market need.[15][full citation needed]\n\nAs of 2012 research on BPM has paid increasing attention to the compliance of business processes. Although a key aspect of business processes is flexibility, as business processes continuously need to adapt to changes in the environment, compliance with business strategy, policies and government regulations should also be ensured.[16] The compliance aspect in BPM is highly important for governmental organizations. As of 2010 BPM approaches in a governmental context largely focus on operational processes and knowledge representation.[17] Although there have been many technical studies on operational business processes in both the public and private sectors, researchers have rarely taken legal compliance activities into account, for instance the legal implementation processes in public-administration bodies.[citation needed]\n\nBPM life-cycle\nBusiness process management activities can be arbitrarily grouped into categories such as design, modeling, execution, monitoring, and optimization.[18]\n\nBusiness Process Management Life-Cycle.svg\nDesign\nProcess design encompasses both the identification of existing processes and the design of \"to-be\" processes. Areas of focus include representation of the process flow, the factors within it, alerts and notifications, escalations, standard operating procedures, service level agreements, and task hand-over mechanisms.\n\nWhether or not existing processes are considered, the aim of this step is to ensure that a correct and efficient theoretical design is prepared.\n\nThe proposed improvement could be in human-to-human, human-to-system or system-to-system workflows, and might target regulatory, market, or competitive challenges faced by the businesses.\n\nThe existing process and the design of new process for various applications will have to synchronise and not cause major outage or process interruption.\n\nModeling\nModeling takes the theoretical design and introduces combinations of variables (e.g., changes in rent or materials costs, which determine how the process might operate under different circumstances).\n\nIt may also involve running \"what-if analysis\"(Conditions-when, if, else) on the processes: \"What if I have 75% of resources to do the same task?\" \"What if I want to do the same job for 80% of the current cost?\".\n\nExecution\n\nThis section possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (February 2015) (Learn how and when to remove this template message)\nOne of the ways to automate processes is to develop or purchase an application that executes the required steps of the process; however, in practice, these applications rarely execute all the steps of the process accurately or completely. Another approach is to use a combination of software and human intervention; however this approach is more complex, making the documentation process difficult.\n\nAs a response to these problems, software has been developed that enables the full business process (as developed in the process design activity) to be defined in a computer language which can be directly executed by the computer. The process models can be run through execution engines that automate the processes directly from the model (e.g. calculating a repayment plan for a loan) or, when a step is too complex to automate, Business Process Modeling Notation (BPMN) provides front-end capability for human input.[19] Compared to either of the previous approaches, directly executing a process definition can be more straightforward and therefore easier to improve. However, automating a process definition requires flexible and comprehensive infrastructure, which typically rules out implementing these systems in a legacy IT environment.\n\nBusiness rules have been used by systems to provide definitions for governing behavior, and a business rule engine can be used to drive process execution and resolution.\n\nMonitoring\nMonitoring encompasses the tracking of individual processes, so that information on their state can be easily seen, and statistics on the performance of one or more processes can be provided. An example of this tracking is being able to determine the state of a customer order (e.g. order arrived, awaiting delivery, invoice paid) so that problems in its operation can be identified and corrected.\n\nIn addition, this information can be used to work with customers and suppliers to improve their connected processes. Examples are the generation of measures on how quickly a customer order is processed or how many orders were processed in the last month. These measures tend to fit into three categories: cycle time, defect rate and productivity.\n\nThe degree of monitoring depends on what information the business wants to evaluate and analyze and how business wants it to be monitored, in real-time, near real-time or ad hoc. Here, business activity monitoring (BAM) extends and expands the monitoring tools generally provided by BPMS.\n\nProcess mining is a collection of methods and tools related to process monitoring. The aim of process mining is to analyze event logs extracted through process monitoring and to compare them with an a priori process model. Process mining allows process analysts to detect discrepancies between the actual process execution and the a priori model as well as to analyze bottlenecks.\n\nOptimization\nProcess optimization includes retrieving process performance information from modeling or monitoring phase; identifying the potential or actual bottlenecks and the potential opportunities for cost savings or other improvements; and then, applying those enhancements in the design of the process. Process mining tools are able to discover critical activities and bottlenecks, creating greater business value.[20]\n\nRe-engineering\nWhen the process becomes too complex or inefficient, and optimization is not fetching the desired output, it is usually recommended by a company steering committee chaired by the president / CEO to re-engineer the entire process cycle. Business process reengineering (BPR) has been used by organizations to attempt to achieve efficiency and productivity at work.\n\nBPM suites\nA market has developed for Enterprise software leveraging the Business Process Management concepts to organize and automate processes. The recent convergence of these software from distinct pieces such as Business rules engine, Business Process Modelling, Business Activity Monitoring and Human Workflow has given birth to integrated Business Process Management Suites. Forrester Research, Inc recognize the BPM suite space through three different lenses:\n\nhuman-centric BPM\nintegration-centric BPM (Enterprise Service Bus)\ndocument-centric BPM (Dynamic Case Management)\nHowever, standalone integration-centric and document-centric offerings have matured into separate, standalone markets.\n\nPractice\n\nExample of Business Process Management (BPM) Service Pattern: This pattern shows how business process management (BPM) tools can be used to implement business processes through the orchestration of activities between people and systems.[21]\nWhile the steps can be viewed as a cycle, economic or time constraints are likely to limit the process to only a few iterations. This is often the case when an organization uses the approach for short to medium term objectives rather than trying to transform the organizational culture. True iterations are only possible through the collaborative efforts of process participants. In a majority of organizations, complexity will require enabling technology (see below) to support the process participants in these daily process management challenges.\n\nTo date, many organizations often start a BPM project or program with the objective of optimizing an area that has been identified as an area for improvement.\n\nCurrently, the international standards for the task have limited BPM to the application in the IT sector, and ISO/IEC 15944 covers the operational aspects of the business. However, some corporations with the culture of best practices do use standard operating procedures to regulate their operational process.[22] Other standards are currently being worked upon to assist in BPM implementation (BPMN, Enterprise Architecture, Business Motivation Model).\n\nBPM technology\nBPM is now considered a critical component of operational intelligence (OI) solutions to deliver real-time, actionable information. This real-time information can be acted upon in a variety of ways - alerts can be sent or executive decisions can be made using real-time dashboards. OI solutions use real-time information to take automated action based on pre-defined rules so that security measures and or exception management processes can be initiated.\n\nAs such, some people view BPM as \"the bridge between Information Technology (IT) and Business.\"[citation needed]. In fact, an argument can be made that this \"holistic approach\" bridges organizational and technological silos.\n\nThere are four critical components of a BPM Suite:\n\nProcess engine � a robust platform for modeling and executing process-based applications, including business rules\nBusiness analytics � enable managers to identify business issues, trends, and opportunities with reports and dashboards and react accordingly\nContent management � provides a system for storing and securing electronic documents, images, and other files\nCollaboration tools � remove intra- and interdepartmental communication barriers through discussion forums, dynamic workspaces, and message boards\nBPM also addresses many of the critical IT issues underpinning these business drivers, including:\n\nManaging end-to-end, customer-facing processes\nConsolidating data and increasing visibility into and access to associated data and information\nIncreasing the flexibility and functionality of current infrastructure and data\nIntegrating with existing systems and leveraging service oriented architecture (SOA)\nEstablishing a common language for business-IT alignment\nValidation of BPMS is another technical issue that vendors and users need to be aware of, if regulatory compliance is mandatory.[23] The validation task could be performed either by an authenticated third party or by the users themselves. Either way, validation documentation will need to be generated. The validation document usually can either be published officially or retained by users.\n\nCloud computing BPM\nCloud computing business process management is the use of (BPM) tools that are delivered as software services (SaaS) over a network. Cloud BPM business logic is deployed on an application server and the business data resides in cloud storage.\n\nMarket\nAccording to Gartner, 20% of all the \"shadow business processes\" will be supported by BPM cloud platforms[citation needed]. Gartner refers to all the hidden organizational processes that are supported by IT departments as part of legacy business processes such as Excel spreadsheets, routing of emails using rules, phone calls routing, etc. These can, of course also be replaced by other technologies such as workflow software.\n\nBenefits\nThe benefits of using cloud BPM services include removing the need and cost of maintaining specialized technical skill sets in-house and reducing distractions from an enterprise's main focus. It offers controlled IT budgeting and enables geographical mobility.\n\nThe details of this are still emerging.[24][full citation needed]\n\nInternet of Things\nThe emerging Internet of Things poses a significant challenge to control and manage the flow of information through large numbers of devices. To cope with this, a new direction known as BPM Everywhere shows promise as way of blending traditional process techniques, with additional capabilities to automate the handling of all the independent devices.\n\nSee also\nApplication service provider\nBPM Everywhere\nBusiness process modeling\nBusiness activity monitoring\nBusiness intelligence\nBusiness-oriented architecture\nBusiness process automation\nBusiness process reengineering\nComparison of business integration software\nEnterprise planning systems\nEnterprise software\nITIL\nManaged services\nWorkflow", "skillName": "Business Management."}
{"id": 89, "category": "Business", "skillText": "Business communication is the sharing of information between people within and outside the organization that is performed for the commercial benefit of the organization. It can also be defined as relaying of information within a business by its people.\n\nContents\n\n    1 Overview\n    2 Effective business communication\n        2.1 Face-to-face\n        2.2 Email\n        2.3 Telephone\n        2.4 Listening\n    3 Choosing Communication Media\n    4 Business Writing Process\n        4.1 STEP1: Planning\n        4.2 STEP2: Drafting\n        4.3 STEP3: Revising\n    5 Organizations\n    6 References\n\nOverview\n\nBusiness communication (or simply \"communication\", in a business context) encompasses topics such as marketing, brand management, customer relations, consumer behavior, advertising, public relations, corporate communication, community engagement, reputation management, interpersonal communication, employee engagement, and event management. It is closely related to the fields of professional communication and technical communication.\n\nMedia channels for business communication include the Internet, print media, radio, television, ambient media, and word of mouth.\n\nBusiness communication can also refer to internal communication that takes place within an organization.\n\nBusiness communication is a common topic included in the curricula of Undergraduate and Master programs of many colleges and universities.\n\nThere are several methods of business communication, including:\n\n    Web-based communication - for better and improved communication, anytime anywhere ...\n    video conferencing which allow people in different locations to hold interactive meetings;\n    Reports - important in documenting the activities of any department;\n    Presentations - very popular method of communication in all types of organizations, usually involving audiovisual material, like copies of reports, or material prepared in Microsoft PowerPoint or Adobe Flash;\n    telephone meetings, which allow for long distance speech;\n    forum boards, which allow people to instantly post information at a centralized location; and\n    face-to-face meetings, which are personal and should be succeeded by a written followup.\n    suggestion box: It is primarily used for upward communication, because some people may hesitate to communicate with management directly, so they opt to give suggestions by drafting one and putting it in the suggestion box.\n\nEffective business communication\n\nA two way information sharing process which involves one party sending a message that is easily understood by the receiving party. Effective communication by business managers facilitates information sharing between company employees and can substantially contribute to its commercial success.[1]\n\nFor business communication to be effective these qualities are essential :\n\n    Establish clear hierarchy\n    Use visual communication\n    Conflict Management\n    Consider Cultural Issues\n    Good Written communication\n\nFace-to-face\n\nFace-to-face communication helps to establish a personal connection and will help sell the product or service to the customer.[2] These interactions can portray a whole different message than written communication as tone, pitch, and body language is observed.[3] Information is easier to access and delivered immediately with interactions rather than waiting for an email or phone call. Conflicts are also easily resolved this way, as verbal and non-verbal cues are observed and acted upon. Communicating professionally is very important as one is representing the company. Speak clearly and ask questions to understand the needs and wants, let the recipient respond as one resolves the issue. Decisions are made more confidently during a face-to-face interaction as the recipient asks questions to understand and move forward with their decision.\nEmail\n\nWhen using email to communicate in the business world, it is important to be careful with the choice of words. Miscommunication is very frequent as the reader doesn’t know what non-verbal cues one is giving off, such as the pitch, tone, or expressions. Before beginning an email, make sure the email address one is using is appropriate and professional as well as the message one is going to send. Again, make sure the information is clear and to the point so the recipient isn’t confused. Make sure one includes their signature, title, and other contact information at the end.[citation needed].\nTelephone\n\nWhen making a business call, make it clear who is on the line and where one is from as well as one's message when on the phone. Smile and have a positive attitude as the recipient will be able to read the caller and that will affect how they react. When leaving a message, make sure one is clear and brief. One should state their name and who they are and the purpose for contacting them. If replying to a voicemail, try to respond as soon as possible and take into consideration the time of day. Don't call too early or too late, as it is important to respect other's time. Also be mindful of where one is and the noise level as well as the people one is around when trying to reach someone by phone.[4]\n\nWhen making a sales call, hope for the person one are trying to connect to does not answer the phone. Leave up to five enticing messages and one's target audience will be ready to speak when one either gets a call back or one calls and reaches the person. The enticing message prepares the person to speak to the representative. It may be that the person is not interested based on what one had said in each voice message. Always be polite and accept that one may have many more to call. If the individual is reached, one might ask if there might be someone better suited for the advertised program.\n\nIf one is calling and leaving voice messages, include time of availability for callbacks. There is nothing worse than a callback coming to one when one is not available. Use the telephone as a great communication tool. Be polite and always put oneself in the other person's position. For more tips on making business calls and leaving enticing messages see Harlan J Brown's book on Telephone Participation.\nListening\n\nWhen listening to another employee or customer speak it is very important to be an avid listener. Here are some obstacles that you might have to overcome:\n\n    Filters and Assumptions\n    Biases and Prejudices\n    Inattention and Impatience\n    Surrounding Environment\n\nA good way to overcome these factors is by using LOTS Better Communication method. This method includes four steps in order to produce good listening skills and the ability to respond with an educated statement. The four steps to this method are:\n\n    Listen\n    Observe\n    Think\n    Speak\n\nDoing all of these things while showing good eye contact and body posture will assure the speaker that he/she is getting full attention from the listeners.\n\nChoice of Means and Mode of Communication - Choosing the right means and mode of communication plays a vital role in the effectiveness of the message being communicated and such choice depends on various factors such as:\n\nOrganization Size and Policy - If the organisation is small, probably more communication will be oral, than in larger organizations where it may organizations where it may be in writing. The policy for communication also would play a major role in influencing one's choice of mode of communication.\n\nCost Factor - The main point to be considered here would be to evaluate wheather the cost involved in sending the message would be commensurate with the results expected.\n\nNature of Message - Whether the message is confidential in nature, urgent or important etc. and whether a matter would require hand delivery or be set by registered post etc. also influences the choice of mode and means of communication.\n\nDistance Involved - Whether the message to be sent is also another vital factor which could influence the choice of means and modes of communication. For example, if a letter is to be sent to a partner in a joint venture in Japan and it is urgent, you would not think of sending someone to personally deliver it.\n\nResources - The resources available to both the sender and receiver would also influence your choice. You can only send a fax if the other person/organization has a fax machine. Therefore we can see that the choice of a particular mode and means of communication will depend on a case to case basis and is influenced by various factors.\nChoosing Communication Media\n\nWhen choosing a media of communication, it is important to consider who are the respective audience and the objective of the message itself. Rich media are more interactive than lean media and provide the opportunity for two-way communication: the receiver can ask questions and express opinions easily in person.[5] To help such decision, one may roughly refer to the continuum shown below.\n\nFrom Richer to Leaner[6]\n\n1.Face-to-Face Meeting\n2.In-Person Oral Presentation\n3.Online Meeting\n4.Videoconferencing\n5.Teleconferencing\n6.Phone Call\n7.Voice Message\n8.Video\n9.Blog\n10.Report\n11.Brochure\n12.Newsletter\n13.Flier\n14.Email\n\n15. Memo\n\nSubliminal method of communication\n\nSubliminal perception refers to the individual ability to perceive and respond to stimuli that are below the threshold or level of consciousness, which proved to influence thoughts, feelings or actions altogether or separately. There are four distinct methods of communicating subliminally. These are visual stimuli in movies, accelerated speech, embedded images in a print advertisement, and suggestiveness which is not normally seen at first glance.Focussing on Subliminal Communication through visual stimuli, Marketing people have adopted this method even incorporating it films and television shows.Subliminal method of communication first made its debut in a 1957 advertisement, during which a brief message flashed, telling viewers to eat popcorn and drink Coca-Cola. Since that time, subliminal communication has occupied a controversial role in the advertising landscape, with some people claiming it's omnipresent, while others emphasize it's not real. As of publication, there is still an ongoing scientific debate about whether subliminal advertising works. Subliminal messaging is a form of advertising in which a subtle message is inserted into a standard ad. This subtle message affects the consumer's behavior, but the consumer does not know she's seen the message. For example, a marketer might incorporate a single frame telling consumers to drink tea in a movie. In print media, advertisers might put hidden images or coded messages into ad text.\n\nArguments for Effectiveness\n\nA 2009 study at the University College of London found that people were especially likely to be affected by negative subliminal communication . For example, a cosmetic advertisement conveying to a consumer that she is ugly might be more effective. Subliminal ads \"prime\" the brain to seek out stimuli that match the message in the advertisement, according to a 1992 study published in \"Personality and Social Psychology Bulletin.\" This can affect behavior, particularly when a message addresses an individual's insecurities or behavioral tendencies and when a consumer is in a context that allows her to act on the ad's message.\nBusiness Writing Process\n\n[7]\n\nThe challenge of the communication process is for the sender and receiver to gain a mutual understanding about the meaning of the message. A writer can put his or her words on paper, but the reader may not react to the words as the writer intended. Most writers are much more effective, successful, and productive if they spend time thinking about the communication situation before beginning to write. Successful writers approach writing as a three- step process that involves planning before starting to write, drafting with the audience (the reader) in mind, and revising the document to determine if it meets the audience’s needs and if it represents the organization well.\nSTEP1: Planning\n\nYou should spend more time planning and revising your document than you spend writing. Dr. Ken Davis suggests effective writers spend as much as 40 percent of writing time on planning the document.\nSTEP2: Drafting\n\nOnce you have planned the purpose of your message, considered how your audience might react to the message, gathered your information, decided on an order for your information, and selected your medium for delivery, you are ready to compose your document. About 20 percent of your writing time should be spent drafting the document.\n\nDo not be concerned with perfection as you draft your message. Write in a conversational tone, without using slang; write as you would speak in a workplace environment. One guideline that helps in the drafting stage is to write as though you are presenting the information to a friend. Rather than thinking of the audience as just “someone out there,” think of the audience as a specific person with whom you are building or maintaining a relationship. Thinking of a friend helps you choose effective words and tone, helps you be clear, and helps you include information helpful to the reader.\nSTEP3: Revising\n\nRevising is more than checking your spelling and punctuation. Revising requires you to check every part of your message to see if it is clear, concise, and correct and will take approximately 40 percent of your writing time. You want to look at every word to see if you selected the most appropriate one, at every sentence to see whether the structure is the best it can be, and at every paragraph to see whether it includes a well-developed argument. Finally review the document design to look for an attractive, professional appearance that meets your employer’s and your reader’s expectations.\nOrganizations\n\n    Founded in 1936 by Shankar is the Association for Business Communication (ABC),[8] originally called the Association of College Teachers of Business Writing, is \"an international organization committed to fostering excellence in business communication scholarship,research ,education, and practice.\"\n    The IEEE Professional Communication Society (PCS) [2] \n    is dedicated to understanding and promoting effective communication in engineering, scientific, and other environments, including business environments. PCS's academic journal,[9] is one of the premier journals in Europe communication. The journal’s readers are engineers, writers, information designers, managers, and others working as scholars, educators, and practitioners who share an interest in the effective communication of technical and business information.\n    The Society for Technical Communication is a professional association dedicated to the advancement of the theory and practice of technical communication. With membership of more than 6,000 technical communicators, it's the largest organization of its type in North America.\n\n\nBusiness intelligence\nBusiness administration\nSociety\nCompany Business Conglomerate\nBusiness organization\nBusiness entity\nCorporate governance\nCorporate titles\nEconomy\nCorporate law\nFinance\nAccounting\nTrade\nOrganization\nSociety\nTypes of management\nBusiness intelligence (BI) can be described as \"a set of techniques and tools for the acquisition and transformation of raw data into meaningful and useful information for business analysis purposes\".[1] The term \"data surfacing\" is also more often associated with BI functionality. BI technologies are capable of handling large amounts of unstructured data to help identify, develop and otherwise create new strategic business opportunities. The goal of BI is to allow for the easy interpretation of these large volumes of data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]\n\nBI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies are reporting, online analytical processing, analytics, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics and prescriptive analytics.\n\nBI can be used to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions include priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a more complete picture which, in effect, creates an \"intelligence\" that cannot be derived by any singular set of data.[3] Amongst myriad uses, BI tools empower organisations to gain insight into new markets, assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts.[4]\nIllustration showing benefits of business intelligence in entrepreneurship. (created by Hamid Q)\n\n1\tComponents\n2\tHistory\n3\tData warehousing\n4\tComparison with competitive intelligence\n5\tComparison with business analytics\n6\tApplications in an enterprise\n7\tPrioritization of projects\n8\tSuccess factors of implementation\n8.1\tBusiness sponsorship\n8.2\tBusiness needs\n8.3\tAmount and quality of available data\n9\tUser aspect\n10\tBI Portals\n11\tMarketplace\n11.1\tIndustry-specific\n12\tSemi-structured or unstructured data\n12.1\tUnstructured data vs. semi-structured data\n12.2\tProblems with semi-structured or unstructured data\n12.3\tThe use of metadata\n13\tFuture\n14\tSee also\n15\tReferences\n16\tBibliography\n17\tExternal links\nComponents\nBusiness intelligence is made up of an increasing number of components including:\n\nMultidimensional aggregation and allocation\nDenormalization, tagging and standardization\nRealtime reporting with analytical alert\nA method of interfacing with unstructured data sources\nGroup consolidation, budgeting and rolling forecasts\nStatistical inference and probabilistic simulation\nKey performance indicators optimization\nVersion control and process management\nOpen item management\nHistory\nThe earliest known use of the term \"Business Intelligence\" is in Richard Millar Devens in the Cyclopædia of Commercial and Business Anecdotes from 1865. Devens used the term to describe how the banker, Sir Henry Furnese, gained profit by receiving and acting upon information about his environment, prior to his competitors. Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news. (Devens, (1865), p. 210). The ability to collect and react accordingly based on the information retrieved, an ability that Furnese excelled in, is today still at the very heart of BI.[5]\n\nIn a 1958 article, IBM researcher Hans Peter Luhn used the term business intelligence. He employed the Webster's dictionary definition of intelligence: \"the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.\"[6]\n\nBusiness intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s. DSS originated in the computer-aided models created to assist with decision making and planning. From DSS, data warehouses, Executive Information Systems, OLAP and business intelligence came into focus beginning in the late 80s.\n\nIn 1989, Howard Dresner (later a Gartner analyst) proposed \"business intelligence\" as an umbrella term to describe \"concepts and methods to improve business decision making by using fact-based support systems.\"[7] It was not until the late 1990s that this usage was widespread.[8]\n\nData warehousing\nOften BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW sometimes combine as \"BI/DW\"[9] or as \"BIDW\". A data warehouse contains a copy of analytical data that facilitates decision support. However, not all data warehouses serve for business intelligence, nor do all business intelligence applications require a data warehouse.\n\nTo distinguish between the concepts of business intelligence and data warehouses, Forrester Research defines business intelligence in one of two ways:\n\nUsing a broad definition: \"Business Intelligence is a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.\"[10] Under this definition, business intelligence also includes technologies such as data integration, data quality, data warehousing, master-data management, text- and content-analytics, and many others that the market sometimes lumps into the \"Information Management\" segment. Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.\nForrester defines the narrower business-intelligence market as, \"...referring to just the top layers of the BI architectural stack such as reporting, analytics and dashboards.\"[11]\nComparison with competitive intelligence\nThough the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.[12]\n\nComparison with business analytics\nBusiness intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[13] One definition contrasts the two, stating that the term business intelligence refers to collecting business data to find information primarily through asking questions, reporting, and online analytical processes. Business analytics, on the other hand, uses statistical and quantitative tools for explanatory and predictive modeling.[14]\n\nIn an alternate definition, Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an \"alerts\" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[15]\n\nApplications in an enterprise\nBusiness intelligence can be applied to the following business purposes, in order to drive business value.[citation needed]\n\nMeasurement  program that creates a hierarchy of performance metrics (see also Metrics Reference Model) and benchmarking that informs business leaders about progress towards business goals (business process management).\nAnalytics  program that builds quantitative processes for a business to arrive at optimal decisions and to perform business knowledge discovery. Frequently involves: data mining, process mining, statistical analysis, predictive analytics, predictive modeling, business process modeling, data lineage, complex event processing and prescriptive analytics.\nReporting/enterprise reporting  program that builds infrastructure for strategic reporting to serve the strategic management of a business, not operational reporting. Frequently involves data visualization, executive information system and OLAP.\nCollaboration/collaboration platform  program that gets different areas (both inside and outside the business) to work together through data sharing and electronic data interchange.\nKnowledge management  program to make the company data-driven through strategies and practices to identify, create, represent, distribute, and enable adoption of insights and experiences that are true business knowledge. Knowledge management leads to learning management and regulatory compliance.\nIn addition to the above, business intelligence can provide a pro-active approach, such as alert functionality that immediately notifies the end-user if certain conditions are met. For example, if some business metric exceeds a pre-defined threshold, the metric will be highlighted in standard reports, and the business analyst may be alerted via e-mail or another monitoring service. This end-to-end process requires data governance, which should be handled by the expert.[citation needed]\n\nPrioritization of projects\nIt can be difficult to provide a positive business case for business intelligence initiatives, and often the projects must be prioritized through strategic initiatives. BI projects can attain higher prioritization within the organization if managers consider the following:\n\nAs described by Kimball[16] the BI manager must determine the tangible benefits such as eliminated cost of producing legacy reports.\nData access for the entire organization must be enforced.[17] In this way even a small benefit, such as a few minutes saved, makes a difference when multiplied by the number of employees in the entire organization.\nAs described by Ross, Weil & Roberson for Enterprise Architecture,[18] managers should also consider letting the BI project be driven by other business initiatives with excellent business cases. To support this approach, the organization must have enterprise architects who can identify suitable business projects.\nUsing a structured and quantitative methodology to create defensible prioritization in line with the actual needs of the organization, such as a weighted decision matrix.[19]\nSuccess factors of implementation\nAccording to Kimball et al., there are three critical areas that organizations should assess before getting ready to do a BI project:[20]\n\nThe level of commitment and sponsorship of the project from senior management.\nThe level of business need for creating a BI implementation.\nThe amount and quality of business data available.\nBusiness sponsorship\nThe commitment and sponsorship of senior management is according to Kimball et al., the most important criteria for assessment.[21] This is because having strong management backing helps overcome shortcomings elsewhere in the project. However, as Kimball et al. state: even the most elegantly designed DW/BI system cannot overcome a lack of business [management] sponsorship.[22]\n\nIt is important that personnel who participate in the project have a vision and an idea of the benefits and drawbacks of implementing a BI system. The best business sponsor should have organizational clout and should be well connected within the organization. It is ideal that the business sponsor is demanding but also able to be realistic and supportive if the implementation runs into delays or drawbacks. The management sponsor also needs to be able to assume accountability and to take responsibility for failures and setbacks on the project. Support from multiple members of the management ensures the project does not fail if one person leaves the steering group. However, having many managers work together on the project can also mean that there are several different interests that attempt to pull the project in different directions, such as if different departments want to put more emphasis on their usage. This issue can be countered by an early and specific analysis of the business areas that benefit the most from the implementation. All stakeholders in the project should participate in this analysis in order for them to feel invested in the project and to find common ground.\n\nAnother management problem that may be encountered before the start of an implementation is an overly aggressive business sponsor. Problems of scope creep occur when the sponsor requests data sets that were not specified in the original planning phase.\n\nBusiness needs\nBecause of the close relationship with senior management, another critical thing that must be assessed before the project begins is whether or not there is a business need and whether there is a clear business benefit by doing the implementation.[23] The needs and benefits of the implementation are sometimes driven by competition and the need to gain an advantage in the market. Another reason for a business-driven approach to implementation of BI is the acquisition of other organizations that enlarge the original organization it can sometimes be beneficial to implement DW or BI in order to create more oversight.\n\nCompanies that implement BI are often large, multinational organizations with diverse subsidiaries.[24] A well-designed BI solution provides a consolidated view of key business data not available anywhere else in the organization, giving management visibility and control over measures that otherwise would not exist.\n\nAmount and quality of available data\nWithout proper data, or with too little quality data, any BI implementation fails; it does not matter how good the management sponsorship or business-driven motivation is. Before implementation it is a good idea to do data profiling. This analysis identifies the content, consistency and structure [..][23] of the data. This should be done as early as possible in the process and if the analysis shows that data is lacking, put the project on hold temporarily while the IT department figures out how to properly collect data.\n\nWhen planning for business data and business intelligence requirements, it is always advisable to consider specific scenarios that apply to a particular organization, and then select the business intelligence features best suited for the scenario.\n\nOften, scenarios revolve around distinct business processes, each built on one or more data sources. These sources are used by features that present that data as information to knowledge workers, who subsequently act on that information. The business needs of the organization for each business process adopted correspond to the essential steps of business intelligence. These essential steps of business intelligence include but are not limited to:\n\nGo through business data sources in order to collect needed data\nConvert business data to information and present appropriately\nQuery and analyze data\nAct on the collected data\nThe quality aspect in business intelligence should cover all the process from the source data to the final reporting. At each step, the quality gates are different:\n\nSource Data:\nData Standardization: make data comparable (same unit, same pattern...)\nMaster Data Management: unique referential\nOperational Data Store (ODS):\nData Cleansing: detect & correct inaccurate data\nData Profiling: check inappropriate value, null/empty\nData warehouse:\nCompleteness: check that all expected data are loaded\nReferential integrity: unique and existing referential over all sources\nConsistency between sources: check consolidated data vs sources\nReporting:\nUniqueness of indicators: only one share dictionary of indicators\nFormula accuracy: local reporting formula should be avoided or checked\nUser aspect\nSome considerations must be made in order to successfully integrate the usage of business intelligence systems in a company. Ultimately the BI system must be accepted and utilized by the users in order for it to add value to the organization.[25][26] If the usability of the system is poor, the users may become frustrated and spend a considerable amount of time figuring out how to use the system or may not be able to really use the system. If the system does not add value to the usersŽ mission, they simply don't use it.[26]\n\nTo increase user acceptance of a BI system, it can be advisable to consult business users at an early stage of the DW/BI lifecycle, for example at the requirements gathering phase.[25] This can provide an insight into the business process and what the users need from the BI system. There are several methods for gathering this information, such as questionnaires and interview sessions.\n\nWhen gathering the requirements from the business users, the local IT department should also be consulted in order to determine to which degree it is possible to fulfill the business's needs based on the available data.[25]\n\nTaking a user-centered approach throughout the design and development stage may further increase the chance of rapid user adoption of the BI system.[26]\n\nBesides focusing on the user experience offered by the BI applications, it may also possibly motivate the users to utilize the system by adding an element of competition. Kimball[25] suggests implementing a function on the Business Intelligence portal website where reports on system usage can be found. By doing so, managers can see how well their departments are doing and compare themselves to others and this may spur them to encourage their staff to utilize the BI system even more.\n\nIn a 2007 article, H. J. Watson gives an example of how the competitive element can act as an incentive.[27] Watson describes how a large call centre implemented performance dashboards for all call agents, with monthly incentive bonuses tied to performance metrics. Also, agents could compare their performance to other team members. The implementation of this type of performance measurement and competition significantly improved agent performance.\n\nBI chances of success can be improved by involving senior management to help make BI a part of the organizational culture, and by providing the users with necessary tools, training, and support.[27] Training encourages more people to use the BI application.[25]\n\nProviding user support is necessary to maintain the BI system and resolve user problems.[26] User support can be incorporated in many ways, for example by creating a website. The website should contain great content and tools for finding the necessary information. Furthermore, helpdesk support can be used. The help desk can be manned by power users or the DW/BI project team.[25]\n\nBI Portals\nA Business Intelligence portal (BI portal) is the primary access interface for Data Warehouse (DW) and Business Intelligence (BI) applications. The BI portal is the user's first impression of the DW/BI system. It is typically a browser application, from which the user has access to all the individual services of the DW/BI system, reports and other analytical functionality. The BI portal must be implemented in such a way that it is easy for the users of the DW/BI application to call on the functionality of the application.[28]\n\nThe BI portal's main functionality is to provide a navigation system of the DW/BI application. This means that the portal has to be implemented in a way that the user has access to all the functions of the DW/BI application.\n\nThe most common way to design the portal is to custom fit it to the business processes of the organization for which the DW/BI application is designed, in that way the portal can best fit the needs and requirements of its users.[29]\n\nThe BI portal needs to be easy to use and understand, and if possible have a look and feel similar to other applications or web content of the organization the DW/BI application is designed for (consistency).\n\nThe following is a list of desirable features for web portals in general and BI portals in particular:\n\nUsable\nUser should easily find what they need in the BI tool.\nContent Rich\nThe portal is not just a report printing tool, it should contain more functionality such as advice, help, support information and documentation.\nClean\nThe portal should be designed so it is easily understandable and not over-complex as to confuse the users\nCurrent\nThe portal should be updated regularly.\nInteractive\nThe portal should be implemented in a way that makes it easy for the user to use its functionality and encourage them to use the portal. Scalability and customization give the user the means to fit the portal to each user.\nValue Oriented\nIt is important that the user has the feeling that the DW/BI application is a valuable resource that is worth working on.\nMarketplace\nThere are a number of business intelligence vendors, often categorized into the remaining independent \"pure-play\" vendors and consolidated \"megavendors\" that have entered the market through a recent trend[30] of acquisitions in the BI industry.[31] The business intelligence market is gradually growing. In 2012 business intelligence services brought in $13.1 billion in revenue.[32]\n\nSome companies adopting BI software decide to pick and choose from different product offerings (best-of-breed) rather than purchase one comprehensive integrated solution (full-service).[33]\n\nIndustry-specific\nSpecific considerations for business intelligence systems have to be taken in some sectors such as governmental banking regulations or healthcare.[34] The information collected by banking institutions and analyzed with BI software must be protected from some groups or individuals, while being fully available to other groups or individuals. Therefore, BI solutions must be sensitive to those needs and be flexible enough to adapt to new regulations and changes to existing law.[citation needed]\n\nSemi-structured or unstructured data\nBusinesses create a huge amount of valuable information in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material and news. According to Merrill Lynch, more than 85% of all business information exists in these forms. These information types are called either semi-structured or unstructured data. However, organizations often only use these documents once.[35]\n\nThe management of semi-structured data is recognized as a major unsolved problem in the information technology industry.[36] According to projections from Gartner (2003), white collar workers spend anywhere from 30 to 40 percent of their time searching, finding and assessing unstructured data. BI uses both structured and unstructured data, but the former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision making.[36][37] Because of the difficulty of properly searching, finding and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task or project. This can ultimately lead to poorly informed decision making.[35]\n\nTherefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[37]\n\nUnstructured data vs. semi-structured data\nUnstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row within each file or document.[citation needed]\n\nMany of these data types, however, like e-mails, word processing text files, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[36] but no specific consensus seems to have been reached.\n\nUnstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.\n\nProblems with semi-structured or unstructured data\nThere are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[38] some of those are:\n\nPhysically accessing unstructured textual data  unstructured data is stored in a huge variety of formats.\nTerminology  Among researchers and analysts, there is a need to develop a standardized terminology.\nVolume of data  As stated earlier, up to 85% of all data exists as semi-structured data. Couple that with the need for word-to-word and semantic analysis.\nSearchability of unstructured textual data  A simple search on some data, e.g. apple, results in links where there is a reference to that precise search term. (Inmon & Nesavich, 2008)[38] gives an example: a search is made on the term felony. In a simple search, the term felony is used, and everywhere there is a reference to felony, a hit to an unstructured document is made. But a simple search is crude. It does not find references to crime, arson, murder, embezzlement, vehicular homicide, and such, even though these crimes are types of felonies.\nThe use of metadata\nTo solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[35] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content  e.g. summaries, topics, people or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.\n\nFuture\nA 2009 paper predicted[39] these developments in the business intelligence market:\n\nBecause of lack of information, processes, and tools, through 2012, more than 35 percent of the top 5,000 global companies regularly fail to make insightful decisions about significant changes in their business and markets.\nBy 2012, business units will control at least 40 percent of the total budget for business intelligence.\nBy 2012, one-third of analytic applications applied to business processes will be delivered through coarse-grained application mashups.\nA 2009 Information Management special report predicted the top BI trends: \"green computing, social networking services, data visualization, mobile BI, predictive analytics, composite applications, cloud computing and multitouch.\".[40] Research undertaken in 2014 indicated that employees are more likely to have access to, and more likely to engage with, cloud-based BI tools than traditional tools.[41]\n\nOther business intelligence trends include the following:\n\nThird party SOA-BI products increasingly address ETL issues of volume and throughput.\nCompanies embrace in-memory processing, 64-bit processing, and pre-packaged analytic BI applications.\nOperational applications have callable BI components, with improvements in response time, scaling, and concurrency.\nNear or real time BI analytics is a baseline expectation.\nOpen source BI software replaces vendor offerings.\nOther lines of research include the combined study of business intelligence and uncertain data.[42][43] In this context, the data used is not assumed to be precise, accurate and complete. Instead, data is considered uncertain and therefore this uncertainty is propagated to the results produced by BI.\n\nAccording to a study by the Aberdeen Group, there has been increasing interest in Software-as-a-Service (SaaS) business intelligence over the past years, with twice as many organizations using this deployment approach as one year ago  15% in 2009 compared to 7% in 2008.[44]\n\nAn article by InfoWorlds Chris Kanaracus points out similar growth data from research firm IDC, which predicts the SaaS BI market will grow 22 percent each year through 2013 thanks to increased product sophistication, strained IT budgets, and other factors.[45]\n\nAn analysis of top 100 Business Intelligence and Analytics scores and ranks the firms based on several open variables[46]\n\nSee also\nAccounting intelligence\nAnalytic applications\nArtificial intelligence marketing\nBusiness Intelligence 2.0\nBusiness process discovery\nBusiness process management\nBusiness activity monitoring\nBusiness service management\nComparison of OLAP Servers\nCustomer dynamics\nData Presentation Architecture\nData visualization\nDecision engineering\nEnterprise planning systems\nInfonomics\nDocument intelligence\nIntegrated business planning\nLocation intelligence\nMedia intelligence\nMeteorological intelligence\nMobile business intelligence\nMultiway Data Analysis\nOperational intelligence\nBusiness Information Systems\nBusiness intelligence tools\nProcess mining\nReal-time business intelligence\nRuntime intelligence\nSales intelligence\nTest and learn\n\nBusiness process management\nBusiness process management (BPM) is a field in operations management that focuses on improving corporate performance by managing and optimising a company's business processes.[1] It can therefore be described as a \"process optimization process.\" It is argued that BPM enables organizations to be more efficient, more effective and more capable of change than a functionally focused, traditional hierarchical management approach.[2] These processes can impact the cost and revenue generation of an organization.\n\nAs a policy-making approach, BPM sees processes as important assets of an organization that must be understood, managed, and developed to announce value-added products and services to clients or customers. This approach closely resembles other total quality management or continual improvement process methodologies and BPM proponents also claim that this approach can be supported, or enabled, through technology.[3] As such, many BPM articles and scholars frequently discuss BPM from one of two viewpoints: people and/or technology.\n\tDefinitions\n2\tChanges in business process management\n3\tBPM life-cycle\n3.1\tDesign\n3.2\tModeling\n3.3\tExecution\n3.4\tMonitoring\n3.5\tOptimization\n3.6\tRe-engineering\n4\tBPM suites\n5\tPractice\n5.1\tBPM technology\n5.2\tCloud computing BPM\n5.2.1\tMarket\n5.2.2\tBenefits\n5.3\tInternet of Things\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nDefinitions\nBPMInstitute.org[4] defines Business Process Management as:\n\nthe definition, improvement and management of a firm's end-to-end enterprise business processes in order to achieve three outcomes crucial to a performance-based, customer-driven firm: 1) clarity on strategic direction, 2) alignment of the firm's resources, and 3) increased discipline in daily operations. Read the article What is BPM Anyway?[5]\nThe Workflow Management Coalition,[6] BPM.com[7] and several other sources[8] have come to agreement on the following definition:\n\nBusiness Process Management (BPM) is a discipline involving any combination of modeling, automation, execution, control, measurement and optimization of business activity flows, in support of enterprise goals, spanning systems, employees, customers and partners within and beyond the enterprise boundaries.\nThe Association Of Business Process Management Professionals[9] defines BPM as:\n\nBusiness Process Management (BPM) is a disciplined approach to identify, design, execute, document, measure, monitor, and control both automated and non-automated business processes to achieve consistent, targeted results aligned with an organizations strategic goals. BPM involves the deliberate, collaborative and increasingly technology-aided definition, improvement, innovation, and management of end-to-end business processes that drive business results, create value, and enable an organization to meet its business objectives with more agility. BPM enables an enterprise to align its business processes to its business strategy, leading to effective overall company performance through improvements of specific work activities either within a specific department, across the enterprise, or between organizations.\nGartner defines Business process management (BPM) as:\n\n\"the discipline of managing processes (rather than tasks) as the means for improving business performance outcomes and operational agility. Processes span organizational boundaries, linking together people, information flows, systems and other assets to create and deliver value to customers and constituents.\"[10]\nIt is common to confuse BPM with a BPM Suite (BPMS). BPM is a professional discipline done by people, while a BPMS is a technological suite of tool designed to help the BPM professional accomplish their goals. BPM should also not be confused with an application or solution that was developed to support a particular process. Suites and solutions represent ways of automating business processes, but automation is only one aspect of BPM.\n\nChanges in business process management\nThe concept of business process may be as traditional as concepts of tasks, department, production, and outputs, arising from job shop scheduling problems in the early 20th Century.[11] The management and improvement approach as of 2010, with formal definitions and technical modeling, has been around since the early 1990s (see business process modeling). Note that the term \"business process\" is sometimes used by IT practitioners as synonymous with the management of middleware processes or with integrating application software tasks.[citation needed]\n\nAlthough BPM initially focused on the automation of business processes with the use of information technology, it has since been extended[by whom?] to integrate human-driven processes in which human interaction takes place in series or parallel with the use of technology. For example, workflow management systems can assign individual steps requiring deploying human intuition or judgment to relevant humans and other tasks in a workflow to a relevant automated system.[12]\n\nMore recent variations such as \"human interaction management\"[13][14] are concerned with the interaction between human workers performing a task.[citation needed]\n\nAs of 2010 technology has allowed the coupling of BPM with other methodologies, such as Six Sigma.[citation needed] Some BPM tools such as SIPOCs, process flows, RACIs, CTQs and histograms allow users to:\n\nvisualize - functions and processes\nmeasure - determine the appropriate measure to determine success\nanalyze - compare the various simulations to determine an optimal improvement\nimprove - select and implement the improvement\ncontrol - deploy this implementation and by use of user-defined dashboards monitor the improvement in real time and feed the performance information back into the simulation model in preparation for the next improvement iteration\nre-engineer - revamp the processes from scratch for better results\nThis brings with it the benefit of being able to simulate changes to business processes based on real-world data (not just on assumed knowledge). Also, the coupling of BPM to industry methodologies allows users to continually streamline and optimize the process to ensure that it is tuned to its market need.[15][full citation needed]\n\nAs of 2012 research on BPM has paid increasing attention to the compliance of business processes. Although a key aspect of business processes is flexibility, as business processes continuously need to adapt to changes in the environment, compliance with business strategy, policies and government regulations should also be ensured.[16] The compliance aspect in BPM is highly important for governmental organizations. As of 2010 BPM approaches in a governmental context largely focus on operational processes and knowledge representation.[17] Although there have been many technical studies on operational business processes in both the public and private sectors, researchers have rarely taken legal compliance activities into account, for instance the legal implementation processes in public-administration bodies.[citation needed]\n\nBPM life-cycle\nBusiness process management activities can be arbitrarily grouped into categories such as design, modeling, execution, monitoring, and optimization.[18]\n\nBusiness Process Management Life-Cycle.svg\nDesign\nProcess design encompasses both the identification of existing processes and the design of \"to-be\" processes. Areas of focus include representation of the process flow, the factors within it, alerts and notifications, escalations, standard operating procedures, service level agreements, and task hand-over mechanisms.\n\nWhether or not existing processes are considered, the aim of this step is to ensure that a correct and efficient theoretical design is prepared.\n\nThe proposed improvement could be in human-to-human, human-to-system or system-to-system workflows, and might target regulatory, market, or competitive challenges faced by the businesses.\n\nThe existing process and the design of new process for various applications will have to synchronise and not cause major outage or process interruption.\n\nModeling\nModeling takes the theoretical design and introduces combinations of variables (e.g., changes in rent or materials costs, which determine how the process might operate under different circumstances).\n\nIt may also involve running \"what-if analysis\"(Conditions-when, if, else) on the processes: \"What if I have 75% of resources to do the same task?\" \"What if I want to do the same job for 80% of the current cost?\".\n\nExecution\n\nThis section possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (February 2015) (Learn how and when to remove this template message)\nOne of the ways to automate processes is to develop or purchase an application that executes the required steps of the process; however, in practice, these applications rarely execute all the steps of the process accurately or completely. Another approach is to use a combination of software and human intervention; however this approach is more complex, making the documentation process difficult.\n\nAs a response to these problems, software has been developed that enables the full business process (as developed in the process design activity) to be defined in a computer language which can be directly executed by the computer. The process models can be run through execution engines that automate the processes directly from the model (e.g. calculating a repayment plan for a loan) or, when a step is too complex to automate, Business Process Modeling Notation (BPMN) provides front-end capability for human input.[19] Compared to either of the previous approaches, directly executing a process definition can be more straightforward and therefore easier to improve. However, automating a process definition requires flexible and comprehensive infrastructure, which typically rules out implementing these systems in a legacy IT environment.\n\nBusiness rules have been used by systems to provide definitions for governing behavior, and a business rule engine can be used to drive process execution and resolution.\n\nMonitoring\nMonitoring encompasses the tracking of individual processes, so that information on their state can be easily seen, and statistics on the performance of one or more processes can be provided. An example of this tracking is being able to determine the state of a customer order (e.g. order arrived, awaiting delivery, invoice paid) so that problems in its operation can be identified and corrected.\n\nIn addition, this information can be used to work with customers and suppliers to improve their connected processes. Examples are the generation of measures on how quickly a customer order is processed or how many orders were processed in the last month. These measures tend to fit into three categories: cycle time, defect rate and productivity.\n\nThe degree of monitoring depends on what information the business wants to evaluate and analyze and how business wants it to be monitored, in real-time, near real-time or ad hoc. Here, business activity monitoring (BAM) extends and expands the monitoring tools generally provided by BPMS.\n\nProcess mining is a collection of methods and tools related to process monitoring. The aim of process mining is to analyze event logs extracted through process monitoring and to compare them with an a priori process model. Process mining allows process analysts to detect discrepancies between the actual process execution and the a priori model as well as to analyze bottlenecks.\n\nOptimization\nProcess optimization includes retrieving process performance information from modeling or monitoring phase; identifying the potential or actual bottlenecks and the potential opportunities for cost savings or other improvements; and then, applying those enhancements in the design of the process. Process mining tools are able to discover critical activities and bottlenecks, creating greater business value.[20]\n\nRe-engineering\nWhen the process becomes too complex or inefficient, and optimization is not fetching the desired output, it is usually recommended by a company steering committee chaired by the president / CEO to re-engineer the entire process cycle. Business process reengineering (BPR) has been used by organizations to attempt to achieve efficiency and productivity at work.\n\nBPM suites\nA market has developed for Enterprise software leveraging the Business Process Management concepts to organize and automate processes. The recent convergence of these software from distinct pieces such as Business rules engine, Business Process Modelling, Business Activity Monitoring and Human Workflow has given birth to integrated Business Process Management Suites. Forrester Research, Inc recognize the BPM suite space through three different lenses:\n\nhuman-centric BPM\nintegration-centric BPM (Enterprise Service Bus)\ndocument-centric BPM (Dynamic Case Management)\nHowever, standalone integration-centric and document-centric offerings have matured into separate, standalone markets.\n\nPractice\n\nExample of Business Process Management (BPM) Service Pattern: This pattern shows how business process management (BPM) tools can be used to implement business processes through the orchestration of activities between people and systems.[21]\nWhile the steps can be viewed as a cycle, economic or time constraints are likely to limit the process to only a few iterations. This is often the case when an organization uses the approach for short to medium term objectives rather than trying to transform the organizational culture. True iterations are only possible through the collaborative efforts of process participants. In a majority of organizations, complexity will require enabling technology (see below) to support the process participants in these daily process management challenges.\n\nTo date, many organizations often start a BPM project or program with the objective of optimizing an area that has been identified as an area for improvement.\n\nCurrently, the international standards for the task have limited BPM to the application in the IT sector, and ISO/IEC 15944 covers the operational aspects of the business. However, some corporations with the culture of best practices do use standard operating procedures to regulate their operational process.[22] Other standards are currently being worked upon to assist in BPM implementation (BPMN, Enterprise Architecture, Business Motivation Model).\n\nBPM technology\nBPM is now considered a critical component of operational intelligence (OI) solutions to deliver real-time, actionable information. This real-time information can be acted upon in a variety of ways - alerts can be sent or executive decisions can be made using real-time dashboards. OI solutions use real-time information to take automated action based on pre-defined rules so that security measures and or exception management processes can be initiated.\n\nAs such, some people view BPM as \"the bridge between Information Technology (IT) and Business.\"[citation needed]. In fact, an argument can be made that this \"holistic approach\" bridges organizational and technological silos.\n\nThere are four critical components of a BPM Suite:\n\nProcess engine  a robust platform for modeling and executing process-based applications, including business rules\nBusiness analytics  enable managers to identify business issues, trends, and opportunities with reports and dashboards and react accordingly\nContent management  provides a system for storing and securing electronic documents, images, and other files\nCollaboration tools  remove intra- and interdepartmental communication barriers through discussion forums, dynamic workspaces, and message boards\nBPM also addresses many of the critical IT issues underpinning these business drivers, including:\n\nManaging end-to-end, customer-facing processes\nConsolidating data and increasing visibility into and access to associated data and information\nIncreasing the flexibility and functionality of current infrastructure and data\nIntegrating with existing systems and leveraging service oriented architecture (SOA)\nEstablishing a common language for business-IT alignment\nValidation of BPMS is another technical issue that vendors and users need to be aware of, if regulatory compliance is mandatory.[23] The validation task could be performed either by an authenticated third party or by the users themselves. Either way, validation documentation will need to be generated. The validation document usually can either be published officially or retained by users.\n\nCloud computing BPM\nCloud computing business process management is the use of (BPM) tools that are delivered as software services (SaaS) over a network. Cloud BPM business logic is deployed on an application server and the business data resides in cloud storage.\n\nMarket\nAccording to Gartner, 20% of all the \"shadow business processes\" will be supported by BPM cloud platforms[citation needed]. Gartner refers to all the hidden organizational processes that are supported by IT departments as part of legacy business processes such as Excel spreadsheets, routing of emails using rules, phone calls routing, etc. These can, of course also be replaced by other technologies such as workflow software.\n\nBenefits\nThe benefits of using cloud BPM services include removing the need and cost of maintaining specialized technical skill sets in-house and reducing distractions from an enterprise's main focus. It offers controlled IT budgeting and enables geographical mobility.\n\nThe details of this are still emerging.[24][full citation needed]\n\nInternet of Things\nThe emerging Internet of Things poses a significant challenge to control and manage the flow of information through large numbers of devices. To cope with this, a new direction known as BPM Everywhere shows promise as way of blending traditional process techniques, with additional capabilities to automate the handling of all the independent devices.\n\nSee also\nApplication service provider\nBPM Everywhere\nBusiness process modeling\nBusiness activity monitoring\nBusiness intelligence\nBusiness-oriented architecture\nBusiness process automation\nBusiness process reengineering\nComparison of business integration software\nEnterprise planning systems\nEnterprise software\nITIL\nManaged services\nWorkflow\n\nBusiness model\n\nA business model is an \"abstract representation of an organization, be it conceptual, textual, and/or graphical, of all core interrelated architectural, co-operational, and financial arrangements designed and developed by an organization presently and in the future, as well as all core products and/or services the organization offers, or will offer, based on these arrangements that are needed to achieve its strategic goals and objectives.\"[1] [2] This definition by Al-Debei, El-Haddadeh and Avison (2008) indicates that value proposition, value architecture (the organizational infrastructure and technological architecture that allows the movement of products, services, and information), value finance (modeling information related to total cost of ownership, pricing methods, and revenue structure), and value network articulate the primary constructs or dimensions of business models.[3]\n\nA business model describes the rationale of how an organization creates, delivers, and captures value,[4] in economic, social, cultural or other contexts. The process of business model construction is part of business strategy.\n\nIn theory and practice, the term business model is used for a broad range of informal and formal descriptions to represent core aspects of a business, including purpose, business process, target customers, offerings, strategies, infrastructure, organizational structures, sourcing, trading practices, and operational processes and policies including culture. The literature has provided very diverse interpretations and definitions of a business model. A systematic review and analysis of manager responses to a survey defines business models as the design of organizational structures to enact a commercial opportunity.[5] Further extensions to this design logic emphasize the use of narrative or coherence in business model descriptions as mechanisms by which entrepreneurs create extraordinarily successful growth firms.[6]\n\nBusiness models are used to describe and classify businesses, especially in an entrepreneurial setting, but they are also used by managers inside companies to explore possibilities for future development. Well-known business models can operate as \"recipes\" for creative managers.[7] Business models are also referred to in some instances within the context of accounting for purposes of public reporting.\n\nContents  [hide] \n1\tHistory\n2\tTheoretical and empirical insights to business models\n2.1\tDesign logic and narrative coherence\n2.2\tComplementarities of business models between partnering firms\n3\tCategorization of business models\n3.1\tV4 BM framework\n3.2\tShift from pipes to platforms\n3.3\tPlatform business models\n4\tApplications\n5\tBusiness model design\n5.1\tDefinitions of business model\n5.1.1\tEconomic consideration\n5.1.2\tComponent consideration\n5.1.3\tStrategic outcome\n6\tDefinitions of business model design or development\n6.1\tDesign themes emphasis of business model\n6.2\tDesign content emphasis of business model design\n7\tExamples of business models\n8\tBusiness model frameworks\n9\tRelated concepts\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nHistory\nOver the years, business models have become much more sophisticated. The bait and hook business model (also referred to as the \"razor and blades business model\" or the \"tied products business model\") was introduced in the early 20th century. This involves offering a basic product at a very low cost, often at a loss (the \"bait\"), then charging compensatory recurring amounts for refills or associated products or services (the \"hook\"). Examples include: razor (bait) and blades (hook); cell phones (bait) and air time (hook); computer printers (bait) and ink cartridge refills (hook); and cameras (bait) and prints (hook). A variant of this model is Adobe, a software developer that gives away its document reader free of charge but charges several hundred dollars for its document writer.\n\nIn the 1950s, new business models came from McDonald's Restaurants and Toyota. In the 1960s, the innovators were Wal-Mart and Hypermarkets. The 1970s saw new business models from FedEx and Toys R Us; the 1980s from Blockbuster, Home Depot, Intel, and Dell Computer; the 1990s from Southwest Airlines, Netflix, eBay, Amazon.com, and Starbucks.\n\nToday, the type of business models might depend on how technology is used. For example, entrepreneurs on the internet have also created entirely new models that depend entirely on existing or emergent technology. Using technology, businesses can reach a large number of customers with minimal costs. In addition, the rise of outsourcing and globalization has meant that business models must also account for strategic sourcing, complex supply chains and moves to collaborative, relational contracting structures.[8]\n\nTheoretical and empirical insights to business models\nDesign logic and narrative coherence\nDesign logic views the business model as an outcome of creating new organizational structures or changing existing structures to pursue a new opportunity. Gerry George and Adam Bock (2011) conducted a comprehensive literature review and surveyed managers to understand how they perceived the components of a business model. In that analysis these authors show that there is a design logic behind how entrepreneurs and managers perceive and explain their business model. In further extensions to the design logic, George and Bock (2012) use case studies and the IBM survey data on business models in large companies, to describe how CEOs and entrepreneurs create narratives or stories in a coherent manner to move the business from one opportunity to another. They also show that when the narrative is incoherent or the components of the story are misaligned, that these businesses tend to fail. They recommend ways in which the entrepreneur or CEO can create strong narratives for change.\n\nComplementarities of business models between partnering firms\nStudying collaborative research and the accessing of external sources of technology, Hummel et al. (2010) found that in deciding on business partners, it is important to make sure that both parties' business models are complementary.[9] For example, they found that it was important to identify the value drivers of potential partners by analyzing their business models, and that it is beneficial to find partner firms that understand key aspects of our own firms business model.[10]\n\nThe University of Tennessee conducted research into highly collaborative business relationships. Researchers codified their research into a sourcing business model known as Vested (also referred to as Vested Outsourcing). Vested is a hybrid sourcing business model in which buyers and suppliers in an outsourcing or business relationship focus on shared values and goals to create an arrangement that is highly collaborative and mutually beneficial to each.[11]\n\nCategorization of business models\nFrom about 2012, some research and experimentation has theorized about a so-called \"liquid business model\".[12][13]\n\nV4 BM framework\nAl-Debei and Avison (2010) V4 BM Framework - four main dimensions encapsulating sixteen elements: Value Proposition, Value Architecture, Value Network, and Value Finance[3]\n\nValue Proposition: This dimension implies that a BM should include a description of the products/services a digital organization offers, or will offer, along with their related information. Furthermore, the BM needs also to describe the value elements incorporated within the offering, as well as the nature of targeted market segment(s) along with their preferences.\nValue Architecture: portrays the concept as a holistic structural design of an organization, including its technological architecture, organizational infrastructure, and their configurations.\nValue Network: depicts the cross-company or inter-organization perspective towards the concept and has gained much attention in the BM literature.\nValue Finance: depicts information related to costing, pricing methods, and revenue structure\nShift from pipes to platforms\nSangeet Paul Choudary (2013) distinguishes between two broad families of business models in an article in Wired magazine.[14] Choudary contrasts pipes (linear business models) with platforms (networked business models). In the case of pipes, firms create goods and services, push them out and sell them to customers. Value is produced upstream and consumed downstream. There is a linear flow, much like water flowing through a pipe. Unlike pipes, platforms do not just create and push stuff out. They allow users to create and consume value.\n\nIn an op-ed on MarketWatch,[15] Choudary, Van Alstyne and Parker further explain how business models are moving from pipes to platforms, leading to disruption of entire industries.\n\nPlatform business models\nThere are three elements to a successful platform business model.[16] The Toolbox creates connection by making it easy for others to plug into the platform. This infrastructure enables interactions between participants. The Magnet creates pull that attracts participants to the platform. For transaction platforms, both producers and consumers must be present to achieve critical mass. The Matchmaker fosters the flow of value by making connections between producers and consumers. Data is at the heart of successful matchmaking, and distinguishes platforms from other business models.\n\nChen (2009) stated that the business model has to take into account the capabilities of Web 2.0, such as collective intelligence, network effects, user-generated content, and the possibility of self-improving systems. He suggested that the service industry such as the airline, traffic, transportation, hotel, restaurant, information and communications technology and online gaming industries will be able to benefit in adopting business models that take into account the characteristics of Web 2.0. He also emphasized that Business Model 2.0 has to take into account not just the technology effect of Web 2.0 but also the networking effect. He gave the example of the success story of Amazon in making huge revenues each year by developing an open platform that supports a community of companies that re-use Amazon's on-demand commerce services.[17][need quotation to verify]\n\nApplications\nMalone et al.[18] found that some business models, as defined by them, indeed performed better than others in a dataset consisting of the largest U.S. firms, in the period 1998 through 2002, while they did not prove whether the existence of a business model mattered.\n\nIn the context of the Software-Cluster, which is funded by the German Federal Ministry of Education and Research, a business model wizard for software companies has been developed. It supports the design and analysis of software business models. The tool's underlying concept and data were published in various scientific publications.\n\nThe concept of a business model has been incorporated into certain accounting standards. For example, the International Accounting Standards Board (IASB) utilizes an \"entity's business model for managing the financial assets\" as a criterion for determining whether such assets should be measured at amortized cost or at fair value in its financial instruments accounting standard, IFRS 9.[19][20][21][22] In their 2013 proposal for accounting for financial instruments, the Financial Accounting Standards Board also proposed a similar use of business model for classifying financial instruments.[23] The concept of business model has also been introduced into the accounting of deferred taxes under International Financial Reporting Standards with 2010 amendments to IAS 12 addressing deferred taxes related to investment property.[24][25][26]\n\nBoth IASB and FASB have proposed using the concept of business model in the context of reporting a lessor's lease income and lease expense within their joint project on accounting for leases.[27][28][29][30][31] In its 2016 lease accounting model, IFRS 16, the IASB chose not to include a criterion of \"stand alone utility\" in its lease definition because \"entities might reach different conclusions for contracts that contain the same rights of use, depending on differences between customers' resources or suppliers' business models.\"[32] The concept has also been proposed as an approach for determining the measurement and classification when accounting for insurance contracts.[33][34] As a result of the increasing prominence the concept of business model has received in the context of financial reporting, the European Financial Reporting Advisory Group (EFRAG), which advises the European Union on endorsement of financial reporting standards, commenced a project on the \"Role of the Business Model in Financial Reporting\" in 2011.[35]\n\nBusiness model design\nBusiness model design refers to the activity of designing a company's business model. It is part of the business development and business strategy process and involves design methods.\n\nDefinitions of business model\nAl-Debei and Avison (2010) define a business model as an abstract representation of an organization. This may be conceptual, textual, and/or graphical, of all core interrelated architectural, co-operational, and financial arrangements designed and developed by an organization presently and in the future, as well all core products and/or services the organization offers, or will offer, based on these arrangements that are needed to achieve its strategic goals and objectives.[1] This definition indicates that value proposition, value architecture, value finance, and value network articulate the primary constructs or dimensions of business models.[3]\n\nEconomic consideration\nAl-Debei and Avison (2010) consider value finance as one of the main dimensions of BM which depicts information related to costing, pricing methods, and revenue structure. Stewart and Zhao (2000) defined the business model as a statement of how a firm will make money and sustain its profit stream over time. [36]\n\nComponent consideration\nOsterwalder et al. (2005) consider the Business Model as the blueprint of how a company does business.[37] Slywotzky (1996) regards the business model as the totality of how a company selects its customers, defines and differentiates it offerings, defines the tasks it will perform itself and those it will outsource, configures its resources, goes to market, creates utility for customers and captures profits. [38]\n\nStrategic outcome\nMayo and Brown (1999) considered the business model as the design of key interdependent systems that create and sustain a competitive business. [39]\n\nDefinitions of business model design or development\nZott and Amit (2009) consider business model design from the perspectives of design themes and design content. Design themes refer to the systems dominant value creation drivers and design content examines in greater detail the activities to be performed, the linking and sequencing of the activities and who will perform the activities.[40]\n\nDesign themes emphasis of business model\n\nEnvironment-Strategy-Structure-Operations (ESSO) Business Model Development\nDeveloping a Framework for Business Model Development with an emphasis on Design Themes, Lim (2010) proposed the Environment-Strategy-Structure-Operations (ESSO) Business Model Development which takes into consideration the alignment of the organizations strategy with the organization's structure, operations, and the environmental factors in achieving competitive advantage in varying combination of cost, quality, time, flexibility, innovation and affective.[41]\n\nDesign content emphasis of business model design\nBusiness model design includes the modeling and description of a company's:\n\nvalue propositions\ntarget customer segments\ndistribution channels\ncustomer relationships\nvalue configurations\ncore capabilities\npartner network\ncost structure\nrevenue model\nBusiness model design is distinct from business modeling. The former refers to defining the business logic of a company at the strategic level, whereas the latter refers to business process design at the operational level.\n\nA business model design template can facilitate the process of designing and describing a company's business model.\n\nDaas et al. (2012) developed a decision support system (DSS) for business model design. In their study a decision support system (DSS) is developed to help SaaS in this process, based on a design approach consisting of a design process that is guided by various design methods.[42]\n\nExamples of business models\nIn the early history of business models it was very typical to define business model types such as bricks-and-mortar or e-broker. However, these types usually describe only one aspect of the business (most often the revenue model). Therefore, more recent literature on business models concentrate on describing a business model as a whole, instead of only the most visible aspects.\n\nThe following examples provide an overview for various business model types that have been in discussion since the invention of term business model:\n\nBricks and clicks business model\nBusiness model by which a company integrates both offline (bricks) and online (clicks) presences. One example of the bricks-and-clicks model is when a chain of stores allows the user to order products online, but lets them pick up their order at a local store.\nCollective business models\nBusiness system, organization or association typically composed of relatively large numbers of businesses, tradespersons or professionals in the same or related fields of endeavor, which pools resources, shares information or provides other benefits for their members. For example, a science park or high-tech campus provides shared resources (e.g. cleanrooms and other lab facilities) to the firms located on its premises, and in addition seeks to create an innovation community among these firms and their employees.[43]\nCutting out the middleman model\nThe removal of intermediaries in a supply chain: \"cutting out the middleman\". Instead of going through traditional distribution channels, which had some type of intermediate (such as a distributor, wholesaler, broker, or agent), companies may now deal with every customer directly, for example via the Internet.\nDirect sales model\nDirect selling is marketing and selling products to consumers directly, away from a fixed retail location. Sales are typically made through party plan, one-to-one demonstrations, and other personal contact arrangements. A text book definition is: \"The direct personal presentation, demonstration, and sale of products and services to consumers, usually in their homes or at their jobs.\"[44]\nDistribution business models, various\nValue-added reseller model\nValue Added Reseller is a model where a business makes something which is resold by other businesses but with modifications which add value to the original product or service. These modifications or additions are mostly industry specific in nature and are essential for the distribution. Businesses going for a VAR model have to develop a VAR network. It is one of the latest collaborative business models which can help in faster development cycles and is adopted by many Technology companies especially software.\nFee in, free out\nBusiness model which works by charging the first client a fee for a service, while offering that service free of charge to subsequent clients.\nFranchise\nFranchising is the practice of using another firm's successful business model. For the franchisor, the franchise is an alternative to building 'chain stores' to distribute goods and avoid investment and liability over a chain. The franchisor's success is the success of the franchisees. The franchisee is said to have a greater incentive than a direct employee because he or she has a direct stake in the business.\n Sourcing business model\n\nA Sourcing Business Model is a type of business model that is applied specifically to business relationships where more than one party needs to work with another party to be successful. It is the combination of two concepts: the contractual relationship framework a company uses with its supplier (transactional, relational, investment based), and the economic model used (transactional, output or outcome-based).\nFreemium business model\nBusiness model that works by offering basic Web services, or a basic downloadable digital product, for free, while charging a premium for advanced or special features.[45]\nPay what you can (PWYC) is a non-profit or for-profit business model which does not depend on set prices for its goods, but instead asks customers to pay what they feel the product or service is worth to them.[46][47][48] It is often used as a promotional tactic,[49] but can also be the regular method of doing business. It is a variation on the gift economy and cross-subsidization, in that it depends on reciprocity and trust to succeed.\n\"Pay what you want\" (PWYW) is sometimes used synonymously, but \"pay what you can\" is often more oriented to charity or socially oriented uses, based more on ability to pay, while \"pay what you want\" is often more broadly oriented to perceived value in combination with willingness and ability to pay.\n\nOther examples of business models are:\n\nAuction business model\nAll-in-one business model\nChemical Leasing\nLow-cost carrier business model\nLoyalty business models\nMonopolistic business model\nMulti-level marketing business model\nNetwork effects business model\nOnline auction business model\nOnline content business model\nOnline media cooperative\nPremium business model\nProfessional open-source model\nPyramid scheme business model\nRazor and blades business model\nServitization of products business model\nSubscription business model\nBusiness model frameworks\nTechnology centric communities have defined \"frameworks\" for business modeling. These frameworks attempt to define a rigorous approach to defining business value streams. It is not clear, however, to what extent such frameworks are actually important for business planning. Business model frameworks represent the core aspect of any company; they involve the totality of how a company selects its customers defines and differentiates its offerings, defines the tasks it will perform itself and those it will outsource, configures its resource, goes to market, creates utility for customers, and captures profits.[50] A business framework involves internal factors (market analysis; products/services promotion; development of trust; social influence and knowledge sharing) and external factors (competitors and technological aspects).[51] A state of the art review on business model frameworks can be found in Krumeich et al. (2012).[52] In the following some frameworks are introduced.\n\nBusiness reference model\nBusiness reference model is a reference model, concentrating on the architectural aspects of the core business of an enterprise, service organization or government agency.\nComponent business model\nTechnique developed by IBM to model and analyze an enterprise. It is a logical representation or map of business components or \"building blocks\" and can be depicted on a single page. It can be used to analyze the alignment of enterprise strategy with the organization's capabilities and investments, identify redundant or overlapping business capabilities, etc.\n\nAlthough Webvan failed in its goal of disintermediating the North American supermarket industry, several supermarket chains (like Safeway Inc.) have launched their own delivery services to target the niche market to which Webvan catered.\nIndustrialization of services business model\nBusiness model used in strategic management and services marketing that treats service provision as an industrial process, subject to industrial optimization procedures\nBusiness Model Canvas\nDeveloped by A. Osterwalder, Yves Pigneur, Alan Smith, and 470 practitioners from 45 countries, the business model canvas [4][53] is one of the most used frameworks for describing the elements of business models.\nRelated concepts\nThe process of business model design is part of business strategy. Business model design and innovation refer to the way a firm (or a network of firms) defines its business logic at the strategic level.\n\nIn contrast, firms implement their business model at the operational level, through their business operations. This refers to their process-level activities, capabilities, functions and infrastructure (for example, their business processes and business process modeling), their organisational structures (e.g. organigrams, workflows, human resources) and systems (e.g. information technology architecture, production lines).\n\nConsequently, an operationally viable and feasible business model requires lateral alignment with the underlining business operations.[54]\n\nThe brand is a consequence of the business model and has a symbiotic relationship with it, because the business model determines the brand promise, and the brand equity becomes a feature of the model. Managing this is a task of integrated marketing.\n\nThe standard terminology and examples of business models do not apply to most nonprofit organizations, since their sources of income are generally not the same as the beneficiaries. The term funding model is generally used instead.[55]\n\nThe model is defined by the organizations vision, mission, and values, as well as sets of boundaries for the organizationwhat products or services it will deliver, what customers or markets it will target, and what supply and delivery channels it will use. While the business model includes high-level strategies and tactical direction for how the organization will implement the model, it also includes the annual goals that set the specific steps the organization intends to undertake in the next year and the measures for their expected accomplishment. Each of these is likely to be part of internal documentation that is available to the internal auditor.\n\nSee also\n\tWikimedia Commons has media related to Business models.\nBusiness plan\nBusiness process modeling\nBusiness reference model\nBusiness rule\nCompetitive advantage\nCore competency\nGrowth platforms\nMarket forms\nMarketing\nMarketing plan\nStrategic management\nStrategy Markup Language\nStrategic planning\nStrategy dynamics\nValue migration\nThe Design of Business\nEnterprise Architecture\nBusiness Model Canvas\nComponent business model\n\nCorporate communication is a set of activities involved in managing and orchestrating all internal and external communications aimed at creating favourable point of view among stakeholders on which the company depends.[1] It is the messages issued by a corporate organization, body, or institute to its audiences, such as employees, media, channel partners and the general public. Organizations aim to communicate the same message to all its stakeholders, to transmit coherence, credibility and ethic. Corporate Communications help organizations explain their mission, combine its many visions and values into a cohesive message to stakeholders. The concept of corporate communication could be seen as an integrative communication structure linking stakeholders to the organization.\n\nContents\n\n    1 Methods and tactics\n    2 Components\n        2.1 Corporate branding\n        2.2 Corporate and organizational identity\n        2.3 Corporate responsibility\n        2.4 Corporate reputation\n        2.5 Crisis communications\n        2.6 Internal/employee communications\n        2.7 Investor relations\n        2.8 Public relations: issues management and media relations\n            2.8.1 Issues management\n            2.8.2 Media relations\n            2.8.3 Company/spokesperson profiling\n    3 References\n\nMethods and tactics\n\nThree principal clusters of task-planning and communication form the backbone of business and the activity of business organizations. These include management communication, marketing communication, and organizational communication.\n\n    Management communication takes place between management and its internal and external audiences. To support management communication, organizations rely heavily on specialists in marketing communication and organizational communication.[citation needed]\n    Marketing communication gets the bulk of the budgets in most organizations, and consists of product advertising, direct mail, personal selling, and sponsorship activities.\n    Organizational communication consist of specialists in public relations, public affairs, investor relations, environmental communications, corporate advertising, and employee communication.\n\nThe responsibilities of corporate communication are:\n\n    to promote the profile of the \"company behind the brand\" (corporate branding)\n    to minimize discrepancies between the company's desired identity and brand features\n    to delegate tasks in communication\n    to formulate and execute effective procedures to make decisions on communication matters\n    to mobilize internal and external support for corporate objectives\n    to coordinate with international business firms\n\nA Conference Board Study of hundreds of the US’s largest firms showed that close to 80 percent have corporate communication functions that include media relations, speech writing, employee communication, corporate advertising, and community relations.[2] The public is often represented by self-appointed activist non-governmental organizations (NGOs) who identify themselves with a particular issue.\n\nMost companies have specialized groups of professionals for communicating with different audiences, such as internal communication, marketing communication, investor relations, government relations and public relations.[1]\nComponents\nCorporate branding\nMain article: Corporate branding\n\nA corporate brand is the perception of a company that unites a group of products or services for the public under a single name, a shared visual identity, and a common set of symbols. The process of corporate branding consists creating favourable associations and positive reputation with both internal and external stakeholders. The purpose of a corporate branding initiative is to generate a positive halo over the products and businesses of the company, imparting more favourable impressions of those products and businesses.\n\nIn more general terms, research suggests that corporate branding is an appropriate strategy for companies to implement when:\n\n    there is significant \"information asymmetry\" between a company and its clients;[3] That is to say customers are much less informed about a company's products than the company itself is;\n    customers perceive a high degree of risk in purchasing the products or services of the company;[4]\n    features of the company behind the brand would be relevant to the product or service a customer is considering purchasing.[5]\n\nCorporate and organizational identity\n\nThere are two approaches for identity:\n\n    Corporate identity is the reality and uniqueness of an organization, which is integrally related to its external and internal image and reputation through corporate communication[6]\n    Organizational identity comprises those characteristics of an organization that its members believe are central, distinctive and enduring. That is, organizational identity consists of those attributes that members feel are fundamental to (central) and uniquely descriptive of (distinctive) the organization and that persist within the organization over time (enduring)\".[7]\n\nFour types of identity can be distinguished:[8][9]\n\n    Perceived identity: The collection of attributes that are seen as typical for the ‘continuity, centrality and uniqueness’ of the organization in the eyes of its members.\n    Projected identity: The self presentations of the organization’s attributes manifested in the implicit and explicit signals which the organization broadcasts to internal and external target audiences through communication and symbols.\n    Desired identity (also called ‘ideal’ identity): The idealized picture that top managers hold of what the organization could evolve into under their leadership.\n    Applied identity: The signals that an organization broadcasts both consciously and unconsciously through behaviors and initiatives at all levels within the organization.\n\nCorporate responsibility\nMain article: Corporate social responsibility\n\nCorporate responsibility (often referred to as corporate social responsibility), corporate citizenship, sustainability, and even conscious capitalism are some of the terms bandied about the news media and corporate marketing efforts as companies jockey to win the trust and loyalty of constituents. Corporate responsibility (CR) constitutes an organization’s respect for society’s interests, demonstrated by taking ownership of the effects its activities have on key constituencies including customers, employees, shareholders, communities, and the environment, in all parts of their operations. In short, CR prompts a corporation to look beyond its traditional bottom line, to the social implications of its business.[10]\nCorporate reputation\nMain article: Reputation management\n\nReputations are overall assessments of organizations by their stakeholders. They are aggregate perceptions by stakeholders of an organization's ability to fulfill their expectations, whether these stakeholders are interested in buying the company's products, working for the company, or investing in the company's shares.[11]\n\nIn 2000, the US-based Council of PR Firms identified seven programs developed by either media organizations or market research firms, used by companies to assess or benchmark their corporate reputations. Of these, only four are conducted regularly and have broad visibility:\n\n    \"America's Most Admired Companies\" by Fortune Magazine;\n    The \"Brand Asset Valuator\" by Young & Rubicam;\n    \"RepTrak\" by Reputation Institute \n    .\n    \"Best Global Brands\" by Interbrand.\n\nCrisis communications\nMain article: Crisis communications\n\nCrisis communication is sometimes considered a sub-specialty of the public relations profession that is designed to protect and defend an individual, company, or organization facing a public challenge to its reputation. These challenges may come in the form of an investigation from a government agency, a criminal allegation, a media inquiry, a shareholders lawsuit, a violation of environmental regulations, or any of a number of other scenarios involving the legal, ethical, or financial standing of the entity. The crisis for organizations can be defined as follows:[10]\n\n    A crisis is a major catastrophe that may occur either naturally or as a result of human error, intervention, or even malicious intent. It can include tangible devastation, such as the destruction of lives or assets, or intangible devastation, such as the loss of an organization's credibility or other reputational damage. The latter outcomes may be the result of management's response to tangible devastation or the result of human error. A crisis usually has significant actual or potential financial impact on a company, and it usually affects multiple constituencies in more than one market.\n\nInternal/employee communications\nMain article: Internal communications\n\nAs the extent of communication grows, many companies create an employee relations (ER) function with dedicated staff to manage the numerous media through which senior managers can communicate among themselves and with the rest of the organization. Internal communication in the 21st century is more than the memos, publications, and broadcasts that comprise it; it’s about building a corporate culture on values that drive organizational excellence. ER specialists are generally expected to fulfill one or more of the following four roles:[12]\n\n    Efficiency: Internal communication is used primarily to disseminate information about corporate activities.\n    Shared meaning: Internal communication is used to build a shared understanding among employees about corporate goals.\n    Connectivity: Internal communication is used mainly to clarify the connectedness of the company's people and activities.\n    Satisfaction: Internal communication is used to improve job satisfaction throughout the company.\n\nInvestor relations\nMain article: Investor relations\n\nThe investor relations (IR) function is used by companies which publicly trade shares on a stock exchange. In such companies, the purpose of the IR specialist is to interface with current and potential financial stakeholders-namely retail investors, institutional investors, and financial analysts.\n\nThe role of investor relations is to fulfill three principal functions:\n\n    comply with regulations;\n    Create a favorable relationship with key financial audiences;\n    contribute to building and maintaining the company's image and reputation.\n\nPublic relations: issues management and media relations\nMain article: Public relations\n\nThe role of the public relations specialist, in many ways, is to communicate with the general public in ways that serve the interests of the company. PR therefore consists of numerous specialty areas that convey information about the company to the public, including sponsorships, events, issues management and media relations. When executing these types of activities, the PR Specialist must incorporate broader corporate messages to convey the company’s strategic positioning. This ensures the PR activities ultimately convey messages that distinguish the company vis-à-vis its competitors and the overall marketplace, while also communicating the company’s value to target audiences.\nIssues management\n\nA key role of the PR specialist is to make the company better known for traits and attributes that build the company’s perceived distinctiveness and competitiveness with the public. In recent years, PR specialists have become increasingly involved in helping companies manage strategic issues – public concerns about their activities that are frequently magnified by special interest groups and NGOs. The role of the PR specialist therefore also consists of issues management, namely the “set of organizational procedures, routines, personnel, and issues”.[13] A strategic issue is one that compels a company to deal with it because there is “ a conflict between two or more identifiable groups over procedural or substantive matters relating to the distribution of positions or resources”.[14]\nMedia relations\n\nTo build better relationships with the media, organizations must cultivate positive relations with influential members of the media. This task might be handled by employees within the company’s media relations department or handled by a public relations firm.\nCompany/spokesperson profiling\n\nThese \"public faces\" are considered authorities in their respective sector/field and ensure the company/organization is in the limelight.\n\n    Managing content of corporate websites and/or other external touch points\n    Managing corporate publications - for the external world\n    Managing print media\n\n\nA job interview is a one-on-one interview consisting of a conversation between a job applicant and a representative of an employer which is conducted to assess whether the applicant should be hired.[1] Interviews are one of the most popularly used devices for employee selection.[2] Interviews vary in the extent to which the questions are structured, from a totally unstructured and free-wheeling conversation, to a structured interview in which an applicant is asked a predetermined list of questions in a specified order; [3] structured interviews are usually more accurate predictors of which applicants will make good employees, according to research studies.[4]\nA pie chart showing the reasons why job interview candidates are rejected.\n\nA job interview typically precedes the hiring decision. The interview is usually preceded by the evaluation of submitted résumés from interested candidates, possibly by examining job applications or reading many resumes. Next, after this screening, a small number of candidates for interviews is selected.\n\nPotential job interview opportunities also include networking events and career fairs. The job interview is considered one of the most useful tools for evaluating potential employees.[5] It also demands significant resources from the employer, yet has been demonstrated to be notoriously unreliable in identifying the optimal person for the job.[5] An interview also allows the candidate to assess the corporate culture and demands of the job.\n\nMultiple rounds of job interviews and/or other candidate selection methods may be used where there are many candidates or the job is particularly challenging or desirable. Earlier rounds sometimes called 'screening interviews' may involve fewer staff from the employers and will typically be much shorter and less in-depth. An increasingly common initial interview approach is the telephone interview. This is especially common when the candidates do not live near the employer and has the advantage of keeping costs low for both sides. Since 2003, interviews have been held through video conferencing software, such as Skype.[6] Once all candidates have been interviewed, the employer typically selects the most desirable candidate(s) and begins the negotiation of a job offer.\n\nContents\n\n    1 Interview constructs\n    2 Interview process\n    3 Types of interview\n        3.1 Structured interview\n            3.1.1 Situational interview questions\n            3.1.2 Behavioral interview questions\n            3.1.3 Other types of interview questions\n        3.2 Specialized interview formats\n            3.2.1 Case\n            3.2.2 Panel\n            3.2.3 Stress\n            3.2.4 Technical\n        3.3 Other interview modes\n            3.3.1 Telephone\n            3.3.2 Video\n    4 Interviewee strategies and behaviors\n        4.1 Nonverbal behaviors\n        4.2 Physical attractiveness\n        4.3 Coaching\n        4.4 Faking\n        4.5 Narcissism\n        4.6 Psychopathy\n    5 Factors impacting on interview effectiveness\n        5.1 Validity and predictive power\n            5.1.1 Interview structure issues\n            5.1.2 Interviewer rating reliability\n        5.2 Applicant reactions\n            5.2.1 Interview design\n            5.2.2 Types of questions\n            5.2.3 Additional factors\n        5.3 Interview anxiety\n            5.3.1 Implications for applicants\n            5.3.2 Implications for organizations\n    6 Legal issues\n        6.1 Applicants with disabilities\n        6.2 Other applicant discrimination: Weight and pregnancy\n    7 See also\n    8 References\n    9 External links\n\nInterview constructs\n\nResearchers have attempted to identify which interview strategies or \"constructs\" can help interviewers choose the best candidate. Research suggests that interviews capture a wide variety of applicant attributes.[7][8][9] Constructs can be classified into three categories: job-relevant content, interviewee performance (behavior unrelated to the job but which influences the evaluation), and job-irrelevant interviewer biases.[10]\n\nJob-relevant interview content Interview questions are generally designed to tap applicant attributes that are specifically relevant to the job for which the person is applying. The job-relevant applicant attributes that the questions purportedly assess are thought to be necessary for one to successfully perform on the job. The job-relevant constructs that have been assessed in the interview can be classified into three categories: general traits, experiential factors, and core job elements. The first category refers to relatively stable applicant traits. The second category refers to job knowledge that the applicant has acquired over time. The third category refers to the knowledge, skills, and abilities associated with the job.\n\nGeneral traits:\n\n    Mental ability: Applicants' capacity to learn and process information[8]\n    Personality: Conscientiousness, agreeableness, emotional stability, extroversion, openness to new experiences[7][8][9]\n    Interest, goals, and values: Applicant motives, goals, and person-organization fit[8]\n\nExperiential factors:\n\n    Experience: Job-relevant knowledge derived from prior experience[8][9]\n    Education: Job-relevant knowledge derived from prior education\n    Training: Job-relevant knowledge derived from prior training\n\nCore job elements:\n\n    Declarative knowledge: Applicants' learned knowledge[9]\n    Procedural skills and abilities: Applicants' ability to complete the tasks required to do the job[11]\n    Motivation: Applicants' willingness to exert the effort required to do the job[12]\n\nInterviewee performance Interviewer evaluations of applicant responses also tend to be colored by how an applicant behaves in the interview. These behaviors may not be directly related to the constructs the interview questions were designed to assess, but can be related to aspects of the job for which they are applying. Applicants without realizing it may engage in a number of behaviors that influence ratings of their performance. The applicant may have acquired these behaviors during training or from previous interview experience. These interviewee performance constructs can also be classified into three categories: social effectiveness skills, interpersonal presentation, and personal/contextual factors.\n\nSocial effectiveness skills:\n\n    Impression management: Applicants' attempt to make sure the interviewer forms a positive impression of them[13][14]\n    Social skills: Applicants' ability to adapt his/her behavior according to the demands of the situation to positively influence the interviewer[15]\n    Self-monitoring: Applicants' regulation of behaviors to control the image presented to the interviewer[16]\n    Relational control: Applicants' attempt to control the flow of the conversation[17]\n\nInterpersonal Presentation:\n\n    Verbal expression: Pitch, rate, pauses[18]\n    Nonverbal behavior: Gaze, smile, hand movement, body orientation[19]\n\nPersonal/contextual factors:\n\n    Interview training: Coaching, mock interviews with feedback[20]\n    Interview experience: Number of prior interviews[21]\n    Interview self-efficacy: Applicants' perceived ability to do well in the interview[22]\n    Interview motivation: Applicants' motivation to succeed in an interview[23]\n\nJob-irrelevant interviewer biases The following are personal and demographic characteristics that can potentially influence interviewer evaluations of interviewee responses. These factors are typically not relevant to whether the individual can do the job (that is, not related to job performance), thus, their influence on interview ratings should be minimized or excluded. In fact, there are laws in many countries that prohibit consideration of many of these protected classes of people when making selection decisions. Using structured interviews with multiple interviewers coupled with training may help reduce the effect of the following characteristics on interview ratings.[24] The list of job-irrelevant interviewer biases is presented below.\n\n    Attractiveness: Applicant physical attractiveness can influence interviewer's evaluation of one's interview performance[19]\n    Race: Whites tend to score higher than Blacks and Hispanics;[25] racial similarity between interviewer and applicant, on the other hand, has not been found to influence interview ratings[24][26]\n    Gender: Females tend to receive slightly higher interview scores than their male counterparts;[7] gender similarity does not seem to influence interview ratings[24]\n    Similarities in background and attitudes: Interviewers perceived interpersonal attraction was found to influence interview ratings[27]\n    Culture: Applicants with an ethnic name and a foreign accent were viewed less favorably than applicants with just an ethnic name and no accent or an applicant with a traditional name with or without an accent[28]\n\nThe extent to which ratings of interviewee performance reflect certain constructs varies widely depending on the level of structure of the interview, the kind of questions asked, interviewer or applicant biases, applicant professional dress or nonverbal behavior, and a host of other factors. For example, some research suggests that applicant's cognitive ability, education, training, and work experiences may be better captured in unstructured interviews, whereas applicant's job knowledge, organizational fit, interpersonal skills, and applied knowledge may be better captured in a structured interview.[8]\n\nFurther, interviews are typically designed to assess a number of constructs. Given the social nature of the interview, applicant responses to interview questions and interviewer evaluations of those responses are sometimes influenced by constructs beyond those the questions were intended to assess, making it extremely difficult to tease out the specific constructs measured during the interview.[29] Reducing the number of constructs the interview is intended to assess may help mitigate this issue. Moreover, of practical importance is whether the interview is a better measure of some constructs in comparison to paper and pencil tests of the same constructs. Indeed, certain constructs (mental ability and skills, experience) may be better measured with paper and pencil tests than during the interview, whereas personality-related constructs seem to be better measured during the interview in comparison to paper and pencil tests of the same personality constructs.[30] In sum, the following is recommended: Interviews should be developed to assess the job relevant constructs identified in the job analysis.[31][32]\nInterview process\nPeople waiting to be interviewed at an employment agency.\n\nOne way to think about the interview process is as three separate, albeit related, phases: (1) the preinterview phase which occurs before the interviewer and candidate meet, (2) the interview phase where the interview is conducted, and (3) the postinterview phase where the interviewer forms judgments of candidate qualifications and makes final decisions.[33] Although separate, these three phases are related. That is, impressions interviewers form early on may affect how they view the person in a later phase.\n\nPreinterview phase: The preinterview phase encompasses the information available to the interviewer beforehand (e.g., resumes, test scores, social networking site information) and the perceptions interviewers form about applicants from this information prior to the actual face-to-face interaction between the two individuals. In this phase, interviewers are likely to already have ideas about the characteristics that would make a person ideal or qualified for the position.[34] Interviewers also have information about the applicant usually in the form of a resume, test scores, or prior contacts with the applicant.[33] Interviewers then often integrate information that they have on an applicant with their ideas about the ideal employee to form a preinterview evaluation of the candidate. In this way, interviewers typically have an impression of you even before the actual face-to-face interview interaction. Nowadays with recent technological advancements, we must be aware that interviewers have an even larger amount of information available on some candidates. For example, interviewers can obtain information from search engines (e.g. Google, Bing, Yahoo), blogs, and even social networks (e.g. Linkedin, Facebook, Twitter). While some of this information may be job-related, some of it may not be. In some cases, a review of Facebook may reveal undesirable behaviors such as drunkenness or drug use. Despite the relevance of the information, any information interviewers obtain about the applicant before the interview is likely to influence their preinterview impression of the candidate. And, why is all this important? It is important because what interviewers think about you before they meet you, can have an effect on how they might treat you in the interview and what they remember about you.[33][35] Furthermore, researchers have found that what interviewers think about the applicant before the interview (preinterview phase) is related to how they evaluate the candidate after the interview, despite how the candidate may have performed during the interview.[36]\n\nInterview phase: The interview phase entails the actual conduct of the interview, the interaction between the interviewer and the applicant. Initial interviewer impressions about the applicant before the interview may influence the amount of time an interviewer spends in the interview with the applicant, the interviewer’s behavior and questioning of the applicant,[37] and the interviewer’s postinterview evaluations.[36] Preinterview impressions also can affect what the interviewer notices about the interviewee, recalls from the interview, and how an interviewer interprets what the applicant says and does in the interview.[35]\n\nAs interviews are typically conducted face-to-face, over the phone, or through video conferencing[38] (e.g. Skype), they are a social interaction between at least two individuals. Thus, the behavior of the interviewer during the interview likely \"leaks\" information to the interviewee. That is, you can sometimes tell during the interview whether the interviewer thinks positively or negatively about you.[33] Knowing this information can actually affect how the applicant behaves, resulting in a self-fulfilling prophecy effect.[37][39] For example, interviewees who feel the interviewer does not think they are qualified may be more anxious and feel they need to prove they are qualified. Such anxiety may hamper how well they actually perform and present themselves during the interview, fulfilling the original thoughts of the interviewer. Alternatively, interviewees who perceive an interviewer believes they are qualified for the job may feel more at ease and comfortable during the exchange, and consequently actually perform better in the interview. It should be noted again, that because of the dynamic nature of the interview, the interaction between the behaviors and thoughts of both parties is a continuous process whereby information is processed and informs subsequent behavior, thoughts, and evaluations.\n\nPostinterview phase: After the interview is conducted, the interviewer must form an evaluation of the interviewee’s qualifications for the position. The interviewer most likely takes into consideration all the information, even from the preinterview phase, and integrates it to form a postinterview evaluation of the applicant. In the final stage of the interview process, the interviewer uses his/her evaluation of the candidate (i.e., in the form of interview ratings or judgment) to make a final decision. Sometimes other selection tools (e.g., work samples, cognitive ability tests, personality tests) are used in combination with the interview to make final hiring decisions; however, interviews remain the most commonly used selection device in North America.[40]\n\nFor interviewees: Although the description of the interview process above focuses on the perspective of the interviewer, job applicants also gather information on the job and/or organization and form impressions prior to the interview.[34] The interview is a two-way exchange and applicants are also making decisions about whether the company is a good fit for them. Essentially, the process model illustrates that the interview is not an isolated interaction, but rather a complex process that begins with two parties forming judgments and gathering information, and ends with a final interviewer decision.\nTypes of interview\n\nInterviews may be 'structured', 'unstructured' and composite: i.e. comprising a combination of both types or interview approach. There are also a large range of more specialised interview approaches. Interviews typically take place face-to-face, though other modes such as telephone or video are increasingly used.\nStructured interview\n\nIn interviews that are considered \"structured interviews,\" there are several types of questions interviewers ask applicants. Two major types are situational questions[41] and behavioral questions (also known as patterned behavioral description interviews).[42] Both types of questions are based on \"critical incidents\" that are required to perform the job[43] but they differ in their focus (see below for descriptions). Critical incidents are relevant tasks that are required for the job and can be collected through interviews or surveys with current employees, managers, or subject matter experts[44][45] One of the first critical incidents techniques ever used in the United States Army asked combat veterans to report specific incidents of effective or ineffective behavior of a leader. The question posed to veterans was \"Describe the officer’s actions. What did he do?\" Their responses were compiled to create a factual definition or \"critical requirements\" of what an effective combat leader is.[43]\n\nPrevious meta-analyses have found mixed results for which type of question will best predict future job performance of an applicant. For example, some studies have shown that situational type questions have better predictability for job performance in interviews,[46][47][48] while, other researchers have found that behavioral type questions are better at predicting future job performance of applicants.[49] In actual interview settings it is not likely that the sole use of just one type of interview question (situational or behavioral) is asked. A range of questions can add variety for both the interviewer and applicant.[45] In addition, the use of high-quality questions, whether behavioral or situational based, is essential to make sure that candidates provide meaningful responses that lead to insight into their capability to perform on the job.[50]\nSituational interview questions\n\nSituational interview questions[41] ask job applicants to imagine a set of circumstances and then indicate how they would respond in that situation; hence, the questions are future oriented. One advantage of situational questions is that all interviewees respond to the same hypothetical situation rather than describe experiences unique to them from their past. Another advantage is that situational questions allow respondents who have had no direct job experience relevant to a particular question to provide a hypothetical response.[51] Two core aspects of the SI are the development of situational dilemmas that employees encounter on the job, and a scoring guide to evaluate responses to each dilemma.[52]\nBehavioral interview questions\n\nBehavioral (experience-based or patterned behavioral) interviews are past-oriented in that they ask respondents to relate what they did in past jobs or life situations that are relevant to the particular job relevant knowledge, skills, and abilities required for success.[53][54] The idea is that past behavior is the best predictor of future performance in similar situations. By asking questions about how job applicants have handled situations in the past that are similar to those they will face on the job, employers can gauge how they might perform in future situations.[51]\n\nBehavioral interview questions include:[55]\n\n    Describe a situation in which you were able to use persuasion to successfully convince someone to see things your way.\n    Give me an example of a time when you set a goal and were able to meet or achieve it.\n    Tell me about a time when you had to use your presentation skills to influence someone's opinion.\n    Give me an example of a time when you had to conform to a policy with which you did not agree.\n\nExamples include the STAR and SOARA techniques.\nOther types of interview questions\n\nOther possible types of questions that may be asked alongside structured interview questions or in a separate interview include: background questions, job experience questions, and puzzle type questions. A brief explanation of each follows.\n\n    Background questions include a focus on work experience, education, and other qualifications.[56] For instance, an interviewer may ask \"What experience have you had with direct sales phone calls?\"\n    Job experience questions may ask candidates to describe or demonstrate job knowledge. These are typically highly specific questions.[57] For example, one question may be \"What steps would you take to conduct a manager training session on safety?\"\n    The puzzle interview was popularized by Microsoft in the 1990s, and is now used in other organizations. The most common types of questions either ask the applicant to solve puzzles or brain teasers (e.g., \"Why are manhole covers round?\") or to solve unusual problems (e.g., \"How would you weigh an airplane without a scale?\").[58]\n\nSpecialized interview formats\nCase\nFurther information: Case interview\n\nA case interview is an interview form used mostly by management consulting firms and investment banks in which the job applicant is given a question, situation, problem or challenge and asked to resolve the situation. The case problem is often a business situation or a business case that the interviewer has worked on in real life. In recent years, company in other sectors like Design, Architecture, Marketing, Advertising, Finance and Strategy have adopted a similar approach to interviewing candidates. Technology has transformed the Case-based and Technical interview process from a purely private in-person experience to an online exchange of job skills and endorsements.\nPanel\n\nAnother type of job interview found throughout the professional and academic ranks is the panel interview. In this type of interview the candidate is interviewed by a group of panelists representing the various stakeholders in the hiring process. Within this format there are several approaches to conducting the interview. Example formats include;\n\n    Presentation format – The candidate is given a generic topic and asked to make a presentation to the panel. Often used in academic or sales-related interviews.\n    Role format – Each panelist is tasked with asking questions related to a specific role of the position. For example, one panelist may ask technical questions, another may ask management questions, another may ask customer service related questions etc.\n    Skeet shoot format – The candidate is given questions from a series of panelists in rapid succession to test his or her ability to handle stress filled situations.\n\nThe benefits of the panel approach to interviewing include: time savings over serial interviewing, more focused interviews as there is often less time spend building rapport with small talk, and \"apples to apples\" comparison because each stake holder/interviewer/panelist gets to hear the answers to the same questions.[59]\nStress\n\nStress interviews are still in common use. One type of stress interview is where the employer uses a succession of interviewers (one at a time or en masse) whose mission is to intimidate the candidate and keep him/her off-balance. The ostensible purpose of this interview: to find out how the candidate handles stress. Stress interviews might involve testing an applicant's behavior in a busy environment. Questions about handling work overload, dealing with multiple projects, and handling conflict are typical.[60]\n\nAnother type of stress interview may involve only a single interviewer who behaves in an uninterested or hostile manner. For example, the interviewer may not make eye contact, may roll his eyes or sigh at the candidate's answers, interrupt, turn his back, take phone calls during the interview, or ask questions in a demeaning or challenging style. The goal is to assess how the interviewee handles pressure or to purposely evoke emotional responses. This technique was also used in research protocols studying stress and type A (coronary-prone) behavior because it would evoke hostility and even changes in blood pressure and heart rate in study subjects. The key to success for the candidate is to de-personalize the process. The interviewer is acting a role, deliberately and calculatedly trying to \"rattle the cage\". Once the candidate realizes that there is nothing personal behind the interviewer's approach, it is easier to handle the questions with aplomb.\n\nExample stress interview questions:\n\n    Sticky situation: \"If you caught a colleague cheating on his expenses, what would you do?\"\n    Putting one on the spot: \"How do you feel this interview is going?\"\n    \"Popping the balloon\": (deep sigh) \"Well, if that's the best answer you can give ... \" (shakes head) \"Okay, what about this one ...?\"\n    Oddball question: \"What would you change about the design of the hockey stick?\"\n    Doubting one's veracity: \"I don't feel like we're getting to the heart of the matter here. Start again – tell me what really makes you tick.\"\n\nCandidates may also be asked to deliver a presentation as part of the selection process. One stress technique is to tell the applicant that they have 20 minutes to prepare a presentation, and then come back to room five minutes later and demand that the presentation be given immediately. The \"Platform Test\" method involves having the candidate make a presentation to both the selection panel and other candidates for the same job. This is obviously highly stressful and is therefore useful as a predictor of how the candidate will perform under similar circumstances on the job. Selection processes in academic, training, airline, legal and teaching circles frequently involve presentations of this sort.\nTechnical\nFurther information: Microsoft Interview\n\nThis kind of interview focuses on problem solving and creativity. The questions aim at the interviewee's problem-solving skills and likely show their ability in solving the challenges faced in the job through creativity. Technical interviews are being conducted online at progressive companies before in-person talks as a way to screen job applicants.\nOther interview modes\nTelephone\nMain article: Telephone interview\n\nTelephone interviews take place if a recruiter wishes to reduce the number of prospective candidates before deciding on a shortlist for face-to-face interviews. They also take place if a job applicant is a significant distance away from the premises of the hiring company, such as abroad or in another state or province.\nVideo\n\nVideo interviews are a modern variation of telephone interviews. Prospective candidates are asked preset questions using computer software then their immediate responses are recorded. These responses are then viewed and evaluated by recruiters to form a shortlist of suitable candidates for face-to-face interviews.\nInterviewee strategies and behaviors\n\nWhile preparing for an interview, prospective employees usually look at what the job posting or job description says in order to get a better understanding of what is expected of them should they get hired. Exceptionally good interviewees look at the wants and needs of a job posting and show off how good they are at those abilities during the interview to impress the interviewer and increase their chances of getting a job.\n\nResearching the company itself is also a good way for interviewees to impress lots of people during an interview. It shows the interviewer that the interviewee is not only knowledgeable about the company's goals and objectives, but also that the interviewee has done their homework and that they make a great effort when they are given an assignment. Researching about the company makes sure that employees are not entirely clueless about the company they are applying for, and at the end of the interview, the interviewee might ask some questions to the interviewer about the company, either to learn more information or to clarify on some points that they might have found during their research. In any case, it impresses the interviewer and it shows that the interviewee is willing to learn more about the company.\n\nMost interviewees also find that practising answering the most common questions asked in interviews helps them prepare for the real one. It minimizes the chance of their being caught off-guard regarding certain questions, prepares their minds to convey the right information in the hopes of impressing the interviewer, and also makes sure that they do not accidentally say something that might not be suitable in an interview situation. The use of head mounted displays such as Google Cardboard, a virtual reality platform in which the user is immersed in a variety of realistic environments, is a new tool used to practice answering interview questions.[61]\n\nInterviewees are generally dressed properly in business attire for the interview, so as to look professional in the eyes of the interviewer. They also bring their résumé, cover letter and references to the interview to supply the interviewer the information they need, and to also cover them in case they forgot to bring any of the papers. Items like cellphones, a cup of coffee and chewing gum are not recommended to bring to an interview, as it can lead to the interviewer perceiving the interviewee as unprofessional and in some cases, even rude.\n\nAbove all, interviewees should be confident and courteous to the interviewer, as they are taking their time off work to participate in the interview. An interview is often the first time an interviewer looks at the interviewee first hand, so it is important to make a good first impression.[62]\nNonverbal behaviors\n\nIt may not only be what you say in an interview that matters, but also how you say it (e.g., how fast you speak) and how you behave during the interview (e.g., hand gestures, eye contact). In other words, although applicants’ responses to interview questions influence interview ratings,[63] their nonverbal behaviors may also affect interviewer judgments.[64] Nonverbal behaviors can be divided into two main categories: vocal cues (e.g., articulation, pitch, fluency, frequency of pauses, speed, etc.) and visual cues (e.g., smiling, eye contact, body orientation and lean, hand movement, posture, etc.).[65] Oftentimes physical attractiveness is included as part of nonverbal behavior as well.[65] There is some debate about how large a role nonverbal behaviors may play in the interview. Some researchers maintain that nonverbal behaviors affect interview ratings a great deal,[63] while others have found that they have a relatively small impact on interview outcomes, especially when considered with applicant qualifications presented in résumés.[66] The relationship between nonverbal behavior and interview outcomes is also stronger in structured interviews than unstructured,[67] and stronger when interviewees’ answers are of high quality.[66]\n\nApplicants’ nonverbal behaviors may influence interview ratings through the inferences interviewers make about the applicant based on their behavior. For instance, applicants who engage in positive nonverbal behaviors such as smiling and leaning forward are perceived as more likable, trustworthy, credible,[65] warmer, successful, qualified, motivated, competent,[68] and social skills.[69] These applicants are also predicted to be better accepted and more satisfied with the organization if hired.[68]\n\nApplicants’ verbal responses and their nonverbal behavior may convey some of the same information about the applicant.[64] However, despite any shared information between content and nonverbal behavior, it is clear that nonverbal behaviors do predict interview ratings to an extent beyond the content of what was said, and thus it is essential that applicants and interviewers alike are aware of their impact. You may want to be careful of what you may be communicating through the nonverbal behaviors you display.\nPhysical attractiveness\n\nTo hire the best applicants for the job, interviewers form judgments, sometimes using applicants’ physical attractiveness. That is, physical attractiveness is usually not necessarily related to how well one can do the job, yet has been found to influence interviewer evaluations and judgments about how suitable an applicant is for the job. Once individuals are categorized as attractive or unattractive, interviewers may have expectations about physically attractive and physically unattractive individuals and then judge applicants based on how well they fit those expectations.[70] As a result, it typically turns out that interviewers will judge attractive individuals more favorably on job-related factors than they judge unattractive individuals. People generally agree on who is and who is not attractive and attractive individuals are judged and treated more positively than unattractive individuals.[71] For example, people who think another is physically attractive tend to have positive initial impressions of that person (even before formally meeting them), perceive the person to be smart, socially competent, and have good social skills and general mental health.[70]\n\nWithin the business domain, physically attractive individuals have been shown to have an advantage over unattractive individuals in numerous ways, that include, but are not limited to, perceived job qualifications, hiring recommendations, predicted job success, and compensation levels.[70] As noted by several researchers, attractiveness may not be the most influential determinant of personnel decisions, but may be a deciding factor when applicants possess similar levels of qualifications.[70] In addition, attractiveness does not provide an advantage if the applicants in the pool are of high quality, but it does provide an advantage in increased hiring rates and more positive job-related outcomes for attractive individuals when applicant quality is low and average.[72]\n\nVocal Attractiveness Just as physical attractiveness is a visual cue, vocal attractiveness is an auditory cue and can lead to differing interviewer evaluations in the interview as well. Vocal attractiveness, defined as an appealing mix of speech rate, loudness, pitch, and variability, has been found to be favorably related to interview ratings and job performance.[73][74] In addition, the personality traits of agreeableness and conscientiousness predict performance more strongly for people with more attractive voices compared to those with less attractive voices.[73]\n\nAs important as it is to understand how physical attractiveness can influence the judgments, behaviors, and final decisions of interviewers, it is equally important to find ways to decrease potential bias in the job interview. Conducting an interview with elements of structure is a one possible way to decrease bias.[75]\nCoaching\n\nAn abundance of information is available to instruct interviewees on strategies for improving their performance in a job interview. Information used by interviewees comes from a variety of sources ranging from popular how-to books to formal coaching programs, sometimes even provided by the hiring organization. Within the more formal coaching programs, there are two general types of coaching. One type of coaching is designed to teach interviewees how to perform better in the interview by focusing on how to behave and present oneself. This type of coaching is focused on improving aspects of the interview that are not necessarily related to the specific elements of performing the job tasks. This type of coaching could include how to dress, how to display nonverbal behaviors (head nods, smiling, eye contact), verbal cues (how fast to speak, speech volume, articulation, pitch), and impression management tactics. Another type of coaching is designed to focus interviewees on the content specifically relevant to describing one’s qualifications for the job, in order to help improve their answers to interview questions. This coaching, therefore, focuses on improving the interviewee’s understanding of the skills, abilities, and traits the interviewer is attempting to assess, and responding with relevant experience that demonstrates these skills.[76] For example, this type of coaching might teach an interviewee to use the STAR approach for answering behavioral interview questions.[77]\n\nA coaching program might include several sections focusing on various aspects of the interview. It could include a section designed to introduce interviewees to the interview process, and explain how this process works (e.g., administration of interview, interview day logistics, different types of interviews, advantages of structured interviews). It could also include a section designed to provide feedback to help the interviewee to improve their performance in the interview, as well as a section involving practice answering example interview questions. An additional section providing general interview tips about how to behave and present oneself could also be included.[78]\n\nIt is useful to consider coaching in the context of the competing goals of the interviewer and interviewee. The interviewee’s goal is typically to perform well (i.e. obtain high interview ratings), in order to get hired. On the other hand, the interviewer’s goal is to obtain job-relevant information, in order to determine whether the applicant has the skills, abilities, and traits believed by the organization to be indicators of successful job performance.[76] Research has shown that how well an applicant does in the interview can be enhanced with coaching.[76][79][80][81] The effectiveness of coaching is due, in part, to increasing the interviewee’s knowledge, which in turn results in better interview performance. Interviewee knowledge refers to knowledge about the interview, such as the types of questions that will be asked, and the content that the interviewer is attempting to assess.[82] Research has also shown that coaching can increase the likelihood that interviewers using a structured interview will accurately choose those individuals who will ultimately be most successful on the job (i.e., increase reliability and validity of the structured interview).[76] Additionally, research has shown that interviewees tend to have positive reactions to coaching, which is often an underlying goal of an interview.[78] Based on research thus far, the effects of coaching tend to be positive for both interviewees and interviewers.\nFaking\n\nInterviewers should be aware that applicants can intentionally distort their responses or fake during the interview and such applicant faking has the potential to influence interview outcomes if present. Two concepts that relate to faking include social desirability (the tendency for people to present themselves in a favorable light[83]), and impression management (conscious or unconscious attempts to influence one’s image during interactions[84]). Faking in the employment interview, then, can be defined as \"deceptive impression management or the conscious distortion of answers to the interview questions in order to obtain a better score on the interview and/or otherwise create favorable perceptions\".[85] Thus, faking in the employment interview is intentional, deceptive, and aimed at improving perceptions of performance.\n\nFaking in the employment interview can be broken down into four elements.[85] The first involves the interviewee portraying him or herself as an ideal job candidate by exaggerating true skills, tailoring answers to better fit the job, and/or creating the impression that personal beliefs, values, and attitudes are similar to those of the organization.\n\nThe second aspect of faking is inventing or completely fabricating one’s image by piecing distinct work experiences together to create better answers, inventing untrue experiences or skills, and portraying others’ experiences or accomplishments as ones’ own.\n\nThirdly, faking might also be aimed at protecting the applicant’s image. This can be accomplished through omitting certain negative experiences, concealing negatively perceived aspects of the applicant’s background, and by separating oneself from negative experiences.\n\nThe fourth and final component of faking involves ingratiating oneself to the interviewer by conforming personal opinions to align with those of the organization, as well as insincerely praising or complimenting the interviewer or organization.\n\nOf all of the various faking behaviors listed, ingratiation tactics were found to be the most prevalent in the employment interview, while flat out making up answers or claiming others’ experiences as one’s own is the least common.[85] However, fabricating true skills appears to be at least somewhat prevalent in employment interviews. One study found that over 80% of participants lied about job-related skills in the interview,[86] presumably to compensate for a lack of job-required skills/traits and further their chances for employment.\n\nMost importantly, faking behaviors have been shown to affect outcomes of employment interviews. For example, the probability of getting another interview or job offer increases when interviewees make up answers.[85]\n\nDifferent interview characteristics also seem to impact the likelihood of faking. Faking behavior is less prevalent, for instance, in past behavioral interviews than in situational interviews, although follow-up questions increased faking behaviors in both types of interviews. Therefore, if practitioners are interested in decreasing faking behaviors among job candidates in employment interview settings, they should utilize structured, past behavioral interviews and avoid the use of probes or follow-up questions.[85]\nNarcissism\nMain article: Narcissism in the workplace\n\nNarcissists typically perform well at job interviews and have a good success rate for landing jobs. Interviews are one of the few social situations where narcissistic behaviours such as boasting actually create a positive impression.[87]\nPsychopathy\nMain article: Psychopathy in the workplace\nSee also: Superficial charm\n\nCorporate psychopaths are readily recruited into organisations because they make a distinctly positive impression at interviews.[88] They appear to be alert, friendly and easy to get along with and talk to. They look like they are of good ability, emotionally well adjusted and reasonable, and these traits make them attractive to those in charge of hiring staff within organisations. Other researchers confirm that psychopaths can present themselves as likeable and personally attractive.[89] Companies often rely on interview performance alone and do not conduct other checks such as taking references. Being accomplished liars helps psychopaths obtain the jobs they want.[90]\nFactors impacting on interview effectiveness\nValidity and predictive power\n\nThere is extant data which puts into question the value of job interviews as a tool for selecting employees. Where the aim of a job interview is ostensibly to choose a candidate who will perform well in the job role, other methods of selection provide greater predictive power and often lower costs.[91]\nInterview structure issues\n\nWhile unstructured interviews are commonly used, structured interviews have yielded much better results and are considered a best practice.[92] Interview structure is defined as \"the reduction in procedural variance across applicants, which can translate into the degree of discretion that an interviewer is allowed in conducting the interview\".[93] Structure in an interview can be compared to a typical paper and pencil test: we would not think it was fair if every test taker was given different questions and a different number of questions on an exam, or if their answers were each graded differently. Yet this is exactly what occurs in an unstructured interview; thus, a structured interview attempts to standardize this popular selection tool. While there is debate surrounding what is meant specifically by a structured interview,[94] there are typically two broad categories of standardization: 1) content structure, and 2) evaluation structure.[95] Content structure includes elements that refer to the actual content of the interview:\n\n    Base questions on attributes that are representative of the job, as indicated by a job analysis\n    Ask the same questions of all interviewees\n    Limit prompting, or follow up questions, that interviewers may ask\n    Ask better questions, such as behavioral description questions\n    Have a longer interview\n    Control ancillary information available to the interviewees, such as resumes\n    Don’t allow questions from applicants during interview\n\nEvaluation structure includes aspects that refer to the actual rating of the interviewee:\n\n    Rate each answer rather than making an overall evaluation at the end of the interview\n    Use anchored rating scales (for an example, see BARS )\n    Have the interviewer take detailed notes\n    Have more than one interviewer view each applicant (i.e. have panel interviews)\n    Have the same interviewers rate each applicant\n    Don’t allow any discussion about the applicants between interviewers\n    Train the interviewers\n    Use statistical procedures to create an overall interview score\n\nIt is important to note that structure should be thought of as a continuum; that is, the degree of structure present in an interview can vary along these various elements listed above.[94]\nInterviewer rating reliability\n\nIn terms of reliability, meta-analytic results provided evidence that interviews can have acceptable levels of interrater reliability, or consistent ratings across interviewers interrater reliability (i.e. .75 or above), when a structured panel interview is used.[96] In terms of criterion-related validity, or how well the interview predicts later job performance criterion validity, meta-analytic results have shown that when compared to unstructured interviews, structured interviews have higher validities, with values ranging from .20-.57 (on a scale from 0 to 1), with validity coefficients increasing with higher degrees of structure.[93][97][98] That is, as the degree of structure in an interview increases, the more likely interviewers can successfully predict how well the person will do on the job, especially when compared to unstructured interviews. In fact, one structured interview that included a) a predetermined set of questions that interviewers were able to choose from, and b) interviewer scoring of applicant answers after each individual question using previously created benchmark answers, showed validity levels comparable to cognitive ability tests (traditionally one of the best predictors of job performance) for entry level jobs.[93]\n\nHonesty and integrity are attributes that can be very hard to determine using a formal job interview process: the competitive environment of the job interview may in fact promote dishonesty. Some experts on job interviews express a degree of cynicism towards the process.[who?]\nApplicant reactions\n\nApplicant reactions to the interview process include specific factors such as; fairness, emotional responses, and attitudes toward the interviewer or the organization.[99] Though the applicant's perception of the interview process may not influence the interviewer(s) ability to distinguish between individuals' suitability, applicants reactions are important as those who react negatively to the selection process are more likely to withdraw from the selection process.[100][101][102] They are less likely to accept a job offer, apply on future occasions,[103] or to speak highly of the organization to others and to be a customer of that business.[100][101][104] Compared to other selection methods, such as personality or cognitive ability tests, applicants, from different cultures may have positive opinions about interviews.[100][105]\nInterview design\n\nInterview design can influence applicants' positive and negative reactions, though research findings on applicants preferences for structured compared to unstructured interviews appear contradictory.[102][106] Applicants' negative reactions to structured interviews may be reduced by providing information about the job and organization.[107] Providing interview questions to applicants before the interview, or telling them how their answers will be evaluated, are also received positively.[108]\nTypes of questions\n\nThe type of questions asked can affect applicant reactions. General questions are viewed more positively than situational or behavioral questions[109] and 'puzzle' interview questions may be perceived as negative being perceived unrelated to the job, unfair, or unclear how to answer.[110] Using questions that discriminating unfairly in law unsurprisingly are viewed negatively with applicants less likely to accept a job offer, or to recommend the organization to others.[111]\n\nSome of the questions and concerns on the mind of the hiring manager include:\n\n    Does this person have the skills I need to get the job done?\n    Will he or she fit in with the department or team?\n    Can I manage this person?\n    Does this person demonstrate honesty, integrity, and a good work ethic?\n    What motivates this person?\n    Do I like this person, and will he or she get along with others?\n    Will he or she focus on tasks and stick to the job until it is done?\n    Will this person perform up to the level the company requires for success?\n\nA sample of intention behind questions asked for understanding observable responses, displayed character, and underlying motivation:\n\n    What did the candidate really do in this job?\n    What role did he or she play, supportive or leading?\n    How much influence did the candidate exert on the outcomes of projects?\n    How did the candidate handle problems that came up?\n    How does this candidate come across?\n    How serious is the candidate about his or her career and this job?\n    Is he or she bright and likable?\n    Did the candidate prepare for this interview?\n    Is the candidate being forthright with information?\n    Does this person communicate well in a somewhat stressful face-to-face conversation?\n    Does the candidate stay focused on the question asked or ramble along?\n    Did the candidate exhibit good judgment in the career moves he or she made?\n    Did the candidate grow in his or her job and take on more responsibilities over time or merely do the same thing repeatedly?\n    Did the candidate demonstrate leadership, integrity, effective communications, teamwork, and persuasion skills (among others)?\n\nAdditional factors\n\nThe 'friendliness' of the interviewer may be equated to fairness of the process and improve the liklihood of accepting a job offer,[112] and face-to-face interviews compared to video conferencing and telephone interviews.[113] In video conferencing interviews the perception of the interviewer may be viewed as less personable, trustworthy, and competent.[114]\nInterview anxiety\n\nInterview anxiety refers to experiencing unpleasant or distressing feelings before or during a job interview.[115] It also reflects feeling apprehensive or tense about participating in an interview.[116] A couple of reasons why job candidates feel a heightened sense of anxiety and nervousness about the employment interview is because they feel they have little to no control over the interview process [117] or because they have to speak with a stranger.[118]\nImplications for applicants\n\nWhether anxieties come from how someone is as a person or from the interview situation itself, these anxious feelings have important consequences for job candidates, such as; limiting an applicant from effectively showing their ability to communicate and their future potential,[119] reducing interview performance and consequent assessment despite potential suitability for the job,[115] reducing likelihood of a second interview compared to less anxious individuals.[120]\nImplications for organizations\n\nApplicants who view the selection process more favorably tend to be more positive about the organization, and are likely to influence an organization’s reputation.[115][121] whearas, in contrast, anxious or uncomfortable during their interview may view an organization less favorably, causing the otherwise qualified candidates not accepting a job offer.[115] If an applicant is nervous, they might not act the same way they would on the job, making it harder for organizations to use the interview for predicting someone’s future job performance.[115]\nLegal issues\n\nIn many countries laws are put into place to prevent organizations from engaging in discriminatory practices against protected classes when selecting individuals for jobs.[122] In the United States, it is unlawful for private employers with 15 or more employees along with state and local government employers to discriminate against applicants based on the following: race, color, sex (including pregnancy), national origin, age (40 or over), disability, or genetic information (note: additional classes may be protected depending on state or local law). More specifically, an employer cannot legally \"fail or refuse to hire or to discharge any individual, or otherwise discriminate against any individual with respect to his compensation, terms, conditions, or privilege of employment\" or \"to limit, segregate, or classify his employees or applicants for employment in any way which would deprive or tend to deprive any individual of employment opportunities or otherwise adversely affect his status as an employee.\"[123][124]\n\nThe Civil Rights Act of 1964 and 1991 (Title VII) were passed into law to prevent the discrimination of individuals due to race, color, religion, sex, or national origin. The Pregnancy Discrimination Act was added as an amendment and protects women if they are pregnant or have a pregnancy-related condition.[125]\n\nThe Age Discrimination in Employment Act of 1967 prohibits discriminatory practice directed against individuals who are 40 years of age and older. Although some states (e.g. New York) do have laws preventing the discrimination of individuals younger than 40, no federal law exists.[126]\n\nThe Americans with Disabilities Act of 1990 protects qualified individuals who currently have or in the past have had a physical or mental disability (current users of illegal drugs are not covered under this Act). A person is a cripple if he has a disability that substantially limits a major life activity, has a history of a disability, is regarded by others as being disabled, or has a physical or mental impairment that is not transitory (lasting or expected to last six months or less) and minor. In order to be covered under this Act, the individual must be qualified for the job. A qualified individual is \"an individual with a disability who, with or without reasonable accommodation, can perform the essential functions of the employment position that such individual holds or desires.\"[127] Unless the disability poses an \"undue hardship,\" reasonable accommodations must be made by the organization. \"In general, an accommodation is any change in the work environment or in the way things are customarily done that enables an individual with a disability to enjoy equal employment opportunities.\"[127] Examples of reasonable accommodations are changing the workspace of an individual in a wheelchair to make it more wheelchair accessible, modifying work schedules, and/or modifying equipment.[128] Employees are responsible for asking for accommodations to be made by their employer.[125]\n\nThe most recent law to be passed is Title II of the Genetic Information Nondiscrimination Act of 2008. In essence, this law prohibits the discrimination of employees or applicants due to an individual’s genetic information and family medical history information.\n\nIn rare circumstances, it is lawful for employers to base hiring decisions on protected class information if it is considered a Bona Fide Occupational Qualification, that is, if it is a \"qualification reasonably necessary to the normal operation of the particular business.\" For example, a movie studio may base a hiring decision on age if the actor they are hiring will play a youthful character in a film.[129]\n\nGiven these laws, organizations are limited in the types of questions they legally are allowed to ask applicants in a job interview. Asking these questions may cause discrimination against protected classes, unless the information is considered a Bona Fide Occupational Qualification. For example, in the majority of situations it is illegal to ask the following questions in an interview as a condition of employment:\n\n    What is your date of birth?[130]\n    Have you ever been arrested for a crime?[130]\n    Do you have any future plans for marriage and children?[130]\n    What are your spiritual beliefs?[131]\n    How many days were you sick last year? Have you ever been treated for mental health problems?[131]\n    What prescription drugs are you currently taking?[131]\n\nApplicants with disabilities\n\nApplicants with disabilities may be concerned with the effect that their disability has on both interview and employment outcomes. Research has concentrated on four key issues: how interviewers rate applicants with disabilities, the reactions of applicants with disabilities to the interview, the effects of disclosing a disability during the interview, and the perceptions different kinds of applicant disabilities may have on interviewer ratings.\n\nThe job interview is a tool used to measure constructs or overall characteristics that are relevant for the job. Oftentimes, applicants will receive a score based on their performance during the interview. Research has found different findings based on interviewers’ perceptions of the disability. For example, some research has found a leniency effect (i.e., applicants with disabilities receive higher ratings than equally qualified non-disabled applicants) in ratings of applicants with disabilities[132][133] Other research, however, has found there is a disconnect between the interview score and the hiring recommendation for applicants with disabilities. That is, even though applicants with disabilities may have received a high interview score, they are still not recommended for employment.[134][135] The difference between ratings and hiring could be detrimental to a company because they may be missing an opportunity to hire a qualified applicant.\n\nA second issue in interview research deals with the applicants’ with disabilities reactions to the interview and applicant perceptions of the interviewers. Applicants with disabilities and able-bodied applicants report similar feelings of anxiety towards an interview.[136] Applicants with disabilities often report that interviewers react nervously and insecurely, which leads such applicants to experience anxiety and tension themselves. The interview is felt to be the part of the selection process where covert discrimination against applicants with disabilities can occur.[136] Many applicants with disabilities feel they cannot disclose (i.e., inform potential employer of disability) or discuss their disability because they want to demonstrate their abilities. If the disability is visible, then disclosure will inevitably occur when the applicant meets the interviewer, so the applicant can decide if they want to discuss their disability. If an applicant has a non-visible disability, however, then that applicant has more of a choice in disclosing and discussing. In addition, applicants who were aware that the recruiting employer already had employed people with disabilities felt they had a more positive interview experience.[136] Applicants should consider if they are comfortable with talking about and answering questions about their disability before deciding how to approach the interview.\n\nResearch has also demonstrated that different types of disabilities have different effects on interview outcomes. Disabilities with a negative stigma and that are perceived as resulting from the actions of the person (e.g., HIV-Positive, substance abuse) result in lower interview scores than disabilities for which the causes are perceived to be out of the individual’s control (e.g., physical birth defect).[135] A physical disability often results in higher interviewer ratings than psychological (e.g., mental illness) or sensory conditions (e.g., Tourette Syndrome).[133][137] In addition, there are differences between the effects of disclosing disabilities that are visible (e.g., wheelchair bound) and non-visible (e.g., Epilepsy) during the interview. When applicants had a non-visible disability and disclosed their disability early in the interview they were not rated more negatively than applicants who did not disclose. In fact, they were liked more than the applicants who did not disclose their disability and were presumed not disabled.[138] Interviewers tend to be impressed by the honesty of the disclosure.[137] Strong caution needs to be taken with applying results from studies about specific disabilities, as these results may not apply to other types of disabilities. Not all disabilities are the same and more research is needed to find whether these results are relevant for other types of disabilities.\n\nSome practical implications for job interviews for applicants with disabilities include research findings that show there are no differences in interviewer responses to a brief, shorter discussion or a detailed, longer discussion about the disability during the interview.[137] Applicants, however, should note that when a non-visible disability is disclosed near the end of the interview, applicants were rated more negatively than early disclosing and non-disclosing applicants. Therefore, it is possible that interviewers feel individuals who delay disclosure may do so out of shame or embarrassment. In addition, if the disability is disclosed after being hired, employers may feel deceived by the new hire and reactions could be less positive than would have been in the interview.[139] If applicants want to disclose their disability during the interview, research shows that a disclosure and/or discussion earlier in the interview approach may afford them some positive interview effects.[140] The positive effects, however, are preceded by the interviewers perception of the applicants’ psychological well-being. That is, when the interviewer perceives the applicant is psychologically well and/or comfortable with his or her disability, there can be positive interviewer effects. In contrast, if the interviewer perceives the applicant as uncomfortable or anxious discussing the disability, this may either fail to garner positive effect or result in more negative interview ratings for the candidate. Caution must again be taken when applying these research findings to other types of disabilities not investigated in the studies discussed above. There are many factors that can influence the interview of an applicant with a disability, such as whether the disability is physical or psychological, visible or non-visible, or whether the applicant is perceived as responsible for the disability or not. Therefore, applicants should make their own conclusions about how to proceed in the interview after comparing their situations with those examined in the research discussed here.\nOther applicant discrimination: Weight and pregnancy\n\nJob applicants who are underweight (to the point of emaciation), overweight or obese may face discrimination in the interview.[141][142] The negative treatment of overweight and obese individuals may stem from beliefs that weight is controllable and those who fail to control their weight are lazy, unmotivated, and lack self-discipline.[143][144] Underweight individuals may also be subject to appearance-related negative treatment.[142] Underweight, overweight and obese applicants are not protected from discrimination by any current United States laws.[141] However, some individuals who are morbidly obese and whose obesity is due to a physiological disorder may be protected against discrimination under the Americans with Disabilities Act.[145]\n\nDiscrimination against pregnant applicants is illegal under the Pregnancy Discrimination Act of 1978, which views pregnancy as a temporary disability and requires employers to treat pregnant applicants the same as all other applicants.[146] Yet, discrimination against pregnant applicants continues both in the United States and internationally.[146][147] Research shows that pregnant applicants compared to non-pregnant applicants are less likely to be recommended for hire.[148][149] Interviewers appear concerned that pregnant applicants are more likely than non-pregnant applicants to miss work and even quit.[149] Organizations who wish to reduce potential discrimination against pregnant applicants should consider implementing structured interviews, although some theoretical work suggests interviewers may still show biases even in these types of interviews.[148][150]\n\nEmployers are using social networking sites like Facebook and LinkedIn to obtain additional information about job applicants.[151][152][153] While these sites may be useful to verify resume information, profiles with pictures also may reveal much more information about the applicant, including issues pertaining to applicant weight and pregnancy.[154] Some employers are also asking potential job candidates for their social media logins which has alarmed many privacy watch dogs and regulators.[155]\n\nProject management\nProject management is the discipline of initiating, planning, executing, controlling, and closing the work of a team to achieve specific goals and meet specific success criteria. A project is a temporary endeavor designed to produce a unique product, service or result with a defined beginning and end (usually time-constrained, and often constrained by funding or deliverables) undertaken to meet unique goals and objectives, typically to bring about beneficial change or added value.[1][2] The temporary nature of projects stands in contrast with business as usual (or operations),[3] which are repetitive, permanent, or semi-permanent functional activities to produce products or services. In practice, the management of these two systems is often quite different, and as such requires the development of distinct technical skills and management strategies.[4]\n\nThe primary challenge of project management is to achieve all of the project goals within the given constraints.[5] This information is usually described in a user or project manual, which is created at the beginning of the development process. The primary constraints are scope, time, quality and budget.[6] The secondary  and more ambitious  challenge is to optimize the allocation of necessary inputs and integrate them to meet pre-defined objectives.\n\nContents  [hide] \n1\tHistory\n2\tApproaches\n2.1\tThe traditional approach\n2.2\tPRINCE2\n2.3\tCritical chain project management\n2.4\tProcess-based management\n2.5\tLean project management\n2.6\tExtreme project management\n2.7\tBenefits realization management\n3\tProcesses\n3.1\tInitiating\n3.2\tPlanning\n3.3\tExecuting\n3.4\tMonitoring and controlling\n3.5\tClosing\n3.6\tProject controlling and project control systems\n4\tTopics\n4.1\tProject managers\n4.2\tProject management types\n4.3\tWork breakdown structure\n4.4\tInternational standards\n4.5\tProject portfolio management\n4.6\tProject management software\n4.7\tVirtual project management\n5\tSee also\n6\tReferences\n7\tExternal links\nHistory\n\nRoman soldiers building a fortress, Trajan's Column 113 AD\nUntil 1900, civil engineering projects were generally managed by creative architects, engineers, and master builders themselves, for example Vitruvius (first century BC), Christopher Wren (16321723), Thomas Telford (17571834) and Isambard Kingdom Brunel (18061859).[7] It was in the 1950s that organizations started to systematically apply project management tools and techniques to complex engineering projects.[8]\n\n\nHenry Gantt (18611919), the father of planning and control techniques\nAs a discipline, project management developed from several fields of application including civil construction, engineering, and heavy defense activity.[9] Two forefathers of project management are Henry Gantt, called the father of planning and control techniques,[10] who is famous for his use of the Gantt chart as a project management tool (alternatively Harmonogram first proposed by Karol Adamiecki[11]); and Henri Fayol for his creation of the five management functions that form the foundation of the body of knowledge associated with project and program management.[12] Both Gantt and Fayol were students of Frederick Winslow Taylor's theories of scientific management. His work is the forerunner to modern project management tools including work breakdown structure (WBS) and resource allocation.\n\nThe 1950s marked the beginning of the modern project management era where core engineering fields come together to work as one. Project management became recognized as a distinct discipline arising from the management discipline with engineering model.[13] In the United States, prior to the 1950s, projects were managed on an ad-hoc basis, using mostly Gantt charts and informal techniques and tools. At that time, two mathematical project-scheduling models were developed. The \"Critical Path Method\" (CPM) was developed as a joint venture between DuPont Corporation and Remington Rand Corporation for managing plant maintenance projects. And the \"Program Evaluation and Review Technique\" or PERT, was developed by the United States Navy in conjunction with the Lockheed Corporation and Booz Allen Hamilton as part of the Polaris missile submarine program.[14]\n\nPERT and CPM are very similar in their approach but still present some differences. CPM is used for projects that assume deterministic activity times; the times at which each activity will be carried out are known. PERT, on the other hand, allows for stochastic activity times; the times at which each activity will be carried out are uncertain or varied. Because of this core difference, CPM and PERT are used in different contexts. These mathematical techniques quickly spread into many private enterprises.\n\n\nPERT network chart for a seven-month project with five milestones\nAt the same time, as project-scheduling models were being developed, technology for project cost estimating, cost management, and engineering economics was evolving, with pioneering work by Hans Lang and others. In 1956, the American Association of Cost Engineers (now AACE International; the Association for the Advancement of Cost Engineering) was formed by early practitioners of project management and the associated specialties of planning and scheduling, cost estimating, and cost/schedule control (project control). AACE continued its pioneering work and in 2006 released the first integrated process for portfolio, program and project management (Total Cost Management Framework).\n\nThe International Project Management Association (IPMA) was founded in Europe in 1967,[15] as a federation of several national project management associations. IPMA maintains its federal structure today and now includes member associations on every continent except Antarctica. IPMA offers a Four Level Certification program based on the IPMA Competence Baseline (ICB).[16] The ICB covers technical, contextual, and behavioral competencies.\n\nIn 1969, the Project Management Institute (PMI) was formed in the USA.[17] PMI publishes A Guide to the Project Management Body of Knowledge (PMBOK Guide), which describes project management practices that are common to \"most projects, most of the time.\" PMI also offers multiple certifications.\n\nApproaches\nThere are a number of approaches for managing project activities including lean, iterative, incremental, and phased approaches.\n\nRegardless of the methodology employed, careful consideration must be given to the overall project objectives, timeline, and cost, as well as the roles and responsibilities of all participants and stakeholders.\n\nThe traditional approach\nA traditional phased approach identifies a sequence of steps to be completed. In the \"traditional approach\",[18] five developmental components of a project can be distinguished (four stages plus control):\n\n\nTypical development phases of an engineering project\ninitiation\nplanning and design\nexecution and construction\nmonitoring and controlling systems\ncompletion and finish point\nMany industries use variations of these project stages. For example, when working on a brick-and-mortar design and construction, projects will typically progress through stages like pre-planning, conceptual design, schematic design, design development, construction drawings (or contract documents), and construction administration. In software development, this approach is often known as the waterfall model,[19] i.e., one series of tasks after another in linear sequence. In software development many organizations have adapted the Rational Unified Process (RUP) to fit this methodology, although RUP does not require or explicitly recommend this practice. Waterfall development works well for small, well defined projects, but often fails in larger projects of undefined and ambiguous nature. The Cone of Uncertainty explains some of this as the planning made on the initial phase of the project suffers from a high degree of uncertainty. This becomes especially true as software development is often the realization of a new or novel product. In projects where requirements have not been finalized and can change, requirements management is used to develop an accurate and complete definition of the behavior of software that can serve as the basis for software development.[20] While the terms may differ from industry to industry, the actual stages typically follow common steps to problem solving\"defining the problem, weighing options, choosing a path, implementation and evaluation.\"\n\nPRINCE2\nMain article: PRINCE2\n\nThe PRINCE2 process model\nPRINCE2 is a structured approach to project management released in 1996 as a generic project management method.[21] It combines the original PROMPT methodology (which evolved into the PRINCE methodology) with IBM's MITP (managing the implementation of the total project) methodology. PRINCE2 provides a method for managing projects within a clearly defined framework.\n\nPRINCE2 focuses on the definition and delivery of products, in particular their quality requirements. As such, it defines a successful project as being output-oriented (not activity- or task-oriented) through creating an agreed set of products[22] that define the scope of the project and provides the basis for planning and control, that is, how then to coordinate people and activities, how to design and supervise product delivery, and what to do if products and therefore the scope of the project has to be adjusted if it does not develop as planned.\n\nIn the method, each process is specified with its key inputs and outputs and with specific goals and activities to be carried out to deliver a project's outcomes as defined by its Business Case. This allows for continuous assessment and adjustment when deviation from the Business Case is required.\n\nPRINCE2 provides a common language for all participants in the project. The governance framework of PRINCE2  its roles and responsibilities  are fully described and require tailoring to suit the complexity of the project and skills of the organisation.[22]\n\nCritical chain project management\nMain article: Critical chain project management\nCritical chain project management (CCPM) is a method of planning and managing project execution designed to deal with uncertainties inherent in managing projects, while taking into consideration limited availability of resources (physical, human skills, as well as management & support capacity) needed to execute projects.\n\nCCPM is an application of the theory of constraints (TOC) to projects. The goal is to increase the flow of projects in an organization (throughput). Applying the first three of the five focusing steps of TOC, the system constraint for all projects is identified as are the resources. To exploit the constraint, tasks on the critical chain are given priority over all other activities. Finally, projects are planned and managed to ensure that the resources are ready when the critical chain tasks must start, subordinating all other resources to the critical chain.\n\nThe project plan should typically undergo resource leveling, and the longest sequence of resource-constrained tasks should be identified as the critical chain. In some cases, such as managing contracted sub-projects, it is advisable to use a simplified approach without resource leveling.\n\nIn multi-project environments, resource leveling should be performed across projects. However, it is often enough to identify (or simply select) a single \"drum\". The drum can be a resource that acts as a constraint across projects, which are staggered based on the availability of that single resource.\n\nOne can also use a \"virtual drum\" by selecting a task or group of tasks (typically integration points) and limiting the number of projects in execution at that stage.\n\nProcess-based management\nMain article: Process-based management\nThe incorporation of process-based management has been driven by the use of Maturity models such as the OPM3 and the CMMI (capability maturity model integration; see this example of a predecessor) and ISO/IEC15504 (SPICE  software process improvement and capability estimation). Unlike SEI's CMM, the OPM3 maturity model describes how to make project management processes capable of performing successfully, consistently, and predictably in order to enact the strategies of an organization .\n\nLean project management\nMain article: Lean project management\nLean project management uses the principles from lean manufacturing to focus on delivering value with less waste and reduced time.\n\nExtreme project management\nMain article: Extreme project management\n\nPlanning and feedback loops in Extreme programming (XP) with the time frames of the multiple loops.\nIn critical studies of project management it has been noted that several PERT based models are not well suited for the multi-project company environment of today.[citation needed] Most of them are aimed at very large-scale, one-time, non-routine projects, and currently all kinds of management are expressed in terms of projects.\n\nUsing complex models for \"projects\" (or rather \"tasks\") spanning a few weeks has been proven to cause unnecessary costs and low maneuverability in several cases.[citation needed] The generalization of Extreme Programming to other kinds of projects is extreme project management, which may be used in combination with the process modeling and management principles of human interaction management.\n\nBenefits realization management\nMain article: Benefits realisation management\nBenefits realization management (BRM) enhances normal project management techniques through a focus on outcomes (the benefits) of a project rather than products or outputs, and then measuring the degree to which that is happening to keep a project on track. This can help to reduce the risk of a completed project being a failure by delivering agreed upon requirements/outputs but failing to deliver the benefits of those requirements.\n\nIn addition, BRM practices aim to ensure the alignment between project outcomes and business strategies. The effectiveness of these practices is supported by recent research evidencing BRM practices influencing project success from a strategic perspective across different countries and industries.[23]\n\nAn example of delivering a project to requirements might be agreeing to deliver a computer system that will process staff data and manage payroll, holiday and staff personnel records. Under BRM the agreement might be to achieve a specified reduction in staff hours required to process and maintain staff data.\n\nProcesses\n\nThe project development stages[24]\nTraditionally, project management includes a number of elements: four to five project management process groups, and a control system. Regardless of the methodology or terminology used, the same basic project management processes or stages of development will be used. Major process groups generally include:[6]\n\nInitiation\nPlanning\nProduction or execution\nMonitoring and controlling\nClosing\nIn project environments with a significant exploratory element (e.g., research and development), these stages may be supplemented with decision points (go/no go decisions) at which the project's continuation is debated and decided. An example is the Phasegate model.\n\nInitiating\n\nInitiating process group processes[24]\nThe initiating processes determine the nature and scope of the project.[25] If this stage is not performed well, it is unlikely that the project will be successful in meeting the business needs. The key project controls needed here are an understanding of the business environment and making sure that all necessary controls are incorporated into the project. Any deficiencies should be reported and a recommendation should be made to fix them.\n\nThe initiating stage should include a plan that encompasses the following areas:\n\nanalyzing the business needs/requirements in measurable goals\nreviewing of the current operations\nfinancial analysis of the costs and benefits including a budget\nstakeholder analysis, including users, and support personnel for the project\nproject charter including costs, tasks, deliverables, and schedules\nPlanning\nAfter the initiation stage, the project is planned to an appropriate level of detail (see example of a flow-chart).[24] The main purpose is to plan time, cost and resources adequately to estimate the work needed and to effectively manage risk during project execution. As with the Initiation process group, a failure to adequately plan greatly reduces the project's chances of successfully accomplishing its goals.\n\nProject planning generally consists of[26]\n\ndetermining how to plan (e.g. by level of detail or Rolling Wave planning);\ndeveloping the scope statement;\nselecting the planning team;\nidentifying deliverables and creating the work breakdown structure;\nidentifying the activities needed to complete those deliverables and networking the activities in their logical sequence;\nestimating the resource requirements for the activities;\nestimating time and cost for activities;\ndeveloping the schedule;\ndeveloping the budget;\nrisk planning;\ngaining formal approval to begin work.\nAdditional processes, such as planning for communications and for scope management, identifying roles and responsibilities, determining what to purchase for the project and holding a kick-off meeting are also generally advisable.\n\nFor new product development projects, conceptual design of the operation of the final product may be performed concurrent with the project planning activities, and may help to inform the planning team when identifying deliverables and planning activities.\n\nExecuting\n\nExecuting process group processes[24]\nThe execution/implementation phase ensures that the project management plans deliverables are executed accordingly. This phase involves proper allocation, co-ordination and management of human resources and any other resources such as material and budgets. The output of this phase is the project deliverables.\n\nMonitoring and controlling\n\nMonitoring and controlling process group processes[24]\nMonitoring and controlling consists of those processes performed to observe project execution so that potential problems can be identified in a timely manner and corrective action can be taken, when necessary, to control the execution of the project. The key benefit is that project performance is observed and measured regularly to identify variances from the project management plan.\n\nMonitoring and controlling includes:[27]\n\nMeasuring the ongoing project activities ('where we are');\nMonitoring the project variables (cost, effort, scope, etc.) against the project management plan and the project performance baseline (where we should be);\nIdentifying corrective actions to address issues and risks properly (How can we get on track again);\nInfluencing the factors that could circumvent integrated change control so only approved changes are implemented.\nIn multi-phase projects, the monitoring and control process also provides feedback between project phases, in order to implement corrective or preventive actions to bring the project into compliance with the project management plan.\n\nProject maintenance is an ongoing process, and it includes:[6]\n\nContinuing support of end-users\nCorrection of errors\nUpdates to the product over time\n\nMonitoring and controlling cycle\nIn this stage, auditors should pay attention to how effectively and quickly user problems are resolved.\n\nOver the course of any construction project, the work scope may change. Change is a normal and expected part of the construction process. Changes can be the result of necessary design modifications, differing site conditions, material availability, contractor-requested changes, value engineering and impacts from third parties, to name a few. Beyond executing the change in the field, the change normally needs to be documented to show what was actually constructed. This is referred to as change management. Hence, the owner usually requires a final record to show all changes or, more specifically, any change that modifies the tangible portions of the finished work. The record is made on the contract documents  usually, but not necessarily limited to, the design drawings. The end product of this effort is what the industry terms as-built drawings, or more simply, as built. The requirement for providing them is a norm in construction contracts. Construction document management is a highly important task undertaken with the aid an online or desktop software system, or maintained through physical documentation. The increasing legality pertaining to the construction industries maintenance of correct documentation has caused the increase in the need for document management systems.\n\nWhen changes are introduced to the project, the viability of the project has to be re-assessed. It is important not to lose sight of the initial goals and targets of the projects. When the changes accumulate, the forecasted result may not justify the original proposed investment in the project. Successful project management identifies these components, and tracks and monitors progress so as to stay within time and budget frames already outlined at the commencement of the project.\n\nClosing\n\nClosing process group processes.[24]\nClosing includes the formal acceptance of the project and the ending thereof. Administrative activities include the archiving of the files and documenting lessons learned.\n\nThis phase consists of:[6]\n\nContract closure: Complete and settle each contract (including the resolution of any open items) and close each contract applicable to the project or project phase.\nProject close: Finalize all activities across all of the process groups to formally close the project or a project phase\nAlso included in this phase is the Post Implementation Review. This is a vital phase of the project for the project team to learn from experiences and apply to future projects. Normally a Post Implementation Review consists of looking at things that went well and analysing things that went badly on the project to come up with lessons learned.\n\nProject controlling and project control systems\nProject controlling should be established as an independent function in project management. It implements verification and controlling function during the processing of a project in order to reinforce the defined performance and formal goals.[28] The tasks of project controlling are also:\n\nthe creation of infrastructure for the supply of the right information and its update\nthe establishment of a way to communicate disparities of project parameters\nthe development of project information technology based on an intranet or the determination of a project key performance indicator system (KPI)\ndivergence analyses and generation of proposals for potential project regulations[29]\nthe establishment of methods to accomplish an appropriate project structure, project workflow organization, project control and governance\ncreation of transparency among the project parameters[30]\nFulfillment and implementation of these tasks can be achieved by applying specific methods and instruments of project controlling. The following methods of project controlling can be applied:\n\ninvestment analysis\ncostbenefit analysis\nvalue benefit analysis\nexpert surveys\nsimulation calculations\nrisk-profile analysis\nsurcharge calculations\nmilestone trend analysis\ncost trend analysis\ntarget/actual-comparison[31]\nProject control is that element of a project that keeps it on-track, on-time and within budget.[27] Project control begins early in the project with planning and ends late in the project with post-implementation review, having a thorough involvement of each step in the process. Projects may be audited or reviewed while the project is in progress. Formal audits are generally risk or compliance-based and management will direct the objectives of the audit. An examination may include a comparison of approved project management processes with how the project is actually being managed.[32] Each project should be assessed for the appropriate level of control needed: too much control is too time consuming, too little control is very risky. If project control is not implemented correctly, the cost to the business should be clarified in terms of errors and fixes.\n\nControl systems are needed for cost, risk, quality, communication, time, change, procurement, and human resources. In addition, auditors should consider how important the projects are to the financial statements, how reliant the stakeholders are on controls, and how many controls exist. Auditors should review the development process and procedures for how they are implemented. The process of development and the quality of the final product may also be assessed if needed or requested. A business may want the auditing firm to be involved throughout the process to catch problems earlier on so that they can be fixed more easily. An auditor can serve as a controls consultant as part of the development team or as an independent auditor as part of an audit.\n\nBusinesses sometimes use formal systems development processes. These help assure that systems are developed successfully. A formal process is more effective in creating strong controls, and auditors should review this process to confirm that it is well designed and is followed in practice. A good formal systems development plan outlines:\n\nA strategy to align development with the organizations broader objectives\nStandards for new systems\nProject management policies for timing and budgeting\nProcedures describing the process\nEvaluation of quality of change\nTopics\nProject managers\nA project manager is a professional in the field of project management. Project managers can have the responsibility of the planning, execution, and closing of any project, typically relating to construction industry, engineering, architecture, computing, and telecommunications. Many other fields in production engineering, design engineering, and heavy industrial have project managers.\n\nA project manager is the person accountable for accomplishing the stated project objectives. Key project management responsibilities include creating clear and attainable project objectives, building the project requirements, and managing the triple constraint for projects, which is cost, time, and scope.\n\nA project manager is often a client representative and has to determine and implement the exact needs of the client, based on knowledge of the firm they are representing. The ability to adapt to the various internal procedures of the contracting party, and to form close links with the nominated representatives, is essential in ensuring that the key issues of cost, time, quality and above all, client satisfaction, can be realized.\n\nProject management types\nProject management can apply to any project, but it is often tailored to accommodate the specific needs of different and highly specialized industries. For example, the construction industry, which focuses on the delivery of things like buildings, roads, and bridges, has developed its own specialized form of project management that it refers to as Construction project management and for which project managers can become trained and certified in.[33] The Information technology industry has also evolved to develop its own form of Project management that is referred to as IT Project management and which specializes in the delivery of technical assets and services that are required to pass through various lifecycle phases such as planning, design, development, testing, and deployment. Biotechnology project management focuses on the intricacies of biotechnology research and development.[34]\n\nFor each type of project management, project managers develop and utilize repeatable templates that are specific to the industry they're dealing with. This allows project plans to become very thorough and highly repeatable, with the specific intent to increase quality, lower delivery costs, and lower time to deliver project results.\n\nWork breakdown structure\nMain article: Work breakdown structure\nThe work breakdown structure (WBS) is a tree structure that shows a subdivision of effort required to achieve an objectivefor example a program, project, and contract. The WBS may be hardware-, product-, service-, or process-oriented (see an example in a NASA reporting structure (2001)).[35]\n\nA WBS can be developed by starting with the end objective and successively subdividing it into manageable components in terms of size, duration, and responsibility (e.g., systems, subsystems, components, tasks, sub-tasks, and work packages), which include all steps necessary to achieve the objective.[20]\n\nThe work breakdown structure provides a common framework for the natural development of the overall planning and control of a contract and is the basis for dividing work into definable increments from which the statement of work can be developed and technical, schedule, cost, and labor hour reporting can be established.[35] The work breakdown structure can be displayed in two forms one in form of a table with subdivision of tasks two in form of an organisational chart.\n\nInternational standards\nThere have been several attempts to develop project management standards, such as:\n\nISO 21500: 2012  Guidance on project management. This is the first project management ISO.\nISO 31000: 2009  Risk management. Risk management is 1 of the 10 knowledge areas of either ISO 21500 or PMBoK5 concept of project management.\nISO/IEC/IEEE 16326-2009  Systems and Software EngineeringLife Cycle ProcessesProject Management[36]\nCapability Maturity Model from the Software Engineering Institute.\nGAPPS, Global Alliance for Project Performance Standards  an open source standard describing COMPETENCIES for project and program managers.\nA Guide to the Project Management Body of Knowledge from the Project Management Institute (PMI)\nHERMES method, Swiss general project management method, selected for use in Luxembourg and international organizations.\nThe ISO standards ISO 9000, a family of standards for quality management systems, and the ISO 10006:2003, for Quality management systems and guidelines for quality management in projects.\nPRINCE2, Projects IN Controlled Environments.\nAssociation for Project Management Body of Knowledge[37]\nTeam Software Process (TSP) from the Software Engineering Institute.\nTotal Cost Management Framework, AACE International's Methodology for Integrated Portfolio, Program and Project Management.\nV-Model, an original systems development method.\nThe Logical framework approach, which is popular in international development organizations.\n[Australian Institute of Project Management] AIPM has 4 levels of certification; CPPP, CPPM, CPPD & CPPE for Certified Practicing Project ... Partner, Manager, Director and Executive.\nProject portfolio management\nMain article: Project portfolio management\nAn increasing number of organizations are using, what is referred to as, project portfolio management (PPM) as a means of selecting the right projects and then using project management techniques[38] as the means for delivering the outcomes in the form of benefits to the performing private or not-for-profit organization.\n\nProject management software\nMain articles: Project management software and Project management information system\nProject management software is software used to help plan, organize, and manage resource pools, develop resource estimates and implement plans. Depending on the sophistication of the software, functionality may include estimation and planning, scheduling, cost control and budget management, resource allocation, collaboration software, communication, decision-making, workflow, quality management, documentation and/or administration systems.[39][40]\n\nVirtual project management\nMain article: Virtual team\nVirtual program management (VPM) is management of a project done by a virtual team, though it rarely may refer to a project implementing a virtual environment[41] It is noted that managing a virtual project is fundamentally different from managing traditional projects,[42] combining concerns of telecommuting and global collaboration (culture, timezones, language).[43]\n\nSee also\nLists\nComparison of project management software\nGlossary of project management\nList of collaborative software\nList of project management topics\nTimeline of project management\nComparison of Kanban software\nRelated fields\nArchitectural engineering\nConstruction management\nCost engineering\nFacilitation (business)\nIndustrial engineering\nProject management software\nProject portfolio management\nProject workforce management\nSoftware project management\nSystems engineering\nAgile Construction\nRelated subjects\nCollaborative project management\nEarned value management\nHuman factors\nProcess architecture\nProject accounting\nProject governance\nProgram management\nProject management simulation\nSmall-scale project management\nSoftware development process\nSystems Development Life Cycle (SDLC)", "skillName": "Business."}
{"id": 90, "category": "Business", "skillText": "Business intelligence\nBusiness administration\nSociety\nCompany Business Conglomerate\nBusiness organization\nBusiness entity\nCorporate governance\nCorporate titles\nEconomy\nCorporate law\nFinance\nAccounting\nTrade\nOrganization\nSociety\nTypes of management\nBusiness intelligence (BI) can be described as \"a set of techniques and tools for the acquisition and transformation of raw data into meaningful and useful information for business analysis purposes\".[1] The term \"data surfacing\" is also more often associated with BI functionality. BI technologies are capable of handling large amounts of unstructured data to help identify, develop and otherwise create new strategic business opportunities. The goal of BI is to allow for the easy interpretation of these large volumes of data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.[2]\n\nBI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies are reporting, online analytical processing, analytics, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics and prescriptive analytics.\n\nBI can be used to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions include priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a more complete picture which, in effect, creates an \"intelligence\" that cannot be derived by any singular set of data.[3] Amongst myriad uses, BI tools empower organisations to gain insight into new markets, assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts.[4]\nIllustration showing benefits of business intelligence in entrepreneurship. (created by Hamid Q)\n\n1\tComponents\n2\tHistory\n3\tData warehousing\n4\tComparison with competitive intelligence\n5\tComparison with business analytics\n6\tApplications in an enterprise\n7\tPrioritization of projects\n8\tSuccess factors of implementation\n8.1\tBusiness sponsorship\n8.2\tBusiness needs\n8.3\tAmount and quality of available data\n9\tUser aspect\n10\tBI Portals\n11\tMarketplace\n11.1\tIndustry-specific\n12\tSemi-structured or unstructured data\n12.1\tUnstructured data vs. semi-structured data\n12.2\tProblems with semi-structured or unstructured data\n12.3\tThe use of metadata\n13\tFuture\n14\tSee also\n15\tReferences\n16\tBibliography\n17\tExternal links\nComponents\nBusiness intelligence is made up of an increasing number of components including:\n\nMultidimensional aggregation and allocation\nDenormalization, tagging and standardization\nRealtime reporting with analytical alert\nA method of interfacing with unstructured data sources\nGroup consolidation, budgeting and rolling forecasts\nStatistical inference and probabilistic simulation\nKey performance indicators optimization\nVersion control and process management\nOpen item management\nHistory\nThe earliest known use of the term \"Business Intelligence\" is in Richard Millar Devens� in the �Cyclop�dia of Commercial and Business Anecdotes� from 1865. Devens used the term to describe how the banker, Sir Henry Furnese, gained profit by receiving and acting upon information about his environment, prior to his competitors. �Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the fall of Namur added to his profits, owing to his early receipt of the news.� (Devens, (1865), p. 210). The ability to collect and react accordingly based on the information retrieved, an ability that Furnese excelled in, is today still at the very heart of BI.[5]\n\nIn a 1958 article, IBM researcher Hans Peter Luhn used the term business intelligence. He employed the Webster's dictionary definition of intelligence: \"the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.\"[6]\n\nBusiness intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s. DSS originated in the computer-aided models created to assist with decision making and planning. From DSS, data warehouses, Executive Information Systems, OLAP and business intelligence came into focus beginning in the late 80s.\n\nIn 1989, Howard Dresner (later a Gartner analyst) proposed \"business intelligence\" as an umbrella term to describe \"concepts and methods to improve business decision making by using fact-based support systems.\"[7] It was not until the late 1990s that this usage was widespread.[8]\n\nData warehousing\nOften BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW sometimes combine as \"BI/DW\"[9] or as \"BIDW\". A data warehouse contains a copy of analytical data that facilitates decision support. However, not all data warehouses serve for business intelligence, nor do all business intelligence applications require a data warehouse.\n\nTo distinguish between the concepts of business intelligence and data warehouses, Forrester Research defines business intelligence in one of two ways:\n\nUsing a broad definition: \"Business Intelligence is a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.\"[10] Under this definition, business intelligence also includes technologies such as data integration, data quality, data warehousing, master-data management, text- and content-analytics, and many others that the market sometimes lumps into the \"Information Management\" segment. Therefore, Forrester refers to data preparation and data usage as two separate but closely linked segments of the business-intelligence architectural stack.\nForrester defines the narrower business-intelligence market as, \"...referring to just the top layers of the BI architectural stack such as reporting, analytics and dashboards.\"[11]\nComparison with competitive intelligence\nThough the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.[12]\n\nComparison with business analytics\nBusiness intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions.[13] One definition contrasts the two, stating that the term business intelligence refers to collecting business data to find information primarily through asking questions, reporting, and online analytical processes. Business analytics, on the other hand, uses statistical and quantitative tools for explanatory and predictive modeling.[14]\n\nIn an alternate definition, Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an \"alerts\" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.[15]\n\nApplications in an enterprise\nBusiness intelligence can be applied to the following business purposes, in order to drive business value.[citation needed]\n\nMeasurement � program that creates a hierarchy of performance metrics (see also Metrics Reference Model) and benchmarking that informs business leaders about progress towards business goals (business process management).\nAnalytics � program that builds quantitative processes for a business to arrive at optimal decisions and to perform business knowledge discovery. Frequently involves: data mining, process mining, statistical analysis, predictive analytics, predictive modeling, business process modeling, data lineage, complex event processing and prescriptive analytics.\nReporting/enterprise reporting � program that builds infrastructure for strategic reporting to serve the strategic management of a business, not operational reporting. Frequently involves data visualization, executive information system and OLAP.\nCollaboration/collaboration platform � program that gets different areas (both inside and outside the business) to work together through data sharing and electronic data interchange.\nKnowledge management � program to make the company data-driven through strategies and practices to identify, create, represent, distribute, and enable adoption of insights and experiences that are true business knowledge. Knowledge management leads to learning management and regulatory compliance.\nIn addition to the above, business intelligence can provide a pro-active approach, such as alert functionality that immediately notifies the end-user if certain conditions are met. For example, if some business metric exceeds a pre-defined threshold, the metric will be highlighted in standard reports, and the business analyst may be alerted via e-mail or another monitoring service. This end-to-end process requires data governance, which should be handled by the expert.[citation needed]\n\nPrioritization of projects\nIt can be difficult to provide a positive business case for business intelligence initiatives, and often the projects must be prioritized through strategic initiatives. BI projects can attain higher prioritization within the organization if managers consider the following:\n\nAs described by Kimball[16] the BI manager must determine the tangible benefits such as eliminated cost of producing legacy reports.\nData access for the entire organization must be enforced.[17] In this way even a small benefit, such as a few minutes saved, makes a difference when multiplied by the number of employees in the entire organization.\nAs described by Ross, Weil & Roberson for Enterprise Architecture,[18] managers should also consider letting the BI project be driven by other business initiatives with excellent business cases. To support this approach, the organization must have enterprise architects who can identify suitable business projects.\nUsing a structured and quantitative methodology to create defensible prioritization in line with the actual needs of the organization, such as a weighted decision matrix.[19]\nSuccess factors of implementation\nAccording to Kimball et al., there are three critical areas that organizations should assess before getting ready to do a BI project:[20]\n\nThe level of commitment and sponsorship of the project from senior management.\nThe level of business need for creating a BI implementation.\nThe amount and quality of business data available.\nBusiness sponsorship\nThe commitment and sponsorship of senior management is according to Kimball et al., the most important criteria for assessment.[21] This is because having strong management backing helps overcome shortcomings elsewhere in the project. However, as Kimball et al. state: �even the most elegantly designed DW/BI system cannot overcome a lack of business [management] sponsorship�.[22]\n\nIt is important that personnel who participate in the project have a vision and an idea of the benefits and drawbacks of implementing a BI system. The best business sponsor should have organizational clout and should be well connected within the organization. It is ideal that the business sponsor is demanding but also able to be realistic and supportive if the implementation runs into delays or drawbacks. The management sponsor also needs to be able to assume accountability and to take responsibility for failures and setbacks on the project. Support from multiple members of the management ensures the project does not fail if one person leaves the steering group. However, having many managers work together on the project can also mean that there are several different interests that attempt to pull the project in different directions, such as if different departments want to put more emphasis on their usage. This issue can be countered by an early and specific analysis of the business areas that benefit the most from the implementation. All stakeholders in the project should participate in this analysis in order for them to feel invested in the project and to find common ground.\n\nAnother management problem that may be encountered before the start of an implementation is an overly aggressive business sponsor. Problems of scope creep occur when the sponsor requests data sets that were not specified in the original planning phase.\n\nBusiness needs\nBecause of the close relationship with senior management, another critical thing that must be assessed before the project begins is whether or not there is a business need and whether there is a clear business benefit by doing the implementation.[23] The needs and benefits of the implementation are sometimes driven by competition and the need to gain an advantage in the market. Another reason for a business-driven approach to implementation of BI is the acquisition of other organizations that enlarge the original organization it can sometimes be beneficial to implement DW or BI in order to create more oversight.\n\nCompanies that implement BI are often large, multinational organizations with diverse subsidiaries.[24] A well-designed BI solution provides a consolidated view of key business data not available anywhere else in the organization, giving management visibility and control over measures that otherwise would not exist.\n\nAmount and quality of available data\nWithout proper data, or with too little quality data, any BI implementation fails; it does not matter how good the management sponsorship or business-driven motivation is. Before implementation it is a good idea to do data profiling. This analysis identifies the �content, consistency and structure [..]�[23] of the data. This should be done as early as possible in the process and if the analysis shows that data is lacking, put the project on hold temporarily while the IT department figures out how to properly collect data.\n\nWhen planning for business data and business intelligence requirements, it is always advisable to consider specific scenarios that apply to a particular organization, and then select the business intelligence features best suited for the scenario.\n\nOften, scenarios revolve around distinct business processes, each built on one or more data sources. These sources are used by features that present that data as information to knowledge workers, who subsequently act on that information. The business needs of the organization for each business process adopted correspond to the essential steps of business intelligence. These essential steps of business intelligence include but are not limited to:\n\nGo through business data sources in order to collect needed data\nConvert business data to information and present appropriately\nQuery and analyze data\nAct on the collected data\nThe quality aspect in business intelligence should cover all the process from the source data to the final reporting. At each step, the quality gates are different:\n\nSource Data:\nData Standardization: make data comparable (same unit, same pattern...)\nMaster Data Management: unique referential\nOperational Data Store (ODS):\nData Cleansing: detect & correct inaccurate data\nData Profiling: check inappropriate value, null/empty\nData warehouse:\nCompleteness: check that all expected data are loaded\nReferential integrity: unique and existing referential over all sources\nConsistency between sources: check consolidated data vs sources\nReporting:\nUniqueness of indicators: only one share dictionary of indicators\nFormula accuracy: local reporting formula should be avoided or checked\nUser aspect\nSome considerations must be made in order to successfully integrate the usage of business intelligence systems in a company. Ultimately the BI system must be accepted and utilized by the users in order for it to add value to the organization.[25][26] If the usability of the system is poor, the users may become frustrated and spend a considerable amount of time figuring out how to use the system or may not be able to really use the system. If the system does not add value to the users� mission, they simply don't use it.[26]\n\nTo increase user acceptance of a BI system, it can be advisable to consult business users at an early stage of the DW/BI lifecycle, for example at the requirements gathering phase.[25] This can provide an insight into the business process and what the users need from the BI system. There are several methods for gathering this information, such as questionnaires and interview sessions.\n\nWhen gathering the requirements from the business users, the local IT department should also be consulted in order to determine to which degree it is possible to fulfill the business's needs based on the available data.[25]\n\nTaking a user-centered approach throughout the design and development stage may further increase the chance of rapid user adoption of the BI system.[26]\n\nBesides focusing on the user experience offered by the BI applications, it may also possibly motivate the users to utilize the system by adding an element of competition. Kimball[25] suggests implementing a function on the Business Intelligence portal website where reports on system usage can be found. By doing so, managers can see how well their departments are doing and compare themselves to others and this may spur them to encourage their staff to utilize the BI system even more.\n\nIn a 2007 article, H. J. Watson gives an example of how the competitive element can act as an incentive.[27] Watson describes how a large call centre implemented performance dashboards for all call agents, with monthly incentive bonuses tied to performance metrics. Also, agents could compare their performance to other team members. The implementation of this type of performance measurement and competition significantly improved agent performance.\n\nBI chances of success can be improved by involving senior management to help make BI a part of the organizational culture, and by providing the users with necessary tools, training, and support.[27] Training encourages more people to use the BI application.[25]\n\nProviding user support is necessary to maintain the BI system and resolve user problems.[26] User support can be incorporated in many ways, for example by creating a website. The website should contain great content and tools for finding the necessary information. Furthermore, helpdesk support can be used. The help desk can be manned by power users or the DW/BI project team.[25]\n\nBI Portals\nA Business Intelligence portal (BI portal) is the primary access interface for Data Warehouse (DW) and Business Intelligence (BI) applications. The BI portal is the user's first impression of the DW/BI system. It is typically a browser application, from which the user has access to all the individual services of the DW/BI system, reports and other analytical functionality. The BI portal must be implemented in such a way that it is easy for the users of the DW/BI application to call on the functionality of the application.[28]\n\nThe BI portal's main functionality is to provide a navigation system of the DW/BI application. This means that the portal has to be implemented in a way that the user has access to all the functions of the DW/BI application.\n\nThe most common way to design the portal is to custom fit it to the business processes of the organization for which the DW/BI application is designed, in that way the portal can best fit the needs and requirements of its users.[29]\n\nThe BI portal needs to be easy to use and understand, and if possible have a look and feel similar to other applications or web content of the organization the DW/BI application is designed for (consistency).\n\nThe following is a list of desirable features for web portals in general and BI portals in particular:\n\nUsable\nUser should easily find what they need in the BI tool.\nContent Rich\nThe portal is not just a report printing tool, it should contain more functionality such as advice, help, support information and documentation.\nClean\nThe portal should be designed so it is easily understandable and not over-complex as to confuse the users\nCurrent\nThe portal should be updated regularly.\nInteractive\nThe portal should be implemented in a way that makes it easy for the user to use its functionality and encourage them to use the portal. Scalability and customization give the user the means to fit the portal to each user.\nValue Oriented\nIt is important that the user has the feeling that the DW/BI application is a valuable resource that is worth working on.\nMarketplace\nThere are a number of business intelligence vendors, often categorized into the remaining independent \"pure-play\" vendors and consolidated \"megavendors\" that have entered the market through a recent trend[30] of acquisitions in the BI industry.[31] The business intelligence market is gradually growing. In 2012 business intelligence services brought in $13.1 billion in revenue.[32]\n\nSome companies adopting BI software decide to pick and choose from different product offerings (best-of-breed) rather than purchase one comprehensive integrated solution (full-service).[33]\n\nIndustry-specific\nSpecific considerations for business intelligence systems have to be taken in some sectors such as governmental banking regulations or healthcare.[34] The information collected by banking institutions and analyzed with BI software must be protected from some groups or individuals, while being fully available to other groups or individuals. Therefore, BI solutions must be sensitive to those needs and be flexible enough to adapt to new regulations and changes to existing law.[citation needed]\n\nSemi-structured or unstructured data\nBusinesses create a huge amount of valuable information in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material and news. According to Merrill Lynch, more than 85% of all business information exists in these forms. These information types are called either semi-structured or unstructured data. However, organizations often only use these documents once.[35]\n\nThe management of semi-structured data is recognized as a major unsolved problem in the information technology industry.[36] According to projections from Gartner (2003), white collar workers spend anywhere from 30 to 40 percent of their time searching, finding and assessing unstructured data. BI uses both structured and unstructured data, but the former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision making.[36][37] Because of the difficulty of properly searching, finding and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task or project. This can ultimately lead to poorly informed decision making.[35]\n\nTherefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.[37]\n\nUnstructured data vs. semi-structured data\nUnstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row within each file or document.[citation needed]\n\nMany of these data types, however, like e-mails, word processing text files, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database. Therefore, it may be more accurate to talk about this as semi-structured documents or data,[36] but no specific consensus seems to have been reached.\n\nUnstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.\n\nProblems with semi-structured or unstructured data\nThere are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,[38] some of those are:\n\nPhysically accessing unstructured textual data � unstructured data is stored in a huge variety of formats.\nTerminology � Among researchers and analysts, there is a need to develop a standardized terminology.\nVolume of data � As stated earlier, up to 85% of all data exists as semi-structured data. Couple that with the need for word-to-word and semantic analysis.\nSearchability of unstructured textual data � A simple search on some data, e.g. apple, results in links where there is a reference to that precise search term. (Inmon & Nesavich, 2008)[38] gives an example: �a search is made on the term felony. In a simple search, the term felony is used, and everywhere there is a reference to felony, a hit to an unstructured document is made. But a simple search is crude. It does not find references to crime, arson, murder, embezzlement, vehicular homicide, and such, even though these crimes are types of felonies.�\nThe use of metadata\nTo solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata.[35] Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content � e.g. summaries, topics, people or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.\n\nFuture\nA 2009 paper predicted[39] these developments in the business intelligence market:\n\nBecause of lack of information, processes, and tools, through 2012, more than 35 percent of the top 5,000 global companies regularly fail to make insightful decisions about significant changes in their business and markets.\nBy 2012, business units will control at least 40 percent of the total budget for business intelligence.\nBy 2012, one-third of analytic applications applied to business processes will be delivered through coarse-grained application mashups.\nA 2009 Information Management special report predicted the top BI trends: \"green computing, social networking services, data visualization, mobile BI, predictive analytics, composite applications, cloud computing and multitouch.\".[40] Research undertaken in 2014 indicated that employees are more likely to have access to, and more likely to engage with, cloud-based BI tools than traditional tools.[41]\n\nOther business intelligence trends include the following:\n\nThird party SOA-BI products increasingly address ETL issues of volume and throughput.\nCompanies embrace in-memory processing, 64-bit processing, and pre-packaged analytic BI applications.\nOperational applications have callable BI components, with improvements in response time, scaling, and concurrency.\nNear or real time BI analytics is a baseline expectation.\nOpen source BI software replaces vendor offerings.\nOther lines of research include the combined study of business intelligence and uncertain data.[42][43] In this context, the data used is not assumed to be precise, accurate and complete. Instead, data is considered uncertain and therefore this uncertainty is propagated to the results produced by BI.\n\nAccording to a study by the Aberdeen Group, there has been increasing interest in Software-as-a-Service (SaaS) business intelligence over the past years, with twice as many organizations using this deployment approach as one year ago � 15% in 2009 compared to 7% in 2008.[44]\n\nAn article by InfoWorld�s Chris Kanaracus points out similar growth data from research firm IDC, which predicts the SaaS BI market will grow 22 percent each year through 2013 thanks to increased product sophistication, strained IT budgets, and other factors.[45]\n\nAn analysis of top 100 Business Intelligence and Analytics scores and ranks the firms based on several open variables[46]\n\nSee also\nAccounting intelligence\nAnalytic applications\nArtificial intelligence marketing\nBusiness Intelligence 2.0\nBusiness process discovery\nBusiness process management\nBusiness activity monitoring\nBusiness service management\nComparison of OLAP Servers\nCustomer dynamics\nData Presentation Architecture\nData visualization\nDecision engineering\nEnterprise planning systems\nInfonomics\nDocument intelligence\nIntegrated business planning\nLocation intelligence\nMedia intelligence\nMeteorological intelligence\nMobile business intelligence\nMultiway Data Analysis\nOperational intelligence\nBusiness Information Systems\nBusiness intelligence tools\nProcess mining\nReal-time business intelligence\nRuntime intelligence\nSales intelligence\nTest and learn", "skillName": "Business Intelligence."}
{"id": 91, "category": "Business", "skillText": "A job interview is a one-on-one interview consisting of a conversation between a job applicant and a representative of an employer which is conducted to assess whether the applicant should be hired.[1] Interviews are one of the most popularly used devices for employee selection.[2] Interviews vary in the extent to which the questions are structured, from a totally unstructured and free-wheeling conversation, to a structured interview in which an applicant is asked a predetermined list of questions in a specified order; [3] structured interviews are usually more accurate predictors of which applicants will make good employees, according to research studies.[4]\nA pie chart showing the reasons why job interview candidates are rejected.\n\nA job interview typically precedes the hiring decision. The interview is usually preceded by the evaluation of submitted résumés from interested candidates, possibly by examining job applications or reading many resumes. Next, after this screening, a small number of candidates for interviews is selected.\n\nPotential job interview opportunities also include networking events and career fairs. The job interview is considered one of the most useful tools for evaluating potential employees.[5] It also demands significant resources from the employer, yet has been demonstrated to be notoriously unreliable in identifying the optimal person for the job.[5] An interview also allows the candidate to assess the corporate culture and demands of the job.\n\nMultiple rounds of job interviews and/or other candidate selection methods may be used where there are many candidates or the job is particularly challenging or desirable. Earlier rounds sometimes called 'screening interviews' may involve fewer staff from the employers and will typically be much shorter and less in-depth. An increasingly common initial interview approach is the telephone interview. This is especially common when the candidates do not live near the employer and has the advantage of keeping costs low for both sides. Since 2003, interviews have been held through video conferencing software, such as Skype.[6] Once all candidates have been interviewed, the employer typically selects the most desirable candidate(s) and begins the negotiation of a job offer.\n\nContents\n\n    1 Interview constructs\n    2 Interview process\n    3 Types of interview\n        3.1 Structured interview\n            3.1.1 Situational interview questions\n            3.1.2 Behavioral interview questions\n            3.1.3 Other types of interview questions\n        3.2 Specialized interview formats\n            3.2.1 Case\n            3.2.2 Panel\n            3.2.3 Stress\n            3.2.4 Technical\n        3.3 Other interview modes\n            3.3.1 Telephone\n            3.3.2 Video\n    4 Interviewee strategies and behaviors\n        4.1 Nonverbal behaviors\n        4.2 Physical attractiveness\n        4.3 Coaching\n        4.4 Faking\n        4.5 Narcissism\n        4.6 Psychopathy\n    5 Factors impacting on interview effectiveness\n        5.1 Validity and predictive power\n            5.1.1 Interview structure issues\n            5.1.2 Interviewer rating reliability\n        5.2 Applicant reactions\n            5.2.1 Interview design\n            5.2.2 Types of questions\n            5.2.3 Additional factors\n        5.3 Interview anxiety\n            5.3.1 Implications for applicants\n            5.3.2 Implications for organizations\n    6 Legal issues\n        6.1 Applicants with disabilities\n        6.2 Other applicant discrimination: Weight and pregnancy\n    7 See also\n    8 References\n    9 External links\n\nInterview constructs\n\nResearchers have attempted to identify which interview strategies or \"constructs\" can help interviewers choose the best candidate. Research suggests that interviews capture a wide variety of applicant attributes.[7][8][9] Constructs can be classified into three categories: job-relevant content, interviewee performance (behavior unrelated to the job but which influences the evaluation), and job-irrelevant interviewer biases.[10]\n\nJob-relevant interview content Interview questions are generally designed to tap applicant attributes that are specifically relevant to the job for which the person is applying. The job-relevant applicant attributes that the questions purportedly assess are thought to be necessary for one to successfully perform on the job. The job-relevant constructs that have been assessed in the interview can be classified into three categories: general traits, experiential factors, and core job elements. The first category refers to relatively stable applicant traits. The second category refers to job knowledge that the applicant has acquired over time. The third category refers to the knowledge, skills, and abilities associated with the job.\n\nGeneral traits:\n\n    Mental ability: Applicants' capacity to learn and process information[8]\n    Personality: Conscientiousness, agreeableness, emotional stability, extroversion, openness to new experiences[7][8][9]\n    Interest, goals, and values: Applicant motives, goals, and person-organization fit[8]\n\nExperiential factors:\n\n    Experience: Job-relevant knowledge derived from prior experience[8][9]\n    Education: Job-relevant knowledge derived from prior education\n    Training: Job-relevant knowledge derived from prior training\n\nCore job elements:\n\n    Declarative knowledge: Applicants' learned knowledge[9]\n    Procedural skills and abilities: Applicants' ability to complete the tasks required to do the job[11]\n    Motivation: Applicants' willingness to exert the effort required to do the job[12]\n\nInterviewee performance Interviewer evaluations of applicant responses also tend to be colored by how an applicant behaves in the interview. These behaviors may not be directly related to the constructs the interview questions were designed to assess, but can be related to aspects of the job for which they are applying. Applicants without realizing it may engage in a number of behaviors that influence ratings of their performance. The applicant may have acquired these behaviors during training or from previous interview experience. These interviewee performance constructs can also be classified into three categories: social effectiveness skills, interpersonal presentation, and personal/contextual factors.\n\nSocial effectiveness skills:\n\n    Impression management: Applicants' attempt to make sure the interviewer forms a positive impression of them[13][14]\n    Social skills: Applicants' ability to adapt his/her behavior according to the demands of the situation to positively influence the interviewer[15]\n    Self-monitoring: Applicants' regulation of behaviors to control the image presented to the interviewer[16]\n    Relational control: Applicants' attempt to control the flow of the conversation[17]\n\nInterpersonal Presentation:\n\n    Verbal expression: Pitch, rate, pauses[18]\n    Nonverbal behavior: Gaze, smile, hand movement, body orientation[19]\n\nPersonal/contextual factors:\n\n    Interview training: Coaching, mock interviews with feedback[20]\n    Interview experience: Number of prior interviews[21]\n    Interview self-efficacy: Applicants' perceived ability to do well in the interview[22]\n    Interview motivation: Applicants' motivation to succeed in an interview[23]\n\nJob-irrelevant interviewer biases The following are personal and demographic characteristics that can potentially influence interviewer evaluations of interviewee responses. These factors are typically not relevant to whether the individual can do the job (that is, not related to job performance), thus, their influence on interview ratings should be minimized or excluded. In fact, there are laws in many countries that prohibit consideration of many of these protected classes of people when making selection decisions. Using structured interviews with multiple interviewers coupled with training may help reduce the effect of the following characteristics on interview ratings.[24] The list of job-irrelevant interviewer biases is presented below.\n\n    Attractiveness: Applicant physical attractiveness can influence interviewer's evaluation of one's interview performance[19]\n    Race: Whites tend to score higher than Blacks and Hispanics;[25] racial similarity between interviewer and applicant, on the other hand, has not been found to influence interview ratings[24][26]\n    Gender: Females tend to receive slightly higher interview scores than their male counterparts;[7] gender similarity does not seem to influence interview ratings[24]\n    Similarities in background and attitudes: Interviewers perceived interpersonal attraction was found to influence interview ratings[27]\n    Culture: Applicants with an ethnic name and a foreign accent were viewed less favorably than applicants with just an ethnic name and no accent or an applicant with a traditional name with or without an accent[28]\n\nThe extent to which ratings of interviewee performance reflect certain constructs varies widely depending on the level of structure of the interview, the kind of questions asked, interviewer or applicant biases, applicant professional dress or nonverbal behavior, and a host of other factors. For example, some research suggests that applicant's cognitive ability, education, training, and work experiences may be better captured in unstructured interviews, whereas applicant's job knowledge, organizational fit, interpersonal skills, and applied knowledge may be better captured in a structured interview.[8]\n\nFurther, interviews are typically designed to assess a number of constructs. Given the social nature of the interview, applicant responses to interview questions and interviewer evaluations of those responses are sometimes influenced by constructs beyond those the questions were intended to assess, making it extremely difficult to tease out the specific constructs measured during the interview.[29] Reducing the number of constructs the interview is intended to assess may help mitigate this issue. Moreover, of practical importance is whether the interview is a better measure of some constructs in comparison to paper and pencil tests of the same constructs. Indeed, certain constructs (mental ability and skills, experience) may be better measured with paper and pencil tests than during the interview, whereas personality-related constructs seem to be better measured during the interview in comparison to paper and pencil tests of the same personality constructs.[30] In sum, the following is recommended: Interviews should be developed to assess the job relevant constructs identified in the job analysis.[31][32]\nInterview process\nPeople waiting to be interviewed at an employment agency.\n\nOne way to think about the interview process is as three separate, albeit related, phases: (1) the preinterview phase which occurs before the interviewer and candidate meet, (2) the interview phase where the interview is conducted, and (3) the postinterview phase where the interviewer forms judgments of candidate qualifications and makes final decisions.[33] Although separate, these three phases are related. That is, impressions interviewers form early on may affect how they view the person in a later phase.\n\nPreinterview phase: The preinterview phase encompasses the information available to the interviewer beforehand (e.g., resumes, test scores, social networking site information) and the perceptions interviewers form about applicants from this information prior to the actual face-to-face interaction between the two individuals. In this phase, interviewers are likely to already have ideas about the characteristics that would make a person ideal or qualified for the position.[34] Interviewers also have information about the applicant usually in the form of a resume, test scores, or prior contacts with the applicant.[33] Interviewers then often integrate information that they have on an applicant with their ideas about the ideal employee to form a preinterview evaluation of the candidate. In this way, interviewers typically have an impression of you even before the actual face-to-face interview interaction. Nowadays with recent technological advancements, we must be aware that interviewers have an even larger amount of information available on some candidates. For example, interviewers can obtain information from search engines (e.g. Google, Bing, Yahoo), blogs, and even social networks (e.g. Linkedin, Facebook, Twitter). While some of this information may be job-related, some of it may not be. In some cases, a review of Facebook may reveal undesirable behaviors such as drunkenness or drug use. Despite the relevance of the information, any information interviewers obtain about the applicant before the interview is likely to influence their preinterview impression of the candidate. And, why is all this important? It is important because what interviewers think about you before they meet you, can have an effect on how they might treat you in the interview and what they remember about you.[33][35] Furthermore, researchers have found that what interviewers think about the applicant before the interview (preinterview phase) is related to how they evaluate the candidate after the interview, despite how the candidate may have performed during the interview.[36]\n\nInterview phase: The interview phase entails the actual conduct of the interview, the interaction between the interviewer and the applicant. Initial interviewer impressions about the applicant before the interview may influence the amount of time an interviewer spends in the interview with the applicant, the interviewer’s behavior and questioning of the applicant,[37] and the interviewer’s postinterview evaluations.[36] Preinterview impressions also can affect what the interviewer notices about the interviewee, recalls from the interview, and how an interviewer interprets what the applicant says and does in the interview.[35]\n\nAs interviews are typically conducted face-to-face, over the phone, or through video conferencing[38] (e.g. Skype), they are a social interaction between at least two individuals. Thus, the behavior of the interviewer during the interview likely \"leaks\" information to the interviewee. That is, you can sometimes tell during the interview whether the interviewer thinks positively or negatively about you.[33] Knowing this information can actually affect how the applicant behaves, resulting in a self-fulfilling prophecy effect.[37][39] For example, interviewees who feel the interviewer does not think they are qualified may be more anxious and feel they need to prove they are qualified. Such anxiety may hamper how well they actually perform and present themselves during the interview, fulfilling the original thoughts of the interviewer. Alternatively, interviewees who perceive an interviewer believes they are qualified for the job may feel more at ease and comfortable during the exchange, and consequently actually perform better in the interview. It should be noted again, that because of the dynamic nature of the interview, the interaction between the behaviors and thoughts of both parties is a continuous process whereby information is processed and informs subsequent behavior, thoughts, and evaluations.\n\nPostinterview phase: After the interview is conducted, the interviewer must form an evaluation of the interviewee’s qualifications for the position. The interviewer most likely takes into consideration all the information, even from the preinterview phase, and integrates it to form a postinterview evaluation of the applicant. In the final stage of the interview process, the interviewer uses his/her evaluation of the candidate (i.e., in the form of interview ratings or judgment) to make a final decision. Sometimes other selection tools (e.g., work samples, cognitive ability tests, personality tests) are used in combination with the interview to make final hiring decisions; however, interviews remain the most commonly used selection device in North America.[40]\n\nFor interviewees: Although the description of the interview process above focuses on the perspective of the interviewer, job applicants also gather information on the job and/or organization and form impressions prior to the interview.[34] The interview is a two-way exchange and applicants are also making decisions about whether the company is a good fit for them. Essentially, the process model illustrates that the interview is not an isolated interaction, but rather a complex process that begins with two parties forming judgments and gathering information, and ends with a final interviewer decision.\nTypes of interview\n\nInterviews may be 'structured', 'unstructured' and composite: i.e. comprising a combination of both types or interview approach. There are also a large range of more specialised interview approaches. Interviews typically take place face-to-face, though other modes such as telephone or video are increasingly used.\nStructured interview\n\nIn interviews that are considered \"structured interviews,\" there are several types of questions interviewers ask applicants. Two major types are situational questions[41] and behavioral questions (also known as patterned behavioral description interviews).[42] Both types of questions are based on \"critical incidents\" that are required to perform the job[43] but they differ in their focus (see below for descriptions). Critical incidents are relevant tasks that are required for the job and can be collected through interviews or surveys with current employees, managers, or subject matter experts[44][45] One of the first critical incidents techniques ever used in the United States Army asked combat veterans to report specific incidents of effective or ineffective behavior of a leader. The question posed to veterans was \"Describe the officer’s actions. What did he do?\" Their responses were compiled to create a factual definition or \"critical requirements\" of what an effective combat leader is.[43]\n\nPrevious meta-analyses have found mixed results for which type of question will best predict future job performance of an applicant. For example, some studies have shown that situational type questions have better predictability for job performance in interviews,[46][47][48] while, other researchers have found that behavioral type questions are better at predicting future job performance of applicants.[49] In actual interview settings it is not likely that the sole use of just one type of interview question (situational or behavioral) is asked. A range of questions can add variety for both the interviewer and applicant.[45] In addition, the use of high-quality questions, whether behavioral or situational based, is essential to make sure that candidates provide meaningful responses that lead to insight into their capability to perform on the job.[50]\nSituational interview questions\n\nSituational interview questions[41] ask job applicants to imagine a set of circumstances and then indicate how they would respond in that situation; hence, the questions are future oriented. One advantage of situational questions is that all interviewees respond to the same hypothetical situation rather than describe experiences unique to them from their past. Another advantage is that situational questions allow respondents who have had no direct job experience relevant to a particular question to provide a hypothetical response.[51] Two core aspects of the SI are the development of situational dilemmas that employees encounter on the job, and a scoring guide to evaluate responses to each dilemma.[52]\nBehavioral interview questions\n\nBehavioral (experience-based or patterned behavioral) interviews are past-oriented in that they ask respondents to relate what they did in past jobs or life situations that are relevant to the particular job relevant knowledge, skills, and abilities required for success.[53][54] The idea is that past behavior is the best predictor of future performance in similar situations. By asking questions about how job applicants have handled situations in the past that are similar to those they will face on the job, employers can gauge how they might perform in future situations.[51]\n\nBehavioral interview questions include:[55]\n\n    Describe a situation in which you were able to use persuasion to successfully convince someone to see things your way.\n    Give me an example of a time when you set a goal and were able to meet or achieve it.\n    Tell me about a time when you had to use your presentation skills to influence someone's opinion.\n    Give me an example of a time when you had to conform to a policy with which you did not agree.\n\nExamples include the STAR and SOARA techniques.\nOther types of interview questions\n\nOther possible types of questions that may be asked alongside structured interview questions or in a separate interview include: background questions, job experience questions, and puzzle type questions. A brief explanation of each follows.\n\n    Background questions include a focus on work experience, education, and other qualifications.[56] For instance, an interviewer may ask \"What experience have you had with direct sales phone calls?\"\n    Job experience questions may ask candidates to describe or demonstrate job knowledge. These are typically highly specific questions.[57] For example, one question may be \"What steps would you take to conduct a manager training session on safety?\"\n    The puzzle interview was popularized by Microsoft in the 1990s, and is now used in other organizations. The most common types of questions either ask the applicant to solve puzzles or brain teasers (e.g., \"Why are manhole covers round?\") or to solve unusual problems (e.g., \"How would you weigh an airplane without a scale?\").[58]\n\nSpecialized interview formats\nCase\nFurther information: Case interview\n\nA case interview is an interview form used mostly by management consulting firms and investment banks in which the job applicant is given a question, situation, problem or challenge and asked to resolve the situation. The case problem is often a business situation or a business case that the interviewer has worked on in real life. In recent years, company in other sectors like Design, Architecture, Marketing, Advertising, Finance and Strategy have adopted a similar approach to interviewing candidates. Technology has transformed the Case-based and Technical interview process from a purely private in-person experience to an online exchange of job skills and endorsements.\nPanel\n\nAnother type of job interview found throughout the professional and academic ranks is the panel interview. In this type of interview the candidate is interviewed by a group of panelists representing the various stakeholders in the hiring process. Within this format there are several approaches to conducting the interview. Example formats include;\n\n    Presentation format – The candidate is given a generic topic and asked to make a presentation to the panel. Often used in academic or sales-related interviews.\n    Role format – Each panelist is tasked with asking questions related to a specific role of the position. For example, one panelist may ask technical questions, another may ask management questions, another may ask customer service related questions etc.\n    Skeet shoot format – The candidate is given questions from a series of panelists in rapid succession to test his or her ability to handle stress filled situations.\n\nThe benefits of the panel approach to interviewing include: time savings over serial interviewing, more focused interviews as there is often less time spend building rapport with small talk, and \"apples to apples\" comparison because each stake holder/interviewer/panelist gets to hear the answers to the same questions.[59]\nStress\n\nStress interviews are still in common use. One type of stress interview is where the employer uses a succession of interviewers (one at a time or en masse) whose mission is to intimidate the candidate and keep him/her off-balance. The ostensible purpose of this interview: to find out how the candidate handles stress. Stress interviews might involve testing an applicant's behavior in a busy environment. Questions about handling work overload, dealing with multiple projects, and handling conflict are typical.[60]\n\nAnother type of stress interview may involve only a single interviewer who behaves in an uninterested or hostile manner. For example, the interviewer may not make eye contact, may roll his eyes or sigh at the candidate's answers, interrupt, turn his back, take phone calls during the interview, or ask questions in a demeaning or challenging style. The goal is to assess how the interviewee handles pressure or to purposely evoke emotional responses. This technique was also used in research protocols studying stress and type A (coronary-prone) behavior because it would evoke hostility and even changes in blood pressure and heart rate in study subjects. The key to success for the candidate is to de-personalize the process. The interviewer is acting a role, deliberately and calculatedly trying to \"rattle the cage\". Once the candidate realizes that there is nothing personal behind the interviewer's approach, it is easier to handle the questions with aplomb.\n\nExample stress interview questions:\n\n    Sticky situation: \"If you caught a colleague cheating on his expenses, what would you do?\"\n    Putting one on the spot: \"How do you feel this interview is going?\"\n    \"Popping the balloon\": (deep sigh) \"Well, if that's the best answer you can give ... \" (shakes head) \"Okay, what about this one ...?\"\n    Oddball question: \"What would you change about the design of the hockey stick?\"\n    Doubting one's veracity: \"I don't feel like we're getting to the heart of the matter here. Start again – tell me what really makes you tick.\"\n\nCandidates may also be asked to deliver a presentation as part of the selection process. One stress technique is to tell the applicant that they have 20 minutes to prepare a presentation, and then come back to room five minutes later and demand that the presentation be given immediately. The \"Platform Test\" method involves having the candidate make a presentation to both the selection panel and other candidates for the same job. This is obviously highly stressful and is therefore useful as a predictor of how the candidate will perform under similar circumstances on the job. Selection processes in academic, training, airline, legal and teaching circles frequently involve presentations of this sort.\nTechnical\nFurther information: Microsoft Interview\n\nThis kind of interview focuses on problem solving and creativity. The questions aim at the interviewee's problem-solving skills and likely show their ability in solving the challenges faced in the job through creativity. Technical interviews are being conducted online at progressive companies before in-person talks as a way to screen job applicants.\nOther interview modes\nTelephone\nMain article: Telephone interview\n\nTelephone interviews take place if a recruiter wishes to reduce the number of prospective candidates before deciding on a shortlist for face-to-face interviews. They also take place if a job applicant is a significant distance away from the premises of the hiring company, such as abroad or in another state or province.\nVideo\n\nVideo interviews are a modern variation of telephone interviews. Prospective candidates are asked preset questions using computer software then their immediate responses are recorded. These responses are then viewed and evaluated by recruiters to form a shortlist of suitable candidates for face-to-face interviews.\nInterviewee strategies and behaviors\n\nWhile preparing for an interview, prospective employees usually look at what the job posting or job description says in order to get a better understanding of what is expected of them should they get hired. Exceptionally good interviewees look at the wants and needs of a job posting and show off how good they are at those abilities during the interview to impress the interviewer and increase their chances of getting a job.\n\nResearching the company itself is also a good way for interviewees to impress lots of people during an interview. It shows the interviewer that the interviewee is not only knowledgeable about the company's goals and objectives, but also that the interviewee has done their homework and that they make a great effort when they are given an assignment. Researching about the company makes sure that employees are not entirely clueless about the company they are applying for, and at the end of the interview, the interviewee might ask some questions to the interviewer about the company, either to learn more information or to clarify on some points that they might have found during their research. In any case, it impresses the interviewer and it shows that the interviewee is willing to learn more about the company.\n\nMost interviewees also find that practising answering the most common questions asked in interviews helps them prepare for the real one. It minimizes the chance of their being caught off-guard regarding certain questions, prepares their minds to convey the right information in the hopes of impressing the interviewer, and also makes sure that they do not accidentally say something that might not be suitable in an interview situation. The use of head mounted displays such as Google Cardboard, a virtual reality platform in which the user is immersed in a variety of realistic environments, is a new tool used to practice answering interview questions.[61]\n\nInterviewees are generally dressed properly in business attire for the interview, so as to look professional in the eyes of the interviewer. They also bring their résumé, cover letter and references to the interview to supply the interviewer the information they need, and to also cover them in case they forgot to bring any of the papers. Items like cellphones, a cup of coffee and chewing gum are not recommended to bring to an interview, as it can lead to the interviewer perceiving the interviewee as unprofessional and in some cases, even rude.\n\nAbove all, interviewees should be confident and courteous to the interviewer, as they are taking their time off work to participate in the interview. An interview is often the first time an interviewer looks at the interviewee first hand, so it is important to make a good first impression.[62]\nNonverbal behaviors\n\nIt may not only be what you say in an interview that matters, but also how you say it (e.g., how fast you speak) and how you behave during the interview (e.g., hand gestures, eye contact). In other words, although applicants’ responses to interview questions influence interview ratings,[63] their nonverbal behaviors may also affect interviewer judgments.[64] Nonverbal behaviors can be divided into two main categories: vocal cues (e.g., articulation, pitch, fluency, frequency of pauses, speed, etc.) and visual cues (e.g., smiling, eye contact, body orientation and lean, hand movement, posture, etc.).[65] Oftentimes physical attractiveness is included as part of nonverbal behavior as well.[65] There is some debate about how large a role nonverbal behaviors may play in the interview. Some researchers maintain that nonverbal behaviors affect interview ratings a great deal,[63] while others have found that they have a relatively small impact on interview outcomes, especially when considered with applicant qualifications presented in résumés.[66] The relationship between nonverbal behavior and interview outcomes is also stronger in structured interviews than unstructured,[67] and stronger when interviewees’ answers are of high quality.[66]\n\nApplicants’ nonverbal behaviors may influence interview ratings through the inferences interviewers make about the applicant based on their behavior. For instance, applicants who engage in positive nonverbal behaviors such as smiling and leaning forward are perceived as more likable, trustworthy, credible,[65] warmer, successful, qualified, motivated, competent,[68] and social skills.[69] These applicants are also predicted to be better accepted and more satisfied with the organization if hired.[68]\n\nApplicants’ verbal responses and their nonverbal behavior may convey some of the same information about the applicant.[64] However, despite any shared information between content and nonverbal behavior, it is clear that nonverbal behaviors do predict interview ratings to an extent beyond the content of what was said, and thus it is essential that applicants and interviewers alike are aware of their impact. You may want to be careful of what you may be communicating through the nonverbal behaviors you display.\nPhysical attractiveness\n\nTo hire the best applicants for the job, interviewers form judgments, sometimes using applicants’ physical attractiveness. That is, physical attractiveness is usually not necessarily related to how well one can do the job, yet has been found to influence interviewer evaluations and judgments about how suitable an applicant is for the job. Once individuals are categorized as attractive or unattractive, interviewers may have expectations about physically attractive and physically unattractive individuals and then judge applicants based on how well they fit those expectations.[70] As a result, it typically turns out that interviewers will judge attractive individuals more favorably on job-related factors than they judge unattractive individuals. People generally agree on who is and who is not attractive and attractive individuals are judged and treated more positively than unattractive individuals.[71] For example, people who think another is physically attractive tend to have positive initial impressions of that person (even before formally meeting them), perceive the person to be smart, socially competent, and have good social skills and general mental health.[70]\n\nWithin the business domain, physically attractive individuals have been shown to have an advantage over unattractive individuals in numerous ways, that include, but are not limited to, perceived job qualifications, hiring recommendations, predicted job success, and compensation levels.[70] As noted by several researchers, attractiveness may not be the most influential determinant of personnel decisions, but may be a deciding factor when applicants possess similar levels of qualifications.[70] In addition, attractiveness does not provide an advantage if the applicants in the pool are of high quality, but it does provide an advantage in increased hiring rates and more positive job-related outcomes for attractive individuals when applicant quality is low and average.[72]\n\nVocal Attractiveness Just as physical attractiveness is a visual cue, vocal attractiveness is an auditory cue and can lead to differing interviewer evaluations in the interview as well. Vocal attractiveness, defined as an appealing mix of speech rate, loudness, pitch, and variability, has been found to be favorably related to interview ratings and job performance.[73][74] In addition, the personality traits of agreeableness and conscientiousness predict performance more strongly for people with more attractive voices compared to those with less attractive voices.[73]\n\nAs important as it is to understand how physical attractiveness can influence the judgments, behaviors, and final decisions of interviewers, it is equally important to find ways to decrease potential bias in the job interview. Conducting an interview with elements of structure is a one possible way to decrease bias.[75]\nCoaching\n\nAn abundance of information is available to instruct interviewees on strategies for improving their performance in a job interview. Information used by interviewees comes from a variety of sources ranging from popular how-to books to formal coaching programs, sometimes even provided by the hiring organization. Within the more formal coaching programs, there are two general types of coaching. One type of coaching is designed to teach interviewees how to perform better in the interview by focusing on how to behave and present oneself. This type of coaching is focused on improving aspects of the interview that are not necessarily related to the specific elements of performing the job tasks. This type of coaching could include how to dress, how to display nonverbal behaviors (head nods, smiling, eye contact), verbal cues (how fast to speak, speech volume, articulation, pitch), and impression management tactics. Another type of coaching is designed to focus interviewees on the content specifically relevant to describing one’s qualifications for the job, in order to help improve their answers to interview questions. This coaching, therefore, focuses on improving the interviewee’s understanding of the skills, abilities, and traits the interviewer is attempting to assess, and responding with relevant experience that demonstrates these skills.[76] For example, this type of coaching might teach an interviewee to use the STAR approach for answering behavioral interview questions.[77]\n\nA coaching program might include several sections focusing on various aspects of the interview. It could include a section designed to introduce interviewees to the interview process, and explain how this process works (e.g., administration of interview, interview day logistics, different types of interviews, advantages of structured interviews). It could also include a section designed to provide feedback to help the interviewee to improve their performance in the interview, as well as a section involving practice answering example interview questions. An additional section providing general interview tips about how to behave and present oneself could also be included.[78]\n\nIt is useful to consider coaching in the context of the competing goals of the interviewer and interviewee. The interviewee’s goal is typically to perform well (i.e. obtain high interview ratings), in order to get hired. On the other hand, the interviewer’s goal is to obtain job-relevant information, in order to determine whether the applicant has the skills, abilities, and traits believed by the organization to be indicators of successful job performance.[76] Research has shown that how well an applicant does in the interview can be enhanced with coaching.[76][79][80][81] The effectiveness of coaching is due, in part, to increasing the interviewee’s knowledge, which in turn results in better interview performance. Interviewee knowledge refers to knowledge about the interview, such as the types of questions that will be asked, and the content that the interviewer is attempting to assess.[82] Research has also shown that coaching can increase the likelihood that interviewers using a structured interview will accurately choose those individuals who will ultimately be most successful on the job (i.e., increase reliability and validity of the structured interview).[76] Additionally, research has shown that interviewees tend to have positive reactions to coaching, which is often an underlying goal of an interview.[78] Based on research thus far, the effects of coaching tend to be positive for both interviewees and interviewers.\nFaking\n\nInterviewers should be aware that applicants can intentionally distort their responses or fake during the interview and such applicant faking has the potential to influence interview outcomes if present. Two concepts that relate to faking include social desirability (the tendency for people to present themselves in a favorable light[83]), and impression management (conscious or unconscious attempts to influence one’s image during interactions[84]). Faking in the employment interview, then, can be defined as \"deceptive impression management or the conscious distortion of answers to the interview questions in order to obtain a better score on the interview and/or otherwise create favorable perceptions\".[85] Thus, faking in the employment interview is intentional, deceptive, and aimed at improving perceptions of performance.\n\nFaking in the employment interview can be broken down into four elements.[85] The first involves the interviewee portraying him or herself as an ideal job candidate by exaggerating true skills, tailoring answers to better fit the job, and/or creating the impression that personal beliefs, values, and attitudes are similar to those of the organization.\n\nThe second aspect of faking is inventing or completely fabricating one’s image by piecing distinct work experiences together to create better answers, inventing untrue experiences or skills, and portraying others’ experiences or accomplishments as ones’ own.\n\nThirdly, faking might also be aimed at protecting the applicant’s image. This can be accomplished through omitting certain negative experiences, concealing negatively perceived aspects of the applicant’s background, and by separating oneself from negative experiences.\n\nThe fourth and final component of faking involves ingratiating oneself to the interviewer by conforming personal opinions to align with those of the organization, as well as insincerely praising or complimenting the interviewer or organization.\n\nOf all of the various faking behaviors listed, ingratiation tactics were found to be the most prevalent in the employment interview, while flat out making up answers or claiming others’ experiences as one’s own is the least common.[85] However, fabricating true skills appears to be at least somewhat prevalent in employment interviews. One study found that over 80% of participants lied about job-related skills in the interview,[86] presumably to compensate for a lack of job-required skills/traits and further their chances for employment.\n\nMost importantly, faking behaviors have been shown to affect outcomes of employment interviews. For example, the probability of getting another interview or job offer increases when interviewees make up answers.[85]\n\nDifferent interview characteristics also seem to impact the likelihood of faking. Faking behavior is less prevalent, for instance, in past behavioral interviews than in situational interviews, although follow-up questions increased faking behaviors in both types of interviews. Therefore, if practitioners are interested in decreasing faking behaviors among job candidates in employment interview settings, they should utilize structured, past behavioral interviews and avoid the use of probes or follow-up questions.[85]\nNarcissism\nMain article: Narcissism in the workplace\n\nNarcissists typically perform well at job interviews and have a good success rate for landing jobs. Interviews are one of the few social situations where narcissistic behaviours such as boasting actually create a positive impression.[87]\nPsychopathy\nMain article: Psychopathy in the workplace\nSee also: Superficial charm\n\nCorporate psychopaths are readily recruited into organisations because they make a distinctly positive impression at interviews.[88] They appear to be alert, friendly and easy to get along with and talk to. They look like they are of good ability, emotionally well adjusted and reasonable, and these traits make them attractive to those in charge of hiring staff within organisations. Other researchers confirm that psychopaths can present themselves as likeable and personally attractive.[89] Companies often rely on interview performance alone and do not conduct other checks such as taking references. Being accomplished liars helps psychopaths obtain the jobs they want.[90]\nFactors impacting on interview effectiveness\nValidity and predictive power\n\nThere is extant data which puts into question the value of job interviews as a tool for selecting employees. Where the aim of a job interview is ostensibly to choose a candidate who will perform well in the job role, other methods of selection provide greater predictive power and often lower costs.[91]\nInterview structure issues\n\nWhile unstructured interviews are commonly used, structured interviews have yielded much better results and are considered a best practice.[92] Interview structure is defined as \"the reduction in procedural variance across applicants, which can translate into the degree of discretion that an interviewer is allowed in conducting the interview\".[93] Structure in an interview can be compared to a typical paper and pencil test: we would not think it was fair if every test taker was given different questions and a different number of questions on an exam, or if their answers were each graded differently. Yet this is exactly what occurs in an unstructured interview; thus, a structured interview attempts to standardize this popular selection tool. While there is debate surrounding what is meant specifically by a structured interview,[94] there are typically two broad categories of standardization: 1) content structure, and 2) evaluation structure.[95] Content structure includes elements that refer to the actual content of the interview:\n\n    Base questions on attributes that are representative of the job, as indicated by a job analysis\n    Ask the same questions of all interviewees\n    Limit prompting, or follow up questions, that interviewers may ask\n    Ask better questions, such as behavioral description questions\n    Have a longer interview\n    Control ancillary information available to the interviewees, such as resumes\n    Don’t allow questions from applicants during interview\n\nEvaluation structure includes aspects that refer to the actual rating of the interviewee:\n\n    Rate each answer rather than making an overall evaluation at the end of the interview\n    Use anchored rating scales (for an example, see BARS )\n    Have the interviewer take detailed notes\n    Have more than one interviewer view each applicant (i.e. have panel interviews)\n    Have the same interviewers rate each applicant\n    Don’t allow any discussion about the applicants between interviewers\n    Train the interviewers\n    Use statistical procedures to create an overall interview score\n\nIt is important to note that structure should be thought of as a continuum; that is, the degree of structure present in an interview can vary along these various elements listed above.[94]\nInterviewer rating reliability\n\nIn terms of reliability, meta-analytic results provided evidence that interviews can have acceptable levels of interrater reliability, or consistent ratings across interviewers interrater reliability (i.e. .75 or above), when a structured panel interview is used.[96] In terms of criterion-related validity, or how well the interview predicts later job performance criterion validity, meta-analytic results have shown that when compared to unstructured interviews, structured interviews have higher validities, with values ranging from .20-.57 (on a scale from 0 to 1), with validity coefficients increasing with higher degrees of structure.[93][97][98] That is, as the degree of structure in an interview increases, the more likely interviewers can successfully predict how well the person will do on the job, especially when compared to unstructured interviews. In fact, one structured interview that included a) a predetermined set of questions that interviewers were able to choose from, and b) interviewer scoring of applicant answers after each individual question using previously created benchmark answers, showed validity levels comparable to cognitive ability tests (traditionally one of the best predictors of job performance) for entry level jobs.[93]\n\nHonesty and integrity are attributes that can be very hard to determine using a formal job interview process: the competitive environment of the job interview may in fact promote dishonesty. Some experts on job interviews express a degree of cynicism towards the process.[who?]\nApplicant reactions\n\nApplicant reactions to the interview process include specific factors such as; fairness, emotional responses, and attitudes toward the interviewer or the organization.[99] Though the applicant's perception of the interview process may not influence the interviewer(s) ability to distinguish between individuals' suitability, applicants reactions are important as those who react negatively to the selection process are more likely to withdraw from the selection process.[100][101][102] They are less likely to accept a job offer, apply on future occasions,[103] or to speak highly of the organization to others and to be a customer of that business.[100][101][104] Compared to other selection methods, such as personality or cognitive ability tests, applicants, from different cultures may have positive opinions about interviews.[100][105]\nInterview design\n\nInterview design can influence applicants' positive and negative reactions, though research findings on applicants preferences for structured compared to unstructured interviews appear contradictory.[102][106] Applicants' negative reactions to structured interviews may be reduced by providing information about the job and organization.[107] Providing interview questions to applicants before the interview, or telling them how their answers will be evaluated, are also received positively.[108]\nTypes of questions\n\nThe type of questions asked can affect applicant reactions. General questions are viewed more positively than situational or behavioral questions[109] and 'puzzle' interview questions may be perceived as negative being perceived unrelated to the job, unfair, or unclear how to answer.[110] Using questions that discriminating unfairly in law unsurprisingly are viewed negatively with applicants less likely to accept a job offer, or to recommend the organization to others.[111]\n\nSome of the questions and concerns on the mind of the hiring manager include:\n\n    Does this person have the skills I need to get the job done?\n    Will he or she fit in with the department or team?\n    Can I manage this person?\n    Does this person demonstrate honesty, integrity, and a good work ethic?\n    What motivates this person?\n    Do I like this person, and will he or she get along with others?\n    Will he or she focus on tasks and stick to the job until it is done?\n    Will this person perform up to the level the company requires for success?\n\nA sample of intention behind questions asked for understanding observable responses, displayed character, and underlying motivation:\n\n    What did the candidate really do in this job?\n    What role did he or she play, supportive or leading?\n    How much influence did the candidate exert on the outcomes of projects?\n    How did the candidate handle problems that came up?\n    How does this candidate come across?\n    How serious is the candidate about his or her career and this job?\n    Is he or she bright and likable?\n    Did the candidate prepare for this interview?\n    Is the candidate being forthright with information?\n    Does this person communicate well in a somewhat stressful face-to-face conversation?\n    Does the candidate stay focused on the question asked or ramble along?\n    Did the candidate exhibit good judgment in the career moves he or she made?\n    Did the candidate grow in his or her job and take on more responsibilities over time or merely do the same thing repeatedly?\n    Did the candidate demonstrate leadership, integrity, effective communications, teamwork, and persuasion skills (among others)?\n\nAdditional factors\n\nThe 'friendliness' of the interviewer may be equated to fairness of the process and improve the liklihood of accepting a job offer,[112] and face-to-face interviews compared to video conferencing and telephone interviews.[113] In video conferencing interviews the perception of the interviewer may be viewed as less personable, trustworthy, and competent.[114]\nInterview anxiety\n\nInterview anxiety refers to experiencing unpleasant or distressing feelings before or during a job interview.[115] It also reflects feeling apprehensive or tense about participating in an interview.[116] A couple of reasons why job candidates feel a heightened sense of anxiety and nervousness about the employment interview is because they feel they have little to no control over the interview process [117] or because they have to speak with a stranger.[118]\nImplications for applicants\n\nWhether anxieties come from how someone is as a person or from the interview situation itself, these anxious feelings have important consequences for job candidates, such as; limiting an applicant from effectively showing their ability to communicate and their future potential,[119] reducing interview performance and consequent assessment despite potential suitability for the job,[115] reducing likelihood of a second interview compared to less anxious individuals.[120]\nImplications for organizations\n\nApplicants who view the selection process more favorably tend to be more positive about the organization, and are likely to influence an organization’s reputation.[115][121] whearas, in contrast, anxious or uncomfortable during their interview may view an organization less favorably, causing the otherwise qualified candidates not accepting a job offer.[115] If an applicant is nervous, they might not act the same way they would on the job, making it harder for organizations to use the interview for predicting someone’s future job performance.[115]\nLegal issues\n\nIn many countries laws are put into place to prevent organizations from engaging in discriminatory practices against protected classes when selecting individuals for jobs.[122] In the United States, it is unlawful for private employers with 15 or more employees along with state and local government employers to discriminate against applicants based on the following: race, color, sex (including pregnancy), national origin, age (40 or over), disability, or genetic information (note: additional classes may be protected depending on state or local law). More specifically, an employer cannot legally \"fail or refuse to hire or to discharge any individual, or otherwise discriminate against any individual with respect to his compensation, terms, conditions, or privilege of employment\" or \"to limit, segregate, or classify his employees or applicants for employment in any way which would deprive or tend to deprive any individual of employment opportunities or otherwise adversely affect his status as an employee.\"[123][124]\n\nThe Civil Rights Act of 1964 and 1991 (Title VII) were passed into law to prevent the discrimination of individuals due to race, color, religion, sex, or national origin. The Pregnancy Discrimination Act was added as an amendment and protects women if they are pregnant or have a pregnancy-related condition.[125]\n\nThe Age Discrimination in Employment Act of 1967 prohibits discriminatory practice directed against individuals who are 40 years of age and older. Although some states (e.g. New York) do have laws preventing the discrimination of individuals younger than 40, no federal law exists.[126]\n\nThe Americans with Disabilities Act of 1990 protects qualified individuals who currently have or in the past have had a physical or mental disability (current users of illegal drugs are not covered under this Act). A person is a cripple if he has a disability that substantially limits a major life activity, has a history of a disability, is regarded by others as being disabled, or has a physical or mental impairment that is not transitory (lasting or expected to last six months or less) and minor. In order to be covered under this Act, the individual must be qualified for the job. A qualified individual is \"an individual with a disability who, with or without reasonable accommodation, can perform the essential functions of the employment position that such individual holds or desires.\"[127] Unless the disability poses an \"undue hardship,\" reasonable accommodations must be made by the organization. \"In general, an accommodation is any change in the work environment or in the way things are customarily done that enables an individual with a disability to enjoy equal employment opportunities.\"[127] Examples of reasonable accommodations are changing the workspace of an individual in a wheelchair to make it more wheelchair accessible, modifying work schedules, and/or modifying equipment.[128] Employees are responsible for asking for accommodations to be made by their employer.[125]\n\nThe most recent law to be passed is Title II of the Genetic Information Nondiscrimination Act of 2008. In essence, this law prohibits the discrimination of employees or applicants due to an individual’s genetic information and family medical history information.\n\nIn rare circumstances, it is lawful for employers to base hiring decisions on protected class information if it is considered a Bona Fide Occupational Qualification, that is, if it is a \"qualification reasonably necessary to the normal operation of the particular business.\" For example, a movie studio may base a hiring decision on age if the actor they are hiring will play a youthful character in a film.[129]\n\nGiven these laws, organizations are limited in the types of questions they legally are allowed to ask applicants in a job interview. Asking these questions may cause discrimination against protected classes, unless the information is considered a Bona Fide Occupational Qualification. For example, in the majority of situations it is illegal to ask the following questions in an interview as a condition of employment:\n\n    What is your date of birth?[130]\n    Have you ever been arrested for a crime?[130]\n    Do you have any future plans for marriage and children?[130]\n    What are your spiritual beliefs?[131]\n    How many days were you sick last year? Have you ever been treated for mental health problems?[131]\n    What prescription drugs are you currently taking?[131]\n\nApplicants with disabilities\n\nApplicants with disabilities may be concerned with the effect that their disability has on both interview and employment outcomes. Research has concentrated on four key issues: how interviewers rate applicants with disabilities, the reactions of applicants with disabilities to the interview, the effects of disclosing a disability during the interview, and the perceptions different kinds of applicant disabilities may have on interviewer ratings.\n\nThe job interview is a tool used to measure constructs or overall characteristics that are relevant for the job. Oftentimes, applicants will receive a score based on their performance during the interview. Research has found different findings based on interviewers’ perceptions of the disability. For example, some research has found a leniency effect (i.e., applicants with disabilities receive higher ratings than equally qualified non-disabled applicants) in ratings of applicants with disabilities[132][133] Other research, however, has found there is a disconnect between the interview score and the hiring recommendation for applicants with disabilities. That is, even though applicants with disabilities may have received a high interview score, they are still not recommended for employment.[134][135] The difference between ratings and hiring could be detrimental to a company because they may be missing an opportunity to hire a qualified applicant.\n\nA second issue in interview research deals with the applicants’ with disabilities reactions to the interview and applicant perceptions of the interviewers. Applicants with disabilities and able-bodied applicants report similar feelings of anxiety towards an interview.[136] Applicants with disabilities often report that interviewers react nervously and insecurely, which leads such applicants to experience anxiety and tension themselves. The interview is felt to be the part of the selection process where covert discrimination against applicants with disabilities can occur.[136] Many applicants with disabilities feel they cannot disclose (i.e., inform potential employer of disability) or discuss their disability because they want to demonstrate their abilities. If the disability is visible, then disclosure will inevitably occur when the applicant meets the interviewer, so the applicant can decide if they want to discuss their disability. If an applicant has a non-visible disability, however, then that applicant has more of a choice in disclosing and discussing. In addition, applicants who were aware that the recruiting employer already had employed people with disabilities felt they had a more positive interview experience.[136] Applicants should consider if they are comfortable with talking about and answering questions about their disability before deciding how to approach the interview.\n\nResearch has also demonstrated that different types of disabilities have different effects on interview outcomes. Disabilities with a negative stigma and that are perceived as resulting from the actions of the person (e.g., HIV-Positive, substance abuse) result in lower interview scores than disabilities for which the causes are perceived to be out of the individual’s control (e.g., physical birth defect).[135] A physical disability often results in higher interviewer ratings than psychological (e.g., mental illness) or sensory conditions (e.g., Tourette Syndrome).[133][137] In addition, there are differences between the effects of disclosing disabilities that are visible (e.g., wheelchair bound) and non-visible (e.g., Epilepsy) during the interview. When applicants had a non-visible disability and disclosed their disability early in the interview they were not rated more negatively than applicants who did not disclose. In fact, they were liked more than the applicants who did not disclose their disability and were presumed not disabled.[138] Interviewers tend to be impressed by the honesty of the disclosure.[137] Strong caution needs to be taken with applying results from studies about specific disabilities, as these results may not apply to other types of disabilities. Not all disabilities are the same and more research is needed to find whether these results are relevant for other types of disabilities.\n\nSome practical implications for job interviews for applicants with disabilities include research findings that show there are no differences in interviewer responses to a brief, shorter discussion or a detailed, longer discussion about the disability during the interview.[137] Applicants, however, should note that when a non-visible disability is disclosed near the end of the interview, applicants were rated more negatively than early disclosing and non-disclosing applicants. Therefore, it is possible that interviewers feel individuals who delay disclosure may do so out of shame or embarrassment. In addition, if the disability is disclosed after being hired, employers may feel deceived by the new hire and reactions could be less positive than would have been in the interview.[139] If applicants want to disclose their disability during the interview, research shows that a disclosure and/or discussion earlier in the interview approach may afford them some positive interview effects.[140] The positive effects, however, are preceded by the interviewers perception of the applicants’ psychological well-being. That is, when the interviewer perceives the applicant is psychologically well and/or comfortable with his or her disability, there can be positive interviewer effects. In contrast, if the interviewer perceives the applicant as uncomfortable or anxious discussing the disability, this may either fail to garner positive effect or result in more negative interview ratings for the candidate. Caution must again be taken when applying these research findings to other types of disabilities not investigated in the studies discussed above. There are many factors that can influence the interview of an applicant with a disability, such as whether the disability is physical or psychological, visible or non-visible, or whether the applicant is perceived as responsible for the disability or not. Therefore, applicants should make their own conclusions about how to proceed in the interview after comparing their situations with those examined in the research discussed here.\nOther applicant discrimination: Weight and pregnancy\n\nJob applicants who are underweight (to the point of emaciation), overweight or obese may face discrimination in the interview.[141][142] The negative treatment of overweight and obese individuals may stem from beliefs that weight is controllable and those who fail to control their weight are lazy, unmotivated, and lack self-discipline.[143][144] Underweight individuals may also be subject to appearance-related negative treatment.[142] Underweight, overweight and obese applicants are not protected from discrimination by any current United States laws.[141] However, some individuals who are morbidly obese and whose obesity is due to a physiological disorder may be protected against discrimination under the Americans with Disabilities Act.[145]\n\nDiscrimination against pregnant applicants is illegal under the Pregnancy Discrimination Act of 1978, which views pregnancy as a temporary disability and requires employers to treat pregnant applicants the same as all other applicants.[146] Yet, discrimination against pregnant applicants continues both in the United States and internationally.[146][147] Research shows that pregnant applicants compared to non-pregnant applicants are less likely to be recommended for hire.[148][149] Interviewers appear concerned that pregnant applicants are more likely than non-pregnant applicants to miss work and even quit.[149] Organizations who wish to reduce potential discrimination against pregnant applicants should consider implementing structured interviews, although some theoretical work suggests interviewers may still show biases even in these types of interviews.[148][150]\n\nEmployers are using social networking sites like Facebook and LinkedIn to obtain additional information about job applicants.[151][152][153] While these sites may be useful to verify resume information, profiles with pictures also may reveal much more information about the applicant, including issues pertaining to applicant weight and pregnancy.[154] Some employers are also asking potential job candidates for their social media logins which has alarmed many privacy watch dogs and regulators.[155]", "skillName": "Job interview."}
{"id": 92, "category": "Business", "skillText": "Project management\nProject management is the discipline of initiating, planning, executing, controlling, and closing the work of a team to achieve specific goals and meet specific success criteria. A project is a temporary endeavor designed to produce a unique product, service or result with a defined beginning and end (usually time-constrained, and often constrained by funding or deliverables) undertaken to meet unique goals and objectives, typically to bring about beneficial change or added value.[1][2] The temporary nature of projects stands in contrast with business as usual (or operations),[3] which are repetitive, permanent, or semi-permanent functional activities to produce products or services. In practice, the management of these two systems is often quite different, and as such requires the development of distinct technical skills and management strategies.[4]\n\nThe primary challenge of project management is to achieve all of the project goals within the given constraints.[5] This information is usually described in a user or project manual, which is created at the beginning of the development process. The primary constraints are scope, time, quality and budget.[6] The secondary � and more ambitious � challenge is to optimize the allocation of necessary inputs and integrate them to meet pre-defined objectives.\n\nContents  [hide]\n1\tHistory\n2\tApproaches\n2.1\tThe traditional approach\n2.2\tPRINCE2\n2.3\tCritical chain project management\n2.4\tProcess-based management\n2.5\tLean project management\n2.6\tExtreme project management\n2.7\tBenefits realization management\n3\tProcesses\n3.1\tInitiating\n3.2\tPlanning\n3.3\tExecuting\n3.4\tMonitoring and controlling\n3.5\tClosing\n3.6\tProject controlling and project control systems\n4\tTopics\n4.1\tProject managers\n4.2\tProject management types\n4.3\tWork breakdown structure\n4.4\tInternational standards\n4.5\tProject portfolio management\n4.6\tProject management software\n4.7\tVirtual project management\n5\tSee also\n6\tReferences\n7\tExternal links\nHistory\n\nRoman soldiers building a fortress, Trajan's Column 113 AD\nUntil 1900, civil engineering projects were generally managed by creative architects, engineers, and master builders themselves, for example Vitruvius (first century BC), Christopher Wren (1632�1723), Thomas Telford (1757�1834) and Isambard Kingdom Brunel (1806�1859).[7] It was in the 1950s that organizations started to systematically apply project management tools and techniques to complex engineering projects.[8]\n\n\nHenry Gantt (1861�1919), the father of planning and control techniques\nAs a discipline, project management developed from several fields of application including civil construction, engineering, and heavy defense activity.[9] Two forefathers of project management are Henry Gantt, called the father of planning and control techniques,[10] who is famous for his use of the Gantt chart as a project management tool (alternatively Harmonogram first proposed by Karol Adamiecki[11]); and Henri Fayol for his creation of the five management functions that form the foundation of the body of knowledge associated with project and program management.[12] Both Gantt and Fayol were students of Frederick Winslow Taylor's theories of scientific management. His work is the forerunner to modern project management tools including work breakdown structure (WBS) and resource allocation.\n\nThe 1950s marked the beginning of the modern project management era where core engineering fields come together to work as one. Project management became recognized as a distinct discipline arising from the management discipline with engineering model.[13] In the United States, prior to the 1950s, projects were managed on an ad-hoc basis, using mostly Gantt charts and informal techniques and tools. At that time, two mathematical project-scheduling models were developed. The \"Critical Path Method\" (CPM) was developed as a joint venture between DuPont Corporation and Remington Rand Corporation for managing plant maintenance projects. And the \"Program Evaluation and Review Technique\" or PERT, was developed by the United States Navy in conjunction with the Lockheed Corporation and Booz Allen Hamilton as part of the Polaris missile submarine program.[14]\n\nPERT and CPM are very similar in their approach but still present some differences. CPM is used for projects that assume deterministic activity times; the times at which each activity will be carried out are known. PERT, on the other hand, allows for stochastic activity times; the times at which each activity will be carried out are uncertain or varied. Because of this core difference, CPM and PERT are used in different contexts. These mathematical techniques quickly spread into many private enterprises.\n\n\nPERT network chart for a seven-month project with five milestones\nAt the same time, as project-scheduling models were being developed, technology for project cost estimating, cost management, and engineering economics was evolving, with pioneering work by Hans Lang and others. In 1956, the American Association of Cost Engineers (now AACE International; the Association for the Advancement of Cost Engineering) was formed by early practitioners of project management and the associated specialties of planning and scheduling, cost estimating, and cost/schedule control (project control). AACE continued its pioneering work and in 2006 released the first integrated process for portfolio, program and project management (Total Cost Management Framework).\n\nThe International Project Management Association (IPMA) was founded in Europe in 1967,[15] as a federation of several national project management associations. IPMA maintains its federal structure today and now includes member associations on every continent except Antarctica. IPMA offers a Four Level Certification program based on the IPMA Competence Baseline (ICB).[16] The ICB covers technical, contextual, and behavioral competencies.\n\nIn 1969, the Project Management Institute (PMI) was formed in the USA.[17] PMI publishes A Guide to the Project Management Body of Knowledge (PMBOK Guide), which describes project management practices that are common to \"most projects, most of the time.\" PMI also offers multiple certifications.\n\nApproaches\nThere are a number of approaches for managing project activities including lean, iterative, incremental, and phased approaches.\n\nRegardless of the methodology employed, careful consideration must be given to the overall project objectives, timeline, and cost, as well as the roles and responsibilities of all participants and stakeholders.\n\nThe traditional approach\nA traditional phased approach identifies a sequence of steps to be completed. In the \"traditional approach\",[18] five developmental components of a project can be distinguished (four stages plus control):\n\n\nTypical development phases of an engineering project\ninitiation\nplanning and design\nexecution and construction\nmonitoring and controlling systems\ncompletion and finish point\nMany industries use variations of these project stages. For example, when working on a brick-and-mortar design and construction, projects will typically progress through stages like pre-planning, conceptual design, schematic design, design development, construction drawings (or contract documents), and construction administration. In software development, this approach is often known as the waterfall model,[19] i.e., one series of tasks after another in linear sequence. In software development many organizations have adapted the Rational Unified Process (RUP) to fit this methodology, although RUP does not require or explicitly recommend this practice. Waterfall development works well for small, well defined projects, but often fails in larger projects of undefined and ambiguous nature. The Cone of Uncertainty explains some of this as the planning made on the initial phase of the project suffers from a high degree of uncertainty. This becomes especially true as software development is often the realization of a new or novel product. In projects where requirements have not been finalized and can change, requirements management is used to develop an accurate and complete definition of the behavior of software that can serve as the basis for software development.[20] While the terms may differ from industry to industry, the actual stages typically follow common steps to problem solving�\"defining the problem, weighing options, choosing a path, implementation and evaluation.\"\n\nPRINCE2\nMain article: PRINCE2\n\nThe PRINCE2 process model\nPRINCE2 is a structured approach to project management released in 1996 as a generic project management method.[21] It combines the original PROMPT methodology (which evolved into the PRINCE methodology) with IBM's MITP (managing the implementation of the total project) methodology. PRINCE2 provides a method for managing projects within a clearly defined framework.\n\nPRINCE2 focuses on the definition and delivery of products, in particular their quality requirements. As such, it defines a successful project as being output-oriented (not activity- or task-oriented) through creating an agreed set of products[22] that define the scope of the project and provides the basis for planning and control, that is, how then to coordinate people and activities, how to design and supervise product delivery, and what to do if products and therefore the scope of the project has to be adjusted if it does not develop as planned.\n\nIn the method, each process is specified with its key inputs and outputs and with specific goals and activities to be carried out to deliver a project's outcomes as defined by its Business Case. This allows for continuous assessment and adjustment when deviation from the Business Case is required.\n\nPRINCE2 provides a common language for all participants in the project. The governance framework of PRINCE2 � its roles and responsibilities � are fully described and require tailoring to suit the complexity of the project and skills of the organisation.[22]\n\nCritical chain project management\nMain article: Critical chain project management\nCritical chain project management (CCPM) is a method of planning and managing project execution designed to deal with uncertainties inherent in managing projects, while taking into consideration limited availability of resources (physical, human skills, as well as management & support capacity) needed to execute projects.\n\nCCPM is an application of the theory of constraints (TOC) to projects. The goal is to increase the flow of projects in an organization (throughput). Applying the first three of the five focusing steps of TOC, the system constraint for all projects is identified as are the resources. To exploit the constraint, tasks on the critical chain are given priority over all other activities. Finally, projects are planned and managed to ensure that the resources are ready when the critical chain tasks must start, subordinating all other resources to the critical chain.\n\nThe project plan should typically undergo resource leveling, and the longest sequence of resource-constrained tasks should be identified as the critical chain. In some cases, such as managing contracted sub-projects, it is advisable to use a simplified approach without resource leveling.\n\nIn multi-project environments, resource leveling should be performed across projects. However, it is often enough to identify (or simply select) a single \"drum\". The drum can be a resource that acts as a constraint across projects, which are staggered based on the availability of that single resource.\n\nOne can also use a \"virtual drum\" by selecting a task or group of tasks (typically integration points) and limiting the number of projects in execution at that stage.\n\nProcess-based management\nMain article: Process-based management\nThe incorporation of process-based management has been driven by the use of Maturity models such as the OPM3 and the CMMI (capability maturity model integration; see this example of a predecessor) and ISO/IEC15504 (SPICE � software process improvement and capability estimation). Unlike SEI's CMM, the OPM3 maturity model describes how to make project management processes capable of performing successfully, consistently, and predictably in order to enact the strategies of an organization .\n\nLean project management\nMain article: Lean project management\nLean project management uses the principles from lean manufacturing to focus on delivering value with less waste and reduced time.\n\nExtreme project management\nMain article: Extreme project management\n\nPlanning and feedback loops in Extreme programming (XP) with the time frames of the multiple loops.\nIn critical studies of project management it has been noted that several PERT based models are not well suited for the multi-project company environment of today.[citation needed] Most of them are aimed at very large-scale, one-time, non-routine projects, and currently all kinds of management are expressed in terms of projects.\n\nUsing complex models for \"projects\" (or rather \"tasks\") spanning a few weeks has been proven to cause unnecessary costs and low maneuverability in several cases.[citation needed] The generalization of Extreme Programming to other kinds of projects is extreme project management, which may be used in combination with the process modeling and management principles of human interaction management.\n\nBenefits realization management\nMain article: Benefits realisation management\nBenefits realization management (BRM) enhances normal project management techniques through a focus on outcomes (the benefits) of a project rather than products or outputs, and then measuring the degree to which that is happening to keep a project on track. This can help to reduce the risk of a completed project being a failure by delivering agreed upon requirements/outputs but failing to deliver the benefits of those requirements.\n\nIn addition, BRM practices aim to ensure the alignment between project outcomes and business strategies. The effectiveness of these practices is supported by recent research evidencing BRM practices influencing project success from a strategic perspective across different countries and industries.[23]\n\nAn example of delivering a project to requirements might be agreeing to deliver a computer system that will process staff data and manage payroll, holiday and staff personnel records. Under BRM the agreement might be to achieve a specified reduction in staff hours required to process and maintain staff data.\n\nProcesses\n\nThe project development stages[24]\nTraditionally, project management includes a number of elements: four to five project management process groups, and a control system. Regardless of the methodology or terminology used, the same basic project management processes or stages of development will be used. Major process groups generally include:[6]\n\nInitiation\nPlanning\nProduction or execution\nMonitoring and controlling\nClosing\nIn project environments with a significant exploratory element (e.g., research and development), these stages may be supplemented with decision points (go/no go decisions) at which the project's continuation is debated and decided. An example is the Phase�gate model.\n\nInitiating\n\nInitiating process group processes[24]\nThe initiating processes determine the nature and scope of the project.[25] If this stage is not performed well, it is unlikely that the project will be successful in meeting the business� needs. The key project controls needed here are an understanding of the business environment and making sure that all necessary controls are incorporated into the project. Any deficiencies should be reported and a recommendation should be made to fix them.\n\nThe initiating stage should include a plan that encompasses the following areas:\n\nanalyzing the business needs/requirements in measurable goals\nreviewing of the current operations\nfinancial analysis of the costs and benefits including a budget\nstakeholder analysis, including users, and support personnel for the project\nproject charter including costs, tasks, deliverables, and schedules\nPlanning\nAfter the initiation stage, the project is planned to an appropriate level of detail (see example of a flow-chart).[24] The main purpose is to plan time, cost and resources adequately to estimate the work needed and to effectively manage risk during project execution. As with the Initiation process group, a failure to adequately plan greatly reduces the project's chances of successfully accomplishing its goals.\n\nProject planning generally consists of[26]\n\ndetermining how to plan (e.g. by level of detail or Rolling Wave planning);\ndeveloping the scope statement;\nselecting the planning team;\nidentifying deliverables and creating the work breakdown structure;\nidentifying the activities needed to complete those deliverables and networking the activities in their logical sequence;\nestimating the resource requirements for the activities;\nestimating time and cost for activities;\ndeveloping the schedule;\ndeveloping the budget;\nrisk planning;\ngaining formal approval to begin work.\nAdditional processes, such as planning for communications and for scope management, identifying roles and responsibilities, determining what to purchase for the project and holding a kick-off meeting are also generally advisable.\n\nFor new product development projects, conceptual design of the operation of the final product may be performed concurrent with the project planning activities, and may help to inform the planning team when identifying deliverables and planning activities.\n\nExecuting\n\nExecuting process group processes[24]\nThe execution/implementation phase ensures that the project management plan�s deliverables are executed accordingly. This phase involves proper allocation, co-ordination and management of human resources and any other resources such as material and budgets. The output of this phase is the project deliverables.\n\nMonitoring and controlling\n\nMonitoring and controlling process group processes[24]\nMonitoring and controlling consists of those processes performed to observe project execution so that potential problems can be identified in a timely manner and corrective action can be taken, when necessary, to control the execution of the project. The key benefit is that project performance is observed and measured regularly to identify variances from the project management plan.\n\nMonitoring and controlling includes:[27]\n\nMeasuring the ongoing project activities ('where we are');\nMonitoring the project variables (cost, effort, scope, etc.) against the project management plan and the project performance baseline (where we should be);\nIdentifying corrective actions to address issues and risks properly (How can we get on track again);\nInfluencing the factors that could circumvent integrated change control so only approved changes are implemented.\nIn multi-phase projects, the monitoring and control process also provides feedback between project phases, in order to implement corrective or preventive actions to bring the project into compliance with the project management plan.\n\nProject maintenance is an ongoing process, and it includes:[6]\n\nContinuing support of end-users\nCorrection of errors\nUpdates to the product over time\n\nMonitoring and controlling cycle\nIn this stage, auditors should pay attention to how effectively and quickly user problems are resolved.\n\nOver the course of any construction project, the work scope may change. Change is a normal and expected part of the construction process. Changes can be the result of necessary design modifications, differing site conditions, material availability, contractor-requested changes, value engineering and impacts from third parties, to name a few. Beyond executing the change in the field, the change normally needs to be documented to show what was actually constructed. This is referred to as change management. Hence, the owner usually requires a final record to show all changes or, more specifically, any change that modifies the tangible portions of the finished work. The record is made on the contract documents � usually, but not necessarily limited to, the design drawings. The end product of this effort is what the industry terms as-built drawings, or more simply, �as built.� The requirement for providing them is a norm in construction contracts. Construction document management is a highly important task undertaken with the aid an online or desktop software system, or maintained through physical documentation. The increasing legality pertaining to the construction industries maintenance of correct documentation has caused the increase in the need for document management systems.\n\nWhen changes are introduced to the project, the viability of the project has to be re-assessed. It is important not to lose sight of the initial goals and targets of the projects. When the changes accumulate, the forecasted result may not justify the original proposed investment in the project. Successful project management identifies these components, and tracks and monitors progress so as to stay within time and budget frames already outlined at the commencement of the project.\n\nClosing\n\nClosing process group processes.[24]\nClosing includes the formal acceptance of the project and the ending thereof. Administrative activities include the archiving of the files and documenting lessons learned.\n\nThis phase consists of:[6]\n\nContract closure: Complete and settle each contract (including the resolution of any open items) and close each contract applicable to the project or project phase.\nProject close: Finalize all activities across all of the process groups to formally close the project or a project phase\nAlso included in this phase is the Post Implementation Review. This is a vital phase of the project for the project team to learn from experiences and apply to future projects. Normally a Post Implementation Review consists of looking at things that went well and analysing things that went badly on the project to come up with lessons learned.\n\nProject controlling and project control systems\nProject controlling should be established as an independent function in project management. It implements verification and controlling function during the processing of a project in order to reinforce the defined performance and formal goals.[28] The tasks of project controlling are also:\n\nthe creation of infrastructure for the supply of the right information and its update\nthe establishment of a way to communicate disparities of project parameters\nthe development of project information technology based on an intranet or the determination of a project key performance indicator system (KPI)\ndivergence analyses and generation of proposals for potential project regulations[29]\nthe establishment of methods to accomplish an appropriate project structure, project workflow organization, project control and governance\ncreation of transparency among the project parameters[30]\nFulfillment and implementation of these tasks can be achieved by applying specific methods and instruments of project controlling. The following methods of project controlling can be applied:\n\ninvestment analysis\ncost�benefit analysis\nvalue benefit analysis\nexpert surveys\nsimulation calculations\nrisk-profile analysis\nsurcharge calculations\nmilestone trend analysis\ncost trend analysis\ntarget/actual-comparison[31]\nProject control is that element of a project that keeps it on-track, on-time and within budget.[27] Project control begins early in the project with planning and ends late in the project with post-implementation review, having a thorough involvement of each step in the process. Projects may be audited or reviewed while the project is in progress. Formal audits are generally risk or compliance-based and management will direct the objectives of the audit. An examination may include a comparison of approved project management processes with how the project is actually being managed.[32] Each project should be assessed for the appropriate level of control needed: too much control is too time consuming, too little control is very risky. If project control is not implemented correctly, the cost to the business should be clarified in terms of errors and fixes.\n\nControl systems are needed for cost, risk, quality, communication, time, change, procurement, and human resources. In addition, auditors should consider how important the projects are to the financial statements, how reliant the stakeholders are on controls, and how many controls exist. Auditors should review the development process and procedures for how they are implemented. The process of development and the quality of the final product may also be assessed if needed or requested. A business may want the auditing firm to be involved throughout the process to catch problems earlier on so that they can be fixed more easily. An auditor can serve as a controls consultant as part of the development team or as an independent auditor as part of an audit.\n\nBusinesses sometimes use formal systems development processes. These help assure that systems are developed successfully. A formal process is more effective in creating strong controls, and auditors should review this process to confirm that it is well designed and is followed in practice. A good formal systems development plan outlines:\n\nA strategy to align development with the organization�s broader objectives\nStandards for new systems\nProject management policies for timing and budgeting\nProcedures describing the process\nEvaluation of quality of change\nTopics\nProject managers\nA project manager is a professional in the field of project management. Project managers can have the responsibility of the planning, execution, and closing of any project, typically relating to construction industry, engineering, architecture, computing, and telecommunications. Many other fields in production engineering, design engineering, and heavy industrial have project managers.\n\nA project manager is the person accountable for accomplishing the stated project objectives. Key project management responsibilities include creating clear and attainable project objectives, building the project requirements, and managing the triple constraint for projects, which is cost, time, and scope.\n\nA project manager is often a client representative and has to determine and implement the exact needs of the client, based on knowledge of the firm they are representing. The ability to adapt to the various internal procedures of the contracting party, and to form close links with the nominated representatives, is essential in ensuring that the key issues of cost, time, quality and above all, client satisfaction, can be realized.\n\nProject management types\nProject management can apply to any project, but it is often tailored to accommodate the specific needs of different and highly specialized industries. For example, the construction industry, which focuses on the delivery of things like buildings, roads, and bridges, has developed its own specialized form of project management that it refers to as Construction project management and for which project managers can become trained and certified in.[33] The Information technology industry has also evolved to develop its own form of Project management that is referred to as IT Project management and which specializes in the delivery of technical assets and services that are required to pass through various lifecycle phases such as planning, design, development, testing, and deployment. Biotechnology project management focuses on the intricacies of biotechnology research and development.[34]\n\nFor each type of project management, project managers develop and utilize repeatable templates that are specific to the industry they're dealing with. This allows project plans to become very thorough and highly repeatable, with the specific intent to increase quality, lower delivery costs, and lower time to deliver project results.\n\nWork breakdown structure\nMain article: Work breakdown structure\nThe work breakdown structure (WBS) is a tree structure that shows a subdivision of effort required to achieve an objective�for example a program, project, and contract. The WBS may be hardware-, product-, service-, or process-oriented (see an example in a NASA reporting structure (2001)).[35]\n\nA WBS can be developed by starting with the end objective and successively subdividing it into manageable components in terms of size, duration, and responsibility (e.g., systems, subsystems, components, tasks, sub-tasks, and work packages), which include all steps necessary to achieve the objective.[20]\n\nThe work breakdown structure provides a common framework for the natural development of the overall planning and control of a contract and is the basis for dividing work into definable increments from which the statement of work can be developed and technical, schedule, cost, and labor hour reporting can be established.[35] The work breakdown structure can be displayed in two forms one in form of a table with subdivision of tasks two in form of an organisational chart.\n\nInternational standards\nThere have been several attempts to develop project management standards, such as:\n\nISO 21500: 2012 � Guidance on project management. This is the first project management ISO.\nISO 31000: 2009 � Risk management. Risk management is 1 of the 10 knowledge areas of either ISO 21500 or PMBoK5 concept of project management.\nISO/IEC/IEEE 16326-2009 � Systems and Software Engineering�Life Cycle Processes�Project Management[36]\nCapability Maturity Model from the Software Engineering Institute.\nGAPPS, Global Alliance for Project Performance Standards � an open source standard describing COMPETENCIES for project and program managers.\nA Guide to the Project Management Body of Knowledge from the Project Management Institute (PMI)\nHERMES method, Swiss general project management method, selected for use in Luxembourg and international organizations.\nThe ISO standards ISO 9000, a family of standards for quality management systems, and the ISO 10006:2003, for Quality management systems and guidelines for quality management in projects.\nPRINCE2, Projects IN Controlled Environments.\nAssociation for Project Management Body of Knowledge[37]\nTeam Software Process (TSP) from the Software Engineering Institute.\nTotal Cost Management Framework, AACE International's Methodology for Integrated Portfolio, Program and Project Management.\nV-Model, an original systems development method.\nThe Logical framework approach, which is popular in international development organizations.\n[Australian Institute of Project Management] AIPM has 4 levels of certification; CPPP, CPPM, CPPD & CPPE for Certified Practicing Project ... Partner, Manager, Director and Executive.\nProject portfolio management\nMain article: Project portfolio management\nAn increasing number of organizations are using, what is referred to as, project portfolio management (PPM) as a means of selecting the right projects and then using project management techniques[38] as the means for delivering the outcomes in the form of benefits to the performing private or not-for-profit organization.\n\nProject management software\nMain articles: Project management software and Project management information system\nProject management software is software used to help plan, organize, and manage resource pools, develop resource estimates and implement plans. Depending on the sophistication of the software, functionality may include estimation and planning, scheduling, cost control and budget management, resource allocation, collaboration software, communication, decision-making, workflow, quality management, documentation and/or administration systems.[39][40]\n\nVirtual project management\nMain article: Virtual team\nVirtual program management (VPM) is management of a project done by a virtual team, though it rarely may refer to a project implementing a virtual environment[41] It is noted that managing a virtual project is fundamentally different from managing traditional projects,[42] combining concerns of telecommuting and global collaboration (culture, timezones, language).[43]\n\nSee also\nLists\nComparison of project management software\nGlossary of project management\nList of collaborative software\nList of project management topics\nTimeline of project management\nComparison of Kanban software\nRelated fields\nArchitectural engineering\nConstruction management\nCost engineering\nFacilitation (business)\nIndustrial engineering\nProject management software\nProject portfolio management\nProject workforce management\nSoftware project management\nSystems engineering\nAgile Construction\nRelated subjects\nCollaborative project management\nEarned value management\nHuman factors\nProcess architecture\nProject accounting\nProject governance\nProgram management\nProject management simulation\nSmall-scale project management\nSoftware development process\nSystems Development Life Cycle (SDLC)", "skillName": "Project management."}
{"id": 93, "category": "Business", "skillText": "Service-oriented modeling\nService-oriented modeling is the discipline of modeling business and software systems, for the purpose of designing and specifying service-oriented business systems within a variety of architectural styles, such as enterprise architecture, application architecture, service-oriented architecture, and cloud computing.\n\nAny service-oriented modeling methodology typically includes a modeling language that can be employed by both the 'problem domain organization' (the Business), and 'solution domain organization' (the Information Technology Department), whose unique perspectives typically influence the 'service' development life-cycle strategy and the projects implemented using that strategy.\n\nService-oriented modeling typically strives to create models that provide a comprehensive view of the analysis, design, and architecture of all 'Software Entities' in an organization, which can be understood by individuals with diverse levels of business and technical understanding. Service-oriented modeling typically encourages viewing software entities as 'assets' (service-oriented assets), and refers to these assets collectively as 'services'.\n\n1\tPopular approaches\n1.1\tService-oriented modeling and architecture\n1.1.1\tLife cycle modeling activities\n1.2\tService-oriented modeling framework (SOMF)\n1.2.1\tLanguage modeling generations\n1.2.2\tTransformation models\n1.2.3\tDiscipline-specific modeling\n2\tMethodological issues\n2.1\tModeling styles\n2.2\tModeling asset patterns\n2.3\tModeling notation\n2.4\tService-oriented conceptualization\n3\tExamples\n4\tCloud computing modeling capabilities\n4.1\tCloud computing modeling examples\n5\tSee also\n6\tReferences\n7\tFurther reading\n8\tExternal links\nPopular approaches\nThere are many different approaches that have been proposed for service modeling, including SOMA and SOMF.\n\nService-oriented modeling and architecture\nIBM announced service-oriented modeling and architecture (SOMA) as the first publicly announced SOA-related methodology in 2004.[1] SOMA refers to the more general domain of service modeling necessary to design and create SOA. SOMA covers a broader scope and implements service-oriented analysis and design (SOAD) through the identification, specification and realization of services, components that realize those services (a.k.a. \"service components\"), and flows that can be used to compose services.\n\nSOMA includes an analysis and design method that extends traditional object-oriented and component-based analysis and design methods to include concerns relevant to and supporting SOA. It consists of three major phases of identification, specification and realization of the three main elements of SOA, namely, services, components that realize those services (aka service components) and flows that can be used to compose services.\n\nSOMA is an end-to-end SOA method for the identification, specification, realization and implementation of services (including information services), components, flows (processes/composition). SOMA builds on current techniques in areas such as domain analysis, functional areas grouping, variability-oriented analysis (VOA) process modeling, component-based development, object-oriented analysis and design and use case modeling. SOMA introduces new techniques such as goal-service modeling, service model creation and a service litmus test to help determine the granularity of a service.\n\nSOMA identifies services, component boundaries, flows, compositions, and information through complementary techniques which include domain decomposition, goal-service modeling and existing asset analysis.\n\nLife cycle modeling activities\nService-oriented modeling and architecture (SOMA) consists of the phases of identification, specification, realization, implementation, deployment and management in which the fundamental building blocks of SOA are identified then refined and implemented in each phase. The fundamental building blocks of SOA consists of services, components, flows and related to them, information, policy and contracts.\n\nService-oriented modeling framework (SOMF)\nService-oriented modeling framework (SOMF) characteristics\nDriving modeling paradigm\nHolistic\nAnthropomorphic\nDiscipline-specific\nVirtual modeling\nVisual modeling\nStyle and pattern oriented\nModeling generations: as-is, to-be, used-to-be\nFederated Architecture\nArchitectural applications\nEnterprise architecture\nBusiness architecture\nApplication Architecture\nService-oriented architecture (SOA)\nCloud computing\nChief business goals\nAsset consolidation\nExpenditure reduction\nTime to market\nBusiness agility\nBusiness and IT alignment\nOverall technological goals\nArchitecture flexibility\nTechnological extensibility\nInteroperable implementations\nSOMF has been devised by author Michael Bell as a holistic and anthropomorphic modeling language for software development that employs disciplines and a universal language to provide tactical and strategic solutions to enterprise problems.[2] The term \"holistic language\" pertains to a modeling language that can be employed to design any application, business and technological environment, either local or distributed. This universality may include design of application-level and enterprise-level solutions, including SOA landscapes, cloud computing, or big data environments. The term \"anthropomorphic\", on the other hand, affiliates the SOMF language with intuitiveness of implementation and simplicity of usage. Furthermore, The SOMF language and its notation has been adopted by Sparx Enterprise Architect modeling platform that enables business architects, technical architects, managers, modelers, developers, and business and technical analysts to pursue the chief SOMF life cycle disciplines.\n\nSOMF is a service-oriented development life cycle methodology, a discipline-specific modeling process. It offers a number of modeling practices and disciplines that contribute to a successful service-oriented life cycle development and modeling during a project (see image on left).\n\n\nSOMF Version 2.0\nIt illustrates the major elements that identify the �what to do� aspects of a service development scheme. These are the modeling pillars that will enable practitioners to craft an effective project plan and to identify the milestones of a service-oriented initiative�either a small or large-scale business or a technological venture.\n\nThe provided image thumb (on the left hand side) depicts the four sections of the modeling framework that identify the general direction and the corresponding units of work that make up a service-oriented modeling strategy: practices, environments, disciplines, and artifacts. These elements uncover the context of a modeling occupation and do not necessarily describe the process or the sequence of activities needed to fulfill modeling goals. These should be ironed out during the project plan � the service-oriented development life cycle strategy � that typically sets initiative boundaries, time frame, responsibilities and accountabilities, and achievable project milestones.\n\nLanguage modeling generations\nSOMF introduces a transparency model by enabling three major modeling time frames, often named modeling generations:\n\nUsed-to-be: Design scheme of software components and related environments that were deployed, configured, and used in the past\nAs-is: Design of software components and corresponding environments that are currently being utilized\nTo-be: Design of software components and corresponding environments that will be deployed, configured, and used in the future\nThese three unique implementation generations can be viewed by SOMF diagrams and their corresponding perspectives to help practitioners to depict business and architectural decisions in the past, current, and future implementations. For example, an architect and a developer can describe the evolution of a system or an application since inception, and explain what were the architecture best practices that drove alterations to these software entities. This capability enables transparency of design and implementation. On the business side, modeling generations can help estimate return on investments and business value. Traceability of business investments and justifications to business initiatives can also be depicted by employing these modeling generations.\n\nTransformation models\n\nSOMF Transformation Models\nSOMF offers eight models of implementation, also known as \"Bell's Transformation Models\", as depicted in the displayed image named SOMF transformation models. Each of these units of work, namely models, identify the methodology, process, platform, best practices, and disciplines by which a practitioner ought to accomplish a modeling task during a project. The illustrated ninth model is the Governance Model, which should be employed to manage the other eight models.\n\nConsider the overall charter of the SOMF implementation models:\n\nDiscovery model: This model should be employed when ascertaining new software entities to provide a solution\nAnalysis model: The analysis model is devised to inspect a software component's feasibility to offer a solution, help analyzing business and technical requirements, and assist with measuring the success of implementation\nDesign model: Facilitates logical design of software entities; and contributes to component relationship, deployment compositions, and establishment of transactions\nTechnical architecture model: This model involves three major architecture perspectives: conceptual architecture, logical architecture, and physical architecture.\nConstruction model: Assists with modeling practices during the source code implementation phase\nQuality assurance model: Certifies software components for production and ensures stability of business and technical continuity\nOperations model: Enables a stable production environment and assures proper deployment and configuration of software entities\nBusiness architecture model: This model fosters proper integration of contextual and structural business formations with software entities\nGovernance model: Offers best practices, standards, and policies for all SOMF implementation models\nDiscipline-specific modeling\n\nSOMF Life Cycle Activities\nSOMF is driven by the development process of services. This approach, also named discipline-specific modeling (DspM), enables business and information technology professionals to focus on modeling deliverables that correspond to a specific software development life cycle stage and event.\n\nThe service-oriented modeling framework (SOMF) introduces five major life cycle modeling activities that drive a service evolution during design-time and run-time. At the design-time phase a service originates as a conceptual entity (conceptual service), later it transforms into a unit of analysis (analysis service), next it transitions into a contractual and logical entity (design service), and finally is established as a concrete service (solution service). The following identify the major contributions of the service-oriented modeling activities:\n\nService-oriented discovery & analysis modeling: Discover and analyze services for granularity, reusability, interoperability, loose-coupling, and identify consolidation opportunities.\nService-oriented business integration modeling: Identify service integration and alignment opportunities with business domains' processes (organizations, products, geographical locations)\nService-oriented logical design modeling: Establish service relationships and message exchange paths. Address service visibility. Craft service logical compositions. Model service transactions\nService-oriented conceptual architecture modeling: Establish an application or enterprise architectural direction. Depict a technological environment. Craft a technological stack. Identify business ownership.\nService-oriented logical architecture modeling: Integrate organizational software assets. Establish logical environment dependencies. Foster service reuse, loose coupling and consolidation.\nMethodological issues\nModeling styles\n\nSOMF modeling styles\nHow can a practitioner model a computing environment? In what type of forms can a group of services be arranged to enable an efficient integrated computing landscape? What would be the best message routes between a service consumer and provider? How can interoperability hurdles be mitigated? How can an organization discover a topology in which systems exchange messages?\n\nSOMF provides five major software modeling styles useful throughout a service life cycle (conceptualization, discovery and analysis, business integration, logical design, conceptual and logical architecture). These modeling styles: circular, hierarchical, network, bus, and star, are illustrated by corresponding \"modeling beams\"�connectors that link services to each other, can assist a software modeler with the following modeling aspects:\n\nIdentify service relationships: contextual and technological affiliations\nEstablish message routes between consumers and services\nProvide efficient service orchestration and choreography methods\nCreate powerful service transaction and behavioral patterns\nOffer valuable service packaging solutions\nIn the illustration on the right you will find the major five service-oriented modeling styles that SOMF offers. Each pattern identifies the various approaches and strategies that one should consider employing when modeling a service ecosystem.\n\nCircular modeling style: enables message exchange in a circular fashion, rather than employing a controller to carry out the distribution of messages. The circular style also offers a conceptual method to affiliating services.\nHierarchical modeling style: offers a relationship pattern between services for the purpose of establishing transactions and message exchange routes between consumers and services. The hierarchical pattern founds parent/child associations between services.\nNetwork modeling style: this pattern establishes �many to many� relationship between services, their peer services, and consumers. The network pattern accentuates on distributed environments and interoperable computing networks.\nStar modeling style: the star pattern advocates arranging services in a star formation, in which the central service passes messages to its extending arms. The Star modeling style is often used in �multi casting� or �publish and subscribe� instances, where �solicitation� or �fire and forget� message styles are involved.\nBus modeling style: illustrates an intermediary service that connects consumers with service providers for the purpose of message exchange duties.\nModeling asset patterns\nThe service-oriented modeling framework (SOMF) introduces four major software formations. These structures are software entities that habitually exist in our computing environments. In addition, the notion of a software component is further abstracted and represented by the universal \"service\" term, which can represent any organizational software asset, such as an object, a software module, a library component, an application, a business process, a database, a database store procedure or trigger, an ESB, a legacy implementation, a Web service, and more. Again, any of these software entities can be named \"service\". The image below illustrates these asset patterns.\n\nSOMF Asset Patterns\nThus, a service is classified by its contextual and structural attributes:\n\nAtomic service: an indivisible software component that is too granular and executes fewer business or technical functionalities. An atomic formation is also a piece of software that typically is not subject to decomposition analysis activities and its business or technological functionality does not justify breakdown to smaller components. Examples: customer address service and checking account balance service.\nComposite service: a composite service structure aggregates smaller and fine-grained services. This hierarchical service formation is characteristically known as coarse-grained entity that encompasses more business or technical processes. A composite service may aggregate atomic or other composite services. Examples: customer checking service that aggregates smaller checking account and savings account services. An application that is composed of sub-systems, an ESB that is composed of routing, orchestration, and data transformation components.\nService cluster: this is a collection of distributed and related services that are gathered because of their mutual business or technological commonalities. A service cluster both affiliates services and combines their offerings to solve a business problem. A cluster structure can aggregate atomic as well as composite formations. Examples: A Mutual Funds service cluster that is composed of related and distributed mutual funds services. A Customer Support service cluster that offers automated bill payments, online account statements, and money transfer capabilities\nCloud of Services: a collection of services that are delivered by a cloud computing implementation. These services can be classified as Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), and more.\nThe below image illustrates these SOMF assets that are being modeled during the analysis phase of a project.\n\nSOMF Analysis Assets\nModeling notation\nAs previously discussed, each SOMF life cycle discipline also offers a specialized notation. For example, the service-oriented discovery and analysis discipline provides a notation to help model the analysis and identification of services.[3] In addition, during the design phase the SOMF design notation offers symbols to help model a logical design, design composition, and a service transaction model.\n\nLet us take a look at the service-oriented discovery and analysis modeling notation. During the service identification and inspection a practitioner should pursue two types of modeling tasks: (1) Contextual analysis and modeling, and (2) Structural analysis and modeling. These activities are performed to produce a service-oriented analysis proposition.\n\nThe below illustration identifies the contextual analysis and modeling operations (represented by analysis symbols)that can be employed to draft an analysis proposition diagram. These operations promote the core service-oriented analysis discipline best practices.\n\n\nService-oriented contextual analysis notation - SOMF 2.0[4]\nHere is a short description for these symbols:\n\nGeneralized: Increases service abstraction level and broadens service offerings\nSpecified: Decreases service abstraction level and limits service offerings\nExpanded: Expands service operations in a distributed environment\nContracted: Trims service operations in a distributed environment\nThe below illustration, on the other hand, depicts the service-oriented structural analysis and modeling notation.\n\n\nService-oriented tructural analysis notation - SOMF 2.0[4]\nHere is a short description for these symbols:\n\nAggregated: Depicts containment of services\nUnified: Joins services by creating a new service\nCompounded: Groups services that offer collaborative solution\nDecomposed: Detaches a child service from its containing parent\nSubtracted: Retires a service\nTransformed: Converts a service structure to another formation (i.e. from Atomic to Composite, etc.,)\nIntersected: Intersects two or more service clusters\nOverlapped: Identifies the overlapping region between two or more service clusters\nExcluded: Isolates the overlapping region of a two ore more intersected service clusters\nClipped: Isolates a service from a distributed environment\nCoupled: Structurally couples two autonomous services in a distributed environment\nDecoupled: Structurally separates two autonomous services in a distributed environment\nCloned: Duplicates an instance of a service by creating a new and identical service\nDe-cloned: Separates cloned services\nBound: Identifies a contract between two services\nUnbound: Identifies a contract cancellation between two services\nOperation Numbering: Illustrates the sequence of analysis and modeling operations\nComment: A place to put comments next to each asset or operation\nService-oriented conceptualization\nThe service-oriented modeling framework (SOMF) advocates that practitioners devise conceptual services to bridge the communication gaps in an organization. These conceptual entities foster the creation of a common language, a vocabulary that can be used during projects to encourage asset reuse and consolidation. One of the most important aspects of the conceptual service paradigm is that it provides direction and defines the scope of a project or a business initiative.\n\nThe conceptualization process then identifies six major �tools� that can facilitate the development of conceptual services.[5]\n\nConcept attribution: this activity pertains to the collection of software products attributes that both describe service�s features and lead to the discovery of organizational taxonomy\nConcept classification: the categorization effort contributes to separation of concerns and the establishment of service identities. This is the process of identifying service dissimilarities\nConcept association: Unlike the classification activity, the association effort enables the discovery of service relationship. These can be business or technological affiliations\nConcept clustering: this discipline is about grouping related conceptual services that collaboratively provide a solution. Clustering is a conceptual operation that can encompass local, remote, and virtual services\nConcept generalization: to raise the abstraction level of a conceptual service, the generalization method increases the conceptual scope of a solution. This approach is typically used when a service scope is too narrow.\nConcept specification: the specification activity enables architects, modelers, and developers to reduce the abstraction level of a service and narrow its conceptual scope.\nExamples\nLet us now view a number of service analysis modeling examples.\n\nClick on small images for full-size version\n\n1. Service Aggregation Example\n\n\n2. Service Decomposition Example\n\n\n3. Service Subtraction Example\n\n\n4. Service Substitution Example\nUse Case 1 depicts a simple aggregation case, in which atomic service A-1 is aggregated in composite service C-1 because of SOA best practice reasons.\nUse Case 2 describes service decomposition. Once again, this is because of an SOA best practice rule.\nUse Case 3 illustrates a service retirement (elimination) employing the �subtracted� analysis operation.\nUse Case 4 represents a common business substitution operation. Atomic service A-3 was retired and replaced with atomic service A-2.\nCloud computing modeling capabilities\n\nSOMF Cloud Computing Model\nThe SOMF cloud computing modeling notation, also known as CCMN, helps illustrate a service architecture scheme whose participating services interact and collaborate in a cloud boundary or beyond. �Cloud boundary� pertains to cloud offerings, which typically provide software, infrastructure, and platform type of services. The term �beyond�, however, implies that any consumer, such as organizations, applications, or remote services can also be a part of the cloud computing venture if they subscribe to the cloud�s services.\n\nThis overall servicing vision embodies the general notion: �everything is a service�, as illustrated on the far right. The ability to abstract services in spite of their location, interoperability challenges, or deployment difficulties, the SOMF cloud computing model represents an elastic cloud computing environment, nimble enough to adapt to changes, and meet time-to-market.\n\nCloud computing modeling examples\nThe introduced examples illustrate cloud design diagrams produced in various software development life cycle stages. In addition, these examples introduce three major cloud modeling spaces, each of which helps modelers to describe service interoperability, integration, message exchange, and collaboration in a deployment environment:\n\nService Containment Space: a modeling space that identifies aggregated service formations, such as composite service or service cluster\nIntraCloud Space: identifies the architecture boundary of a cloud landscape\nExtraCloud Space: defines service architecture/s external to a cloud boundary\nOrganizational Boundary: a modeling area dedicated to service modeling, typically owned by an organization\nClick on small images for full-size version\n\n1. Cloud Logical Design Service Relationships Diagram\n\n\n2. Cloud Logical Design Composition Diagram\n\n\n3. Cloud Analysis Proposition Diagram\n\n\n4. Cloud Delivery Model Diagram\n\n\n5. Cloud Deployment Diagram\nExample 1 depicts (simple and high-level) a logical design relationship diagram, which illustrates associations between three services (composite, atomic, and service cluster), each of which reside within a distinct organizational boundary: North Side Inc., East Side Inc., and West Side Inc. These organizations communicate to a design public cloud by �apparent bidirectional� connectors, depicting the message paths between these entities\nExample 2 shows a logical design composition diagram which illustrates detail offerings of a cloud, denoted by the IntraCloud Space (a space allocated to services within a cloud), which contains two composite services, a service cluster, and an atomic service, forming a circular message delivery path by using circular beams type of connectors that form a message exchange pattern). The ExtraCloud Space (a space allocated to services outside of a cloud), on the other hand, contains services that are not offered by the cloud: a composite service and two atomic services, communicating by network beams (type of connectors that form a message exchange pattern). Finally, the IntraCloud Space and the ExtraCloud Space are linked by the network beam, depicting relationship between two composite services, each of which located on the opposite side of the aisle\nExample 3 shows an analysis proposition diagram, typically created during the analysis phase of a project, in which two organizations exchange messages over a network: Public Cloud Inc. and New York Computers Inc. The former contains an IntraCloud and ExtraCloud spaces, offering various aggregated services bound by contracts. The later contains a private cloud that consists of a service cluster and a composite service. These two organizations are bound by a contract supported by two different composite services, each of which resides in one organizational boundary\nExample 4 illustrates a general cloud delivery model diagram in which a community cloud delivers two types of services: Software as a Service (SaaS) and Platform as a Service (PaaS). Note the attributes of each of the delivery model.\nExample 5 depicts a cloud computing deployment diagram that contains three different geographical locations: Continent, Region, and Zone.\nSee also\nLayered queueing network", "skillName": "Service-oriented modeling."}
{"id": 94, "category": "Business", "skillText": "Business model\n\nA business model is an \"abstract representation of an organization, be it conceptual, textual, and/or graphical, of all core interrelated architectural, co-operational, and financial arrangements designed and developed by an organization presently and in the future, as well as all core products and/or services the organization offers, or will offer, based on these arrangements that are needed to achieve its strategic goals and objectives.\"[1] [2] This definition by Al-Debei, El-Haddadeh and Avison (2008) indicates that value proposition, value architecture (the organizational infrastructure and technological architecture that allows the movement of products, services, and information), value finance (modeling information related to total cost of ownership, pricing methods, and revenue structure), and value network articulate the primary constructs or dimensions of business models.[3]\n\nA business model describes the rationale of how an organization creates, delivers, and captures value,[4] in economic, social, cultural or other contexts. The process of business model construction is part of business strategy.\n\nIn theory and practice, the term business model is used for a broad range of informal and formal descriptions to represent core aspects of a business, including purpose, business process, target customers, offerings, strategies, infrastructure, organizational structures, sourcing, trading practices, and operational processes and policies including culture. The literature has provided very diverse interpretations and definitions of a business model. A systematic review and analysis of manager responses to a survey defines business models as the design of organizational structures to enact a commercial opportunity.[5] Further extensions to this design logic emphasize the use of narrative or coherence in business model descriptions as mechanisms by which entrepreneurs create extraordinarily successful growth firms.[6]\n\nBusiness models are used to describe and classify businesses, especially in an entrepreneurial setting, but they are also used by managers inside companies to explore possibilities for future development. Well-known business models can operate as \"recipes\" for creative managers.[7] Business models are also referred to in some instances within the context of accounting for purposes of public reporting.\n\nContents  [hide]\n1\tHistory\n2\tTheoretical and empirical insights to business models\n2.1\tDesign logic and narrative coherence\n2.2\tComplementarities of business models between partnering firms\n3\tCategorization of business models\n3.1\tV4 BM framework\n3.2\tShift from pipes to platforms\n3.3\tPlatform business models\n4\tApplications\n5\tBusiness model design\n5.1\tDefinitions of business model\n5.1.1\tEconomic consideration\n5.1.2\tComponent consideration\n5.1.3\tStrategic outcome\n6\tDefinitions of business model design or development\n6.1\tDesign themes emphasis of business model\n6.2\tDesign content emphasis of business model design\n7\tExamples of business models\n8\tBusiness model frameworks\n9\tRelated concepts\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nHistory\nOver the years, business models have become much more sophisticated. The bait and hook business model (also referred to as the \"razor and blades business model\" or the \"tied products business model\") was introduced in the early 20th century. This involves offering a basic product at a very low cost, often at a loss (the \"bait\"), then charging compensatory recurring amounts for refills or associated products or services (the \"hook\"). Examples include: razor (bait) and blades (hook); cell phones (bait) and air time (hook); computer printers (bait) and ink cartridge refills (hook); and cameras (bait) and prints (hook). A variant of this model is Adobe, a software developer that gives away its document reader free of charge but charges several hundred dollars for its document writer.\n\nIn the 1950s, new business models came from McDonald's Restaurants and Toyota. In the 1960s, the innovators were Wal-Mart and Hypermarkets. The 1970s saw new business models from FedEx and Toys R Us; the 1980s from Blockbuster, Home Depot, Intel, and Dell Computer; the 1990s from Southwest Airlines, Netflix, eBay, Amazon.com, and Starbucks.\n\nToday, the type of business models might depend on how technology is used. For example, entrepreneurs on the internet have also created entirely new models that depend entirely on existing or emergent technology. Using technology, businesses can reach a large number of customers with minimal costs. In addition, the rise of outsourcing and globalization has meant that business models must also account for strategic sourcing, complex supply chains and moves to collaborative, relational contracting structures.[8]\n\nTheoretical and empirical insights to business models\nDesign logic and narrative coherence\nDesign logic views the business model as an outcome of creating new organizational structures or changing existing structures to pursue a new opportunity. Gerry George and Adam Bock (2011) conducted a comprehensive literature review and surveyed managers to understand how they perceived the components of a business model. In that analysis these authors show that there is a design logic behind how entrepreneurs and managers perceive and explain their business model. In further extensions to the design logic, George and Bock (2012) use case studies and the IBM survey data on business models in large companies, to describe how CEOs and entrepreneurs create narratives or stories in a coherent manner to move the business from one opportunity to another. They also show that when the narrative is incoherent or the components of the story are misaligned, that these businesses tend to fail. They recommend ways in which the entrepreneur or CEO can create strong narratives for change.\n\nComplementarities of business models between partnering firms\nStudying collaborative research and the accessing of external sources of technology, Hummel et al. (2010) found that in deciding on business partners, it is important to make sure that both parties' business models are complementary.[9] For example, they found that it was important to identify the value drivers of potential partners by analyzing their business models, and that it is beneficial to find partner firms that understand key aspects of our own firm�s business model.[10]\n\nThe University of Tennessee conducted research into highly collaborative business relationships. Researchers codified their research into a sourcing business model known as Vested (also referred to as Vested Outsourcing). Vested is a hybrid sourcing business model in which buyers and suppliers in an outsourcing or business relationship focus on shared values and goals to create an arrangement that is highly collaborative and mutually beneficial to each.[11]\n\nCategorization of business models\nFrom about 2012, some research and experimentation has theorized about a so-called \"liquid business model\".[12][13]\n\nV4 BM framework\nAl-Debei and Avison (2010) V4 BM Framework - four main dimensions encapsulating sixteen elements: Value Proposition, Value Architecture, Value Network, and Value Finance[3]\n\nValue Proposition: This dimension implies that a BM should include a description of the products/services a digital organization offers, or will offer, along with their related information. Furthermore, the BM needs also to describe the value elements incorporated within the offering, as well as the nature of targeted market segment(s) along with their preferences.\nValue Architecture: portrays the concept as a holistic structural design of an organization, including its technological architecture, organizational infrastructure, and their configurations.\nValue Network: depicts the cross-company or inter-organization perspective towards the concept and has gained much attention in the BM literature.\nValue Finance: depicts information related to costing, pricing methods, and revenue structure\nShift from pipes to platforms\nSangeet Paul Choudary (2013) distinguishes between two broad families of business models in an article in Wired magazine.[14] Choudary contrasts pipes (linear business models) with platforms (networked business models). In the case of pipes, firms create goods and services, push them out and sell them to customers. Value is produced upstream and consumed downstream. There is a linear flow, much like water flowing through a pipe. Unlike pipes, platforms do not just create and push stuff out. They allow users to create and consume value.\n\nIn an op-ed on MarketWatch,[15] Choudary, Van Alstyne and Parker further explain how business models are moving from pipes to platforms, leading to disruption of entire industries.\n\nPlatform business models\nThere are three elements to a successful platform business model.[16] The Toolbox creates connection by making it easy for others to plug into the platform. This infrastructure enables interactions between participants. The Magnet creates pull that attracts participants to the platform. For transaction platforms, both producers and consumers must be present to achieve critical mass. The Matchmaker fosters the flow of value by making connections between producers and consumers. Data is at the heart of successful matchmaking, and distinguishes platforms from other business models.\n\nChen (2009) stated that the business model has to take into account the capabilities of Web 2.0, such as collective intelligence, network effects, user-generated content, and the possibility of self-improving systems. He suggested that the service industry such as the airline, traffic, transportation, hotel, restaurant, information and communications technology and online gaming industries will be able to benefit in adopting business models that take into account the characteristics of Web 2.0. He also emphasized that Business Model 2.0 has to take into account not just the technology effect of Web 2.0 but also the networking effect. He gave the example of the success story of Amazon in making huge revenues each year by developing an open platform that supports a community of companies that re-use Amazon's on-demand commerce services.[17][need quotation to verify]\n\nApplications\nMalone et al.[18] found that some business models, as defined by them, indeed performed better than others in a dataset consisting of the largest U.S. firms, in the period 1998 through 2002, while they did not prove whether the existence of a business model mattered.\n\nIn the context of the Software-Cluster, which is funded by the German Federal Ministry of Education and Research, a business model wizard for software companies has been developed. It supports the design and analysis of software business models. The tool's underlying concept and data were published in various scientific publications.\n\nThe concept of a business model has been incorporated into certain accounting standards. For example, the International Accounting Standards Board (IASB) utilizes an \"entity's business model for managing the financial assets\" as a criterion for determining whether such assets should be measured at amortized cost or at fair value in its financial instruments accounting standard, IFRS 9.[19][20][21][22] In their 2013 proposal for accounting for financial instruments, the Financial Accounting Standards Board also proposed a similar use of business model for classifying financial instruments.[23] The concept of business model has also been introduced into the accounting of deferred taxes under International Financial Reporting Standards with 2010 amendments to IAS 12 addressing deferred taxes related to investment property.[24][25][26]\n\nBoth IASB and FASB have proposed using the concept of business model in the context of reporting a lessor's lease income and lease expense within their joint project on accounting for leases.[27][28][29][30][31] In its 2016 lease accounting model, IFRS 16, the IASB chose not to include a criterion of \"stand alone utility\" in its lease definition because \"entities might reach different conclusions for contracts that contain the same rights of use, depending on differences between customers' resources or suppliers' business models.\"[32] The concept has also been proposed as an approach for determining the measurement and classification when accounting for insurance contracts.[33][34] As a result of the increasing prominence the concept of business model has received in the context of financial reporting, the European Financial Reporting Advisory Group (EFRAG), which advises the European Union on endorsement of financial reporting standards, commenced a project on the \"Role of the Business Model in Financial Reporting\" in 2011.[35]\n\nBusiness model design\nBusiness model design refers to the activity of designing a company's business model. It is part of the business development and business strategy process and involves design methods.\n\nDefinitions of business model\nAl-Debei and Avison (2010) define a business model as an abstract representation of an organization. This may be conceptual, textual, and/or graphical, of all core interrelated architectural, co-operational, and financial arrangements designed and developed by an organization presently and in the future, as well all core products and/or services the organization offers, or will offer, based on these arrangements that are needed to achieve its strategic goals and objectives.[1] This definition indicates that value proposition, value architecture, value finance, and value network articulate the primary constructs or dimensions of business models.[3]\n\nEconomic consideration\nAl-Debei and Avison (2010) consider value finance as one of the main dimensions of BM which depicts information related to costing, pricing methods, and revenue structure. Stewart and Zhao (2000) defined the business model as ��a statement of how a firm will make money and sustain its profit stream over time.�� [36]\n\nComponent consideration\nOsterwalder et al. (2005) consider the Business Model as the blueprint of how a company does business.[37] Slywotzky (1996) regards the business model as ��the totality of how a company selects its customers, defines and differentiates it offerings, defines the tasks it will perform itself and those it will outsource, configures its resources, goes to market, creates utility for customers and captures profits.�� [38]\n\nStrategic outcome\nMayo and Brown (1999) considered the business model as ��the design of key interdependent systems that create and sustain a competitive business.�� [39]\n\nDefinitions of business model design or development\nZott and Amit (2009) consider business model design from the perspectives of design themes and design content. Design themes refer to the system�s dominant value creation drivers and design content examines in greater detail the activities to be performed, the linking and sequencing of the activities and who will perform the activities.[40]\n\nDesign themes emphasis of business model\n\nEnvironment-Strategy-Structure-Operations (ESSO) Business Model Development\nDeveloping a Framework for Business Model Development with an emphasis on Design Themes, Lim (2010) proposed the Environment-Strategy-Structure-Operations (ESSO) Business Model Development which takes into consideration the alignment of the organization�s strategy with the organization's structure, operations, and the environmental factors in achieving competitive advantage in varying combination of cost, quality, time, flexibility, innovation and affective.[41]\n\nDesign content emphasis of business model design\nBusiness model design includes the modeling and description of a company's:\n\nvalue propositions\ntarget customer segments\ndistribution channels\ncustomer relationships\nvalue configurations\ncore capabilities\npartner network\ncost structure\nrevenue model\nBusiness model design is distinct from business modeling. The former refers to defining the business logic of a company at the strategic level, whereas the latter refers to business process design at the operational level.\n\nA business model design template can facilitate the process of designing and describing a company's business model.\n\nDaas et al. (2012) developed a decision support system (DSS) for business model design. In their study a decision support system (DSS) is developed to help SaaS in this process, based on a design approach consisting of a design process that is guided by various design methods.[42]\n\nExamples of business models\nIn the early history of business models it was very typical to define business model types such as bricks-and-mortar or e-broker. However, these types usually describe only one aspect of the business (most often the revenue model). Therefore, more recent literature on business models concentrate on describing a business model as a whole, instead of only the most visible aspects.\n\nThe following examples provide an overview for various business model types that have been in discussion since the invention of term business model:\n\nBricks and clicks business model\nBusiness model by which a company integrates both offline (bricks) and online (clicks) presences. One example of the bricks-and-clicks model is when a chain of stores allows the user to order products online, but lets them pick up their order at a local store.\nCollective business models\nBusiness system, organization or association typically composed of relatively large numbers of businesses, tradespersons or professionals in the same or related fields of endeavor, which pools resources, shares information or provides other benefits for their members. For example, a science park or high-tech campus provides shared resources (e.g. cleanrooms and other lab facilities) to the firms located on its premises, and in addition seeks to create an innovation community among these firms and their employees.[43]\nCutting out the middleman model\nThe removal of intermediaries in a supply chain: \"cutting out the middleman\". Instead of going through traditional distribution channels, which had some type of intermediate (such as a distributor, wholesaler, broker, or agent), companies may now deal with every customer directly, for example via the Internet.\nDirect sales model\nDirect selling is marketing and selling products to consumers directly, away from a fixed retail location. Sales are typically made through party plan, one-to-one demonstrations, and other personal contact arrangements. A text book definition is: \"The direct personal presentation, demonstration, and sale of products and services to consumers, usually in their homes or at their jobs.\"[44]\nDistribution business models, various\nValue-added reseller model\nValue Added Reseller is a model where a business makes something which is resold by other businesses but with modifications which add value to the original product or service. These modifications or additions are mostly industry specific in nature and are essential for the distribution. Businesses going for a VAR model have to develop a VAR network. It is one of the latest collaborative business models which can help in faster development cycles and is adopted by many Technology companies especially software.\nFee in, free out\nBusiness model which works by charging the first client a fee for a service, while offering that service free of charge to subsequent clients.\nFranchise\nFranchising is the practice of using another firm's successful business model. For the franchisor, the franchise is an alternative to building 'chain stores' to distribute goods and avoid investment and liability over a chain. The franchisor's success is the success of the franchisees. The franchisee is said to have a greater incentive than a direct employee because he or she has a direct stake in the business.\n� Sourcing business model\n\nA Sourcing Business Model is a type of business model that is applied specifically to business relationships where more than one party needs to work with another party to be successful. It is the combination of two concepts: the contractual relationship framework a company uses with its supplier (transactional, relational, investment based), and the economic model used (transactional, output or outcome-based).\nFreemium business model\nBusiness model that works by offering basic Web services, or a basic downloadable digital product, for free, while charging a premium for advanced or special features.[45]\nPay what you can (PWYC) is a non-profit or for-profit business model which does not depend on set prices for its goods, but instead asks customers to pay what they feel the product or service is worth to them.[46][47][48] It is often used as a promotional tactic,[49] but can also be the regular method of doing business. It is a variation on the gift economy and cross-subsidization, in that it depends on reciprocity and trust to succeed.\n\"Pay what you want\" (PWYW) is sometimes used synonymously, but \"pay what you can\" is often more oriented to charity or socially oriented uses, based more on ability to pay, while \"pay what you want\" is often more broadly oriented to perceived value in combination with willingness and ability to pay.\n\nOther examples of business models are:\n\nAuction business model\nAll-in-one business model\nChemical Leasing\nLow-cost carrier business model\nLoyalty business models\nMonopolistic business model\nMulti-level marketing business model\nNetwork effects business model\nOnline auction business model\nOnline content business model\nOnline media cooperative\nPremium business model\nProfessional open-source model\nPyramid scheme business model\nRazor and blades business model\nServitization of products business model\nSubscription business model\nBusiness model frameworks\nTechnology centric communities have defined \"frameworks\" for business modeling. These frameworks attempt to define a rigorous approach to defining business value streams. It is not clear, however, to what extent such frameworks are actually important for business planning. Business model frameworks represent the core aspect of any company; they involve �the totality of how a company selects its customers defines and differentiates its offerings, defines the tasks it will perform itself and those it will outsource, configures its resource, goes to market, creates utility for customers, and captures profits�.[50] A business framework involves internal factors (market analysis; products/services promotion; development of trust; social influence and knowledge sharing) and external factors (competitors and technological aspects).[51] A state of the art review on business model frameworks can be found in Krumeich et al. (2012).[52] In the following some frameworks are introduced.\n\nBusiness reference model\nBusiness reference model is a reference model, concentrating on the architectural aspects of the core business of an enterprise, service organization or government agency.\nComponent business model\nTechnique developed by IBM to model and analyze an enterprise. It is a logical representation or map of business components or \"building blocks\" and can be depicted on a single page. It can be used to analyze the alignment of enterprise strategy with the organization's capabilities and investments, identify redundant or overlapping business capabilities, etc.\n\nAlthough Webvan failed in its goal of disintermediating the North American supermarket industry, several supermarket chains (like Safeway Inc.) have launched their own delivery services to target the niche market to which Webvan catered.\nIndustrialization of services business model\nBusiness model used in strategic management and services marketing that treats service provision as an industrial process, subject to industrial optimization procedures\nBusiness Model Canvas\nDeveloped by A. Osterwalder, Yves Pigneur, Alan Smith, and 470 practitioners from 45 countries, the business model canvas [4][53] is one of the most used frameworks for describing the elements of business models.\nRelated concepts\nThe process of business model design is part of business strategy. Business model design and innovation refer to the way a firm (or a network of firms) defines its business logic at the strategic level.\n\nIn contrast, firms implement their business model at the operational level, through their business operations. This refers to their process-level activities, capabilities, functions and infrastructure (for example, their business processes and business process modeling), their organisational structures (e.g. organigrams, workflows, human resources) and systems (e.g. information technology architecture, production lines).\n\nConsequently, an operationally viable and feasible business model requires lateral alignment with the underlining business operations.[54]\n\nThe brand is a consequence of the business model and has a symbiotic relationship with it, because the business model determines the brand promise, and the brand equity becomes a feature of the model. Managing this is a task of integrated marketing.\n\nThe standard terminology and examples of business models do not apply to most nonprofit organizations, since their sources of income are generally not the same as the beneficiaries. The term funding model is generally used instead.[55]\n\nThe model is defined by the organization�s vision, mission, and values, as well as sets of boundaries for the organization�what products or services it will deliver, what customers or markets it will target, and what supply and delivery channels it will use. While the business model includes high-level strategies and tactical direction for how the organization will implement the model, it also includes the annual goals that set the specific steps the organization intends to undertake in the next year and the measures for their expected accomplishment. Each of these is likely to be part of internal documentation that is available to the internal auditor.\n\nSee also\n\tWikimedia Commons has media related to Business models.\nBusiness plan\nBusiness process modeling\nBusiness reference model\nBusiness rule\nCompetitive advantage\nCore competency\nGrowth platforms\nMarket forms\nMarketing\nMarketing plan\nStrategic management\nStrategy Markup Language\nStrategic planning\nStrategy dynamics\nValue migration\nThe Design of Business\nEnterprise Architecture\nBusiness Model Canvas\nComponent business model", "skillName": "Business Model."}
{"id": 95, "category": "Business", "skillText": "Corporate communication is a set of activities involved in managing and orchestrating all internal and external communications aimed at creating favourable point of view among stakeholders on which the company depends.[1] It is the messages issued by a corporate organization, body, or institute to its audiences, such as employees, media, channel partners and the general public. Organizations aim to communicate the same message to all its stakeholders, to transmit coherence, credibility and ethic. Corporate Communications help organizations explain their mission, combine its many visions and values into a cohesive message to stakeholders. The concept of corporate communication could be seen as an integrative communication structure linking stakeholders to the organization.\n\nContents\n\n    1 Methods and tactics\n    2 Components\n        2.1 Corporate branding\n        2.2 Corporate and organizational identity\n        2.3 Corporate responsibility\n        2.4 Corporate reputation\n        2.5 Crisis communications\n        2.6 Internal/employee communications\n        2.7 Investor relations\n        2.8 Public relations: issues management and media relations\n            2.8.1 Issues management\n            2.8.2 Media relations\n            2.8.3 Company/spokesperson profiling\n    3 References\n\nMethods and tactics\n\nThree principal clusters of task-planning and communication form the backbone of business and the activity of business organizations. These include management communication, marketing communication, and organizational communication.\n\n    Management communication takes place between management and its internal and external audiences. To support management communication, organizations rely heavily on specialists in marketing communication and organizational communication.[citation needed]\n    Marketing communication gets the bulk of the budgets in most organizations, and consists of product advertising, direct mail, personal selling, and sponsorship activities.\n    Organizational communication consist of specialists in public relations, public affairs, investor relations, environmental communications, corporate advertising, and employee communication.\n\nThe responsibilities of corporate communication are:\n\n    to promote the profile of the \"company behind the brand\" (corporate branding)\n    to minimize discrepancies between the company's desired identity and brand features\n    to delegate tasks in communication\n    to formulate and execute effective procedures to make decisions on communication matters\n    to mobilize internal and external support for corporate objectives\n    to coordinate with international business firms\n\nA Conference Board Study of hundreds of the US’s largest firms showed that close to 80 percent have corporate communication functions that include media relations, speech writing, employee communication, corporate advertising, and community relations.[2] The public is often represented by self-appointed activist non-governmental organizations (NGOs) who identify themselves with a particular issue.\n\nMost companies have specialized groups of professionals for communicating with different audiences, such as internal communication, marketing communication, investor relations, government relations and public relations.[1]\nComponents\nCorporate branding\nMain article: Corporate branding\n\nA corporate brand is the perception of a company that unites a group of products or services for the public under a single name, a shared visual identity, and a common set of symbols. The process of corporate branding consists creating favourable associations and positive reputation with both internal and external stakeholders. The purpose of a corporate branding initiative is to generate a positive halo over the products and businesses of the company, imparting more favourable impressions of those products and businesses.\n\nIn more general terms, research suggests that corporate branding is an appropriate strategy for companies to implement when:\n\n    there is significant \"information asymmetry\" between a company and its clients;[3] That is to say customers are much less informed about a company's products than the company itself is;\n    customers perceive a high degree of risk in purchasing the products or services of the company;[4]\n    features of the company behind the brand would be relevant to the product or service a customer is considering purchasing.[5]\n\nCorporate and organizational identity\n\nThere are two approaches for identity:\n\n    Corporate identity is the reality and uniqueness of an organization, which is integrally related to its external and internal image and reputation through corporate communication[6]\n    Organizational identity comprises those characteristics of an organization that its members believe are central, distinctive and enduring. That is, organizational identity consists of those attributes that members feel are fundamental to (central) and uniquely descriptive of (distinctive) the organization and that persist within the organization over time (enduring)\".[7]\n\nFour types of identity can be distinguished:[8][9]\n\n    Perceived identity: The collection of attributes that are seen as typical for the ‘continuity, centrality and uniqueness’ of the organization in the eyes of its members.\n    Projected identity: The self presentations of the organization’s attributes manifested in the implicit and explicit signals which the organization broadcasts to internal and external target audiences through communication and symbols.\n    Desired identity (also called ‘ideal’ identity): The idealized picture that top managers hold of what the organization could evolve into under their leadership.\n    Applied identity: The signals that an organization broadcasts both consciously and unconsciously through behaviors and initiatives at all levels within the organization.\n\nCorporate responsibility\nMain article: Corporate social responsibility\n\nCorporate responsibility (often referred to as corporate social responsibility), corporate citizenship, sustainability, and even conscious capitalism are some of the terms bandied about the news media and corporate marketing efforts as companies jockey to win the trust and loyalty of constituents. Corporate responsibility (CR) constitutes an organization’s respect for society’s interests, demonstrated by taking ownership of the effects its activities have on key constituencies including customers, employees, shareholders, communities, and the environment, in all parts of their operations. In short, CR prompts a corporation to look beyond its traditional bottom line, to the social implications of its business.[10]\nCorporate reputation\nMain article: Reputation management\n\nReputations are overall assessments of organizations by their stakeholders. They are aggregate perceptions by stakeholders of an organization's ability to fulfill their expectations, whether these stakeholders are interested in buying the company's products, working for the company, or investing in the company's shares.[11]\n\nIn 2000, the US-based Council of PR Firms identified seven programs developed by either media organizations or market research firms, used by companies to assess or benchmark their corporate reputations. Of these, only four are conducted regularly and have broad visibility:\n\n    \"America's Most Admired Companies\" by Fortune Magazine;\n    The \"Brand Asset Valuator\" by Young & Rubicam;\n    \"RepTrak\" by Reputation Institute \n    .\n    \"Best Global Brands\" by Interbrand.\n\nCrisis communications\nMain article: Crisis communications\n\nCrisis communication is sometimes considered a sub-specialty of the public relations profession that is designed to protect and defend an individual, company, or organization facing a public challenge to its reputation. These challenges may come in the form of an investigation from a government agency, a criminal allegation, a media inquiry, a shareholders lawsuit, a violation of environmental regulations, or any of a number of other scenarios involving the legal, ethical, or financial standing of the entity. The crisis for organizations can be defined as follows:[10]\n\n    A crisis is a major catastrophe that may occur either naturally or as a result of human error, intervention, or even malicious intent. It can include tangible devastation, such as the destruction of lives or assets, or intangible devastation, such as the loss of an organization's credibility or other reputational damage. The latter outcomes may be the result of management's response to tangible devastation or the result of human error. A crisis usually has significant actual or potential financial impact on a company, and it usually affects multiple constituencies in more than one market.\n\nInternal/employee communications\nMain article: Internal communications\n\nAs the extent of communication grows, many companies create an employee relations (ER) function with dedicated staff to manage the numerous media through which senior managers can communicate among themselves and with the rest of the organization. Internal communication in the 21st century is more than the memos, publications, and broadcasts that comprise it; it’s about building a corporate culture on values that drive organizational excellence. ER specialists are generally expected to fulfill one or more of the following four roles:[12]\n\n    Efficiency: Internal communication is used primarily to disseminate information about corporate activities.\n    Shared meaning: Internal communication is used to build a shared understanding among employees about corporate goals.\n    Connectivity: Internal communication is used mainly to clarify the connectedness of the company's people and activities.\n    Satisfaction: Internal communication is used to improve job satisfaction throughout the company.\n\nInvestor relations\nMain article: Investor relations\n\nThe investor relations (IR) function is used by companies which publicly trade shares on a stock exchange. In such companies, the purpose of the IR specialist is to interface with current and potential financial stakeholders-namely retail investors, institutional investors, and financial analysts.\n\nThe role of investor relations is to fulfill three principal functions:\n\n    comply with regulations;\n    Create a favorable relationship with key financial audiences;\n    contribute to building and maintaining the company's image and reputation.\n\nPublic relations: issues management and media relations\nMain article: Public relations\n\nThe role of the public relations specialist, in many ways, is to communicate with the general public in ways that serve the interests of the company. PR therefore consists of numerous specialty areas that convey information about the company to the public, including sponsorships, events, issues management and media relations. When executing these types of activities, the PR Specialist must incorporate broader corporate messages to convey the company’s strategic positioning. This ensures the PR activities ultimately convey messages that distinguish the company vis-à-vis its competitors and the overall marketplace, while also communicating the company’s value to target audiences.\nIssues management\n\nA key role of the PR specialist is to make the company better known for traits and attributes that build the company’s perceived distinctiveness and competitiveness with the public. In recent years, PR specialists have become increasingly involved in helping companies manage strategic issues – public concerns about their activities that are frequently magnified by special interest groups and NGOs. The role of the PR specialist therefore also consists of issues management, namely the “set of organizational procedures, routines, personnel, and issues”.[13] A strategic issue is one that compels a company to deal with it because there is “ a conflict between two or more identifiable groups over procedural or substantive matters relating to the distribution of positions or resources”.[14]\nMedia relations\n\nTo build better relationships with the media, organizations must cultivate positive relations with influential members of the media. This task might be handled by employees within the company’s media relations department or handled by a public relations firm.\nCompany/spokesperson profiling\n\nThese \"public faces\" are considered authorities in their respective sector/field and ensure the company/organization is in the limelight.\n\n    Managing content of corporate websites and/or other external touch points\n    Managing corporate publications - for the external world\n    Managing print media", "skillName": "Corporate communication."}
{"id": 96, "category": "Research", "skillText": "The scholarly method or scholarship is the body of principles and practices used by scholars to make their claims about the world as valid and trustworthy as possible, and to make them known to the scholarly public. It is the methods that systemically advance the teaching, research, and practice of a given scholarly or academic field of study through rigorous inquiry. Scholarship is noted by its significance to its particular profession, and is creative, can be documented, can be replicated or elaborated, and can be and is peer-reviewed through various methods.[1]\n\nContents\n\n    1 Methods\n    2 Ethical issues to consider when doing research\n    3 See also\n    4 References\n\nMethods\n\nOriginally started to reconcile the philosophy of the ancient classical philosophers with medieval Christian theology, scholasticism is not a philosophy or theology in itself but a tool and method for learning which places emphasis on dialectical reasoning. The primary purpose of scholasticism is to find the answer to a question or to resolve a contradiction. It was once well known for its application in medieval theology, but was eventually applied to classical philosophy and many other fields of study.\n\nThe historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. The question of the nature, and indeed the possibility, of sound historical method is raised in the philosophy of history, as a question of epistemology. History guidelines commonly used by historians in their work require external criticism, internal criticism, and synthesis.\n\nThe empirical method is generally taken to mean the collection of data on which to base a hypothesis or derive a conclusion in science. It is part of the scientific method, but is often mistakenly assumed to be synonymous with other methods. The empirical method is not sharply defined and is often contrasted with the precision of experiments, where data is derived from the systematic manipulation of variables. The experimental method investigates causal relationships among variables. An experiment is a cornerstone of the empirical approach to acquiring data about the world and is used in both natural sciences and social sciences. An experiment can be used to help solve practical problems and to support or negate theoretical assumptions.\n\nThe scientific method refers to a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry must be based on gathering observable, empirical and measurable evidence subject to specific principles of reasoning.[2] A scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.[3]\nEthical issues to consider when doing research\n\nThere are numerous examples of ethical concerns, including:\n\n    Confidentiality of information used;\n    Participants’ anonymity;\n    Consent by participants;\n    Security and benefits to individuals.\n\nSee also\n\tWikiquote has quotations related to: Scholarship\n\tLook up scholar, scholarly, or scholarship in Wiktionary, the free dictionary.\n\n    Academia\n    Academic authorship\n    Academic publishing\n    Discipline (academia)\n    Doctor (title)\n    Historical revisionism\n    History of scholarship\n    Manual of style\n    Professor\n    Research\n    Rigour\n    Source criticism\n    Urtext edition", "skillName": "ScholarlyMethod."}
{"id": 97, "category": "Research", "skillText": "Undergraduate research is the exploration of a specific topic within a field by an undergraduate student that makes an original contribution to the discipline.[1] It is a fairly recent concept in the academic community, with roots in the nineteenth and twentieth centuries. The creation of MIT’s Undergraduate Research Opportunities Program (UROP) in 1969 encouraged an explosion in popularity. Undergraduate research programs were fairly common by the 1990s. Students may work on their own, collaborate with faculty members, or seek enrollment in a research program within their field. Both faculty members and students experience advantages and disadvantages when collaborating on research. Undergraduate research can be conducted in the sciences (both biological and physical) and in the humanities. The research approach differs depending on the field and the focus. Undergraduate research is often required for acceptance to graduate and professional schools.\n\nContents\n\n    1 History\n    2 Value\n    3 Disciplinary variations\n        3.1 Humanities\n        3.2 Sciences\n            3.2.1 Chemistry\n            3.2.2 Biology\n            3.2.3 Physics\n            3.2.4 Geology\n    4 Students\n    5 Faculty\n        5.1 Advantages\n        5.2 Disadvantages\n    6 See also\n    7 References\n    8 Further reading\n\nHistory\n\nAccording to Edward Ayers, undergraduate research is a “relatively recent” development in higher education, although it has its roots in early nineteenth and twentieth-century practice. In 1810, Wilhelm von Humboldt founded the University of Berlin, which created the model for undergraduate research. During the nineteenth century, many Americans went to Germany for graduate education. Many Americans began to call for a transition towards the German education systems, with specialized disciplines and majors. This paved the way for undergraduate education. Mentions and praises of undergraduate research could be found in journals and magazines by the early 1900s and in 1912, the University of Chicago established the undergraduate research prize in memory of Howard Ricketts. In 1969, Margaret MacVicar established the Undergraduate Research Opportunities Program at MIT. It was considered the first Undergraduate Research Program. The Council on Undergraduate Research (CUR) was established in 1978 as a faculty development initiative; its headquarters are in Washington, DC. The National Conference on Undergraduate Research (NCUR) was formed in 1987 and hosted its first conference at UNC Asheville. Recent NCUR events have attracted 3000 students. CUR and NCUR merged in 2010.Since the 1990s, many universities and colleges have instituted programs and offices[2] meant to foster research at the undergraduate level.[3] The Council on Undergraduate Research Quarterly contributes to the growth and development of undergraduate research endeavors across colleges and universities by highlighting best practices, models for mentoring, and the assessment of undergraduate research. The goal of the journal is to provide useful and inspiring information about student-faculty collaborative research and scholarship from all disciplines at all types of institutions in the United States and abroad.\nValue\n\nUndergraduate research is defined broadly to include scientific inquiry, creative activity, and scholarship. An undergraduate research project might result in a musical composition, a work of art, an agricultural field experiment, or an analysis of historical documents. The key is that the project produces some original work.[4] There are many benefits to undergraduate research including, but not limited to, real world applications, research and professional experience, and better relationships with faculty and peers. According to Heather Thiry, “Through coursework and out-of-class experiences, students described learning to work and think independently, to take responsibility for their own learning, and to take initiative to solve problems on their own rather than relying on experts for the answers.”[5] In addition, institutions find value in promoting undergraduate research to recruit and retain students and to prepare them for graduate studies[6] Undergraduate research \"days\" at state capitols is an effective way to showcase the effective of learning and teaching through research.[7] The Council on Undergraduate Research (CUR) inspired many states to create state versions of its Posters on the Hill event, which takes place each April in Washington, DC.\nDisciplinary variations\n\nResearch methods vary depending on the field of study a student decides to participate in. These fields of study are more commonly broken up into humanities and sciences. Humanities include, but are not limited to, fields such as history, literature, philosophy, and the performing arts. Sciences include physics, biology, psychology, and chemistry.\nHumanities\n\nThe undergraduate research in humanities generally promotes the same values as that in the sciences. Such values include collaboration, unique thought, and interdisciplinary works. It differs from the sciences in that it is based more on theoretical research than laboratory research. For example, undergraduate research in the performing arts consists of the developing of new performing pieces as experimentation. Though not viewed as research in the traditional sense, this is a form of experimentation.[8] Because of this differentiation, this field of undergraduate research is not as well funded as it is for the sciences.[3] Also, there are many college students interested in studying the humanities; in actuality, more STEM-related majors take humanities courses than humanities-related majors take STEM courses.[9]\n\nUniversities have utilized course-based research since 1998, but few have applied it to the humanities. However, they have increasingly done so with time.[10] For example, since 2001, the University of Washington has established a lecture-discussion panel as well as an annual Undergraduate Research Symposium and Summer Institute with the purpose of supporting undergraduate research in the humanities; it does so through providing incentives such as funding for research.Volumes on research in specific humanities disciplines have begun to appear, particularly in English[11] and Religious Studies.\nSciences\n\n        The science disciplines are more lab-oriented than the humanities, although fieldwork can occasionally play an important role in the research process.\n\nChemistry\n\n        Undergraduate research can be done through self-directed experiments under the guidance of an advisor. Most work is completed in the lab because chemists learn by working in the lab and experience is gained with the lab equipment. The students gain a greater understanding of the material while advances are made in science.[12]\n\nBiology\n\n        It is a combination of working in the laboratory and in the environment for the purpose of better understanding the world around us.[13]\n\nPhysics\n\n        Undergraduate research for the field of physics is always hands on and practical. Students tend to focus on the “why-nots” and the “what-ifs.”[14]\n\nGeology\n\n        Undergraduate work in the geology field has strong ties to both climatology and environmental sciences. While work takes place in the lab for analysis, there is a distinct amount of work to be done in the field like studying fossilized remains, minerals, or other geologic formations to better understand trends in the past and the future.[15]\n\nStudents\n\nUndergraduate research provides opportunities for independent research, professional student/teacher relationships, and experience in the field of study. Undergraduate research is a tool for students to gain knowledge about their field using their own methods and not relying on a professor or supervisor to walk them through the study. Independent research will help students feel confident and competent when performing tasks in their future career.[16] The results of undergraduate research can sometimes be published in peer-reviewed venues, such as an undergraduate research journal dedicated specifically to such work, or in traditional academic journals with the student as a coauthor.\n\nBoth students and faculty recognize the benefits of undergraduate research. Faculty from Harvey Mudd College, Wellesley College, and Grinnell College were surveyed, and listed benefits such as opportunities to work and think independently, learning through reading literature and communication, increased problem solving skills, and an appreciation of what scientists do. Student responses from this study included an enhancement of professional or academic credentials, as well as the clarification of a career path. The author of this study stated that the \"benefits [students] value result from a good relationship with and expert guidance from a mentor.\" The greatest benefit to both professor and student is the personal guidance and knowledge shared between the two – far greater a reward than fancy equipment or a diploma.\n\nMany graduate programs and professional schools require undergraduate research. For example, admissions boards for medical schools look for research experience when admitting students. Therefore, not only will pre-medical students benefit from learning how to apply hypothesis-based research, but they will also be more likely to be accepted to medical school if they have participated in undergraduate research.[17] This holds true for graduate programs in a variety of subjects.\nFaculty\n\nFaculty members involved in undergraduate research perform two different roles. Faculty members may act as primary researchers who lead the investigation and provide support for students, and in other circumstances, undergraduate research is led by the student. In this case, the faculty member acts as an advisor to keep students on track and provide assistance when needed.\nAdvantages\n\nUndergraduate involvement in research may increase efficiency as well as provide other skill sets. In institutions that focus on their professors’ role as teachers, administrators reward faculty members’ involvement in undergraduate research. Some faculty members report, according to Scott Windham, that working with students on undergraduate research gives them the personal satisfaction of helping students grow and professionally develop.[18] Showcasing students’ research at undergraduate conferences highlights the hard work of the students and their faculty mentors. Faculty members might also become better teachers due to their work on cutting-edge research. Colleges may also assess undergraduate research as part of departmental productivity. Taking part in undergraduate research can help faculty members in the long run because the students they work with can train future undergraduate researchers. Colleges might also give more funding to faculty research if they work with undergraduate students.[19]\nDisadvantages\n\nAccording to Scott Windham, \"Publication may be limited to second tier journals if an undergraduate student is listed as a co-author.\"[18] As a result, in some institutions, working with undergraduate students can negatively affect tenure and promotion opportunities. Some observers report fear that undergraduate research may exploit students in order to promote the faculty member’s career. In addition, faculty members perceive that research with students may keep them from conducting their own research, will require more time or resources, or will not help them with their professional development. Many faculty members do not receive teaching credit for mentoring undergraduate research, nor do they receive any other types of rewards, incentives, or compensation. A problem also arises when the student is not prepared properly or if the student is not motivated enough.[20]\nSee also\n\n    Research\n    Undergraduate education", "skillName": "UndergraduateResearch."}
{"id": 98, "category": "Research", "skillText": "Research comprises \"creative work undertaken on a systematic basis in order to increase the stock of knowledge, including knowledge of humans, culture and society, and the use of this stock of knowledge to devise new applications.\"[1] It is used to establish or confirm facts, reaffirm the results of previous work, solve new or existing problems, support theorems, or develop new theories. A research project may also be an expansion on past work in the field. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects, or the project as a whole. The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, or the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life,technological,etc.\n\nContents\n\n    1 Forms of research\n    2 Etymology\n    3 Definitions\n    4 Steps in conducting research\n    5 Scientific research\n    6 Historical method\n    7 Research methods\n        7.1 Research method controversies\n            7.1.1 Quantitative vs. Qualitative war\n            7.1.2 Anti-methodology\n            7.1.3 Methodological academic imperialism\n    8 Professionalisation\n        8.1 In Russia\n    9 Publishing\n    10 Research funding\n    11 Original research\n        11.1 Different forms\n    12 Artistic research\n    13 See also\n    14 References\n    15 Further reading\n    16 External links\n\nForms of research\n\nScientific research is a systematic way of gathering data and harnessing curiosity. This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research is funded by public authorities, by charitable organizations and by private groups, including many companies. Scientific research can be subdivided into different classifications according to their academic and application disciplines. Scientific research is a widely used criterion for judging the standing of an academic institution, such as business schools, but some argue that such is an inaccurate assessment of the institution, because the quality of research does not tell about the quality of teaching (these do not necessarily correlate).[2]\n\nResearch in the humanities involves different methods such as for example hermeneutics and semiotics, and a different, more relativist epistemology. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past.\n\nArtistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth.\nEtymology\nAristotle, 384 BC – 322 BC, - one of the early figures in the development of the scientific method.[3]\n\nThe word research is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\" a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'.[4] The earliest recorded use of the term was in 1577.[4]\nDefinitions\n\nResearch has been defined in a number of different ways.\n\nA broad definition of research is given by Martyn Shuttleworth - \"In the broadest sense of the word, the definition of research includes any gathering of data, information and facts for the advancement of knowledge.\"[5]\n\nAnother definition of research is given by Creswell who states that - \"Research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". It consists of three steps: Pose a question, collect data to answer the question, and present an answer to the question.[6]\n\nThe Merriam-Webster Online Dictionary defines research in more detail as \"a studious inquiry or examination; especially investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\".[4]\nSteps in conducting research\n\nResearch is often conducted using the hourglass model structure of research.[7] The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:[8]\n\n    Identification of research problem\n    Literature review\n    Specifying the purpose of research\n    Determine specific research questions\n    Specification of a conceptual framework, usually a set of hypotheses[9]\n    Choice of a methodology (for data collection)\n    Data collection\n    Verify data\n    Analyzing and interpreting the data\n    Reporting and evaluating research\n    Communicating the research findings and, possibly, recommendations\n\nThe steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps.[10] Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study.[11] The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in confirming or failing to reject the Null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the flip approach: starting with articulating findings and discussion of them, moving \"up\" to identification research problem that emerging in the findings and literature review introducing the findings. The flip approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings fully emerged and interpreted.\n\nRudolph Rummel says, \"... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results.\"[12]\n\nPlato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrase in the following way, \"If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\"[13]\nScientific research\nMain article: Scientific method\nPrimary scientific research being carried out at the Microscopy Laboratory of the Idaho National Laboratory.\nScientific research equipment at MIT.\n\nGenerally, research is understood to follow a certain structural process. Though step order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:\n\n    Observations and Formation of the topic: Consists of the subject area of ones interest and following that subject area to conduct subject related research. The subject area should not be randomly chosen since it requires reading a vast amount of literature on the topic to determine the gap in the literature the researcher intends to narrow. A keen interest in the chosen subject area is advisable. The research will have to be justified by linking its importance to already existing knowledge about the topic.\n    Hypothesis: A testable prediction which designates the relationship between two or more variables.\n    Conceptual definition: Description of a concept by relating it to other concepts.\n    Operational definition: Details in regards to defining the variables and how they will be measured/assessed in the study.\n    Gathering of data: Consists of identifying a population and selecting samples, gathering information from and/or about these samples by using specific research instruments. The instruments used for data collection must be valid and reliable.\n    Analysis of data: Involves breaking down the individual pieces of data in order to draw conclusions about it.\n    Data Interpretation: This can be represented through tables, figures and pictures, and then described in words.\n    Test, revising of hypothesis\n    Conclusion, reiteration if necessary\n\nA common misconception is that a hypothesis will be proven (see, rather, Null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.\n\nA useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which state no relationship or difference between the independent or dependent variables. A null hypothesis uses a sample of all possible people to make a conclusion about the population.[14]\nHistorical method\nMain article: Historical method\nGerman historian Leopold von Ranke (1795-1886), considered to be one of the founders of modern source-based history.\n\nThe historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:[15]\n\n    Identification of origin date\n    Evidence of localization\n    Recognition of authorship\n    Analysis of data\n    Identification of integrity\n    Attribution of credibility\n\nResearch methods\nThe research room at the New York Public Library, an example of secondary research in progress.\nMaurice Hilleman is credited with saving more lives than any other scientist of the 20th century.[16]\n\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\n\n    Exploratory research, which helps to identify and define a problem or question.\n    Constructive research, which tests theories and proposes solutions to a problem or question.\n    Empirical research, which tests the feasibility of a solution using empirical evidence.\n\nThere are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\n\nQualitative research\n    Understanding of human behavior and the reasons that govern such behavior. Asking a broad question and collecting data in the form of words, images, video etc that is analyzed and searching for themes. This type of research aims to investigate a question without attempting to quantifiably measure variables or look to potential relationships between variables. It is viewed as more restrictive in testing hypotheses because it can be expensive and time-consuming, and typically limited to a single set of research subjects.[citation needed] Qualitative research is often used as a method of exploratory research as a basis for later quantitative research hypotheses.[citation needed] Qualitative research is linked with the philosophical and theoretical stance of social constructionism.\n\nQuantitative research\n    Systematic empirical investigation of quantitative properties and phenomena and their relationships. Asking a narrow question and collecting numerical data to analyze utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).[17] Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism.\n\nThe quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories.[citation needed] These methods produce results that are easy to summarize, compare, and generalize.[citation needed] Quantitative research is concerned with testing hypotheses derived from theory and/or being able to estimate the size of a phenomenon of interest. Depending on the research question, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment).[citation needed] If this is not feasible, the researcher may collect data on participant and situational characteristics in order to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.[18]\n\nIn either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible.[19]\n\nMixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common.[20]\n\nBig data has brought big impacts on research methods that now researchers do not put much effort on data collection, and also methods to analyze easily available huge amount of data have also changed.[21]\n\nNonempirical refers to an approach that is grounded in theory as opposed to using observation and experimentation to achieve the outcome. As such, nonempirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool existing and established knowledge. Nonempirical is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose within life and in science. A simple example of a nonempirical task could the prototyping of a new drug using a differentiated application of existing knowledge; similarly, it could be the development of a business process in the form of a flow chart and texts where all the ingredients are from established knowledge. Empirical research on the other hand seeks to create new knowledge through observations and experiments in which established knowledge can either be contested or supplements.\nResearch method controversies\n\nThere have been many controversies about research methods stemmed from a philosophical positivism promise to distinguish the science from other practices (especially religion) by its method. This promise leads to methodological hegemony and methodology wars where diverse researchers, often coming from opposing paradigms, try to impose their own methodology on the entire field or even on the science practice in general as the only legitimate one.[citation needed]\nQuantitative vs. Qualitative war\nAnti-methodology\n\nAccording to this view, general scientific methodology does not exist and attempts to impose it on scientists is counterproductive. Each particular research with its emerging particular inquiries requires and should produce its own way (method) of researching. Similar to the art practice, the notion of methodology has to be replaced with the notion of research mastery.[22]\nMethodological academic imperialism\n\nEpistemologies of different national sciences and cultural communities may differ and, thus, they may produce different methods of research. For example, psychological research in Russia tends to be rooted in philosophy while in the US and UK in empirism.[23][24][25] Rich countries (and dominant cultural communities within them) and their national sciences may dominate scientific discourse through funding and publications. This academic hegemony can translate into impositions of certain research methodologies through the gatekeeping process of international academic publications, conference presentation selection, institutional review boards, and funding.[26]\nProfessionalisation\nGlobe icon.\n\tThe examples and perspective in this section may not represent a worldwide view of the subject. Please improve this article and discuss the issue on the talk page. (January 2014) (Learn how and when to remove this template message)\nSee also: Academic ranks, Academics, and Scientists\n\nIn several national and private academic systems, the professionalization of research has resulted in formal job titles.\nIn Russia\n\nIn present-day Russia, the former Soviet Union and in some Post-Soviet states the term researcher (Russian: Научный сотрудник, nauchny sotrudnik) is both a generic term for a person who carried out scientific research, as well as a job position within the frameworks of the USSR Academy of Sciences, Soviet universities, and in other research-oriented establishments. The term is also sometimes translated as research fellow, research associate, etc.\n\nThe following ranks are known:\n\n    Junior Researcher (Junior Research Associate)\n    Researcher (Research Associate)\n    Senior Researcher (Senior Research Associate)\n    Leading Researcher (Leading Research Associate)[27]\n    Chief Researcher (Chief Research Associate)\n\nPublishing\nCover of the first issue of Nature, 4 November 1869.\n\nAcademic publishing describes a system that is necessary in order for academic scholars to peer review the work and make it available for a wider audience. The system varies widely by field, and is also always changing, if often slowly. Most academic work is published in journal article or book form. There is also a large body of research that exists in either a thesis or dissertation form. These forms of research can be found in databases explicitly for theses and dissertations. In publishing, STM publishing is an abbreviation for academic publications in science, technology, and medicine.\n\nMost established academic fields have their own scientific journals and other outlets for publication, though many academic journals are somewhat interdisciplinary, and publish work from several distinct fields or subfields. The kinds of publications that are accepted as contributions of knowledge or research vary greatly between fields; from the print to the electronic format. A study suggests that researchers should not give great consideration to findings that are not replicated frequently.[28] It has also been suggested that all published studies should be subjected to some measure for assessing the validity or reliability of its factors in order to prevent the publication of unproven findings.[29] Business models are different in the electronic environment. Since about the early 1990s, licensing of electronic resources, particularly journals, has been very common. Presently, a major trend, particularly with respect to scholarly journals, is open access.[30] There are two main forms of open access: open access publishing, in which the articles or the whole journal is freely available from the time of publication, and self-archiving, where the author makes a copy of their own work freely available on the web.\nResearch funding\nMain article: Funding of science\n\nMost funding for scientific research comes from three major sources: corporate research and development departments; private foundations, for example, the Bill and Melinda Gates Foundation; and government research councils such as the National Institutes of Health in the USA[31] and the Medical Research Council in the UK. These are managed primarily through universities and in some cases through military contractors. Many senior researchers (such as group leaders) spend a significant amount of their time applying for grants for research funds. These grants are necessary not only for researchers to carry out their research, but also as a source of merit.\n\nThe Social Psychology Network provides a comprehensive list of U.S. Government and private foundation funding sources.\nOriginal research\nOriginal research redirects here, for the Wikipedia policy see Wikipedia:No original research\n\nOriginal research is research that is not exclusively based on a summary, review or synthesis of earlier publications on the subject of research. This material is of a primary source character. The purpose of the original research is to produce new knowledge, rather than to present the existing knowledge in a new form (e.g., summarized or classified).[32][33]\nDifferent forms\n\nOriginal research can take a number of forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced, or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.[34]\n\nThe degree of originality of the research is among major criteria for articles to be published in academic journals and usually established by means of peer review.[35] Graduate students are commonly required to perform original research as part of a dissertation.[36]\nArtistic research\n\nThe controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines.[37] One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.[38]\n\nArtistic research has been defined by the University of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner - \"Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods and criticality. Through presented documentation, the insights gained shall be placed in a context.\"[39] Artistic research aims to enhance knowledge and understanding with presentation of the arts.[40] For a survey of the central problematics of today's Artistic Research, see Giaco Schiesser.[41]\n\nAccording to artist Hakan Topal, in artistic research, \"perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities\".[42] Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.[43]\n\nThe Society for Artistic Research (SAR) publishes the triannual Journal for Artistic Research (JAR),[44][45] an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the Research Catalogue (RC),[46][47][48] a searchable, documentary database of artistic research, to which anyone can contribute.\n\nPatricia Leavy addresses eight arts-based research (ABR) genres, they are: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.[49]\nSee also\n\n    European Charter for Researchers\n    Undergraduate research\n    Internet research\n    List of countries by research and development spending\n    Open research\n    Operations research\n    Participatory action research\n    Primary research\n    Psychological research methods\n    Research-intensive cluster\n    Scholarly research\n    Secondary research\n    Society for Artistic Research\n    Timeline of the history of scientific method", "skillName": "RM2."}
{"id": 99, "category": "Research", "skillText": "The Collins Paperback English Dictionary defines research as a “systematic investigation\nto establish facts or collect information on a subject [14].” Vaishnavi and Kuechler [23] de-\nfine research as “an activity that contributes to the understanding of a phenomenon.” By\nthese definitions, reading a first-year textbook would, hopefully, be research for a student.\nVaishnavi and Kuechler [23] go on to define a phenomenon as “a set of behaviours of some\nentity(ies) that is found interesting by the researcher or by a group,” and understanding\nas “knowledge that allows prediction of the behaviour of some aspect of the phenomenon.”\nAcademic research usually includes the idea that research adds something that is, in some\nsense, new to the body of knowledge. The Office of Research and Innovation of Edith\nCowan University state that research “comprises creative work undertaken on a system-\natic basis in order to increase the stock of knowledge, including knowledge of man (sic),\nculture and society, and the use of this stock of knowledge to devise new applications”\n[17]. Other concepts that are associated with academic research include rigour, relevance\nand a systematic approach. These are the kinds of things that journal reviewers look for\nwhen trying to evaluate the quality of research.\nResearch is often divided into Basic and Applied research. Wikipedia [26] states that\n“Basic research (also called fundamental or pure research) has as its primary objective\nthe advancement of knowledge and the theoretical understanding of the relations among\nvariables. It is exploratory and often driven by the researcher’s curiosity, interest, or hunch.\nIt is conducted without any practical end in mind, although it may have unexpected results\npointing to practical applications.” On the other hand, “Applied research is done to solve\nspecific, practical questions; its primary aim is not to gain knowledge for its own sake”\n[26].\n2.1\nWhy do we do research?\nWe do research to gain understanding. There are three main reasons why we might wish\nto gain understanding. Firstly, because humans are more curious than cats. We would\ndo research to satisfy our curiosity, even if there were no practical application for the\nunderstanding obtained. Secondly, as mentioned above, we do research in order to predict\nthe behaviour of an entity. An example of this would be seismic research that helps us\npredict the occurrence of earthquakes. This has practical value in that it may allow us\nto save lives, even though we cannot actually change the phenomenon. Thirdly, we do\nresearch in order to change the behaviour of an entity. If we can predict an entity’s\nbehaviour, then we can also predict the effect on that behaviour if we change the entity\nin some way. A good example of this is medical research, where we try to understand the\nprocesses that cause a disease, so that we can prevent or cure it.\n2.2\nWhat do we do research on?\nWhat kinds of phenomena do we do research on? Simon [20] divides the universe into\nthe natural and the artificial. Natural phenomena are those that occur ‘naturally’ in\nthe world, such as earthquakes, diseases and human behaviour. Artificial phenomena are\nthose that are created by man, for the purpose of satisfying man’s desires and achieving\nhis goals. Natural Science, or natural research, is concerned with understanding and\nexplaining natural phenomena. The Science of the Artificial [20], also known as Design\nScience [23], is concerned with man-made, artificial phenomena.\n2.3\nTypes of research\nIf one reads almost any book on how to do research, one will immediately be presented\nwith one of two ways to do research: Quantitative or Qualitative [16, 22]. However,\nthese are just methodologies, or prescriptions of how to do a part of research. Much of\nthe literature on research divides the types of research into Positivist and Interpretivist\nviewpoints [23], although some authors disagree with this distinction [25]. The viewpoint\nthat is assumed by a particular researcher will affect all of their research, including the\nmethodologies they choose to use. These viewpoints, and their associated methodologies\nwill be discussed in more detail in §5.3.\n2.4\nHow do we do research?\nThe Natural Science research process is composed of two activities, discovery and justi-\nfication [9]. Discovery is the process of generating and proposing scientific claims. This\nprocess is not well understood, and is inherently creative. Justification is the process of\ntesting the claims for validity. This is usually done by trying to prove the claim false, as\na single negative instance can do so, while innumerable positive instances cannot prove a\nclaim true [12]. Most research methodologies are prescriptions of how to gather data and\ntest claims; that is, they are prescriptions for the justification process and say nothing\nabout the discovery process.\n2.5\nHow do we measure the quality of research?\nThe way in which the quality of a particular piece of research is measured depends on\nthe viewpoint that was adopted by the researchers conducting the research. In Natural\nScience, the measure of claims and theories is their explanatory power. “Good” claims are\nconsistent with observed facts, and provide deep, encompassing and accurate predictions\nof future observations [12].\n3\nWhat is operations research?\nAs with many other applied fields, there are two components to the discipline of OR.\nThere is the practice of the discipline, and there is research into the tools and methods of\nthe discipline. An analogy of this is the practice of medicine. The General Practitioner\nor Specialist uses their knowledge, experience, tools and methods to diagnose and treat\npatients. This is not research as they are not adding anything new to the body of medical\nknowledge, although they may be providing new understanding to the patient. At the\nsame time there are the medical researchers who are doing research to increase their\nunderstanding of the human body, and to use that understanding to develop better tools\n\nand methods. Sometimes an individual will combine practice and research at the same\ntime, but they are still distinct actives. The difference between medicine and OR is that\nmedicine does not claim that the practice is also research in its name!\n3.1\nResearch into operations research\nResearch into OR seeks to advance the practice of OR by various means, including, but\nnot limited to:\n• Developing new, or improved, models of various systems.\n• Developing new, or improved, algorithms for solving models.\n• Developing new, or improved, methodologies.\n• Developing new, or improved, tools such as software.\n• Increasing the understanding of phenomena that affect the implementation or adop-\ntion of OR models and methodologies.\nResearch that seeks to improve the practice of OR is generally applied research, but it\nmay be considered basic research if the practical application is only potential.\n3.2\nPractice of operations research\nThe Institute for Operations Research and the Management Sciences (INFORMS) de-\nscribes OR as “the discipline of applying advanced analytical methods to help make better\ndecisions” [5]. They go on to expand this explanation as follows: “By using techniques\nsuch as mathematical modelling to analyse complex situations, operations research gives\nexecutives the power to make more effective decisions and build more productive systems”\n[5].\nWhen a practitioner applies a standard tool, such as linear programming, to solve a\nproblem from a class that is well understood, such as resource allocation, is he conducting\nresearch? He is definitely providing new knowledge to the organisation for whom he is\nsolving the problem, but he is not adding anything new to the body of OR knowledge.\nThe author would argue that in the general sense of research as “systematic investigation\nto establish facts or collect information on a subject [14]”, then this is research. However,\nis it research in the sense of research as “creative work undertaken on a systematic basis\nin order to increase the stock of knowledge” [17]?\nA brief review of 20 abstracts from Volume 52 (2006) of Management Science revealed\nthat 50% of the published articles were Research into OR. A further 30% of articles\nwere not OR, but rather articles on Management. Only 20% of the articles reviewed\ndescribed projects that could be considered Practice of OR. However, all of these projects\napproached the research from a Natural Science perspective. They were projects that\nanalysed a particular system in order to gain greater understanding. There appears to be\nno attempt to build a model, or to ‘solve the problem’. This would seem to indicate that,\nin the eyes of Management Science at least, the practice of OR is not seen as research.", "skillName": "IsOperationsResearchReallyResearch."}
{"id": 100, "category": "Research", "skillText": "Secondary research (also known as desk research) involves the summary, collation and/or synthesis of existing research rather than primary research, in which data are collected from, for example, research subjects or experiments.[1]\n\nCare should be taken to distinguish secondary research from primary research that uses raw secondary data sources. The key of distinction is whether the secondary source used has already been analyzed and interpreted by the primary authors.\n\nThe term is widely used in health research, legal research and market research. The principal methodology in health secondary research is the systematic review, commonly using meta-analytic statistical techniques, but other methods of synthesis, like realist reviews and meta-narrative[2] reviews, have been developed in recent years. Such secondary research uses the primary research of others typically in the form of research publications and reports.\n\nIn a market research context, secondary research is taken to include the reuse, by a second party, of any data collected by a first party or parties.\n\nIn archaeology and landscape history, desk research is contrasted with fieldwork.\n\nSometimes, secondary research is required in the preliminary stages of research to determine what is known already and what new data is required or else to inform research design. At other times, it may be the only research technique used.\n\nA key performance area in secondary research is the full citation of original sources, usually in the form of a complete listing or annotated listing.\n\nSecondary sources could include previous research reports, newspapers, magazines and journals as well as government and NGO statistics.\nSee also\n\n    Meta-analysis\n    Primary research\n    Experiments", "skillName": "Secondary_research."}
{"id": 101, "category": "Research", "skillText": "Science is a systematic and logical approach to discovering how things in the universe work. It is also the body of knowledge accumulated through the discoveries about all the things in the universe. \n\nThe word \"science\" is derived from the Latin word scientia, which is knowledge based on demonstrable and reproducible data, according to the Merriam-Webster Dictionary. True to this definition, science aims for measurable results through testing and analysis. Science is based on fact, not opinion or preferences. The process of science is designed to challenge ideas through research. One important aspect of the scientific process is that it is focuses only on the natural world, according to the University of California. Anything that is considered supernatural does not fit into the definition of science.\nThe scientific method\n\nWhen conducting research, scientists use the scientific method to collect measurable, empirical evidence in an experiment related to a hypothesis (often in the form of an if/then statement), the results aiming to support or contradict a theory.\n\nThe steps of the scientific method go something like this:\n\n    Make an observation or observations.\n    Ask questions about the observations and gather information.\n    Form a hypothesis — a tentative description of what’s been observed, and make predictions based on that hypothesis.\n    Test the hypothesis and predictions in an experiment that can be reproduced.\n    Analyze the data and draw conclusions; accept or reject the hypothesis or modify the hypothesis if necessary.\n    Reproduce the experiment until there are no discrepancies between observations and theory. “Replication of methods and results is my favorite step in the scientific method,\" Moshe Pritsker, a former post-doctoral researcher at Harvard Medical School and CEO of JoVE, told Live Science. \"The reproducibility of published experiments is the foundation of science. No reproducibility – no science.\"\n\nSome key underpinnings to the scientific method:\n\n    The hypothesis must be testable and falsifiable, according to North Carolina State University. Falsifiable means that there must be a possible negative answer to the hypothesis.\n    Research must involve deductive reasoning and inductive reasoning. Deductive reasoning is the process of using true premises to reach a logical true conclusion while inductive reasoning takes the opposite approach.\n    An experiment should include a dependent variable (which does not change) and an independent variable (which does change).\n    An experiment should include an experimental group and a control group. The control group is what the experimental group is compared against.\n\nScientific theories and laws\n\nThe scientific method and science in general can be frustrating. A theory is almost never proven, though a few theories do become scientific laws. One example would be the laws of conservation of energy, which is the first law of thermodynamics. Dr. Linda Boland, a neurobiologist and chairperson of the biology department at the University of Richmond, Virginia, told Live Science that this is her favorite scientific law. \"This is one that guides much of my research on cellular electrical activity and it states that energy cannot be created nor destroyed, only changed in form. This law continually reminds me of the many forms of energy,\" she said.\n\nLaws are generally considered to be without exception, though some laws have been modified over time after further testing found discrepancies. This does not mean theories are not meaningful. For a hypothesis to become a theory, rigorous testing must occur, typically across multiple disciplines by separate groups of scientists. Saying something is “just a theory” is a layperson’s term that has no relationship to science. To most people a theory is a hunch. In science a theory is the framework for observations and facts, Jaime Tanner, a professor of biology at Marlboro College, told Live Science", "skillName": "RM5."}
{"id": 102, "category": "Research", "skillText": "The scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge.[2] To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning.[3] The Oxford Dictionaries Online define the scientific method as \"a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses.\"[4]\n\nThe scientific method is an ongoing process, which usually begins with observations about the natural world. Human beings are naturally inquisitive, so they often come up with questions about things they see or hear and often develop ideas (hypotheses) about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways, including making further observations about nature. In general, the strongest tests of hypotheses come from carefully controlled and replicated experiments that gather empirical data. Depending on how well the tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported a general theory may be developed.[1]\n\nAlthough procedures vary from one field of inquiry to another, identifiable features are frequently shared in common between them. The overall process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions.[5][6] A hypothesis is a conjecture, based on knowledge obtained while formulating the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments. Under modern interpretations, a scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.[7]\n\nThe purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis.[8] Experiments can take place in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars, and so on. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles.[9] Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order.[10] Some philosophers and scientists have argued that there is no scientific method. For example, Lee Smolin[11] and Paul Feyerabend (in his Against Method). Nola and Sankey remark that \"For some, the whole idea of a theory of scientific method is yester-year’s debate ...\".[12]\nContents\n\n    1 Overview\n        1.1 Process\n        1.2 DNA example\n        1.3 Other components\n    2 Scientific inquiry\n        2.1 Properties of scientific inquiry\n        2.2 Beliefs and biases\n    3 Elements of the scientific method\n        3.1 Characterizations\n        3.2 Hypothesis development\n        3.3 Predictions from the hypothesis\n        3.4 Experiments\n        3.5 Evaluation and improvement\n        3.6 Confirmation\n    4 Models of scientific inquiry\n        4.1 Classical model\n        4.2 Pragmatic model\n    5 Communication and community\n        5.1 Peer review evaluation\n        5.2 Documentation and replication\n        5.3 Dimensions of practice\n    6 Philosophy and sociology of science\n        6.1 Role of chance in discovery\n    7 History\n    8 Relationship with mathematics\n    9 Relationship with statistics\n    10 See also\n        10.1 Problems and issues\n        10.2 History, philosophy, sociology\n    11 Notes\n    12 References\n    13 Further reading\n    14 External links\n\nOverview\n\n    The DNA example below is a synopsis of this method\n\nIbn al-Haytham (Alhazen), 965–1039 Iraq. A polymath, considered by some to be the father of modern scientific methodology, due to his emphasis on experimental data and reproducibility of its results.[13][14]\nJohannes Kepler (1571–1630). \"Kepler shows his keen logical sense in detailing the whole process by which he finally arrived at the true orbit. This is the greatest piece of Retroductive reasoning ever performed.\" – C. S. Peirce, c. 1896, on Kepler's reasoning through explanatory hypotheses[15]\nAccording to Morris Kline,[16] \"Modern science owes its present flourishing state to a new scientific method which was fashioned almost entirely by Galileo Galilei\" (1564−1642). Dudley Shapere[17] takes a more measured view of Galileo's contribution.\n\nThe scientific method is the process by which science is carried out.[18] As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time.[19][20][21][22][23][24] This model can be seen to underlay the scientific revolution.[25] One thousand years ago, Alhazen argued the importance of forming questions and subsequently testing them,[26] an approach which was advocated by Galileo in 1638 with the publication of Two New Sciences.[27] The current method is based on a hypothetico-deductive model[28] formulated in the 20th century, although it has undergone significant revision since first proposed (for a more formal discussion, see below).\nProcess\n\nThe overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct.[5] There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, they are better considered as general principles.[29] Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. As noted by William Whewell (1794–1866), \"invention, sagacity, [and] genius\"[10] are required at every step.\nFormulation of a question\n\nThe question can refer to the explanation of a specific observation, as in \"Why is the sky blue?\", but can also be open-ended, as in \"How can I design a drug to cure this particular disease?\" This stage frequently involves looking up and evaluating evidence from previous experiments, personal scientific observations or assertions, and/or the work of other scientists. If the answer is already known, a different question that builds on the previous evidence can be posed. When applying the scientific method to scientific research, determining a good question can be very difficult and affects the final outcome of the investigation.[30]\nHypothesis\n\nA hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain the observed behavior of a part of our universe. The hypothesis might be very specific, e.g., Einstein's equivalence principle or Francis Crick's \"DNA makes RNA makes protein\",[31] or it might be broad, e.g., unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about some population. For example, the population might be people with a particular disease. The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false, e.g., that the new drug does nothing and that any cures are due to chance effects. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, e.g., that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested.\nPrediction\n\nThis step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).[32]\nTesting\n\nThis is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from a hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk.[8] Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem–Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually.\nAnalysis\n\nThis involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence, or to build up many answers to highly specific questions in order to answer a single broader question.\nDNA example\nDNA icon (25x25).png \tThe basic elements of the scientific method are illustrated by the following example from the discovery of the structure of DNA:\n\n    Question: Previous investigation of DNA had determined its chemical composition (the four nucleotides), the structure of each individual nucleotide, and other properties. It had been identified as the carrier of genetic information by the Avery–MacLeod–McCarty experiment in 1944,[33] but the mechanism of how genetic information was stored in DNA was unclear.\n    Hypothesis: Linus Pauling, Francis Crick and James D. Watson hypothesized that DNA had a helical structure.[34]\n    Prediction: If DNA had a helical structure, its X-ray diffraction pattern would be X-shaped.[35][36] This prediction was determined using the mathematics of the helix transform, which had been derived by Cochran, Crick and Vand[37] (and independently by Stokes). This prediction was a mathematical construct, completely independent from the biological problem at hand.\n    Experiment: Rosalind Franklin crystallized pure DNA and performed X-ray diffraction to produce photo 51. The results showed an X-shape.\n    Analysis: When Watson saw the detailed diffraction pattern, he immediately recognized it as a helix.[38][39] He and Crick then produced their model, using this information along with the previously known information about DNA's composition and about molecular interactions such as hydrogen bonds.[40]\n\nThe discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article.\nOther components\n\nThe scientific method also includes other components required even when all the iterations of the steps above have been completed:[41]\nReplication\n\nIf an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.[42]\nExternal review\n\nThe process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.[43]\nData recording and sharing\n\nScientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others.[44] Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.[45]\nScientific inquiry\n\nScientific inquiry generally aims to obtain knowledge in the form of testable explanations that can be used to predict the results of future experiments. This allows scientists to gain a better understanding of the topic being studied, and later be able to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it is to continue explaining a body of evidence better than its alternatives. The most successful explanations, which explain and make accurate predictions in a wide range of circumstances, are often called scientific theories.\n\nMost experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding is typically the result of a gradual process of development over time, sometimes across different domains of science.[46] Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question is more powerful than its alternatives at explaining the evidence. Often the explanations are altered over time, or explanations are combined to produce new explanations.\nProperties of scientific inquiry\n\nScientific knowledge is closely tied to empirical findings, and can remain subject to falsification if new experimental observation incompatible with it is found. That is, no theory can ever be considered final, since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to be related to how long it has persisted without major alteration to its core principles.\n\nTheories can also subject to subsumption by other theories. For example, thousands of years of scientific observations of the planets were explained almost perfectly by Newton's laws. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicting and explaining other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.[47]\n\nSince new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors.[47] For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world;[48][49] its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology.\nBeliefs and biases\nFlying gallop falsified; see image below\nMuybridge's photographs of The Horse in Motion, 1878, were used to answer the question whether all four feet of a galloping horse are ever off the ground at the same time. This demonstrates a use of photography in science.\n\nScientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy. The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).\n\nA historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touches the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together.[50] Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true.[2] In contrast to this standard in the scientific method, poorly attested beliefs can be believed and acted upon via a less rigorous heuristic,[51] sometimes taking advantage of the narrative fallacy that when narrative is constructed its elements become easier to believe.[52][53] Sometimes, these have their elements assumed a priori, or contain some other logical or methodological flaw in the process that ultimately produced them.[54]\nElements of the scientific method\n\nThere are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.\n\n    Four essential elements[55][56][57] of the scientific method[58] are iterations,[59][60] recursions,[61] interleavings, or orderings of the following:\n\n        Characterizations (observations,[62] definitions, and measurements of the subject of inquiry)\n        Hypotheses[63][64] (theoretical, hypothetical explanations of observations and measurements of the subject)[65]\n        Predictions (reasoning including deductive reasoning[66] from the hypothesis or theory)\n        Experiments[67] (tests of all of the above)\n\nEach element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as \"the scientific method\".[68]\n\nThe scientific method is not a single recipe: it requires intelligence, imagination, and creativity.[69] In this sense, it is not a mindless set of standards and procedures to follow, but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's Principia. On the contrary, if the astronomically large, the vanishingly small, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase our confidence in Newton's work.\n\nA linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:[70]\n\n    Define a question\n    Gather information and resources (observe)\n    Form an explanatory hypothesis\n    Test the hypothesis by performing an experiment and collecting data in a reproducible manner\n    Analyze the data\n    Interpret the data and draw conclusions that serve as a starting point for new hypothesis\n    Publish results\n    Retest (frequently done by other scientists)\n\nThe iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.\n\nWhile this schema outlines a typical hypothesis/testing method,[71] it should also be noted that a number of philosophers, historians and sociologists of science (perhaps most notably Paul Feyerabend) claim that such descriptions of scientific method have little relation to the ways that science is actually practiced.\nCharacterizations\n\nThe scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The subjects can also be called unsolved problems or the unknowns.) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.\n\nThe systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.\n\n    I am not accustomed to saying anything with certainty after only one or two observations.\n    — Andreas Vesalius, (1546)[72]\n\nUncertainty\n\nMeasurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken.\nDefinition\n\nMeasurements demand the use of operational definitions of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or \"idealized\" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of \"mass\" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.\n\nThe scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.\n\nNew theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, \"I do not define time, space, place and motion, as being well known to all.\" Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood.[73] In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them.\nDNA-characterizations\nDNA icon (25x25).png\n\nThe history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's transforming principle).[33] But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle.[74] ..2. DNA-hypotheses\nAnother example: precession of Mercury\nPrecession of the perihelion (exaggerated)\n\nThe characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Einstein as a possible early test of his theory of General Relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century.\nHypothesis development\nMain article: Hypothesis formation\n\nA hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.\n\nNormally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.\n\nScientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Charles Sanders Peirce, borrowing a page from Aristotle (Prior Analytics, 2.25) described the incipient stages of inquiry, instigated by the \"irritation of doubt\" to venture a plausible guess, as abductive reasoning. The history of science is filled with stories of scientists claiming a \"flash of inspiration\", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.\n\nWilliam Glen observes that\n\n    the success of a hypothesis, or its service to science, lies not simply in its perceived \"truth\", or power to displace, subsume or reduce a predecessor idea, but perhaps more in its ability to stimulate the research that will illuminate ... bald suppositions and areas of vagueness.[75]\n\nIn general scientists tend to look for theories that are \"elegant\" or \"beautiful\". In contrast to the usual English use of these terms, they here refer to a theory in accordance with the known facts, which is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses.\nDNA-hypotheses\nDNA icon (25x25).png\n\nLinus Pauling proposed that DNA might be a triple helix.[76] This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong[77] and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) ..3. DNA-predictions\nPredictions from the hypothesis\nMain article: Prediction in science\n\nAny useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.\n\nIt is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.\n\nIf the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. Thus, much scientifically based speculation might convince one (or many) that the hypothesis that other intelligent species exist is true. But since there no experiment now known which can test this hypothesis, science itself can have little to say about the possibility. In future, some new technique might lead to an experimental test and the speculation would then become part of accepted science.\nDNA-predictions\nDNA icon (25x25).png\n\nJames D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'.[36][78] This prediction followed from the work of Cochran, Crick and Vand[37] (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.\n\nIn their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, \"It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material\".[79] ..4. DNA-experiments\nAnother example: general relativity\nEinstein's prediction (1907): Light bends in a gravitational field\n\nEinstein's theory of General Relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.[80]\nExperiments\nMain article: Experiment\n\nOnce predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed, when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is.[81] Factor analysis is one technique for discovering the important factor in an effect.\n\nDepending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment which tests the aerodynamical hypotheses used for constructing the plane.\n\nScientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).[82]\nDNA-experiments\nDNA icon (25x25).png\n\nWatson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape \nand was able to confirm the structure was helical.[38][39] This rekindled Watson and Crick's model building and led to the correct structure. ..1. DNA-characterizations\nEvaluation and improvement\n\nThe scientific method is iterative. At any stage it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.\n\nOther scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility.\nDNA-iterations\nDNA icon (25x25).png\n\nAfter considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts,[83][84][85] Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it.[40][86] They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. ..DNA Example\nConfirmation\n\nScience is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.[87]\n\nTo protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including Nature and Science, have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center.\nModels of scientific inquiry\nMain article: Models of scientific inquiry\nClassical model\n\nThe classical model of scientific inquiry derives from Aristotle,[88] who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.\nPragmatic model\nSee also: Pragmatic theory of truth\n\nIn 1877,[19] Charles Sanders Peirce (/ˈpɜːrs/ like \"purse\"; 1839–1914) characterized inquiry in general not as the pursuit of truth per se but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless.[89] He outlined four methods of settling opinion, ordered from least to most successful:\n\n    The method of tenacity (policy of sticking to initial belief) – which brings comforts and decisiveness but leads to trying to ignore contrary information and others' views as if truth were intrinsically private, not public. It goes against the social impulse and easily falters since one may well notice when another's opinion is as good as one's own initial opinion. Its successes can shine but tend to be transitory.[90]\n    The method of authority – which overcomes disagreements but sometimes brutally. Its successes can be majestic and long-lived, but it cannot operate thoroughly enough to suppress doubts indefinitely, especially when people learn of other societies present and past.\n    The method of the a priori – which promotes conformity less brutally but fosters opinions as something like tastes, arising in conversation and comparisons of perspectives in terms of \"what is agreeable to reason.\" Thereby it depends on fashion in paradigms and goes in circles over time. It is more intellectual and respectable but, like the first two methods, sustains accidental and capricious beliefs, destining some minds to doubt it.\n    The scientific method – the method wherein inquiry regards itself as fallible and purposely tests itself and criticizes, corrects, and improves itself.\n\nPeirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research,[91] which in turn should not be trammeled by the other methods and practical ends; reason's \"first rule\" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry.[92] The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth per se but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.[19][22]\n\nFor Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators would reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points.[93] In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to \"Do the science.\" Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge.[94] As inference, \"logic is rooted in the social principle\" since it depends on a standpoint that is, in a sense, unlimited.[95]\n\nPaying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in \"A Neglected Argument\"[5] except as otherwise noted):\n\n    Abduction (or retroduction). Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, on the basis of tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea so as to account in a simple, economical way for a surprising or complicative phenomenon. Oftenest, even a well-prepared mind guesses wrong. But the modicum of success of our guesses far exceeds that of sheer luck and seems born of attunement to nature by instincts developed or inherent, especially insofar as best guesses are optimally plausible and simple in the sense, said Peirce, of the \"facile and natural\", as by Galileo's natural light of reason and as distinct from \"logical simplicity\". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and, without it, there is no hope of sufficiently expediting inquiry (often multi-generational) toward new truths.[96] Coordinative method leads from abducing a plausible hypothesis to judging it for its testability[97] and for how its trial would economize inquiry itself.[98] Peirce calls his pragmatism \"the logic of abduction\".[99] His pragmatic maxim is: \"Consider what effects that might conceivably have practical bearings you conceive the objects of your conception to have. Then, your conception of those effects is the whole of your conception of the object\".[93] His pragmatism is a method of reducing conceptual confusions fruitfully by equating the meaning of any conception with the conceivable practical implications of its object's conceived effects—a method of experimentational mental reflection hospitable to forming hypotheses and conducive to testing them. It favors efficiency. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if uncostly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has instinctive plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be chosen for trial strategically, for their caution (for which Peirce gave as example the game of Twenty Questions), breadth, and incomplexity.[100] One can hope to discover only that which time would reveal through a learner's sufficient experience anyway, so the point is to expedite it; the economy of research is what demands the leap, so to speak, of abduction and governs its art.[98]\n    Deduction. Two stages:\n        Explication. Unclearly premissed, but deductive, analysis of the hypothesis in order to render its parts as clear as possible.\n        Demonstration: Deductive Argumentation, Euclidean in procedure. Explicit deduction of hypothesis's consequences as predictions, for induction to test, about evidence to be found. Corollarial or, if needed, Theorematic.\n    Induction. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning in general[93]) that the real is only the object of the final opinion to which adequate investigation would lead;[101] anything to which no such process would ever lead would not be real. Induction involving ongoing tests or observations follows a method which, sufficiently persisted in, will diminish its error below any predesignate degree. Three stages:\n        Classification. Unclearly premissed, but inductive, classing of objects of experience under general ideas.\n        Probation: direct inductive argumentation. Crude (the enumeration of instances) or gradual (new estimate of proportion of truth in the hypothesis after each test). Gradual induction is qualitative or quantitative; if qualitative, then dependent on weightings of qualities or characters;[102] if quantitative, then dependent on measurements, or on statistics, or on countings.\n        Sentential Induction. \"...which, by inductive reasonings, appraises the different probations singly, then their combinations, then makes self-appraisal of these very appraisals themselves, and passes final judgment on the whole result\".\n\nCommunication and community\nSee also: Scientific community and Scholarly communication\n\nFrequently the scientific method is employed not only by a single person, but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment.\nPeer review evaluation\n\nScientific journals use a process of peer review, in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three) fellow (usually anonymous) scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer review process can have limitations when considering research outside the conventional scientific paradigm: problems of \"groupthink\" can interfere with open and fair deliberation of some new research.[103]\nDocumentation and replication\nMain article: Reproducibility\n\nSometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results.\nArchiving\n\nResearchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery.\nData sharing\n\nWhen additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research.\nLimitations\n\nSince it is impossible for a scientist to record everything that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'.\nDimensions of practice\nFurther information: Rhetoric of science\n\nThe primary constraints on contemporary science are:\n\n    Publication, i.e. Peer review\n    Resources (mostly funding)\n\nIt has not always been like this: in the old days of the \"gentleman scientist\" funding (and to a lesser extent publication) were far weaker constraints.\n\nBoth of these constraints indirectly require scientific method – work that violates the constraints will be difficult to publish and difficult to get funded. Journals require submitted papers to conform to \"good scientific practice\" and to a degree this can be enforced by peer review. Originality, importance and interest are more important – see for example the author guidelines \nfor Nature.\nPhilosophy and sociology of science\nSee also: Philosophy of science and Sociology of science\n\nPhilosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world.[104] These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized. More generally, the scientific method can be recognized as an idealization.[105]\n\nThomas Kuhn examined the history of science in his The Structure of Scientific Revolutions, and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.\n\nNorwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the \"theory laden\" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description.[106] He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a \"different\" sun rise despite the same physiological phenomenon. Kuhn[107] and Feyerabend[108] acknowledge the pioneering significance of his work.\n\nKuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the \"route from theory to measurement can almost never be traveled backward\". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that \"once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed\".[109]\n\nPaul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book Against Method he argues that scientific progress is not the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'.[110] Criticisms such as his led to the strong programme, a radical approach to the sociology of science.\n\nThe postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.[111]\nRole of chance in discovery\nMain article: Role of chance in scientific discoveries\n\nSomewhere between 33% and 50% of all scientific discoveries are estimated to have been stumbled upon, rather than sought out. This may explain why scientists so often express that they were lucky.[112] Louis Pasteur is credited with the famous saying that \"Luck favours the prepared mind\", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected.[112][113] This is what Nassim Nicholas Taleb calls \"Anti-fragility\"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.[23]\n\nPsychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they think is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.[112][113]\nHistory\nMain article: History of scientific method\nSee also: Timeline of the history of scientific method", "skillName": "RM3."}
{"id": 103, "category": "Research", "skillText": "Key Concepts of the Scientific Method\n\nResearch Methodology\n\nThere are several important aspects to research methodology. This is a summary of the key concepts in scientific research and an attempt to erase some common misconceptions in science.\n\nSteps of the scientific method are shaped like an hourglass - starting from general questions, narrowing down to focus on one specific aspect, and designing research where we can observe and analyze this aspect. At last, we conclude and generalize to the real world.\nFormulating a Research Problem\n\nResearchers organize their research by formulating and defining a research problem. This helps them focus the research process so that they can draw conclusions reflecting the real world in the best possible way.\n\n\nHypothesis\n\nIn research, a hypothesis is a suggested explanation of a phenomenon.\n\nA null hypothesis is a hypothesis which a researcher tries to disprove. Normally, the null hypothesis represents the current view/explanation of an aspect of the world that the researcher wants to challenge.\n\nResearch methodology involves the researcher providing an alternative hypothesis, a research hypothesis, as an alternate way to explain the phenomenon.\n\nThe researcher tests the hypothesis to disprove the null hypothesis, not because he/she loves the research hypothesis, but because it would mean coming closer to finding an answer to a specific problem. The research hypothesis is often based on observations that evoke suspicion that the null hypothesis is not always correct.\n\nIn the Stanley Milgram Experiment, the null hypothesis was that the personality determined whether a person would hurt another person, while the research hypothesis was that the role, instructions and orders were much more important in determining whether people would hurt others.\n\nReasoning Cycle - Scientific Research\nVariables\n\nA variable is something that changes. It changes according to different factors. Some variables change easily, like the stock-exchange value, while other variables are almost constant, like the name of someone. Researchers are often seeking to measure variables.\n\nThe variable can be a number, a name, or anything where the value can change.\n\nAn example of a variable is temperature. The temperature varies according to other variable and factors. You can measure different temperature inside and outside. If it is a sunny day, chances are that the temperature will be higher than if it's cloudy. Another thing that can make the temperature change is whether something has been done to manipulate the temperature, like lighting a fire in the chimney.\n\nIn research, you typically define variables according to what you're measuring. The independent variable is the variable which the researcher would like to measure (the cause), while the dependent variable is the effect (or assumed effect), dependent on the independent variable. These variables are often stated in experimental research, in a hypothesis, e.g. \"what is the effect of personality on helping behavior?\"\n\nIn explorative research methodology, e.g. in some qualitative research, the independent and the dependent variables might not be identified beforehand. They might not be stated because the researcher does not have a clear idea yet on what is really going on.\n\nConfounding variables are variables with a significant effect on the dependent variable that the researcher failed to control or eliminate - sometimes because the researcher is not aware of the effect of the confounding variable. The key is to identify possible confounding variables and somehow try to eliminate or control them.\nOperationalization\n\nOperationalization is to take a fuzzy concept (conceptual variables), such as 'helping behavior', and try to measure it by specific observations, e.g. how likely are people to help a stranger with problems.\n\nOperationalization in Research\n\nSee also:\n\nConceptual Variables\nChoosing the Research Method", "skillName": "DSRM01_Scientific_method."}
{"id": 104, "category": "Research", "skillText": "Accuracy dispute\n\tThis article appears to contradict the article History of scientific method. Please see discussion on the linked talk page. Please do not remove this message until the contradictions are resolved. (June 2015)\n\tThis section may contain an excessive amount of intricate detail that may only interest a specific audience. Please help by spinning off or relocating any relevant information, and removing excessive detail that may be against Wikipedia's inclusion policy. (June 2015) (Learn how and when to remove this template message)\nAristotle, 384 BCE – 322 BCE. \"As regards his method, Aristotle is recognized as the inventor of scientific method because of his refined analysis of logical implications contained in demonstrative discourse, which goes well beyond natural logic and does not owe anything to the ones who philosophized before him.\" – Riccardo Pozzo[114]\n\nThe development of the scientific method emerges in the history of science itself. Ancient Egyptian documents describe empirical methods in astronomy,[115] mathematics,[116] and medicine.[117] The Greeks made contributions to the scientific method, most notably through Aristotle in his six works of logic collected as the Organon. Aristotle's inductive-deductive method used inductions from observations to infer general principles, deductions from those principles to check against further observations, and more cycles of induction and deduction to continue the advance of knowledge[118]\n\nAccording to Karl Popper, Parmenides (fl. 5th century BCE) had conceived an axiomatic-deductive method.[119] According to David Lindberg, Aristotle (4th century BCE) wrote about the scientific method even if he and his followers did not actually follow what he said.[67] Lindberg also notes that Ptolemy (2nd century CE) and Ibn al-Haytham (11th century CE) are among the early examples of people who carried out scientific experiments.[120] Also, John Losee writes that \"the Physics and the Metaphysics contain discussions of certain aspects of scientific method\", of which, he says \"Aristotle viewed scientific inquiry as a progression from observations to general principles and back to observations.\"[121]\n\nEarly Christian leaders such as Clement of Alexandria (150–215) and Basil of Caesarea (330–379) encouraged future generations to view the Greek wisdom as \"handmaidens to theology\" and science was considered a means to more accurate understanding of the Bible and of God.[122]:pp.4–5 Augustine of Hippo (354–430) who contributed great philosophical wealth to the Latin Middle Ages, advocated the study of science and was wary of philosophies that disagreed with the Bible, such as astrology and the Greek belief that the world had no beginning.[122]:p.5 This Christian accommodation with Greek science \"laid a foundation for the later widespread, intensive study of natural philosophy during the Late Middle Ages.\"[122]:pp.8,9 However, the division of Latin-speaking Western Europe from the Greek-speaking East,[122]:p.18 followed by barbarian invasions, the Plague of Justinian, and the Islamic conquests,[123] resulted in the West largely losing access to Greek wisdom.\n\nBy the 8th century Islam had conquered the Christian lands[124] of Syria, Iraq, Iran and Egypt.[125] This swift conquest further severed Western Europe from many of the great works of Aristotle, Plato, Euclid and others, many of which were housed in the great library of Alexandria. Having come upon such a wealth of knowledge, the Arabs, who viewed non-Arab languages as inferior, even as a source of pollution,[126] employed conquered Christians and Jews to translate these works from the native Greek and Syriac into Arabic.[127]\n\nThus equipped, Arab philosopher Alhazen (Ibn al-Haytham) performed optical and physiological experiments, reported in his manifold works, the most famous being Book of Optics (1021).[128] He was thus a forerunner of scientific method, having understood that a controlled environment involving experimentation and measurement is required in order to draw educated conclusions. Other Arab polymaths of the same era produced copious works on mathematics, philosophy, astronomy and alchemy. Most stuck closely to Aristotle, being hesitant to admit that some of Aristotle's thinking was errant,[129] while others strongly criticized him.\n\nDuring these years, occasionally a paraphrased translation from the Arabic, which itself had been translated from Greek and Syriac, might make its way to the West for scholarly study. It was not until 1204, during which the Latins conquered and took Constantinople from the Byzantines in the name of the fourth Crusade, that a renewed scholarly interest in the original Greek manuscripts began to grow. Due to the new easier access to the libraries of Constantinople by Western scholars, a certain revival in the study and analysis of the original Greek texts by Western scholars began.[130] From that point a functional scientific method that would launch modern science was on the horizon.\n\nGrosseteste (1175–1253), an English statesman, scientist and Christian theologian, was \"the principal figure\" in bringing about \"a more adequate method of scientific inquiry\" by which \"medieval scientists were able eventually to outstrip their ancient European and Muslim teachers\" (Dales 1973, p. 62). ... His thinking influenced Roger Bacon, who spread Grosseteste's ideas from Oxford to the University of Paris during a visit there in the 1240s. From the prestigious universities in Oxford and Paris, the new experimental science spread rapidly throughout the medieval universities: \"And so it went to Galileo, William Gilbert, Francis Bacon, William Harvey, Descartes, Robert Hooke, Newton, Leibniz, and the world of the seventeenth century\" (Crombie 1953, p. 15). \"So it went to us as well \" (Gauch 2003, pp. 52–53).\nRoger Bacon (c. 1214 – c. 1292) is sometimes credited as one of the earliest European advocates of the modern scientific method inspired by the works of Aristotle.[131]\n\nRoger Bacon (c. 1214 – c. 1292), an English thinker and experimenter, is recognized by many to be the father of modern scientific method. His view that mathematics was essential to a correct understanding of natural philosophy was considered to be 400 years ahead of its time.[132]:2 He was viewed as \"a lone genius proclaiming the truth about time,\" having correctly calculated the calendar[132]:3 His work in optics provided the platform on which Newton, Descartes, Huygens and others later transformed the science of light. Bacon's groundbreaking advances were due largely to his discovery that experimental science must be based on mathematics. (186–187) His works Opus Majus and De Speculis Comburentibus contain many \"carefully drawn diagrams showing Bacon's meticulous investigations into the behavior of light.\"[132]:66 He gives detailed descriptions of systematic studies using prisms and measurements by which he shows how a rainbow functions.[132]:200\n\nOthers who advanced scientific method during this era included Albertus Magnus (c. 1193 – 1280), Theodoric of Freiberg, (c. 1250 – c. 1310), William of Ockham (c. 1285 – c. 1350), and Jean Buridan (c. 1300 – c. 1358). These were not only scientists but leaders of the church – Christian archbishops, friars and priests.\n\nBy the late 15th century, the physician-scholar Niccolò Leoniceno was finding errors in Pliny's Natural History. As a physician, Leoniceno was concerned about these botanical errors propagating to the materia medica on which medicines were based.[133] To counter this, a botanical garden was established at Orto botanico di Padova, University of Padua (in use for teaching by 1546), in order that medical students might have empirical access to the plants of a pharmacopia. The philosopher and physician Francisco Sanches was led by his medical training at Rome, 1571–73, and by the philosophical skepticism recently placed in the European mainstream by the publication of Sextus Empiricus' \"Outlines of Pyrrhonism\", to search for a true method of knowing (modus sciendi), as nothing clear can be known by the methods of Aristotle and his followers[134] – for example, syllogism fails upon circular reasoning. Following the physician Galen's method of medicine, Sanches lists the methods of judgement and experience, which are faulty in the wrong hands,[135] and we are left with the bleak statement That Nothing is Known (1581). This challenge was taken up by René Descartes in the next generation (1637), but at the least, Sanches warns us that we ought to refrain from the methods, summaries, and commentaries on Aristotle, if we seek scientific knowledge. In this, he is echoed by Francis Bacon, also influenced by skepticism; Sanches cites the humanist Juan Luis Vives who sought a better educational system, as well as a statement of human rights as a pathway for improvement of the lot of the poor.\n\nThe modern scientific method crystallized no later than in the 17th and 18th centuries. In his work Novum Organum (1620) – a reference to Aristotle's Organon – Francis Bacon outlined a new system of logic to improve upon the old philosophical process of syllogism.[136] Then, in 1637, René Descartes established the framework for scientific method's guiding principles in his treatise, Discourse on Method. The writings of Alhazen, Bacon and Descartes are considered critical in the historical development of the modern scientific method, as are those of John Stuart Mill.[137]\n\nIn the late 19th century, Charles Sanders Peirce proposed a schema that would turn out to have considerable influence in the development of current scientific methodology generally. Peirce accelerated the progress on several fronts. Firstly, speaking in broader context in \"How to Make Our Ideas Clear\" (1878) \n, Peirce outlined an objectively verifiable method to test the truth of putative knowledge on a way that goes beyond mere foundational alternatives, focusing upon both deduction and induction. He thus placed induction and deduction in a complementary rather than competitive context (the latter of which had been the primary trend at least since David Hume, who wrote in the mid-to-late 18th century). Secondly, and of more direct importance to modern method, Peirce put forth the basic schema for hypothesis/testing that continues to prevail today. Extracting the theory of inquiry from its raw materials in classical logic, he refined it in parallel with the early development of symbolic logic to address the then-current problems in scientific reasoning. Peirce examined and articulated the three fundamental modes of reasoning that, as discussed above in this article, play a role in inquiry today, the processes that are currently known as abductive, deductive, and inductive inference. Thirdly, he played a major role in the progress of symbolic logic itself – indeed this was his primary specialty.\n\nBeginning in the 1930s, Karl Popper argued that there is no such thing as inductive reasoning.[138] All inferences ever made, including in science, are purely[139] deductive according to this view. Accordingly, he claimed that the empirical character of science has nothing to do with induction – but with the deductive property of falsifiability that scientific hypotheses have. Contrasting his views with inductivism and positivism, he even denied the existence of the scientific method: \"(1) There is no method of discovering a scientific theory (2) There is no method for ascertaining the truth of a scientific hypothesis, i.e., no method of verification; (3) There is no method for ascertaining whether a hypothesis is 'probable', or probably true\".[140] Instead, he held that there is only one universal method, a method not particular to science: The negative method of criticism, or colloquially termed trial and error. It covers not only all products of the human mind, including science, mathematics, philosophy, art and so on, but also the evolution of life. Following Peirce and others, Popper argued that science is fallible and has no authority.[140] In contrast to empiricist-inductivist views, he welcomed metaphysics and philosophical discussion and even gave qualified support to myths[141] and pseudosciences.[142] Popper's view has become known as critical rationalism.\n\nAlthough science in a broad sense existed before the modern era, and in many historical civilizations (as described above), modern science is so distinct in its approach and successful in its results that it now defines what science is in the strictest sense of the term.[143]\nRelationship with mathematics\n\nScience is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines can clearly distinguish what is known from what is unknown at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be falsifiable (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.[144]\n\nMathematical work and scientific work can inspire each other.[145] For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).\n\nNevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, The Unreasonable Effectiveness of Mathematics in the Natural Sciences, is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.\n\nGeorge Pólya's work on problem solving,[146] the construction of mathematical proofs, and heuristic[147][148] show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps.\n\tMathematical method \tScientific method\n1 \tUnderstanding \tCharacterization from experience and observation\n2 \tAnalysis \tHypothesis: a proposed explanation\n3 \tSynthesis \tDeduction: prediction from the hypothesis\n4 \tReview/Extend \tTest and experiment\n\nIn Pólya's view, understanding involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; analysis, which Pólya takes from Pappus,[149] involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; synthesis is the strict Euclidean exposition of step-by-step details[150] of the proof; review involves reconsidering and re-examining the result and the path taken to it.\n\nGauss, when asked how he came about his theorems, once replied \"durch planmässiges Tattonieren\" (through systematic palpable experimentation).[151]\n\nImre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work.[152] In like manner to science, where truth is sought, but certainty is not found, in Proofs and refutations (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré (Proofs and Refutations, 1976).)\n\nLakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In Proofs and Refutations, Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.[153]\nRelationship with statistics\n\nThe scientific method has been extremely successful in bringing the world out of medieval times, especially once it was combined with industrial processes.[154] However, when the scientific method employs statistics as part of its arsenal, there are a number of both mathematical and practical issues that can have a deleterious effect on the reliability of the output of the scientific methods. This is outlined in detail in the most downloaded 2005 scientific paper \"Why Most Published Research Findings Are False\"[155] ever by John Ioannidis.\n\nThe particular points raised are statistical (\"The smaller the studies conducted in a scientific field, the less likely the research findings are to be true\" and \"The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.\") and economical (\"The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true\" and \"The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.\") Hence: \"Most research findings are false for most research designs and for most fields\" and \"As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings.\" However: \"Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds,\" which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield most new discoveries.\nSee also\n\n    Armchair theorizing\n    Confirmability\n    Contingency\n    Empirical limits in science\n    Evidence-based medicine\n    Fuzzy logic\n    Inquiry\n    Information theory\n    Logic\n    Methodology\n        Historical\n        Philosophical\n        Phronetic\n        Scholarly\n    Operationalization\n    Quantitative research\n    Replication crisis\n    Social research\n    Statistical hypothesis testing\n    Strong inference\n    Testability", "skillName": "RM4."}
{"id": 105, "category": "Research", "skillText": "A systematic review is a type of literature review that collects and critically analyzes multiple research studies or papers. A review of existing studies is often quicker and cheaper than embarking on a new study. Researchers use methods that are selected before one or more research questions are formulated, and then they aim to find and analyze studies that relate to and answer those questions.[1] Systematic reviews of randomized controlled trials are key in the practice of evidence-based medicine.[2]\n\nAn understanding of systematic reviews, and how to implement them in practice, is highly recommended for professionals involved in the delivery of health care. Besides health interventions, systematic reviews may examine clinical tests, public health interventions, environmental interventions,[3] social interventions, adverse effects, and economic evaluations.[4][5] Systematic reviews are not limited to medicine and are quite common in all other sciences where data are collected, published in the literature, and an assessment of methodological quality for a precisely defined subject would be helpful.[6]\n\nContents\n\n    1 Characteristics\n    2 Stages of a systematic review\n    3 Cochrane Collaboration\n    4 Strengths and weaknesses\n    5 See also\n    6 References\n    7 External links\n\nCharacteristics\n\nA systematic review aims to provide a complete, exhaustive summary of current literature relevant to a research question. The first step in conducting a systematic review is to perform a thorough search of the literature for relevant papers. The Methodology section of a systematic review will list all of the databases and citation indexes that were searched such as Web of Science, Embase, and PubMed and any individual journals that were searched. The titles and abstracts of identified articles are checked against pre-determined criteria for eligibility and relevance to form an inclusion set. This set will relate back to the research problem. Each included study may be assigned an objective assessment of methodological quality preferably by using methods conforming to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement (the current guideline)[7] or the high quality standards of Cochrane collaboration.[8]\n\nSystematic reviews often, but not always, use statistical techniques (meta-analysis) to combine results of eligible studies, or at least use scoring of the levels of evidence depending on the methodology used. An additional rater may be consulted to resolve any scoring differences between raters.[6] Systematic review is often applied in the biomedical or healthcare context, but it can be applied in any field of research. Groups like the Campbell Collaboration are promoting the use of systematic reviews in policy-making beyond just healthcare.\n\nA systematic review uses an objective and transparent approach for research synthesis, with the aim of minimizing bias. While many systematic reviews are based on an explicit quantitative meta-analysis of available data, there are also qualitative reviews which adhere to standards for gathering, analyzing and reporting evidence.[9] The EPPI-Centre has been influential in developing methods for combining both qualitative and quantitative research in systematic reviews.[10] The PRISMA statement[11] suggests a standardized way to ensure a transparent and complete reporting of systematic reviews, and is now required for this kind of research by more than 170 medical journals worldwide.[12]\n\nRecent developments in systematic reviews include realist reviews,[13] and the meta-narrative approach.[14][15] These approaches try to overcome the problems of methodological and epistemological heterogeneity in the diverse literatures existing on some subjects.\nStages of a systematic review\n\nThe main stages of a systematic review are:\nA visualisation of data being 'extracted' and 'combined' in a Cochrane systematic review.[16]\n\n    Defining a question and agreeing an objective method.[16]\n    A search for relevant data from research that matches certain criteria. For example, only selecting research that is good quality and answers the defined question.[16]\n    'Extraction' of relevant data. This can include how the research was done (often called the method or 'intervention'), who participated in the research (including how many people), how it was paid for (for example funding sources) and what happened (the outcomes).[16]\n    Assess the quality of the data by judging it against criteria identified at the first stage.[16]\n    Analyse and combine the data (using complex statistical methods) which give an overall result from all of the data. This combination of data can be visualised using a blobbogram (also called a forest plot).[16] The diamond in the blobbogram represents the combined results of all the data included. Because this combined result uses data from more sources than just one data set, it’s considered more reliable and better evidence, as the more data there is, the more confident we can be of conclusions.[16]\n\nOnce these stages are complete, the review may be published, disseminated and translated into practice after being adopted as evidence.\nCochrane Collaboration\n\nThe Cochrane Collaboration is a group of over 31,000 specialists in healthcare who systematically review randomised trials of the effects of prevention, treatments and rehabilitation as well as health systems interventions. When appropriate, they also include the results of other types of research. Cochrane Reviews are published in The Cochrane Database of Systematic Reviews section of the Cochrane Library. The 2015 impact factor for The Cochrane Database of Systematic Reviews was 6.103, and it was ranked 12th in the “Medicine, General & Internal” category.[17] There are six types of Cochrane Review:[18][19][20][21]\n\n    Intervention reviews assess the benefits and harms of interventions used in healthcare and health policy.\n    Diagnostic test accuracy reviews assess how well a diagnostic test performs in diagnosing and detecting a particular disease.\n    Methodology reviews address issues relevant to how systematic reviews and clinical trials are conducted and reported.\n    Qualitative reviews synthesize qualitative and quantitative evidence to address questions on aspects other than effectiveness.[9]\n    Prognosis reviews address the probable course or future outcome(s) of people with a health problem.\n    Overviews of Systematic Reviews (OoRs) are a new type of study in order to compile multiple evidence from systematic reviews into a single document that is accessible and useful to serve as a friendly front end for the Cochrane Collaboration with regard to healthcare decision-making.\n\nThe Cochrane Collaboration provides a handbook for systematic reviewers of interventions which \"provides guidance to authors for the preparation of Cochrane Intervention reviews.\"[8] The Cochrane Handbook outlines eight general steps for preparing a systematic review:[8]\n\n    Defining the review question(s) and developing criteria for including studies\n    Searching for studies\n    Selecting studies and collecting data\n    Assessing risk of bias in included studies\n    Analysing data and undertaking meta-analyses\n    Addressing reporting biases\n    Presenting results and \"summary of findings\" tables\n    Interpreting results and drawing conclusions\n\nThe Cochrane Handbook forms the basis of two sets of standards for the conduct and reporting of Cochrane Intervention Reviews (MECIR - Methodological Expectations of Cochrane Intervention Reviews)[22]\n\nThe Cochrane Collaboration logo visually represents how results from some systematic reviews can be explained.[23] The lines within illustrate the summary results from an iconic systematic review showing the benefit of corticosteroids, which 'has probably saved thousands of premature babies'.[24]\nStrengths and weaknesses\n\nWhile systematic reviews are regarded as the strongest form of medical evidence, a review of 300 studies found that not all systematic reviews were equally reliable, and that their reporting can be improved by a universally agreed upon set of standards and guidelines.[25] A further study by the same group found that of 100 systematic reviews monitored, 7% needed updating at the time of publication, another 4% within a year, and another 11% within 2 years; this figure was higher in rapidly changing fields of medicine, especially cardiovascular medicine.[26] A 2003 study suggested that extending searches beyond major databases, perhaps into grey literature, would increase the effectiveness of reviews.[27]\n\nRoberts and colleagues highlighted the problems with systematic reviews, particularly those conducted by the Cochrane Collaboration, noting that published reviews are often biased, out of date and excessively long.[28] They criticized Cochrane reviews as not being sufficiently critical in the selection of trials and including too many of low quality. They proposed several solutions, including limiting studies in meta-analyses and reviews to registered clinical trials, requiring that original data be made available for statistical checking, paying greater attention to sample size estimates, and eliminating dependence on only published data.\n\nSome of these difficulties were noted early on as described by Altman: \"much poor research arises because researchers feel compelled for career reasons to carry out research that they are ill equipped to perform, and nobody stops them.\"[29] Methodological limitations of meta-analysis have also been noted.[30] Another concern is that the methods used to conduct a systematic review are sometimes changed once researchers see the available trials they are going to include.[31] Bloggers have described retractions of systematic reviews and published reports of studies included in published systematic reviews.[32][33][34]\n\nSystematic reviews are increasingly prevalent in other fields, such as international development research.[35] Subsequently, a number of donors – most notably the UK Department for International Development (DFID) and AusAid – are focusing more attention and resources on testing the appropriateness of systematic reviews in assessing the impacts of development and humanitarian interventions.[35]\nSee also\n\n    Critical appraisal\n    Literature review\n    Peer review\n    Review journal", "skillName": "DSRM02_Systematic_study."}
{"id": 106, "category": "Research", "skillText": "What are the research objectives?\n\nIn general, research objectives describe what we expect to achieve by a project.\n\nResearch objectives are usually expressed in lay terms and are directed as much to the client as to the researcher. Research objectives may be linked with a hypothesis or used as a statement of purpose in a study that does not have a hypothesis.\n\nEven if the nature of the research has not been clear to the layperson from the hypotheses, s/he should be able to understand the research from the objectives.\n\nA statement of research objectives can serve to guide the activities of research. Consider the following examples.\n\n    Objective: To describe what factors farmers take into account in making such decisions as whether to adopt a new technology or what crops to grow.\n    Objective: To develop a budget for reducing pollution by a particular enterprise.\n    Objective: To describe the habitat of the giant panda in China.\n\nIn the above examples the intent of the research is largely descriptive.\n\n    In the case of the first example, the research will end the study by being able to specify factors which emerged in household decisions.\n    In the second, the result will be the specification of a pollution reduction budget.\n    In the third, creating a picture of the habitat of the giant panda in China.\n\nThese observations might prompt researchers to formulate hypotheses which could be tested in another piece of research. So long as the aim of the research is exploratory, ie to describe what is, rather than to test an explanation for what is, a research objective will provide an adequate guide to the research.\n\n\nResearch comprises \"creative work undertaken on a systematic basis in order to increase the stock of knowledge, including knowledge of humans, culture and society, and the use of this stock of knowledge to devise new applications.\"[1] It is used to establish or confirm facts, reaffirm the results of previous work, solve new or existing problems, support theorems, or develop new theories. A research project may also be an expansion on past work in the field. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects, or the project as a whole. The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, or the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life,technological,etc.\n\nContents\n\n    1 Forms of research\n    2 Etymology\n    3 Definitions\n    4 Steps in conducting research\n    5 Scientific research\n    6 Historical method\n    7 Research methods\n        7.1 Research method controversies\n            7.1.1 Quantitative vs. Qualitative war\n            7.1.2 Anti-methodology\n            7.1.3 Methodological academic imperialism\n    8 Professionalisation\n        8.1 In Russia\n    9 Publishing\n    10 Research funding\n    11 Original research\n        11.1 Different forms\n    12 Artistic research\n    13 See also\n    14 References\n    15 Further reading\n    16 External links\n\nForms of research\n\nScientific research is a systematic way of gathering data and harnessing curiosity. This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research is funded by public authorities, by charitable organizations and by private groups, including many companies. Scientific research can be subdivided into different classifications according to their academic and application disciplines. Scientific research is a widely used criterion for judging the standing of an academic institution, such as business schools, but some argue that such is an inaccurate assessment of the institution, because the quality of research does not tell about the quality of teaching (these do not necessarily correlate).[2]\n\nResearch in the humanities involves different methods such as for example hermeneutics and semiotics, and a different, more relativist epistemology. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past.\n\nArtistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth.\nEtymology\nAristotle, 384 BC – 322 BC, - one of the early figures in the development of the scientific method.[3]\n\nThe word research is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\" a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'.[4] The earliest recorded use of the term was in 1577.[4]\nDefinitions\n\nResearch has been defined in a number of different ways.\n\nA broad definition of research is given by Martyn Shuttleworth - \"In the broadest sense of the word, the definition of research includes any gathering of data, information and facts for the advancement of knowledge.\"[5]\n\nAnother definition of research is given by Creswell who states that - \"Research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". It consists of three steps: Pose a question, collect data to answer the question, and present an answer to the question.[6]\n\nThe Merriam-Webster Online Dictionary defines research in more detail as \"a studious inquiry or examination; especially investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\".[4]\nSteps in conducting research\n\nResearch is often conducted using the hourglass model structure of research.[7] The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:[8]\n\n    Identification of research problem\n    Literature review\n    Specifying the purpose of research\n    Determine specific research questions\n    Specification of a conceptual framework, usually a set of hypotheses[9]\n    Choice of a methodology (for data collection)\n    Data collection\n    Verify data\n    Analyzing and interpreting the data\n    Reporting and evaluating research\n    Communicating the research findings and, possibly, recommendations\n\nThe steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps.[10] Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study.[11] The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in confirming or failing to reject the Null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the flip approach: starting with articulating findings and discussion of them, moving \"up\" to identification research problem that emerging in the findings and literature review introducing the findings. The flip approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings fully emerged and interpreted.\n\nRudolph Rummel says, \"... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results.\"[12]\n\nPlato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrase in the following way, \"If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\"[13]\nScientific research\nMain article: Scientific method\nPrimary scientific research being carried out at the Microscopy Laboratory of the Idaho National Laboratory.\nScientific research equipment at MIT.\n\nGenerally, research is understood to follow a certain structural process. Though step order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:\n\n    Observations and Formation of the topic: Consists of the subject area of ones interest and following that subject area to conduct subject related research. The subject area should not be randomly chosen since it requires reading a vast amount of literature on the topic to determine the gap in the literature the researcher intends to narrow. A keen interest in the chosen subject area is advisable. The research will have to be justified by linking its importance to already existing knowledge about the topic.\n    Hypothesis: A testable prediction which designates the relationship between two or more variables.\n    Conceptual definition: Description of a concept by relating it to other concepts.\n    Operational definition: Details in regards to defining the variables and how they will be measured/assessed in the study.\n    Gathering of data: Consists of identifying a population and selecting samples, gathering information from and/or about these samples by using specific research instruments. The instruments used for data collection must be valid and reliable.\n    Analysis of data: Involves breaking down the individual pieces of data in order to draw conclusions about it.\n    Data Interpretation: This can be represented through tables, figures and pictures, and then described in words.\n    Test, revising of hypothesis\n    Conclusion, reiteration if necessary\n\nA common misconception is that a hypothesis will be proven (see, rather, Null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.\n\nA useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which state no relationship or difference between the independent or dependent variables. A null hypothesis uses a sample of all possible people to make a conclusion about the population.[14]\nHistorical method\nMain article: Historical method\nGerman historian Leopold von Ranke (1795-1886), considered to be one of the founders of modern source-based history.\n\nThe historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:[15]\n\n    Identification of origin date\n    Evidence of localization\n    Recognition of authorship\n    Analysis of data\n    Identification of integrity\n    Attribution of credibility\n\nResearch methods\nThe research room at the New York Public Library, an example of secondary research in progress.\nMaurice Hilleman is credited with saving more lives than any other scientist of the 20th century.[16]\n\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\n\n    Exploratory research, which helps to identify and define a problem or question.\n    Constructive research, which tests theories and proposes solutions to a problem or question.\n    Empirical research, which tests the feasibility of a solution using empirical evidence.\n\nThere are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\n\nQualitative research\n    Understanding of human behavior and the reasons that govern such behavior. Asking a broad question and collecting data in the form of words, images, video etc that is analyzed and searching for themes. This type of research aims to investigate a question without attempting to quantifiably measure variables or look to potential relationships between variables. It is viewed as more restrictive in testing hypotheses because it can be expensive and time-consuming, and typically limited to a single set of research subjects.[citation needed] Qualitative research is often used as a method of exploratory research as a basis for later quantitative research hypotheses.[citation needed] Qualitative research is linked with the philosophical and theoretical stance of social constructionism.\n\nQuantitative research\n    Systematic empirical investigation of quantitative properties and phenomena and their relationships. Asking a narrow question and collecting numerical data to analyze utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).[17] Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism.\n\nThe quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories.[citation needed] These methods produce results that are easy to summarize, compare, and generalize.[citation needed] Quantitative research is concerned with testing hypotheses derived from theory and/or being able to estimate the size of a phenomenon of interest. Depending on the research question, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment).[citation needed] If this is not feasible, the researcher may collect data on participant and situational characteristics in order to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.[18]\n\nIn either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible.[19]\n\nMixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common.[20]\n\nBig data has brought big impacts on research methods that now researchers do not put much effort on data collection, and also methods to analyze easily available huge amount of data have also changed.[21]\n\nNonempirical refers to an approach that is grounded in theory as opposed to using observation and experimentation to achieve the outcome. As such, nonempirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool existing and established knowledge. Nonempirical is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose within life and in science. A simple example of a nonempirical task could the prototyping of a new drug using a differentiated application of existing knowledge; similarly, it could be the development of a business process in the form of a flow chart and texts where all the ingredients are from established knowledge. Empirical research on the other hand seeks to create new knowledge through observations and experiments in which established knowledge can either be contested or supplements.\nResearch method controversies\n\nThere have been many controversies about research methods stemmed from a philosophical positivism promise to distinguish the science from other practices (especially religion) by its method. This promise leads to methodological hegemony and methodology wars where diverse researchers, often coming from opposing paradigms, try to impose their own methodology on the entire field or even on the science practice in general as the only legitimate one.[citation needed]\nQuantitative vs. Qualitative war\nAnti-methodology\n\nAccording to this view, general scientific methodology does not exist and attempts to impose it on scientists is counterproductive. Each particular research with its emerging particular inquiries requires and should produce its own way (method) of researching. Similar to the art practice, the notion of methodology has to be replaced with the notion of research mastery.[22]\nMethodological academic imperialism\n\nEpistemologies of different national sciences and cultural communities may differ and, thus, they may produce different methods of research. For example, psychological research in Russia tends to be rooted in philosophy while in the US and UK in empirism.[23][24][25] Rich countries (and dominant cultural communities within them) and their national sciences may dominate scientific discourse through funding and publications. This academic hegemony can translate into impositions of certain research methodologies through the gatekeeping process of international academic publications, conference presentation selection, institutional review boards, and funding.[26]\nProfessionalisation\nGlobe icon.\n\tThe examples and perspective in this section may not represent a worldwide view of the subject. Please improve this article and discuss the issue on the talk page. (January 2014) (Learn how and when to remove this template message)\nSee also: Academic ranks, Academics, and Scientists\n\nIn several national and private academic systems, the professionalization of research has resulted in formal job titles.\nIn Russia\n\nIn present-day Russia, the former Soviet Union and in some Post-Soviet states the term researcher (Russian: Научный сотрудник, nauchny sotrudnik) is both a generic term for a person who carried out scientific research, as well as a job position within the frameworks of the USSR Academy of Sciences, Soviet universities, and in other research-oriented establishments. The term is also sometimes translated as research fellow, research associate, etc.\n\nThe following ranks are known:\n\n    Junior Researcher (Junior Research Associate)\n    Researcher (Research Associate)\n    Senior Researcher (Senior Research Associate)\n    Leading Researcher (Leading Research Associate)[27]\n    Chief Researcher (Chief Research Associate)\n\nPublishing\nCover of the first issue of Nature, 4 November 1869.\n\nAcademic publishing describes a system that is necessary in order for academic scholars to peer review the work and make it available for a wider audience. The system varies widely by field, and is also always changing, if often slowly. Most academic work is published in journal article or book form. There is also a large body of research that exists in either a thesis or dissertation form. These forms of research can be found in databases explicitly for theses and dissertations. In publishing, STM publishing is an abbreviation for academic publications in science, technology, and medicine.\n\nMost established academic fields have their own scientific journals and other outlets for publication, though many academic journals are somewhat interdisciplinary, and publish work from several distinct fields or subfields. The kinds of publications that are accepted as contributions of knowledge or research vary greatly between fields; from the print to the electronic format. A study suggests that researchers should not give great consideration to findings that are not replicated frequently.[28] It has also been suggested that all published studies should be subjected to some measure for assessing the validity or reliability of its factors in order to prevent the publication of unproven findings.[29] Business models are different in the electronic environment. Since about the early 1990s, licensing of electronic resources, particularly journals, has been very common. Presently, a major trend, particularly with respect to scholarly journals, is open access.[30] There are two main forms of open access: open access publishing, in which the articles or the whole journal is freely available from the time of publication, and self-archiving, where the author makes a copy of their own work freely available on the web.\nResearch funding\nMain article: Funding of science\n\nMost funding for scientific research comes from three major sources: corporate research and development departments; private foundations, for example, the Bill and Melinda Gates Foundation; and government research councils such as the National Institutes of Health in the USA[31] and the Medical Research Council in the UK. These are managed primarily through universities and in some cases through military contractors. Many senior researchers (such as group leaders) spend a significant amount of their time applying for grants for research funds. These grants are necessary not only for researchers to carry out their research, but also as a source of merit.\n\nThe Social Psychology Network provides a comprehensive list of U.S. Government and private foundation funding sources.\nOriginal research\nOriginal research redirects here, for the Wikipedia policy see Wikipedia:No original research\n\nOriginal research is research that is not exclusively based on a summary, review or synthesis of earlier publications on the subject of research. This material is of a primary source character. The purpose of the original research is to produce new knowledge, rather than to present the existing knowledge in a new form (e.g., summarized or classified).[32][33]\nDifferent forms\n\nOriginal research can take a number of forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced, or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.[34]\n\nThe degree of originality of the research is among major criteria for articles to be published in academic journals and usually established by means of peer review.[35] Graduate students are commonly required to perform original research as part of a dissertation.[36]\nArtistic research\n\nThe controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines.[37] One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.[38]\n\nArtistic research has been defined by the University of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner - \"Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods and criticality. Through presented documentation, the insights gained shall be placed in a context.\"[39] Artistic research aims to enhance knowledge and understanding with presentation of the arts.[40] For a survey of the central problematics of today's Artistic Research, see Giaco Schiesser.[41]\n\nAccording to artist Hakan Topal, in artistic research, \"perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities\".[42] Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.[43]\n\nThe Society for Artistic Research (SAR) publishes the triannual Journal for Artistic Research (JAR),[44][45] an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the Research Catalogue (RC),[46][47][48] a searchable, documentary database of artistic research, to which anyone can contribute.\n\nPatricia Leavy addresses eight arts-based research (ABR) genres, they are: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.[49]\nSee also\n\n    European Charter for Researchers\n    Undergraduate research\n    Internet research\n    List of countries by research and development spending\n    Open research\n    Operations research\n    Participatory action research\n    Primary research\n    Psychological research methods\n    Research-intensive cluster\n    Scholarly research\n    Secondary research\n    Society for Artistic Research\n    Timeline of the history of scientific method", "skillName": "DSRM06_Contribute_research_objectives."}
{"id": 107, "category": "Research", "skillText": "Research question\n\nInterest in a particular topic usually begins the research process, but it is the familiarity with the subject that helps define an appropriate research question for a study.1 Questions then arise out of a perceived knowledge deficit within a subject area or field of study.2 Indeed, Haynes suggests that it is important to know “where the boundary between current knowledge and ignorance lies.”1 The challenge in developing an appropriate research question is in determining which clinical uncertainties could or should be studied and also rationalizing the need for their investigation.\n\nIncreasing one’s knowledge about the subject of interest can be accomplished in many ways. Appropriate methods include systematically searching the literature, in-depth interviews and focus groups with patients (and proxies) and interviews with experts in the field. In addition, awareness of current trends and technological advances can assist with the development of research questions.2 It is imperative to understand what has been studied about a topic to date in order to further the knowledge that has been previously gathered on a topic. Indeed, some granting institutions (e.g., Canadian Institute for Health Research) encourage applicants to conduct a systematic review of the available evidence if a recent review does not already exist and preferably a pilot or feasibility study before applying for a grant for a full trial.\n\nIn-depth knowledge about a subject may generate a number of questions. It then becomes necessary to ask whether these questions can be answered through one study or if more than one study needed.1 Additional research questions can be developed, but several basic principles should be taken into consideration.1 All questions, primary and secondary, should be developed at the beginning and planning stages of a study. Any additional questions should never compromise the primary question because it is the primary research question that forms the basis of the hypothesis and study objectives. It must be kept in mind that within the scope of one study, the presence of a number of research questions will affect and potentially increase the complexity of both the study design and subsequent statistical analyses, not to mention the actual feasibility of answering every question.1 A sensible strategy is to establish a single primary research question around which to focus the study plan.3 In a study, the primary research question should be clearly stated at the end of the introduction of the grant proposal, and it usually specifies the population to be studied, the intervention to be implemented and other circumstantial factors.4\n\nHulley and colleagues2 have suggested the use of the FINER criteria in the development of a good research question (Box 1). The FINER criteria highlight useful points that may increase the chances of developing a successful research project. A good research question should specify the population of interest, be of interest to the scientific community and potentially to the public, have clinical relevance and further current knowledge in the field (and of course be compliant with the standards of ethical boards and national research standards).\nBox 1\nFINER criteria for a good research question\nF\tFeasible\t\n\n    Adequate number of subjects\n    Adequate technical expertise\n    Affordable in time and money\n    Manageable in scope\n\nI\tInteresting\t\n\n    Getting the answer intrigues investigator, peers and community\n\nN\tNovel\t\n\n    Confirms, refutes or extends previous findings\n\nE\tEthical\t\n\n    Amenable to a study that institutional review board will approve\n\nR\tRelevant\t\n\n    To scientific knowledge\n    To clinical and health policy\n    To future research\n\nAdapted with permission from Wolters Kluwer Health.2\n\nWhereas the FINER criteria outline the important aspects of the question in general, a useful format to use in the development of a specific research question is the PICO format — consider the population (P) of interest, the intervention (I) being studied, the comparison (C) group (or to what is the intervention being compared) and the outcome of interest (O).3,5,6 Often timing (T) is added to PICO (Box 2) — that is, “Over what time frame will the study take place?”1 The PICOT approach helps generate a question that aids in constructing the framework of the study and subsequently in protocol development by alluding to the inclusion and exclusion criteria and identifying the groups of patients to be included. Knowing the specific population of interest, intervention (and comparator) and outcome of interest may also help the researcher identify an appropriate outcome measurement tool.7 The more defined the population of interest, and thus the more stringent the inclusion and exclusion criteria, the greater the effect on the interpretation and subsequent applicability and generalizability of the research findings.1,2 A restricted study population (and exclusion criteria) may limit bias and increase the internal validity of the study; however, this approach will limit external validity of the study and, thus, the generalizability of the findings to the practical clinical setting. Conversely, a broadly defined study population and inclusion criteria may be representative of practical clinical practice but may increase bias and reduce the internal validity of the study.\nBox 2\nPICOT criteria1\nP\tPopulation (patients)\t\n\n    What specific patient population are you interested in?\n\nI\tIntervention (for intervention studies only)\t\n\n    What is your investigational intervention?\n\nC\tComparison group\t\n\n    What is the main alternative to compare with the intervention?\n\nO\tOutcome of interest\t\n\n    What do you intend to accomplish, measure, improve or affect?\n\nT\tTime\t\n\n    What is the appropriate follow-up time to assess outcome\n\nA poorly devised research question may affect the choice of study design, potentially lead to futile situations and, thus, hamper the chance of determining anything of clinical significance, which will then affect the potential for publication. Without devoting appropriate resources to developing the research question, the quality of the study and subsequent results may be compromised. During the initial stages of any research study, it is therefore imperative to formulate a research question that is both clinically relevant and answerable.\nResearch hypothesis\n\nThe primary research question should be driven by the hypothesis rather than the data.1,2 That is, the research question and hypothesis should be developed before the start of the study. This sounds intuitive; however, if we take, for example, a database of information, it is potentially possible to perform multiple statistical comparisons of groups within the database to find a statistically significant association. This could then lead one to work backward from the data and develop the “question.” This is counterintuitive to the process because the question is asked specifically to then find the answer, thus collecting data along the way (i.e., in a prospective manner). Multiple statistical testing of associations from data previously collected could potentially lead to spuriously positive findings of association through chance alone.2 Therefore, a good hypothesis must be based on a good research question at the start of a trial and, indeed, drive data collection for the study.\n\nThe research or clinical hypothesis is developed from the research question and then the main elements of the study — sampling strategy, intervention (if applicable), comparison and outcome variables — are summarized in a form that establishes the basis for testing, statistical and ultimately clinical significance.3 For example, in a research study comparing computer-assisted acetabular component insertion versus freehand acetabular component placement in patients in need of total hip arthroplasty, the experimental group would be computer-assisted insertion and the control/conventional group would be free-hand placement. The investigative team would first state a research hypothesis. This could be expressed as a single outcome (e.g., computer-assisted acetabular component placement leads to improved functional outcome) or potentially as a complex/composite outcome; that is, more than one outcome (e.g., computer-assisted acetabular component placement leads to both improved radiographic cup placement and improved functional outcome).\n\nHowever, when formally testing statistical significance, the hypothesis should be stated as a “null” hypothesis.2 The purpose of hypothesis testing is to make an inference about the population of interest on the basis of a random sample taken from that population. The null hypothesis for the preceding research hypothesis then would be that there is no difference in mean functional outcome between the computer-assisted insertion and free-hand placement techniques. After forming the null hypothesis, the researchers would form an alternate hypothesis stating the nature of the difference, if it should appear. The alternate hypothesis would be that there is a difference in mean functional outcome between these techniques. At the end of the study, the null hypothesis is then tested statistically. If the findings of the study are not statistically significant (i.e., there is no difference in functional outcome between the groups in a statistical sense), we cannot reject the null hypothesis, whereas if the findings were significant, we can reject the null hypothesis and accept the alternate hypothesis (i.e., there is a difference in mean functional outcome between the study groups), errors in testing notwithstanding. In other words, hypothesis testing confirms or refutes the statement that the observed findings did not occur by chance alone but rather occurred because there was a true difference in outcomes between these surgical procedures. The concept of statistical hypothesis testing is complex, and the details are beyond the scope of this article.\n\nAnother important concept inherent in hypothesis testing is whether the hypotheses will be 1-sided or 2-sided. A 2-sided hypothesis states that there is a difference between the experimental group and the control group, but it does not specify in advance the expected direction of the difference. For example, we asked whether there is there an improvement in outcomes with computer-assisted surgery or whether the outcomes worse with computer-assisted surgery. We presented a 2-sided test in the above example because we did not specify the direction of the difference. A 1-sided hypothesis states a specific direction (e.g., there is an improvement in outcomes with computer-assisted surgery). A 2-sided hypothesis should be used unless there is a good justification for using a 1-sided hypothesis. As Bland and Atlman 8 stated, “One-sided hypothesis testing should never be used as a device to make a conventionally nonsignificant difference significant.”\n\nThe research hypothesis should be stated at the beginning of the study to guide the objectives for research. Whereas the investigators may state the hypothesis as being 1-sided (there is an improvement with treatment), the study and investigators must adhere to the concept of clinical equipoise. According to this principle, a clinical (or surgical) trial is ethical only if the expert community is uncertain about the relative therapeutic merits of the experimental and control groups being evaluated.9 It means there must exist an honest and professional disagreement among expert clinicians about the preferred treatment.9\n\nDesigning a research hypothesis is supported by a good research question and will influence the type of research design for the study. Acting on the principles of appropriate hypothesis development, the study can then confidently proceed to the development of the research objective.\nResearch objective\n\nThe primary objective should be coupled with the hypothesis of the study. Study objectives define the specific aims of the study and should be clearly stated in the introduction of the research protocol.7 From our previous example and using the investigative hypothesis that there is a difference in functional outcomes between computer-assisted acetabular component placement and free-hand placement, the primary objective can be stated as follows: this study will compare the functional outcomes of computer-assisted acetabular component insertion versus free-hand placement in patients undergoing total hip arthroplasty. Note that the study objective is an active statement about how the study is going to answer the specific research question. Objectives can (and often do) state exactly which outcome measures are going to be used within their statements. They are important because they not only help guide the development of the protocol and design of study but also play a role in sample size calculations and determining the power of the study.7 These concepts will be discussed in other articles in this series.\n\nFrom the surgeon’s point of view, it is important for the study objectives to be focused on outcomes that are important to patients and clinically relevant. For example, the most methodologically sound randomized controlled trial comparing 2 techniques of distal radial fixation would have little or no clinical impact if the primary objective was to determine the effect of treatment A as compared to treatment B on intraoperative fluoroscopy time. However, if the objective was to determine the effect of treatment A as compared to treatment B on patient functional outcome at 1 year, this would have a much more significant impact on clinical decision-making. Second, more meaningful surgeon–patient discussions could ensue, incorporating patient values and preferences with the results from this study.6,7 It is the precise objective and what the investigator is trying to measure that is of clinical relevance in the practical setting.\n\nThe following is an example from the literature about the relation between the research question, hypothesis and study objectives:\n\nStudy: Warden SJ, Metcalf BR, Kiss ZS, et al. Low-intensity pulsed ultrasound for chronic patellar tendinopathy: a randomized, double-blind, placebo-controlled trial. Rheumatology 2008;47:467–71.\n\nResearch question: How does low-intensity pulsed ultrasound (LIPUS) compare with a placebo device in managing the symptoms of skeletally mature patients with patellar tendinopathy?\n\nResearch hypothesis: Pain levels are reduced in patients who receive daily active-LIPUS (treatment) for 12 weeks compared with individuals who receive inactive-LIPUS (placebo).\n\nObjective: To investigate the clinical efficacy of LIPUS in the management of patellar tendinopathy symptoms.\nConclusion\n\nThe development of the research question is the most important aspect of a research project. A research project can fail if the objectives and hypothesis are poorly focused and underdeveloped. Useful tips for surgical researchers are provided in Box 3. Designing and developing an appropriate and relevant research question, hypothesis and objectives can be a difficult task. The critical appraisal of the research question used in a study is vital to the application of the findings to clinical practice. Focusing resources, time and dedication to these 3 very important tasks will help to guide a successful research project, influence interpretation of the results and affect future publication efforts.", "skillName": "RMO."}
{"id": 108, "category": "Research", "skillText": "The selection of the research method is crucial for what conclusions you can make about a phenomenon. It affects what you can say about the cause and factors influencing the phenomenon.\n\nIt is also important to choose a research method which is within the limits of what the researcher can do. Time, money, feasibility, ethics and availability to measure the phenomenon correctly are examples of issues constraining the research.\nChoosing the Measurement\n\nChoosing the scientific measurements are also crucial for getting the correct conclusion. Some measurements might not reflect the real world, because they do not measure the phenomenon as it should.\nResults\nSignificance Test\n\nTo test a hypothesis, quantitative research uses significance tests to determine which hypothesis is right.\n\nThe significance test can show whether the null hypothesis is more likely correct than the research hypothesis. Research methodology in a number of areas like social sciences depends heavily on significance tests.\n\nA significance test may even drive the research process in a whole new direction, based on the findings.\n\nThe t-test (also called the Student's T-Test) is one of many statistical significance tests, which compares two supposedly equal sets of data to see if they really are alike or not. The t-test helps the researcher conclude whether a hypothesis is supported or not.\nDrawing Conclusions\n\nDrawing a conclusion is based on several factors of the research process, not just because the researcher got the expected result. It has to be based on the validity and reliability of the measurement, how good the measurement was to reflect the real world and what more could have affected the results.\n\nThe observations are often referred to as 'empirical evidence' and the logic/thinking leads to the conclusions. Anyone should be able to check the observation and logic, to see if they also reach the same conclusions.\n\nErrors of the observations may stem from measurement-problems, misinterpretations, unlikely random events etc.\n\nA common error is to think that correlation implies a causal relationship. This is not necessarily true.\nGeneralization\n\nGeneralization is to which extent the research and the conclusions of the research apply to the real world. It is not always so that good research will reflect the real world, since we can only measure a small portion of the population at a time.\n\nGeneralization in Research\n\n\nValidity and Reliability\n\nValidity refers to what degree the research reflects the given research problem, while Reliability refers to how consistent a set of measurements are.\n\nValidity and Reliability\n\nTypes of validity:\n\n    External Validity\n    Population Validity\n    Ecological Validity\n    Internal Validity\n    Content Validity\n    Face Validity\n    Construct Validity\n    Convergent and Discriminant Validity\n    Test Validity\n    Criterion Validity\n    Concurrent Validity\n    Predictive Validity\n\nA definition of reliability may be \"Yielding the same or compatible results in different clinical experiments or statistical trials\" (the free dictionary). Research methodology lacking reliability cannot be trusted. Replication studies are a way to test reliability.\n\nTypes of Reliability:\n\n    Test-Retest Reliability\n    Interrater Reliability\n    Internal Consistency Reliability\n    Instrument Reliability\n    Statistical Reliability\n    Reproducibility\n\nBoth validity and reliability are important aspects of the research methodology to get better explanations of the world.\nErrors in Research\n\nLogically, there are two types of errors when drawing conclusions in research:\n\nType 1 error is when we accept the research hypothesis when the null hypothesis is in fact correct.\n\nType 2 error is when we reject the research hypothesis even if the null hypothesis is wrong.", "skillName": "RM1."}
{"id": 109, "category": "Research", "skillText": "Primary research involves the collection of original primary data by researchers. It is often undertaken after researchers have gained some insight into an issue by reviewing secondary research or by analyzing previously collected primary data.[clarification needed] It can be accomplished through various methods, including questionnaires and telephone interviews in market research, or experiments and direct observations in the physical sciences, amongst others. The distinction between primary research and secondary research is crucial among market-research professionals.[citation needed]\n\nContents\n\n    1 Details\n        1.1 Advantages\n        1.2 Disadvantages\n    2 See also\n    3 References\n\nDetails\n\nThe term primary research is widely used in academic research, market research and competitive intelligence.\n\nThere are advantages and disadvantages to primary research:\nAdvantages\n\n    The researcher can focus on both qualitative and quantitative issues.\n    Specific research issues are addressed as the researcher customizes the search design.\n    Primary research enables the marketer to focus on specific subjects and the researcher to have a higher control over how the information is collected. Taking that into account, the researcher can decide on such requirements as size of project, time frame and goal.\n    Primary research is more acute and up to date\n\nDisadvantages\n\n    Compared to secondary research, primary data may be very expensive in preparing and carrying out the research. Costs can be incurred in producing the paper for questionnaires or the equipment for an experiment of some sort.\n    To be done properly, primary data collection requires the development and execution of a research plan. It takes longer to undertake primary research than to acquire secondary data.\n    Some research projects, while potentially offering information that could prove quite valuable, may not be within the reach of a researcher.\n    By the time that the research is complete, it may be out of date.\n    A low response rate has to be expected.\n\nAn example of primary research in opinion research is if the government wants to know if people are pleased with how the government is being run, so it hands out questionnaires to the public asking if it is happy and, if not, how the government is to improve.\n\nAn example of primary research in the physical sciences is ic the transition temperature of high-temperature superconductors can be increased by varying the composition of the superconducting material. The scientist will modify the composition of the high-Tc material in various ways and then measure the transition temperature of the new material, as a function of its composition.\n\nAll research, primary or secondary, depends eventually on the collection of primary research data.\nSee also\n\n    Academic research\n    Secondary research\n    Market research\n    Marketing research\n    Focus group\n    Academic authorship\n    Expert network\n    Original research\n    Basic research", "skillName": "Primary_Research."}
{"id": 110, "category": "Research", "skillText": "For the purposes of various statutory returns (such as research income figures returned under the Research Activity Survey and published by the Higher Education Statistics Agency), research is defined by the conventions set out in the Frascati Manual.\n\nThe Frascati Manual is the internationally recognised methodology for collecting and using R&D statistics. It defines research as follows:\nResearch and experimental development (R&D) comprise creative work undertaken on a systematic basis in order to increase the stock of knowledge, including knowledge of man, culture and society, and the use of this stock of knowledge to devise new applications.\n\nThe term R&D covers three activities: basic research, applied research and experimental development.\n\n    Basic research is experimental or theoretical work undertaken primarily to acquire new knowledge of the underlying foundation of phenomena and observable facts, without any particular application or use in view.\n    Applied research is also original investigation undertaken in order to acquire new knowledge. It is, however, directed primarily towards a specific practical aim or objective.\n    Experimental development is systematic work, drawing on existing knowledge gained from research and/or practical experience, which is directed to producing new materials, products or devices, to installing new processes, systems and services, or to improving substantially those already produced or installed. R&D covers both formal R&D in R&D units and informal or occasional R&D in other units.\n\nThe Frascati Manual lists situations where certain activities are to be excluded from R&D except when carried out solely or primarily for the purposes of an R&D project. These include: routine testing and analysis of materials, components, products, processes, etc; feasibility studies; routine software development; general purpose data collection. The later stages of some clinical drug trials may be more akin to routine testing, particularly in cases where the original research has been done by a drug company or other contractor.\n\nThe Frascati Manual contains the following examples of the type of work included under the three components of R&D:\n\n    The determination of the amino acid sequence of an antibody molecule would be basic research. Investigations undertaken in an effort to distinguish between antibodies for various diseases would be applied research. Experimental development would then consist of devising a method for synthesising the antibody for a particular disease on the basis of knowledge of its structure and clinically testing the effectiveness of the synthesised antibody on patients who have agreed to accept experimental advanced treatment.\n\n    Theoretical investigation of the factors determining regional variations in economic growth is basic research; however, such investigation performed for the purpose of developing government policy would be applied research. The development of operational models, based upon laws revealed through research and aimed at modifying regional disparities, would be experimental development.", "skillName": "FrascatiDefinitionofResearch."}
{"id": 111, "category": "Research", "skillText": "Systematic Study is a special way to learn school subjects. It is both easy and successful. If you study systematically your grades will improve and you might actually spend less time studying. We think that Systematic Study is smart study.\nQ: Why should you use Systematic Study?\n\nYou should study systematically so you can make good choices about how and when you study. Most people just study without thinking about how they will learn. This may mean they read their textbooks or go over notes they took in class. These students often just close their books or notes when they have finished reading or reviewing.\n\n    This is usually a poor way to study for several reasons:\n\n        Reading or going over notes may not be the best way to study the material.\n        No specific study activity was chosen. The students just did the first activity (reading or going over notes) that came to their minds.\n        No attempt was made to see what they learned. These students did not test themselves.\n        No plan to be successful was made.\n\nWith Systematic Study you will learn to choose and plan how you study. This will make learning easier and faster.\nQ: Why do you need to learn to study?\n\nWe can't say that you do. But we do know that most students can learn to study better. Learning in school can be both difficult and time consuming. No one is born knowing how to do her or his best in school. We learn to study by listening to teachers and our parents and watching others. But, study is private; it happens in our minds. So we can't see exactly what others do. Most of us do what seems to be best and get into the habit of always studying one way.\n\nPsychologists who study school learning have discovered that most students could learn more in less time if they changed how they study. This means using some new study actions and different attitudes during study. The purpose of STUDY SMART is to help you learn these study actions and attitudes.\n\nYou can think of learning to study better as similar to a carpenter with a tool box or a cook with cooking utensils. If they had just one tool - a screwdriver or a mixing spoon - their chances of success would be limited. However, if both had a complete set of tools or utensils, they would be more likely to succeed.\n\nThe same is true for studying. If students have only one way to study, they will have less chance to succeed than if they have many. In STUDY SMART you will learn many ways to study so, like the good carpenter, you will have a full box of study tools.\n\n\n\n\nQ: What does it take to improve study?\n\nImproving any skill or ability takes some work and desire. The same is true with study. While we can show you more effective study skills, no one can study and learn for you. As you know, a mixing bowl does a cook no good unless it is used. The same is true of these skills. You will have to use them in your study.\n\nSometimes using new study skills may not be easy and you may find some study ideas don't work right away. If this happens, do not give up. New ideas and actions take time to learn and get used to. All the ideas, skills, activities, and attitudes presented in STUDY SMART do work.\n\nYou must also be willing to change what you do when you study. This will take some effort and some time. But, you will get big rewards in the future when you learn more in less time and remember it better. So, practice the skills and ideas you learn in STUDY SMART. You will find study will be easier and more successful.\nQ: What will you learn about study?\n\nFirst, you will learn a Systematic Study approach. This will help you think about study differently. You will use this approach to choose study skills and study times and to make study plans. We call this study approach PAT. PAT stands for Prepare to study, Act to learn, and Test yourself.\n\nSecond, you will learn how to use your abilities more effectively. We think all students can do well in school if they use their abilities well. We will help you read faster and remember more, take good notes, listen better in class so you can hear test questions, improve your memory for facts and ideas, organize your time, and write better papers.\n\nSTUDY SMART has 24 lessons. You should do one or two lessons each week. After you work through a lesson, you should apply the ideas to your study for at least one week using the practice suggestions in each lesson.\n\nIn this way you will continue to add new ways to study but also master each new skill without feeling overwhelmed. If you have problems or questions, talk to your teacher or counselor. Most people do have questions, so don't feel like you are having special problems if all these ideas don't work right away.\nQ: Where do these STUDY SMART ideas come from?\n\nWe have developed these ideas from talking to and working with successful students. These are people like you who make very good grades in school.\n\nThey may be different from you because they appear to learn very quickly and effortlessly. Some people believe these students do well because they are especially smart or have very high intelligence scores. But, this usually is not the reason students are successful in school.\n\nThe main difference between successful and less successful students is how and why they study. If you study like more successful students, you can be just as successful. STUDY SMART will show you how to study like the most successful students.\nQ: How do you get started?\n\nBegin by changing how you think about study. This will not be easy; but, it is a very important part of changing and improving your study.\n\nFirst, we want you to learn PAT, the components of Systematic Study.\n\nPrepare: This is an important part of study just as it is for athletes, travelers, and people in all occupations.\n\nAthletes Prepare by stretching and warming up their muscles before they compete. They also make a game plan or strategy to use their skills. The same goes for travelers who choose a route to follow, buy tickets, make reservations, etc.\n\nThe best students Prepare when they study. They think about what they need to learn, they choose how to study, they select where they will study, and they decide how to determine if they were successful. Preparing is the first part of study; you will learn how to Prepare for class and home study.\n\nThink about how you usually begin your study. What do you do? Open a book and begin reading? Grumble to yourself about having to study? Turn the radio to your favorite station? Find your favorite program on TV? Lay down on your bed?\n\n\n\nAct: This part of study involves using the best skills to achieve the results you desire. When you are finished with STUDY SMART, you will have many skills or study tools to choose from. Developing many skills is important. Most good students use many skills because they choose the best skills for the study task which they have been assigned.\n\nNow, think about how you choose to study. What do you do? The first thing you think of? The only thing you know? Or, do you consider several different ways to learn, memorize and understand?\n\nTest: This is the part of study where you Test yourself to see if you learned successfully. It is interesting that good students rarely rely only on teachers to Test them. They Test themselves every time they finish studying.\n\nNow, think about how you finish studying. What do you do? Close the book and heave a sigh of relief? Be grateful you don't have to read another chapter? Fall asleep? Listen to a favorite song? Turn on the TV? Call a friend on the phone?\n\nWrite a description of how you typically end your study.\nQ: How do you begin to improve your study?\n\nBeing a better student requires change. As you work through STUDY SMART you will learn many ways to study differently. You may already have some ideas of how you can change your study to be better. The best place to begin is to look carefully at what you are doing now. For the next week, keep a diary using Form 1.1 of what you do while you study.\n\n    Include:\n\n        When you start and stop (time and date).\n        What you study.\n        What you did to Prepare.\n        What Actions you took to learn.\n        What you did to Test yourself.\n\nAfter about a week, identify the best things you did and the things you think you should change while you study. These should include skills such as reading and note taking, attitudes such as \"disorganized\" and \"unsystematic\", mental approaches such as \"bored\" and \"not interested\", and physical states such as \"tired\" and \"sore from athletic practice.\"", "skillName": "SS."}
{"id": 112, "category": "Software", "skillText": "SAS (Statistical Analysis System)[1] is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.\n\nSAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.\n\nContents\n\n    1 Technical overview and terminology\n    2 History\n        2.1 Origins\n        2.2 Development\n        2.3 Recent history\n    3 Software products\n        3.1 Comparison to other products\n    4 Adoption\n    5 See also\n    6 References\n    7 Further reading\n    8 External links\n\nTechnical overview and terminology\n\nSAS is a software suite that can mine, alter, manage and retrieve data from a variety of sources and perform statistical analysis on it.[2] SAS provides a graphical point-and-click user interface for non-technical users and more advanced options through the SAS programming language.[2] In order to use Statistical Analysis System, Data should be in an Excel table format or SAS format.[3] SAS programs have a DATA step, which retrieves and manipulates data, usually creating a SAS data set, and a PROC step, which analyzes the data.[4]\n\nEach step consists of a series of statements.[5] The DATA step has executable statements that result in the software taking an action, and declarative statements that provide instructions to read a data set or alter the data's appearance.[4] The DATA step has two phases, compilation and execution. In the compilation phase, declarative statements are processed and syntax errors are identified. Afterwards, the execution phase processes each executable statement sequentially.[6] Data sets are organized into tables with rows called \"observations\" and columns called \"variables\". Additionally, each piece of data has a descriptor and a value.[4][7]\n\nThe PROC step consists of PROC statements that call upon named procedures. Procedures perform analysis and reporting on data sets to produce statistics, analyses and graphics. There are more than 300 procedures and each one contains a substantial body of programming and statistical work.[4] PROC statements can also display results, sort data or perform other operations.[5] SAS Macros are pieces of code or variables that are coded once and referenced to perform repetitive tasks.[8]\n\nSAS data can be published in HTML, PDF, Excel and other formats using the Output Delivery System, which was first introduced in 2007.[9] The SAS Enterprise Guide is SAS' point-and-click interface. It generates code to manipulate data or perform analysis automatically and does not require SAS programming experience to use.[10]\n\nThe SAS software suite has more than 200[11] components[12][13] Some of the SAS components include:[2][12][14]\n\n    Base SAS – Basic procedures and data management\n    SAS/STAT – Statistical analysis\n    SAS/GRAPH – Graphics and presentation\n    SAS/OR – Operations research\n    SAS/ETS – Econometrics and Time Series Analysis\n    SAS/IML – Interactive matrix language\n    SAS/AF – Applications facility\n    SAS/QC – Quality control\n    SAS/INSIGHT – Data mining\n    SAS/PH – Clinical trial analysis\n    Enterprise Miner – data mining\n    Enterprise Guide - GUI based code editor & project manager\n    SAS EBI - Suite of Business Intelligence Applications\n    SAS Grid Manager - Manager of SAS grid computing environment\n\nHistory\nOrigins\n\nThe development of SAS began in 1966 after North Carolina State University re-hired Anthony Barr[15] to program his analysis of variance and regression software so that it would run on IBM System/360 computers.[16] The project was funded by the National Institute of Health[17] and was originally intended to analyze agricultural data[12][18] to improve crop yields.[19] Barr was joined by student James Goodnight, who developed the software's statistical routines, and the two became project-leaders.[15][16][20] In 1968, Barr and Goodnight integrated new multiple regression and analysis of variance routines.[21][22] In 1972, after issuing the first release of SAS, the project lost its funding.[17] According to Goodnight, this was because NIH only wanted to fund projects with medical applications.[23] Goodnight continued teaching at the university for a salary of $1 and access to mainframe computers for use with the project,[17] until it was funded by the University Statisticians of the Southern Experiment Stations the following year.[16][23] John Sall joined the project in 1973 and contributed to the software's econometrics, time series, and matrix algebra. Another early participant, Caroll G. Perkins, contributed to SAS' early programming. Jolayne W. Service and Jane T. Helwig created SAS' first documentation.[21]\n\nThe first versions of SAS were named after the year in which they were released.[24] In 1971, SAS 71 was published as a limited release.[2][25] It was used only on IBM mainframes and had the main elements of SAS programming, such as the DATA step and the most common procedures in the PROC step.[24] The following year a full version was released as SAS 72, which introduced the MERGE statement and added features for handling missing data or combining data sets.[26] In 1976, Barr, Goodnight, Sall, and Helwig removed the project from North Carolina State and incorporated it into SAS Institute, Inc.[27]\nDevelopment\n\nSAS was re-designed in SAS 76 with an open architecture that allowed for compilers and procedures. The INPUT and INFILE statements were improved so they could read most data formats used by IBM mainframes. Generating reports was also added through the PUT and FILE statements. The ability to analyze general linear models was also added[28] as was the FORMAT procedure, which allowed developers to customize the appearance of data.[24] In 1979, SAS 79 added support for the CMS operating system and introduced the DATASETS procedure. Three years later, SAS 82 introduced an early macro language and the APPEND procedure.[24]\n\nSAS version 4 had limited features, but made SAS more accessible. Version 5 introduced a complete macro language, array subscripts, and a full-screen interactive user interface called Display Manager.[24] In 1985, SAS was rewritten in the C programming language. This allowed for the SAS' Multivendor Architecture that allows the software to run on UNIX, MS-DOS, and Windows. It was previously written in PL/I, Fortran, and assembly language.[20][24]\n\nIn the 1980s and 1990s, SAS released a number of components to complement Base SAS. SAS/GRAPH, which produces graphics, was released in 1980, as well as the SAS/ETS component, which supports econometric and time series analysis. A component intended for pharmaceutical users, SAS/PH-Clinical, was released in the 1990s. The Food and Drug Administration standardized on SAS/PH-Clinical for new drug applications in 2002.[20] Vertical products like SAS Financial Management and SAS Human Capital Management (then called CFO Vision and HR Vision respectively) were also introduced.[29] JMP was developed by SAS co-founder John Sall and a team of developers to take advantage of the graphical user interface introduced in the 1984 Apple Macintosh[30] and shipped for the first time in 1989.[30] Updated versions of JMP were released continuously after 2002 with the most recent release being from 2012.[31][32][33][34][35][36][37]\n\nSAS version 6 was used throughout the 1990s and was available on a wider range of operating systems, including Macintosh, OS/2, Silicon Graphics, and Primos. SAS introduced new features through dot-releases. From 6.06 to 6.09, a user interface based on the windows paradigm was introduced and support for SQL was added.[24] Version 7 introduced the Output Delivery System (ODS) and an improved text editor. ODS was improved upon in successive releases. For example, more output options were added in version 8. The number of operating systems that were supported was reduced to UNIX, Windows and z/OS, and Linux was added.[24][38] SAS version 8 and SAS Enterprise Miner were released in 1999.[20]\nRecent history\n\nIn 2002, the Text Miner software was introduced. Text Miner analyzes text data like emails for patterns in Business Intelligence applications.[39] In 2004, SAS Version 9.0 was released, which was dubbed \"Project Mercury\" and was designed to make SAS accessible to a broader range of business users.[40][41] Version 9.0 added custom user interfaces based on the user’s role and established the point-and-click user interface of SAS Enterprise Guide as the software's primary graphical user interface (GUI).[40] The Customer Relationship Management (CRM) features were improved in 2004 with SAS Interaction Management.[42] In 2008 SAS announced Project Unity, designed to integrate data quality, data integration and master data management.[43]\n\nSAS sued World Programming, the developers of a competing implementation, World Programming System, alleging that they had infringed SAS's copyright in part by implementing the same functionality. This case was referred from the United Kingdom's High Court of Justice to the European Court of Justice on 11 August 2010.[44] In May 2012, the European Court of Justice ruled in favor of World Programming, finding that \"the functionality of a computer program and the programming language cannot be protected by copyright.\"[45]\n\nA free version was introduced for students in 2010.[46] SAS Social Media Analytics, a tool for social media monitoring, engagement and sentiment analysis, was also released that year.[47][48] SAS Rapid Predictive Modeler (RPM), which creates basic analytical models using Microsoft Excel, was introduced that same year.[48][49] JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel.[50][51] The following year, a High Performance Computing appliance was made available in a partnership with Teradata and EMC Greenplum.[52][53] In 2011, the company released Enterprise Miner 7.1.[54] The company introduced 27 data management products from October 2013 to October 2014 and updates to 160 others.[55] At the 2015 SAS Global Forum, it announced several new products that were specialized for different industries, as well as new training software.[56]\nSoftware products\n\nAs of 2011 SAS's largest set of products is its line for customer intelligence. Numerous SAS modules for web, social media and marketing analytics may be used to profile customers and prospects, predict their behaviors and manage and optimize communications.[37][57] SAS also provides the SAS Fraud Framework. The framework's primary functionality is to monitor transactions across different applications, networks and partners and use analytics to identify anomalies that are indicative of fraud.[58][59][60][61] SAS Enterprise GRC (Governance, Risk and Compliance) provides risk modeling, scenario analysis and other functions[62][63] in order to manage and visualize risk, compliance and corporate policies.[64] There is also a SAS Enterprise Risk Management product-set designed primarily for banks and financial services organizations.[65]\n\nSAS' products for monitoring and managing the operations of IT systems are collectively referred to as SAS IT Management Solutions.[66] SAS collects data from various IT assets on performance and utilization, then creates reports and analyses.[67] SAS' Performance Management products consolidate and provide graphical displays for key performance indicators (KPIs) at the employee, department and organizational level.[68][69] The SAS Supply Chain Intelligence product suite is offered for supply chain needs, such as forecasting product demand, managing distribution and inventory and optimizing pricing.[70] There is also a \"SAS for Sustainability Management\" set of software to forecast environmental, social and economic effects and identify causal relationships between operations and an impact on the environmental or ecosystem.[71]\n\nSAS has product sets for specific industries, such as government, retail, telecommunications and aerospace and for marketing optimization or high-performance computing.[72]\nComparison to other products\nSee also: Comparison of statistical packages\n\nIn a 2005 article for the Journal of Marriage and Family comparing statistical packages from SAS and its competitors Stata and SPSS, Alan C. Acock wrote that SAS programs provide \"extraordinary range of data analysis and data management tasks,\" but were difficult to use and learn.[73] SPSS and Stata, meanwhile, were both easier to learn (with better documentation) but had less capable analytic abilities, though these could be expanded with paid (in SPSS) or free (in Stata) add-ons. Acock concluded that SAS was best for power users, while occasional users would benefit most from SPSS and Stata.[73] A comparison by the University of California, Los Angeles, gave similar results.[74]\n\nCompetitors such as Revolution Analytics and Alpine Data Labs advertise their products as considerably cheaper than SAS'. In a 2011 comparison, Doug Henschen of InformationWeek found that start-up fees for the three are similar, though he admitted that the starting fees were not necessarily the best basis for comparison.[75] SAS' business model is not weighted as heavily on initial fees for its programs, instead focusing on revenue from annual subscription fees.[76][77]\nAdoption\n\nAccording to IDC, SAS is the largest market-share holder in \"advanced analytics\" with 35.4 percent of the market as of 2013.[78] It is the fifth largest market-share holder for business intelligence (BI) software with a 6.9% share[79] and the largest independent vendor. It competes in the BI market against conglomerates, such as SAP BusinessObjects, IBM Cognos, SPSS Modeler, Oracle Hyperion, and Microsoft BI.[80] SAS has been named in the Gartner Leader's Quadrant for Data Integration Tools[81] and for Business Intelligence and Analytical Platforms.[82] A study published in 2011 in BMC Health Services Research found that SAS was used in 42.6 percent of data analyses in health service research, based on a sample of 1,139 articles drawn from three journals.[83]", "skillName": "SAS."}
{"id": 113, "category": "Software", "skillText": "JMP (pronounced \"jump\") is a computer program for statistics developed by the JMP business unit of SAS Institute. It was launched in 1989[1] to take advantage of the graphical user interface introduced by the Macintosh. It has since been improved and made available for the Windows operating system. JMP is used in applications such as Six Sigma, quality control and engineering, design of experiments and scientific research.\n\nThe software consists of five products: JMP, JMP Pro, JMP Clinical, JMP Genomics and the JMP Graph Builder App for the iPad. A scripting language is also available. The software is focused on exploratory analytics, whereby users investigate and explore data, rather than testing a hypothesis.\n\nContents\n\n    1 History\n    2 Software\n    3 JMP Scripting Language (JSL)\n    4 Notable applications\n    5 See also\n    6 References\n    7 Further reading\n    8 External links\n\nHistory\nVersion 1.0 of JMP from 1989\n\nJMP was developed in the 1980s by John Sall and a team of developers to make use of the graphical user interface introduced by the 1984 Apple Macintosh.[2] It originally stood for \"John's Macintosh Project\"[3] or “John’s Macintosh Product”[4] and was first released in October 1989.[2] It was used mostly by scientists and engineers for design of experiments (DOE), quality and productivity support (Six Sigma), and reliability modeling.[5] Semiconductor manufacturers were also among JMP’s early adopters.[6]\n\nInteractive graphics and other features were added in 1991[7][8] with version 2.0. Version 2 was twice the size as the original, though it was still delivered on a floppy disk. It required 2 MB of memory and came with 700 pages of documentation.[9] Support for Microsoft Windows was added in 1994.[4][10] JMP was re-written[11] with version 3 in 1999.[12][13] Version 4, released in 2002, could import data from a wider variety of data sources[14] and added support for surface plots.[8] Version 4 also added time series forecasting and new smoothing models, such as the seasonal smoothing method, called Winter's Method, and ARIMA (Autoregressive Integrated Moving Average). It was also the first version to support JSL, JMP Scripting Language.[15]\n\nIn 2005, data mining tools like a decision tree and neural net were added with version 5[16] as well as Linux support, which was later withdrawn in JMP 9.[5] Later in 2005, JMP 6 was introduced.[6][17] JMP began integrating with SAS in version 7.0 in 2007 and has strengthened this integration ever since. Users can write SAS code in JMP, connect to SAS servers, and retrieve and use data from SAS. Support for bubble plots was added in version 7.[5][18] JMP 7 also improved data visualization and diagnostics.[19]\n\nJMP 8 was released in 2009 with new drag-and-drop features and a 64-bit version to take advantage of advances in the Mac operating system.[20] It also added a new user interface for building graphs, tools for choice experiments and support for Life Distributions.[21] According to Scientific Computing, the software had improvements in \"graphics, QA, ease-of-use, SAS integration and data management areas.\"[22] JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel.[23][24] The main screen was rebuilt and enhancements were made to simulations, graphics and a new Degradation platform.[25] In March 2012, version 10 made improvements in data mining, predictive analytics, and automated model building.[26][27]\nSoftware\nScreenshot of different data displays in JMP\n\nJMP consists of JMP, JMP Pro, JMP Clinical and JMP Genomics,[27] as well as the Graph Builder iPad App.[28] JMP Clinical and JMP Genomics combine JMP with SAS software.[27]\n\nJMP software is partly focused on exploratory data analysis and visualization. It is designed for users to investigate data to learn something unexpected, as opposed to confirming a hypothesis.[4][27][29] JMP links statistical data to graphics representing them, so users can drill down or up to explore the data and various visual representations of it.[14][30][31] Its primary applications are for designed experiments and analyzing statistical data from industrial processes.[6]\n\nJMP is a desktop application with a wizard-based user interface, while SAS can be installed on servers. It runs in-memory, instead of on disk storage.[27] According to a review in Pharmaceutical Statistics, JMP is often used as a graphical front-end for a SAS system, which performs the statistical analysis and tabulations.[32] JMP Genomics, used for analyzing and visualizing genomics data,[33] requires a SAS component to operate and can access SAS/Genetics and SAS/STAT procedures or invoke SAS macros.[32] JMP Clinical, used for analyzing clinical trial data, can package SAS code within the JSL scripting language and convert SAS code to JMP.[18]\n\nJMP is also the name of the SAS Institute business unit that develops JMP. As of 2011 it had 180 employees and 250,000 users.[27]\nJMP Scripting Language (JSL)\n\nThe JMP Scripting Language (JSL) is an interpreted language for recreating analytic results and for automating or extending the functionality of JMP software.[34]:29 JSL was first introduced in JMP version 4 in 2000.[35]:1 JSL has a LISP-like syntax, structured as a series of expressions. All [rpgramming elements, including if-then statemenst and loops, are implemented as JSL functions. Data tables, display elements and analyses are represented by objects in JSL that are manipulated with named messages. Users may write JSL scripts to perform analyses and visualizations not available in the point-and-click interface or to automate a series of commands, such as weekly reports.[34] SAS, R, and Matlab code can also be executed using JSL.[36]\nNotable applications\nJMP being used in the WildTrack FIT system\n\nIn 2007, a wildlife monitoring organization, WildTrack, started using JMP with the Footprint Identification Technology (FIT) system to identify individual endangered animals by their footprints.[37] In 2009, the Chicago Botanic Garden used JMP to analyze DNA data from tropical breadfruit. Researchers determined that the seedless, starchy fruit was created by the deliberate hybridization of two fruits, the breadnut and the dugdug.[38] The Herzenberg Laboratory at Stanford has integrated JMP with the Fluorescence Activated Cell Sorter (FACS). The FACS system is used to study HIV, cancer, stem-cells and oceanography.[39]", "skillName": "JMP."}
{"id": 114, "category": "Software", "skillText": "Git (/ɡɪt/[6]) is a version control system that is used for software development[7] and other version control tasks. As a distributed revision control system it is aimed at speed,[8] data integrity,[9] and support for distributed, non-linear workflows.[10] Git was created by Linus Torvalds in 2005 for development of the Linux kernel, with other kernel developers contributing to its initial development.[11]\n\nAs with most other distributed version control systems, and unlike most client–server systems, every Git directory on every computer is a full-fledged repository with complete history and full version-tracking capabilities, independent of network access or a central server.[12] Like the Linux kernel, Git is free software distributed under the terms of the GNU General Public License version 2.\n\nContents\n\n    1 History\n    2 Design\n        2.1 Characteristics\n        2.2 Data structures\n        2.3 References\n    3 Implementations\n    4 Git server\n    5 Adoption\n    6 Security\n    7 See also\n    8 References\n    9 External links\n\nHistory\n\nGit development began in April 2005, after many developers of the Linux kernel gave up access to BitKeeper, a proprietary source control management (SCM) system that they had previously used to maintain the project.[13] The copyright holder of BitKeeper, Larry McVoy, had withdrawn free use of the product after claiming that Andrew Tridgell had reverse-engineered the BitKeeper protocols.[14]\n\nTorvalds wanted a distributed system that he could use like BitKeeper, but none of the available free systems met his needs, particularly in terms of performance. Torvalds cited an example of a source-control management system requiring 30 seconds to apply a patch and update all associated metadata, and noted that this would not scale to the needs of Linux kernel development, where syncing with fellow maintainers could require 250 such actions at a time. For his design criteria, he specified that patching should take no more than three seconds,[8] and added three additional points:\n\n    Take Concurrent Versions System (CVS) as an example of what not to do; if in doubt, make the exact opposite decision[10]\n    Support a distributed, BitKeeper-like workflow[10]\n    Include very strong safeguards against corruption, either accidental or malicious[9]\n\nThese criteria eliminated every then-existing version-control system except Monotone. Performance considerations excluded this, too.[10] So immediately after the 2.6.12-rc2 Linux kernel development release, Torvalds set out to write his own system.[10]\n\nTorvalds quipped about the name git (which means \"unpleasant person\" in British English slang): \"I'm an egotistical bastard, and I name all my projects after myself. First 'Linux', now 'git'.\"[15][16] The man page describes Git as \"the stupid content tracker\".[17] The readme file of the source code elaborates further:[18]\n\nThe name \"git\" was given by Linus Torvalds when he wrote the very\nfirst version. He described the tool as \"the stupid content tracker\"\nand the name as (depending on your mood):\n\n - random three-letter combination that is pronounceable, and not\n   actually used by any common UNIX command.  The fact that it is a\n   mispronunciation of \"get\" may or may not be relevant.\n - stupid. contemptible and despicable. simple. Take your pick from the\n   dictionary of slang.\n - \"global information tracker\": you're in a good mood, and it actually\n   works for you. Angels sing, and a light suddenly fills the room.\n - \"goddamn idiotic truckload of sh*t\": when it breaks\n\nThe development of Git began on 3 April 2005.[19] Torvalds announced the project on 6 April;[20] it became self-hosting as of 7 April.[19] The first merge of multiple branches took place on 18 April.[21] Torvalds achieved his performance goals; on 29 April, the nascent Git was benchmarked recording patches to the Linux kernel tree at the rate of 6.7 per second.[22] On 16 June Git managed the kernel 2.6.12 release.[23]\n\nTorvalds turned over maintenance on 26 July 2005 to Junio Hamano, a major contributor to the project.[24] Hamano was responsible for the 1.0 release on 21 December 2005, and remains the project's maintainer.[25]\nVersion \tOriginal release date \tLatest version \tRelease date\n0.99 \t2005-07-11 \t0.99.9n \t2005-12-15\n1.0 \t2005-12-21 \t1.0.13 \t2006-01-27\n1.1 \t2006-01-08 \t1.1.6 \t2006-01-30\n1.2 \t2006-02-12 \t1.2.6 \t2006-04-08\n1.3 \t2006-04-18 \t1.3.3 \t2006-05-16\n1.4 \t2006-06-10 \t1.4.4.5 \t2008-07-16\n1.5 \t2007-02-14 \t1.5.6.6 \t2008-12-17\n1.6 \t2008-08-17 \t1.6.6.3 \t2010-12-15\n1.7 \t2010-02-13 \t1.7.12.4 \t2012-10-17\n1.8 \t2012-10-21 \t1.8.5.6 \t2014-12-17\n1.9 \t2014-02-14 \t1.9.5 \t2014-12-17\n2.0 \t2014-05-28 \t2.0.5 \t2014-12-17\n2.1 \t2014-08-16 \t2.1.4 \t2014-12-17\n2.2 \t2014-11-26 \t2.2.3 \t2015-09-04\n2.3 \t2015-02-05 \t2.3.10 \t2015-09-29\n2.4 \t2015-04-30 \t2.4.11 \t2016-03-17\n2.5 \t2015-07-27 \t2.5.5 \t2016-03-17\n2.6 \t2015-09-28 \t2.6.6 \t2016-03-17\n2.7 \t2015-10-04 \t2.7.4 \t2016-03-17\n2.8 \t2016-03-28 \t2.8.4 \t2016-06-06\n2.9 \t2016-06-13 \t2.9.0 \t2016-06-13\nLegend:\nOld version\nOlder version, still supported\nLatest version\nLatest preview version\nDesign\n\nGit's design was inspired by BitKeeper and Monotone.[26][27] Git was originally designed as a low-level version control system engine on top of which others could write front ends, such as Cogito or StGIT.[27] The core Git project has since become a complete version control system that is usable directly.[28] While strongly influenced by BitKeeper, Torvalds deliberately avoided conventional approaches, leading to a unique design.[29]\nCharacteristics\n\nGit's design is a synthesis of Torvalds's experience with Linux in maintaining a large distributed development project, along with his intimate knowledge of file system performance gained from the same project and the urgent need to produce a working system in short order. These influences led to the following implementation choices:\n\nStrong support for non-linear development\n    Git supports rapid branching and merging, and includes specific tools for visualizing and navigating a non-linear development history. A core assumption in Git is that a change will be merged more often than it is written, as it is passed around various reviewers. Branches in Git are very lightweight: A branch in Git is only a reference to a single commit. With its parental commits, the full branch structure can be constructed.\nDistributed development\n    Like Darcs, BitKeeper, Mercurial, SVK, Bazaar and Monotone, Git gives each developer a local copy of the entire development history, and changes are copied from one such repository to another. These changes are imported as additional development branches, and can be merged in the same way as a locally developed branch.\nCompatibility with existing systems/protocols\n    Repositories can be published via HTTP, FTP, rsync (until Git 2.8.0[30]), or a Git protocol over either a plain socket, or ssh. Git also has a CVS server emulation, which enables the use of existing CVS clients and IDE plugins to access Git repositories. Subversion and svk repositories can be used directly with git-svn.\nEfficient handling of large projects\n    Torvalds has described Git as being very fast and scalable,[31] and performance tests done by Mozilla[32] showed it was an order of magnitude faster than some version-control systems, and fetching version history from a locally stored repository can be one hundred times faster than fetching it from the remote server.[33]\nCryptographic authentication of history\n    The Git history is stored in such a way that the ID of a particular version (a commit in Git terms) depends upon the complete development history leading up to that commit. Once it is published, it is not possible to change the old versions without it being noticed. The structure is similar to a Merkle tree, but with additional data at the nodes as well as the leaves.[34] (Mercurial and Monotone also have this property.)\nToolkit-based design\n    Git was designed as a set of programs written in C, and a number of shell scripts that provide wrappers around those programs.[35] Although most of those scripts have since been rewritten in C for speed and portability, the design remains, and it is easy to chain the components together.[36]\nPluggable merge strategies\n    As part of its toolkit design, Git has a well-defined model of an incomplete merge, and it has multiple algorithms for completing it, culminating in telling the user that it is unable to complete the merge automatically and that manual editing is required.\nGarbage accumulates unless collected\n    Aborting operations or backing out changes will leave useless dangling objects in the database. These are generally a small fraction of the continuously growing history of wanted objects. Git will automatically perform garbage collection when enough loose objects have been created in the repository. Garbage collection can be called explicitly using git gc --prune.[37]\nPeriodic explicit object packing\n    Git stores each newly created object as a separate file. Although individually compressed, this takes a great deal of space and is inefficient. This is solved by the use of packs that store a large number of objects in a single file (or network byte stream) called packfile, delta-compressed among themselves. Packs are compressed using the heuristic that files with the same name are probably similar, but do not depend on it for correctness. A corresponding index file is created for each packfile, telling the offset of each object in the packfile. Newly created objects (newly added history) are still stored singly, and periodic repacking is required to maintain space efficiency. The process of packing the repository can be very computationally expensive. By allowing objects to exist in the repository in a loose, but quickly generated format, Git allows the expensive pack operation to be deferred until later when time does not matter (e.g. the end of the work day). Git does periodic repacking automatically but manual repacking is also possible with the git gc command. For data integrity, both packfile and its index have SHA-1 checksum inside, and also the file name of packfile contains a SHA-1 checksum. To check integrity, run the git fsck command.\n\nAnother property of Git is that it snapshots directory trees of files. The earliest systems for tracking versions of source code, SCCS and RCS, worked on individual files and emphasized the space savings to be gained from interleaved deltas (SCCS) or delta encoding (RCS) the (mostly similar) versions. Later revision control systems maintained this notion of a file having an identity across multiple revisions of a project. However, Torvalds rejected this concept.[38] Consequently, Git does not explicitly record file revision relationships at any level below the source code tree.\n\nImplicit revision relationships have some significant consequences:\n\n    It is slightly more expensive to examine the change history of a single file than the whole project.[39] To obtain a history of changes affecting a given file, Git must walk the global history and then determine whether each change modified that file. This method of examining history does, however, let Git produce with equal efficiency a single history showing the changes to an arbitrary set of files. For example, a subdirectory of the source tree plus an associated global header file is a very common case.\n    Renames are handled implicitly rather than explicitly. A common complaint with CVS is that it uses the name of a file to identify its revision history, so moving or renaming a file is not possible without either interrupting its history, or renaming the history and thereby making the history inaccurate. Most post-CVS revision control systems solve this by giving a file a unique long-lived name (analogous to an inode number) that survives renaming. Git does not record such an identifier, and this is claimed as an advantage.[40][41] Source code files are sometimes split or merged as well as simply renamed,[42] and recording this as a simple rename would freeze an inaccurate description of what happened in the (immutable) history. Git addresses the issue by detecting renames while browsing the history of snapshots rather than recording it when making the snapshot.[43] (Briefly, given a file in revision N, a file of the same name in revision N−1 is its default ancestor. However, when there is no like-named file in revision N−1, Git searches for a file that existed only in revision N−1 and is very similar to the new file.) However, it does require more CPU-intensive work every time history is reviewed, and a number of options to adjust the heuristics. This mechanism does not always work; sometimes a file that is renamed with changes in the same commit is read as a deletion of the old file and the creation of a new file. Developers can work around this limitation by committing the rename and changes separately.\n\nGit implements several merging strategies; a non-default can be selected at merge time:[44]\n\n    resolve: the traditional three-way merge algorithm.\n    recursive: This is the default when pulling or merging one branch, and is a variant of the three-way merge algorithm.\n\n        When there are more than one common ancestors that can be used for three-way merge, it creates a merged tree of the common ancestors and uses that as the reference tree for the three-way merge. This has been reported to result in fewer merge conflicts without causing mis-merges by tests done on actual merge commits taken from Linux 2.6 kernel development history. Additionally this can detect and handle merges involving renames.\n        — Linus Torvalds[45]\n\n    octopus: This is the default when merging more than two heads.\n\nData structures\n\nGit's primitives are not inherently a source code management (SCM) system. Torvalds explains,[46]\n\n    In many ways you can just see git as a filesystem – it's content-addressable, and it has a notion of versioning, but I really really designed it coming at the problem from the viewpoint of a filesystem person (hey, kernels is what I do), and I actually have absolutely zero interest in creating a traditional SCM system.\n\nFrom this initial design approach, Git has developed the full set of features expected of a traditional SCM,[28] with features mostly being created as needed, then refined and extended over time.\nSome data flows and storage levels in the Git revision control system.\n\nGit has two data structures: a mutable index (also called stage or cache) that caches information about the working directory and the next revision to be committed; and an immutable, append-only object database.\n\nThe object database contains four types of objects:\n\n    A blob (binary large object) is the content of a file. Blobs have no file name, time stamps, or other metadata.\n    A tree object is the equivalent of a directory. It contains a list of file names, each with some type bits and the name of a blob or tree object that is that file, symbolic link, or directory's contents. This object describes a snapshot of the source tree.\n    A commit object links tree objects together into a history. It contains the name of a tree object (of the top-level source directory), a time stamp, a log message, and the names of zero or more parent commit objects.\n    A tag object is a container that contains reference to another object and can hold additional meta-data related to another object. Most commonly, it is used to store a digital signature of a commit object corresponding to a particular release of the data being tracked by Git.\n\nThe index serves as connection point between the object database and the working tree.\n\nEach object is identified by a SHA-1 hash of its contents. Git computes the hash, and uses this value for the object's name. The object is put into a directory matching the first two characters of its hash. The rest of the hash is used as the file name for that object.\n\nGit stores each revision of a file as a unique blob. The relationships between the blobs can be found through examining the tree and commit objects. Newly added objects are stored in their entirety using zlib compression. This can consume a large amount of disk space quickly, so objects can be combined into packs, which use delta compression to save space, storing blobs as their changes relative to other blobs.\n\nGit servers typically listen on TCP port 9418.[47]\nReferences\n\nEvery object in the Git database which is not referred to may be cleaned up by using a garbage collection command, or automatically. An object may be referenced by another object, or an explicit reference. Git knows different types of references. The commands to create, move, and delete references vary. \"git show-ref\" lists all references. Some types are:\n\n    heads: refers to an object locally\n    remotes: refers to an object which exists in a remote repository\n    stash: refers to an object not yet committed\n    meta: e.g. a configuration in a bare repository, user rights; the refs/meta/config namespace was introduced resp gets used by Gerrit[clarification needed][48]\n    tags: see above\n\nImplementations\ngitg is a graphical front-end using GTK+\n\nGit is primarily developed on Linux, although it also supports most major operating systems including BSD, Solaris, OS X, and Microsoft Windows.[49]\n\nThe first Microsoft Windows \"port\" of Git was primarily a Linux emulation framework that hosts the Linux version. Installing Git under Windows creates a similarly named Program Files directory containing 5,236 files in 580 directories. These include the MinGW port of the GNU Compiler Collection, Perl 5, msys2.0, itself a fork of Cygwin, a Unix-like emulation environment for Windows, and various other Windows ports or emulations of Linux utilities and libraries. Currently native Windows builds of Git are distributed as 32 and 64-bit installers.\n\nThe JGit implementation of Git is a pure Java software library, designed to be embedded in any Java application. JGit is used in the Gerrit code review tool and in EGit, a Git client for the Eclipse IDE.[50]\n\nThe Dulwich implementation of Git is a pure Python software component for Python 2.[51]\n\nThe libgit2 implementation of Git is an ANSI C software library with no other dependencies, which can be built on multiple platforms including Microsoft Windows, Linux, Mac OS X, and BSD.[52] It has bindings for many programming languages, including Ruby, Python and Haskell.[53][54][55]\n\nJS-Git is a JavaScript implementation of a subset of Git.[56]\nGit server\n\nAs Git is a distributed version control system, it can be used as a server out of the box. Dedicated Git server software helps, amongst other features, to add access control, display the contents of a Git repository via the web, and help managing multiple repositories. Remote file store and shell access: A Git repository can be cloned to a shared file system, and accessed by other persons. It can also be accessed via remote shell just by having the Git software installed and allowing a user to log in.[57]\n\nGit daemon, instaweb\n    Git daemon allows users to share their own repository to colleagues quickly. Git instaweb allows users to provide web view to the repository. As of 2014-04 instaweb does not work on Windows. Both can be seen in the line of Mercurial's \"hg serve\".[58][59]\nGitolite\n    Gitolite is an access control layer on top of Git, providing fine access control to Git repositories. It relies on other software to remotely view the repositories on the server.[60][61]\nApache Allura\n    Apache Allura is an open-source forge software for managing source code repositories, bug reports, discussions, wiki pages, blogs and more for any number of individual projects.\nGerrit\n    Gerrit provides two out of three functionalities: access control and managing repositories. It uses jGit. To view repositories it is combined e.g. with Gitiles or GitBlit.\nGitblit\n    Gitblit can provide all three functions, but is in larger installations used as repository browser installed with gerrit for access control and management of repositories.[62][63] Gitblit can also provide the sync option for other repository.\nGitiles\n    Gitiles is a simple repository browser, usually used together with gerrit.[64][65]\nBonobo Git Server\n    Bonobo Git Server is a simple Git server for Windows implemented as an ASP.NET gateway.[66] It relies on the authentication mechanisms provided by Windows Internet Information Services, thus it does not support SSH access but can be easily integrated with Active Directory.\nGitorious\n    Gitorious is the free software behind the Git repository hosting service of the same name. In March 2015, Gitorious was acquired by GitLab.[67]\nGitLab\n    GitLab provides a software repository service. It offers a web interface like GitHub, and is written in Ruby.\n\nDjacket \n    Djacket is a free and open source Git server like GitHub, meant for personal and small business usages. It is written in Python and Django framework.\nGogs\n    Gogs is another self-hosted Git service written in Go. It provides similar features to GitLab as a web interface.[68]\nRhodeCode\n    RhodeCode is an open source self-hosted platform for behind-the-firewall source code management. It provides centralized control over all Git, Mercurial and Subversion repositories, with common authentication and permission management. RhodeCode allows forking, pull requests distributed revision control, and code reviews via a web interface. RhodeCode is written in Python and Pyramid framework. [69]\nGitHub\n    GitHub is a website where copies of Git repositories can be uploaded. It is a Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. Unlike Git, which is strictly a command-line tool, GitHub provides a web-based graphical interface and desktop as well as mobile integration. It also provides access control and several collaboration features such as wikis, task management, bug tracking and other features that can be helpful for projects. It allows collaboration with other people on projects. It does that by providing a centralized location to share the repository, a web-based interface to view it, and features like forking, pull requests distributed revision control, issues, and wikis.\nBitbucket\n    Bitbucket is a web-based hosting service for projects that use either the Git (since October 2011) or the Mercurial (since launch) revision control systems. Bitbucket offers both commercial plans and free accounts.\nSourcegraph\n    Sourcegraph is a self-hosted Git service, written in Go. It also provides jump-to-definition, cross-reference, and semantic search features (by analyzing code in several languages).[70]\nCommercial products\n    Commercial programs are also available to be installed on premises, amongst them GitHub (using native Git, available as a vm), RhodeCode (custom front-end web interface, native Git in the backend), Stash (using a custom front-end and native Git in the backend), Team Foundation Server (using libgit2).[71]\n\nRelated article: Comparison of source code hosting facilities\nAdoption\n\nThe Eclipse Foundation reported in its annual community survey that as of May 2014, Git is now the most widely used source code management tool, with 42.9% of professional software developers reporting that they use Git as their primary source control system[72] compared with 36.3% in 2013, 32% in 2012; or for Git responses excluding use of GitHub: 33.3% in 2014, 30.3% in 2013, 27.6% in 2012 and 12.8% in 2011.[73] Open source directory Black Duck Open Hub reports a similar uptake among open source projects.[74]\n\nThe UK IT jobs website itjobswatch.co.uk reports that as of late December 2014, 23.58% of UK permanent software development job openings have cited Git,[75] ahead of 16.34% for Subversion,[76] 11.58% for Microsoft Team Foundation Server,[77] 1.62% for Mercurial,[78] and 1.13% for Visual SourceSafe.[79]\nSecurity\n\nOn 17 December 2014, an exploit was found affecting the Windows and Mac versions of the Git client. An attacker could perform arbitrary code execution on a Windows or Mac computer with Git installed by creating a malicious Git tree (directory) named .git (a directory in Git repositories that stores all the data of the repository) in a different case (such as .GIT or .Git, needed because Git doesn't allow the all-lowercase version of .git to be created manually) with malicious files in the .git/hooks subdirectory (a folder with executable files that Git runs) on a repository that the attacker made or on a repository that the attacker can modify. If a Windows or Mac user \"pulls\" (downloads) a version of the repository with the malicious directory, then switches to that directory, the .git directory will be overwritten (due to the case-insensitive nature of the Windows and Mac filesystems) and the malicious executable files in .git/hooks may be run, which results in the attacker's commands being executed. An attacker could also modify the .git/config configuration file, which allows the attacker to create malicious Git aliases (aliases for Git commands or external commands) or modify existing aliases to execute malicious commands when run. The vulnerability was patched in version 2.2.1 of Git, released on 17 December 2014, and announced on the next day.[80][81]\n\nGit version 2.6.1, released on 29 September 2015, contained a patch for a security vulnerability (CVE-2015-7545)[82] which allowed arbitrary code execution.[83] The vulnerability was exploitable if an attacker could convince a victim to clone a specific URL, as the arbitrary commands were embedded in the URL itself.[84] An attacker could use the exploit via a man-in-the-middle attack if the connection was unencrypted,[84] as they could redirect the user to a URL of their choice. Recursive clones were also vulnerable, since they allowed the controller of a repository to specify arbitrary URLs via the gitmodules file.[84]\n\nGit uses SHA-1 hashes internally, even though SHA-1 is looking cryptographically weak compared to newer cryptographic hash functions. Linus has responded that the hash was mostly to guard against accidental corruption, and the security a cryptographically secure hash gives was just an accidental side effect, with the main security being signing elsewhere.[85][86] Though Linus has also referenced an attack on kernel.org where a malicious attacker modified data in a bitkeeper version control system, something which may not be prevented without a secure hash.[87]", "skillName": "Git."}
{"id": 115, "category": "Software", "skillText": "SPSS Statistics is a software package used for statistical analysis. Long produced by SPSS Inc., it was acquired by IBM in 2009. The current versions (2015) are officially named IBM SPSS Statistics. Companion products in the same family are used for survey authoring and deployment (IBM SPSS Data Collection), data mining (IBM SPSS Modeler), text analytics, and collaboration and deployment (batch and automated scoring services).\n\nThe software name originally stood for Statistical Package for the Social Sciences (SPSS),[2] reflecting the original market, although the software is now popular in other fields as well, including the health sciences and marketing.\n\nContents\n\n    1 Overview\n    2 Versions and ownership history\n    3 See also\n    4 Notes\n    5 References\n    6 External links\n\nOverview\n\nSPSS is a widely used program for statistical analysis in social science. It is also used by market researchers, health researchers, survey companies, government, education researchers, marketing organizations, data miners,[3] and others. The original SPSS manual (Nie, Bent & Hull, 1970) has been described as one of \"sociology's most influential books\" for allowing ordinary researchers to do their own statistical analysis.[4] In addition to statistical analysis, data management (case selection, file reshaping, creating derived data) and data documentation (a metadata dictionary was stored in the datafile) are features of the base software.\n\nStatistics included in the base software:\n\n    Descriptive statistics: Cross tabulation, Frequencies, Descriptives, Explore, Descriptive Ratio Statistics\n    Bivariate statistics: Means, t-test, ANOVA, Correlation (bivariate, partial, distances), Nonparametric tests\n    Prediction for numerical outcomes: Linear regression\n    Prediction for identifying groups: Factor analysis, cluster analysis (two-step, K-means, hierarchical), Discriminant\n\nThe many features of SPSS Statistics are accessible via pull-down menus or can be programmed with a proprietary 4GL command syntax language. Command syntax programming has the benefits of reproducibility, simplifying repetitive tasks, and handling complex data manipulations and analyses. Additionally, some complex applications can only be programmed in syntax and are not accessible through the menu structure. The pull-down menu interface also generates command syntax: this can be displayed in the output, although the default settings have to be changed to make the syntax visible to the user. They can also be pasted into a syntax file using the \"paste\" button present in each menu. Programs can be run interactively or unattended, using the supplied Production Job Facility.\n\nAdditionally a \"macro\" language can be used to write command language subroutines. A Python programmability extension can access the information in the data dictionary and data and dynamically build command syntax programs. The Python programmability extension, introduced in SPSS 14, replaced the less functional SAX Basic \"scripts\" for most purposes, although SaxBasic remains available. In addition, the Python extension allows SPSS to run any of the statistics in the free software package R. From version 14 onwards, SPSS can be driven externally by a Python or a VB.NET program using supplied \"plug-ins\". (From Version 20 onwards, these two scripting facilities, as well as many scripts, are included on the installation media and are normally installed by default.)\n\nSPSS Statistics places constraints on internal file structure, data types, data processing, and matching files, which together considerably simplify programming. SPSS datasets have a two-dimensional table structure, where the rows typically represent cases (such as individuals or households) and the columns represent measurements (such as age, sex, or household income). Only two data types are defined: numeric and text (or \"string\"). All data processing occurs sequentially case-by-case through the file. Files can be matched one-to-one and one-to-many, but not many-to-many.\n\nThe graphical user interface has two views which can be toggled by clicking on one of the two tabs in the bottom left of the SPSS Statistics window. The 'Data View' shows a spreadsheet view of the cases (rows) and variables (columns). Unlike spreadsheets, the data cells can only contain numbers or text, and formulas cannot be stored in these cells. The 'Variable View' displays the metadata dictionary where each row represents a variable and shows the variable name, variable label, value label(s), print width, measurement type, and a variety of other characteristics. Cells in both views can be manually edited, defining the file structure and allowing data entry without using command syntax. This may be sufficient for small datasets. Larger datasets such as statistical surveys are more often created in data entry software, or entered during computer-assisted personal interviewing, by scanning and using optical character recognition and optical mark recognition software, or by direct capture from online questionnaires. These datasets are then read into SPSS.\n\nSPSS Statistics can read and write data from ASCII text files (including hierarchical files), other statistics packages, spreadsheets and databases. SPSS Statistics can read and write to external relational database tables via ODBC and SQL.\n\nStatistical output is to a proprietary file format (*.spv file, supporting pivot tables) for which, in addition to the in-package viewer, a stand-alone reader can be downloaded. The proprietary output can be exported to text or Microsoft Word, PDF, Excel, and other formats. Alternatively, output can be captured as data (using the OMS command), as text, tab-delimited text, PDF, XLS, HTML, XML, SPSS dataset or a variety of graphic image formats (JPEG, PNG, BMP and EMF).\nThe SPSS logo used prior to the renaming in January 2010.\n\nSPSS Statistics Server is a version of SPSS Statistics with a client/server architecture. It had some features not available in the desktop version, such as scoring functions. (Scoring functions are included in the desktop version from version 19.)\nVersions and ownership history\n\nThe software was released in its first version in 1968 as the Statistical Package for the Social Sciences (SPSS) after being developed by Norman H. Nie, Dale H. Bent, and C. Hadlai Hull. Those principals incorporated as SPSS Inc. in 1975. Early versions of SPSS Statistics were designed for batch processing on mainframes, including for example IBM and ICL versions, originally using punched cards for input. A processing run read a command file of SPSS commands and either a raw input file of fixed format data with a single record type, or a 'getfile' of data saved by a previous run. To save precious computer time an 'edit' run could be done to check command syntax without analysing the data. From version 10 (SPSS-X) in 1983, data files could contain multiple record types.\n\nSPSS Statistics versions 16.0 and later run under Windows, Mac, and Linux. The graphical user interface is written in Java. The Mac OS version is provided as a Universal binary, making it fully compatible with both PowerPC and Intel-based Mac hardware.\n\nPrior to SPSS 16.0, different versions of SPSS were available for Windows, Mac OS X and Unix. The Windows version was updated more frequently and had more features than the versions for other operating systems.[citation needed]\n\nSPSS Statistics version 13.0 for Mac OS X was not compatible with Intel-based Macintosh computers, due to the Rosetta emulation software causing errors in calculations. SPSS Statistics 15.0 for Windows needed a downloadable hotfix to be installed in order to be compatible with Windows Vista.\n\nSPSS Inc announced on July 28, 2009 that it was being acquired by IBM for US$1.2 billion.[5] Because of a dispute about ownership of the name \"SPSS\", between 2009 and 2010, the product was referred to as PASW (Predictive Analytics SoftWare).[6] As of January 2010, it became \"SPSS: An IBM Company\". Complete transfer of business to IBM was done by October 1, 2010. By that date, SPSS: An IBM Company ceased to exist. IBM SPSS is now fully integrated into the IBM Corporation, and is one of the brands under IBM Software Group's Business Analytics Portfolio, together with IBM Algorithmics, IBM Cognos and IBM OpenPages.", "skillName": "SPSS."}
{"id": 116, "category": "Software", "skillText": "Waikato Environment for Knowledge Analysis (Weka) is a popular suite of machine learning software written in Java, developed at the University of Waikato, New Zealand. It is free software licensed under the GNU General Public License.\n\nContents\n\n    1 Description\n    2 User interfaces\n    3 Extension packages\n    4 History\n    5 Related tools\n    6 See also\n    7 References\n    8 External links\n\nDescription\n\nWeka (pronounced to rhyme with Mecca) is a workbench[1] that contains a collection of visualization tools and algorithms for data analysis and predictive modeling, together with graphical user interfaces for easy access to these functions. The original non-Java version of Weka was a Tcl/Tk front-end to (mostly third-party) modeling algorithms implemented in other programming languages, plus data preprocessing utilities in C, and a Makefile-based system for running machine learning experiments. This original version was primarily designed as a tool for analyzing data from agricultural domains,[2][3] but the more recent fully Java-based version (Weka 3), for which development started in 1997, is now used in many different application areas, in particular for educational purposes and research. Advantages of Weka include:\n\n    Free availability under the GNU General Public License.\n    Portability, since it is fully implemented in the Java programming language and thus runs on almost any modern computing platform.\n    A comprehensive collection of data preprocessing and modeling techniques.\n    Ease of use due to its graphical user interfaces.\n\nWeka supports several standard data mining tasks, more specifically, data preprocessing, clustering, classification, regression, visualization, and feature selection. All of Weka's techniques are predicated on the assumption that the data is available as one flat file or relation, where each data point is described by a fixed number of attributes (normally, numeric or nominal attributes, but some other attribute types are also supported). Weka provides access to SQL databases using Java Database Connectivity and can process the result returned by a database query. It is not capable of multi-relational data mining, but there is separate software for converting a collection of linked database tables into a single table that is suitable for processing using Weka.[4] Another important area that is currently not covered by the algorithms included in the Weka distribution is sequence modeling..\nUser interfaces\n\nWeka's main user interface is the Explorer, but essentially the same functionality can be accessed through the component-based Knowledge Flow interface and from the command line. There is also the Experimenter, which allows the systematic comparison of the predictive performance of Weka's machine learning algorithms on a collection of datasets.\n\nThe Explorer interface features several panels providing access to the main components of the workbench:\n\n    The Preprocess panel has facilities for importing data from a database, a comma-separated values (CSV) file, etc., and for preprocessing this data using a so-called filtering algorithm. These filters can be used to transform the data (e.g., turning numeric attributes into discrete ones) and make it possible to delete instances and attributes according to specific criteria.\n    The Classify panel enables applying classification and regression algorithms (indiscriminately called classifiers in Weka) to the resulting dataset, to estimate the accuracy of the resulting predictive model, and to visualize erroneous predictions, receiver operating characteristic (ROC) curves, etc., or the model itself (if the model is amenable to visualization like, e.g., a decision tree).\n    The Associate panel provides access to association rule learners that attempt to identify all important interrelationships between attributes in the data.\n    The Cluster panel gives access to the clustering techniques in Weka, e.g., the simple k-means algorithm. There is also an implementation of the expectation maximization algorithm for learning a mixture of normal distributions.\n    The Select attributes panel provides algorithms for identifying the most predictive attributes in a dataset.\n    The Visualize panel shows a scatter plot matrix, where individual scatter plots can be selected and enlarged, and analyzed further using various selection operators.\n\nExtension packages\n\nIn version 3.7.2 (thus not available in the stable \"book\" version of Weka), a package manager was added to allow the easier installation of extension packages.[5] Some functionality that used to be included with Weka prior to this version has since been moved into such extension packages, but this change also makes it easier for other to contribute extensions to Weka and to maintain the software, as this modular architecture allows independent updates of the Weka core and individual extensions.\nHistory\n\n    In 1993, the University of Waikato in New Zealand began development of the original version of Weka, which became a mix of Tcl/Tk, C, and Makefiles.\n    In 1997, the decision was made to redevelop Weka from scratch in Java, including implementations of modeling algorithms.[6]\n    In 2005, Weka received the SIGKDD Data Mining and Knowledge Discovery Service Award.[7][8]\n    In 2006, Pentaho Corporation acquired an exclusive licence to use Weka for business intelligence.[citation needed] It forms the data mining and predictive analytics component of the Pentaho business intelligence suite.\n    All-time ranking[9] on Sourceforge.net as of 2011-08-26, 243 (with 2,487,213 downloads)\n\nRelated tools\n\n    Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI) is a similar project to Weka with a focus on cluster analysis, i.e., unsupervised methods.\n    KNIME is a machine learning and data mining software implemented in Java.\n    Massive Online Analysis (MOA) is an open-source project for large scale mining of data streams, also developed at the University of Waikato in New Zealand.\n    Neural Designer is a data mining software based on deep learning techniques written in C++.\n    Orange is a similar open-source project for data mining, machine learning and visualization written in Python and C++.\n    RapidMiner is a commercial machine learning framework implemented in Java which integrates Weka.", "skillName": "Weka."}
{"id": 117, "category": "Software", "skillText": "Oracle Data Mining (ODM) is an option of Oracle Corporation's Relational Database Management System (RDBMS) Enterprise Edition (EE). It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\nOracle Data Mining Developer(s) \tOracle Corporation\nStable release \t11gR2 / September, 2009\nType \tdata mining and analytics\nLicense \tproprietary\nWebsite \tOracle Data Mining \n\nContents\n\n    1 Overview\n    2 History\n    3 Functionality\n    4 Input sources and data preparation\n    5 Graphical user interface: Oracle Data Miner\n    6 PL/SQL and Java interfaces\n    7 SQL scoring functions\n    8 PMML\n    9 Predictive Analytics MS Excel Add-In\n    10 References and further reading\n    11 See also\n    12 References\n    13 External links\n\nOverview\n\nOracle implements a variety of data mining algorithms inside the Oracle relational database. These implementations are integrated right into the Oracle database kernel, and operate natively on data stored in the relational database tables. This eliminates the need for extraction or transfer of data into standalone mining/analytic servers. The relational database platform is leveraged to securely manage models and efficiently execute SQL queries on large volumes of data. The system is organized around a few generic operations providing a general unified interface for data mining functions. These operations include functions to create, apply, test, and manipulate data mining models. Models are created and stored as database objects, and their management is done within the database - similar to tables, views, indexes and other database objects.\n\nIn data mining, the process of using a model to derive predictions or descriptions of behavior that is yet to occur is called \"scoring\". In traditional analytic workbenches, a model built in the analytic engine has to be deployed in a mission-critical system to score new data, or the data is moved from relational tables into the analytical workbench - most workbenches offer proprietary scoring interfaces. ODM simplifies model deployment by offering Oracle SQL functions to score data stored right in the database. This way, the user/application developer can leverage the full power of Oracle SQL - in terms of the ability to pipeline and manipulate the results over several levels, and in terms of parallelizing and partitioning data access for performance.\n\nModels can be created and managed by one of several means. (Oracle Data Miner) is a graphical user interface that steps the user through the process of creating, testing, and applying models (e.g. along the lines of the CRISP-DM methodology). Application and tools developers can embed predictive and descriptive mining capabilities using PL/SQL or Java APIs. Business analysts can quickly experiment with, or demonstrate the power of, predictive analytics using Oracle Spreadsheet Add-In for Predictive Analytics, a dedicated Microsoft Excel adaptor interface. ODM offers a choice of well known machine learning approaches such as Decision Trees, Naive Bayes, Support vector machines, Generalized linear model (GLM) for predictive mining, Association rules, K-means and Orthogonal Partitioning[1][2] Clustering, and Non-negative matrix factorization for descriptive mining. A minimum description length based technique to grade the relative importance of an input mining attributes for a given problem is also provided. Most Oracle Data Mining functions also allow text mining by accepting Text (unstructured data) attributes as input. Users do not need to configure text mining options, this is handled behind the scenes by the Database_options database option.\nHistory\n\nOracle Data Mining was first introduced in 2002 and its releases are named according to the corresponding Oracle database release:\n\n    Oracle Data Mining 9iR2 (9.2.0.1.0 - May 2002)\n    Oracle Data Mining 10gR1 (10.1.0.2.0 - February 2004)\n    Oracle Data Mining 10gR2 (10.2.0.1.0 - July 2005)\n    Oracle Data Mining 11gR1 (11.1 - September 2007)\n    Oracle Data Mining 11gR2 (11.2 - September 2009)\n\nOracle Data Mining is a logical successor of the Darwin data mining toolset developed by Thinking Machines Corporation in the mid-1990s and later distributed by Oracle after its acquisition of Thinking Machines in 1999. However, the product itself is a complete redesign and rewrite from ground-up - while Darwin was a classic GUI-based analytical workbench, ODM offers a data mining development/deployment platform integrated into the Oracle database, along with the Oracle Data Miner GUI.\n\nThe Oracle Data Miner 11gR2 New Workflow GUI was previewed at Oracle Open World 2009. An updated Oracle Data Miner GUI was released in 2012. It is free, and is available as an extension to Oracle SQL Developer 3.1 .\nFunctionality\n\nAs of release 11gR1 Oracle Data Mining contains the following data mining functions:\n\n    Data transformation and model analysis:\n        Data sampling, binning, discretization, and other data transformations.\n        Model exploration, evaluation and analysis.\n    Feature selection (Attribute Importance).\n        Minimum description length (MDL).\n    Classification.\n        Naive Bayes (NB).\n        Generalized linear model (GLM) for Logistic regression.\n        Support Vector Machine (SVM).\n        Decision Trees (DT).\n    Anomaly detection.\n        One-class Support Vector Machine (SVM).\n    Regression\n        Support Vector Machine (SVM).\n        Generalized linear model (GLM) for Multiple regression\n    Clustering:\n        Enhanced k-means (EKM).\n        Orthogonal Partitioning Clustering (O-Cluster).[1][2]\n    Association rule learning:\n        Itemsets and association rules (AM).\n    Feature extraction.\n        Non-negative matrix factorization (NMF).\n    Text and spatial mining:\n        Combined text and non-text columns of input data.\n        Spatial/GIS data.\n\nInput sources and data preparation\n\nMost Oracle Data Mining functions accept as input one relational table or view. Flat data can be combined with transactional data through the use of nested columns, enabling mining of data involving one-to-many relationships (e.g. a star schema). The full functionality of SQL can be used when preparing data for data mining, including dates and spatial data.\n\nOracle Data Mining distinguishes numerical, categorical, and unstructured (text) attributes. The product also provides utilities for data preparation steps prior to model building such as outlier treatment, discretization, normalization and binning (sorting in general speak)\nGraphical user interface: Oracle Data Miner\n\nUsers can access Oracle Data Mining through Oracle Data Miner, a GUI client application that provides access to the data mining functions and structured templates (called Mining Activities) that automatically prescribe the order of operations, perform required data transformations, and set model parameters. The user interface also allows the automated generation of Java and/or SQL code associated with the data-mining activities. The Java Code Generator is an extension to Oracle JDeveloper. An independent interface also exists: the Spreadsheet Add-In for Predictive Analytics which enables access to the Oracle Data Mining Predictive Analytics PL/SQL package from Microsoft Excel.\n\nFrom version 11.2 of the Oracle database, Oracle Data Miner integrates with Oracle SQL Developer.[3]\nPL/SQL and Java interfaces\n\nOracle Data Mining provides a native PL/SQL package (DBMS_DATA_MINING) to create, destroy, describe, apply, test, export and import models. The code below illustrates a typical call to build a classification model:\n\nBEGIN\n  DBMS_DATA_MINING.CREATE_MODEL (\n    model_name          => 'credit_risk_model', \n    function            => DBMS_DATA_MINING.classification, \n    data_table_name     => 'credit_card_data', \n    case_id_column_name => 'customer_id', \n    target_column_name  => 'credit_risk',\n    settings_table_name => 'credit_risk_model_settings');\nEND;\n\nwhere 'credit_risk_model' is the model name, built for the express purpose of classifying future customers' 'credit_risk', based on training data provided in the table 'credit_card_data', each case distinguished by a unique 'customer_id', with the rest of the model parameters specified through the table 'credit_risk_model_settings'.\n\nOracle Data Mining also supports a Java API consistent with the Java Data Mining (JDM) standard for data mining (JSR-73) for enabling integration with web and Java EE applications and to facilitate portability across platforms.\nSQL scoring functions\n\nAs of release 10gR2, Oracle Data Mining contains built-in SQL functions for scoring data mining models. These single-row functions support classification, regression, anomaly detection, clustering, and feature extraction. The code below illustrates a typical usage of a classification model:\n\nSELECT customer_name\n  FROM credit_card_data\n WHERE PREDICTION (credit_risk_model USING *) = 'LOW' AND customer_value = 'HIGH';\n\nPMML\n\nIn Release 11gR2 (11.2.0.2), ODM supports the import of externally created PMML for some of the data mining models. PMML is an XML-based standard for representing data mining models.\nPredictive Analytics MS Excel Add-In\n\nThe PL/SQL package DBMS_PREDICTIVE_ANALYTICS automates the data mining process including data preprocessing, model building and evaluation, and scoring of new data. The PREDICT operation is used for predicting target values classification or regression while EXPLAIN ranks attributes in order of influence in explaining a target column feature selection. The new 11g feature PROFILE finds customer segments and their profiles, given a target attribute. These operations can be used as part of an operational pipeline providing actionable results or displayed for interpretation by end users.\nReferences and further reading\n\n    T. H. Davenport, Competing on Analytics \n    , Harvard Business Review, January 2006.\n    I. Ben-Gal,Outlier detection \n    , In: Maimon O. and Rockach L. (Eds.) Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers,\" Kluwer Academic Publishers, 2005, ISBN 0-387-24435-2.\n    M. M. Campos, P. J. Stengard, and B. L. Milenova, Data-centric Automated Data Mining. In proceedings of the Fourth International Conference on Machine Learning and Applications 2005, 15–17 December 2005. pp8, ISBN 0-7695-2495-8\n    M. F. Hornick, Erik Marcade, and Sunil Venkayala. Java Data Mining: Strategy, Standard, and Practice. Morgan-Kaufmann, 2006, ISBN 0-12-370452-9.\n    B. L. Milenova, J. S. Yarmus, and M. M. Campos. SVM in Oracle database 10g: removing the barriers to widespread adoption of support vector machines. In Proceedings of the 31st international Conference on Very Large Data Bases (Trondheim, Norway, August 30 - September 2, 2005). pp1152–1163, ISBN 1-59593-154-6.\n    B. L. Milenova and M. M. Campos. O-Cluster: scalable clustering of large high dimensional data sets. In proceedings of the 2002 IEEE International Conference on Data Mining: ICDM 2002. pp290–297, ISBN 0-7695-1754-4.\n    P. Tamayo, C. Berger, M. M. Campos, J. S. Yarmus, B. L.Milenova, A. Mozes, M. Taft, M. Hornick, R. Krishnan, S.Thomas, M. Kelly, D. Mukhin, R. Haberstroh, S. Stephens and J. Myczkowski. Oracle Data Mining - Data Mining in the Database Environment. In Part VII of Data Mining and Knowledge Discovery Handbook, Maimon, O.; Rokach, L. (Eds.) 2005, p315-1329, ISBN 0-387-24435-2.\n    Brendan Tierney, Predictive Analytics using Oracle Data Miner: for the data scientist, oracle analyst, oracle developer & DBA, Oracle Press, McGraw Hill, Spring 2014.", "skillName": "Oracle Data Mining."}
{"id": 118, "category": "Software", "skillText": "GNU Octave is software featuring a high-level programming language, primarily intended for numerical computations. Octave helps in solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with MATLAB. It may also be used as a batch-oriented language. Since it is part of the GNU Project, it is free software under the terms of the GNU General Public License.\n\nOctave is one of the major free alternatives to MATLAB, another being Scilab.[4][5][6][7] Scilab however puts less emphasis on (bidirectional) syntactic compatibility with MATLAB than Octave does.[4][8][9]\n\nContents\n\n    1 History\n    2 Developments\n    3 Technical details\n    4 Octave, the language\n    5 Notable features\n        5.1 Command and variable name completion\n        5.2 Command history\n        5.3 Data structures\n        5.4 Short-circuit boolean operators\n        5.5 Increment and decrement operators\n        5.6 Unwind-protect\n        5.7 Variable-length argument lists\n        5.8 Variable-length return lists\n        5.9 C++ integration\n    6 MATLAB compatibility\n        6.1 Syntax compatibility\n        6.2 Function compatibility\n    7 User interfaces\n    8 See also\n    9 References\n    10 Further reading\n    11 External links\n        11.1 Documentation\n        11.2 Numerical packages and libraries interfacing with GNU Octave\n            11.2.1 Numerical tools\n            11.2.2 Plotting tools\n                11.2.2.1 MATLAB-like IDEs\n                11.2.2.2 Other GUIs\n                11.2.2.3 Web-based user interfaces (WUI)\n\nHistory\n\nThe project was conceived around 1988. At first it was intended to be a companion to a chemical reactor design course. Real development was started by John W. Eaton in 1992. The first alpha release dates back to January 4, 1993 and on February 17, 1994 version 1.0 was released. Version 4.0.0 was released on May 29, 2015.\n\nThe program is named after Octave Levenspiel, a former professor of the principal author. Levenspiel is known for his ability to perform quick back-of-the-envelope calculations.[10]\nDevelopments\n\nIn addition to use on desktops for personal scientific computing, Octave is used in academia and industry. For example, Octave was used on a massive parallel computer at Pittsburgh supercomputing center to find vulnerabilities related to guessing social security numbers.[11]\nTechnical details\n\n    Octave is written in C++ using the C++ standard library.\n    Octave uses an interpreter to execute the Octave scripting language.\n    Octave is extensible using dynamically loadable modules.\n    Octave interpreter has an OpenGL-based graphics engine to create plots, graphs and charts and to save or print them. Alternatively, gnuplot can be used for the same purpose.\n    Octave versions 3.8.0 and later include a Graphical User Interface (GUI) in addition to the traditional Command Line Interface (CLI).\n\nOctave, the language\n\nThe Octave language is an interpreted programming language. It is a structured programming language (similar to C) and supports many common C standard library functions, and also certain UNIX system calls and functions.[12] However, it does not support passing arguments by reference.[13]\n\nOctave programs consist of a list of function calls or a script. The syntax is matrix-based and provides various functions for matrix operations. It supports various data structures and allows object-oriented programming.[14]\n\nIts syntax is very similar to MATLAB, and careful programming of a script will allow it to run on both Octave and MATLAB.[15]\n\nBecause Octave is made available under the GNU General Public License, it may be freely changed, copied and used.[10] The program runs on Microsoft Windows and most Unix and Unix-like operating systems, including OS X.[16]\nNotable features\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2013) (Learn how and when to remove this template message)\nCommand and variable name completion\n\nTyping a TAB character on the command line causes Octave to attempt to complete variable, function, and file names (similar to Bash's tab completion). Octave uses the text before the cursor as the initial portion of the name to complete. [17]\nCommand history\n\nWhen running interactively, Octave saves the commands typed in an internal buffer so that they can be recalled and edited.\nData structures\n\nOctave includes a limited amount of support for organizing data in structures. In this example, we see a structure \"x\" with elements \"a\", \"b\", and \"c\", (an integer, an array, and a string, respectively):\n\noctave:1> x.a = 1; x.b = [1, 2; 3, 4]; x.c = \"string\";\noctave:2> x.a\nans =  1\noctave:3> x.b\nans =\n\n   1   2\n   3   4\n\noctave:4> x.c\nans = string\noctave:5> x\nx =\n{\n  a =  1\n  b =\n\n     1   2\n     3   4\n\n  c = string\n}\n\nShort-circuit boolean operators\n\nOctave's '&&' and '||' logical operators are evaluated in a short-circuit fashion (like the corresponding operators in the C language), in contrast to the element-by-element operators '&' and '|'.\nIncrement and decrement operators\nMain article: Increment and decrement operators\n\nOctave includes the C-like increment and decrement operators '++' and '--' in both their prefix and postfix forms. Octave also does augmented assignment, e.g. 'x += 5'.\nUnwind-protect\n\nOctave supports a limited form of exception handling modelled after the 'unwind_protect' \nof Lisp. The general form of an unwind_protect block looks like this:\n\nunwind_protect\n   body\nunwind_protect_cleanup\n   cleanup\nend_unwind_protect\n\nAs a general rule, GNU Octave recognizes as termination of a given 'block' either the keyword 'end' (which is compatible with the MATLAB language) or a more specific keyword 'end_block'. As a consequence, an 'unwind_protect' block can be terminated either with the keyword 'end_unwind_protect' as in the example, or with the more portable keyword 'end'.\n\nThe cleanup part of the block is always executed. In case an exception is raised by the body part, cleanup is executed immediately before propagating the exception outside the block 'unwind_protect'.\n\nGNU Octave also supports another form of exception handling (compatible with the MATLAB language):\n\ntry\n   body\ncatch\n   exception_handling\nend\n\nThis latter form differs from an 'unwind_protect' block in two ways. First, exception_handling is only executed when an exception is raised by body. Second, after the execution of exception_handling the exception is not propagated outside the block (unless a 'rethrow( lasterror )' statement is purposely inserted within the exception_handling code).\nVariable-length argument lists\n\nOctave has a mechanism for handling functions that take an unspecified number of arguments without explicit upper limit. To specify a list of zero or more arguments, use the special argument varargin as the last (or only) argument in the list.\n\nfunction s = plus (varargin)\n   if (nargin==0)\n      s = 0;\n   else\n      s = varargin{1} + plus (varargin{2:nargin});\n   end\nend\n\nVariable-length return lists\n\nA function can be set up to return any number of values by using the special return value varargout. For example:\n\nfunction varargout = multiassign (data)\n   for k=1:nargout\n      varargout{k} = data(:,k);\n   end\nend\n\nC++ integration\n\nIt is also possible to execute Octave code directly in a C++ program. For example, here is a code snippet for calling rand([10,1]):\n\n#include <octave/oct.h>\n...\nColumnVector NumRands(2);\nNumRands(0) = 10;\nNumRands(1) = 1;\noctave_value_list f_arg, f_ret;\nf_arg(0) = octave_value(NumRands);\nf_ret = feval(\"rand\", f_arg, 1);\nMatrix unis(f_ret(0).matrix_value());\n\nC and C++ code can be integrated into GNU Octave by creating oct files, or using the Matlab compatible MEX files.\nMATLAB compatibility\n\nOctave has been built with MATLAB compatibility in mind, and shares many features with MATLAB:\n\n    Matrices as fundamental data type.\n    Built-in support for complex numbers.\n    Powerful built-in math functions and extensive function libraries.\n    Extensibility in the form of user-defined functions.\n\nIn fact, Octave treats incompatibility with MATLAB as a bug;[18] therefore, it can be considered a software clone, which doesn't infringe software copyright as per Lotus v. Borland court case.\nSyntax compatibility\n\nThere are a few purposeful, albeit minor, syntax additions \n:\n\n    Comment lines can be prefixed with the # character as well as the % character;\n    Various C-based operators ++, --, +=, *=, /= are supported;\n    Elements can be referenced without creating a new variable by cascaded indexing, e.g. [1:10](3);\n    Strings can be defined with the \" character as well as the ' character;\n    When the variable type is single, Octave calculates the \"mean\" in the single-domain (Matlab in double-domain) which is faster but gives less accurate results;\n    Blocks can also be terminated with more specific Control structure keywords, i.e., endif, endfor, endwhile, etc.;\n    Functions can be defined within scripts and at the Octave prompt;\n    All operators perform automatic broadcasting or singleton expansion.\n    Presence of a do-until loop (similar to do-while in C).\n\nFunction compatibility\n\nMany of the numerous MATLAB functions are available in GNU Octave, some of them are accessible through packages via Octave-forge, but not all of MATLAB functions are available in GNU Octave. List of unavailable functions exists in Octave, and developers are seeking for help to implement them. Looking for function __unimplemented.m__, leads to the list of unimplemented functions \n.\n\nUnimplemented functions are also categorized in Image \n, Mapping \n, Optimization \n, Signal \n, and Statistics \npackages.\n\nWhen an unimplemented function is called the following error message is shown:\n\n  octave:1> quad2d\n  warning: quad2d is not implemented. Consider using dblquad.\n\n  Please read <http://www.octave.org/missing.html> to learn how you can\n  contribute missing functionality.\n  warning: called from\n      __unimplemented__ at line 523 column 5\n  error: 'quad2d' undefined near line 1 column 1\n\nUser interfaces\nUntil version 3.8, Octave did not come with a graphical user interface (GUI)/integrated development environment (IDE) by default. However, an official graphical interface based on Qt has now been migrated to the main source repository and is available with Octave 3.8, but not as the default interface.[19] It has become the default interface with the release of Octave 4.0.[20] Several 3rd-party graphical front-ends have been developed.", "skillName": "Octave."}
{"id": 119, "category": "Software", "skillText": "MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and fourth-generation programming language. A proprietary programming language developed by MathWorks, MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, Java, Fortran and Python.\n\nAlthough MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.\n\nIn 2004, MATLAB had around one million users across industry and academia.[3] MATLAB users come from various backgrounds of engineering, science, and economics.\n\nContents\n\n    1 History\n    2 Syntax\n        2.1 Variables\n        2.2 Vectors and matrices\n        2.3 Structures\n        2.4 Functions\n        2.5 Function handles\n        2.6 Classes and object-oriented programming\n    3 Graphics and graphical user interface programming\n    4 Interfacing with other languages\n    5 License\n    6 Alternatives\n    7 Release history\n    8 File extensions\n        8.1 MATLAB\n        8.2 Simulink\n        8.3 Simscape\n        8.4 MuPAD\n        8.5 Third-party\n    9 Easter eggs\n    10 See also\n    11 Notes\n    12 References\n    13 External links\n\nHistory\n\nCleve Moler, the chairman of the computer science department at the University of New Mexico, started developing MATLAB in the late 1970s.[4] He designed it to give his students access to LINPACK and EISPACK without them having to learn Fortran. It soon spread to other universities and found a strong audience within the applied mathematics community. Jack Little, an engineer, was exposed to it during a visit Moler made to Stanford University in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in C and founded MathWorks in 1984 to continue its development. These rewritten libraries were known as JACKPAC.[5] In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, LAPACK.[6]\n\nMATLAB was first adopted by researchers and practitioners in control engineering, Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of linear algebra, numerical analysis, and is popular amongst scientists involved in image processing.[4]\nSyntax\n\nThe MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical shell or executing text files containing MATLAB code.[7]\nVariables\n\nVariables are defined using the assignment operator, =. MATLAB is a weakly typed programming language because types are implicitly converted.[8] It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects,[9] and that their type can change. Values can come from constants, from computation involving values of other variables, or from the output of a function. For example:\n\n>> x = 17\nx =\n 17\n\n>> x = 'hat'\nx =\nhat\n\n>> y = x + 0\ny =\n       104        97       116\n\n>> x = [3*4, pi/2]\nx =\n   12.0000    1.5708\n\n>> y = 3*sin(x)\ny =\n   -1.6097    3.0000\n\nVectors and matrices\n\nA simple array is defined using the colon syntax: init:increment:terminator. For instance:\n\n>> array = 1:2:9\narray =\n 1 3 5 7 9\n\ndefines a variable named array (or assigns a new value to an existing variable with the name array) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the init value), increments with each step from the previous value by 2 (the increment value), and stops once it reaches (or to avoid exceeding) 9 (the terminator value).\n\n>> array = 1:3:9\narray =\n 1 4 7\n\nthe increment value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.\n\n>> ari = 1:5\nari =\n 1 2 3 4 5\n\nassigns to the variable named ari an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.\n\nIndexing is one-based,[10] which is the usual convention for matrices in mathematics, although not for some programming languages such as C, C++, and Java.\n\nMatrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).\n\n>> A = [16 3 2 13; 5 10 11 8; 9 6 7 12; 4 15 14 1]\nA =\n 16  3  2 13\n  5 10 11  8\n  9  6  7 12\n  4 15 14  1\n\n>> A(2,3)\nans =\n 11\n\nSets of indices can be specified by expressions such as \"2:4\", which evaluates to [2, 3, 4]. For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:\n\n>> A(2:4,3:4)\nans =\n 11 8\n 7 12\n 14 1\n\nA square identity matrix of size n can be generated using the function eye, and matrices of any size with zeros or ones can be generated with the functions zeros and ones, respectively.\n\n>> eye(3,3)\nans =\n 1 0 0\n 0 1 0\n 0 0 1\n\n>> zeros(2,3)\nans =\n 0 0 0\n 0 0 0\n\n>> ones(2,3)\nans =\n 1 1 1\n 1 1 1\n\nMost MATLAB functions can accept matrices and will apply themselves to each element. For example, mod(2*J,n) will multiply every element in \"J\" by 2, and then reduce each element modulo \"n\". MATLAB does include standard \"for\" and \"while\" loops, but (as in other similar applications such as R), using the vectorized notation often produces code that is faster to execute. This code, excerpted from the function magic.m, creates a magic square M for odd values of n (MATLAB function meshgrid is used here to generate square matrices I and J containing 1:n).\n\n[J,I] = meshgrid(1:n);\nA = mod(I + J - (n + 3) / 2, n);\nB = mod(I + 2 * J - 2, n);\nM = n * A + B + 1;\n\nStructures\n\nMATLAB has structure data types.[11] Since all variables in MATLAB are arrays, a more adequate name is \"structure array\", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names[12] (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.[13]\nFunctions\n\nWhen creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores. Functions are also often case sensitive.\nFunction handles\n\nMATLAB supports elements of lambda calculus by introducing function handles,[14] or function references, which are implemented either in .m files or anonymous[15]/nested functions.[16]\nClasses and object-oriented programming\n\nMATLAB's support for object-oriented programming includes classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics.[17] However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has handle as a super-class (for reference classes) or not (for value classes).[18]\n\nMethod call behavior is different between value and reference classes. For example, a call to a method\n\nobject.method();\n\ncan alter any member of object only if object is an instance of a reference class.\n\nAn example of a simple class is provided below.\n\nclassdef hello\n    methods\n        function greet(this)\n            disp('Hello!')\n        end\n    end\nend\n\nWhen put into a file named hello.m, this can be executed with the following commands:\n\n>> x = hello;\n>> x.greet();\nHello!\n\nGraphics and graphical user interface programming\n\nMATLAB supports developing applications with graphical user interface (GUI) features. MATLAB includes GUIDE[19] (GUI development environment) for graphically designing GUIs.[20] It also has tightly integrated graph-plotting features. For example, the function plot can be used to produce a graph from two vectors x and y. The code:\n\nx = 0:pi/100:2*pi;\ny = sin(x);\nplot(x,y)\n\nproduces the following figure of the sine function:\n\nMatlab plot sin.svg\n\nA MATLAB program can produce three-dimensional graphics using the functions surf, plot3 or mesh.\n\n[X,Y] = meshgrid(-10:0.25:10,-10:0.25:10);\nf = sinc(sqrt((X/pi).^2+(Y/pi).^2));\nmesh(X,Y,f);\naxis([-10 10 -10 10 -0.3 1])\nxlabel('{\\bfx}')\nylabel('{\\bfy}')\nzlabel('{\\bfsinc} ({\\bfR})')\nhidden off\n\n\t    \t\n\n[X,Y] = meshgrid(-10:0.25:10,-10:0.25:10);\nf = sinc(sqrt((X/pi).^2+(Y/pi).^2));\nsurf(X,Y,f);\naxis([-10 10 -10 10 -0.3 1])\nxlabel('{\\bfx}')\nylabel('{\\bfy}')\nzlabel('{\\bfsinc} ({\\bfR})')\n\nThis code produces a wireframe 3D plot of the two-dimensional unnormalized sinc function: \t    \tThis code produces a surface 3D plot of the two-dimensional unnormalized sinc function:\nMATLAB mesh sinc3D.svg \t    \tMATLAB surf sinc3D.svg\n\nIn MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.[21]\nInterfacing with other languages\n\nMATLAB can call functions and subroutines written in the programming languages C or Fortran.[22] A wrapper function is created allowing MATLAB data types to be passed and returned. The dynamically loadable object files created by compiling such functions are termed \"MEX-files\" (for MATLAB executable).[23][24] Since 2014 increasing two-way interfacing with Python is being added.[25][26]\n\nLibraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB,[27][28] and many MATLAB libraries (for example XML or SQL support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox[29] which is sold separately by MathWorks, or using an undocumented mechanism called JMI (Java-to-MATLAB Interface),[30][31] (which should not be confused with the unrelated Java Metadata Interface that is also called JMI).\n\nAs alternatives to the MuPAD based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to Maple or Mathematica.[32][33]\n\nLibraries also exist to import and export MathML.[34]\nLicense\n\nMATLAB is a proprietary product of MathWorks, so users are subject to vendor lock-in.[3][35] Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with .NET[36] or Java[37] application building environment, future development will still be tied to the MATLAB language.\n\nEach toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.\n\nIt has been reported that European Union (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor.[38] The regulators dropped the investigation after the complainant withdrew their accusation and no evidence of wrongdoing was found.[39]\nAlternatives\nSee also: list of numerical analysis software and comparison of numerical analysis software\n\nMATLAB has a number of competitors.[40] Commercial competitors include Mathematica, TK Solver, Maple, and IDL. There are also free open source alternatives to MATLAB, in particular GNU Octave, Scilab, FreeMat, Julia, and Sage which are intended to be mostly compatible with the MATLAB language. Among other languages that treat arrays as basic entities (array programming languages) are APL, Fortran 90 and higher, S-Lang, as well as the statistical languages R and S. There are also libraries to add similar functionality to existing languages, such as IT++ for C++, Perl Data Language for Perl, ILNumerics for .NET, NumPy/SciPy/matplotlib for Python, SciLua \n/Torch for Lua, SciRuby \nfor Ruby, and Numeric.js \nfor JavaScript.\n\nGNU Octave is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see MATLAB Compatibility of GNU Octave). Therefore, GNU Octave attempts to provide a software clone of MATLAB.\nRelease history\nVersion[41] \tRelease name \tNumber \tBundled JVM \tYear \tRelease date \tNotes\nMATLAB 1.0 \t\t\t\t1984 \t\t\nMATLAB 2 \t\t\t\t1986 \t\t\nMATLAB 3 \t\t\t\t1987 \t\t\nMATLAB 3.5 \t\t\t\t1990 \t\tRan on DOS but needed at least a 386 processor; version 3.5m needed math coprocessor\nMATLAB 4 \t\t\t\t1992 \t\t\nMATLAB 4.2c \t\t\t\t1994 \t\tRan on Windows 3.1x, needed a math coprocessor\nMATLAB 5.0 \tVolume 8 \t\t\t1996 \tDecember, 1996 \tUnified releases across all platforms\nMATLAB 5.1 \tVolume 9 \t\t\t1997 \tMay, 1997 \t\nMATLAB 5.1.1 \tR9.1 \t\t\t\t\nMATLAB 5.2 \tR10 \t\t\t1998 \tMarch, 1998 \t\nMATLAB 5.2.1 \tR10.1 \t\t\t\t\nMATLAB 5.3 \tR11 \t\t\t1999 \tJanuary, 1999 \t\nMATLAB 5.3.1 \tR11.1 \t\t\tNovember, 1999 \t\nMATLAB 6.0 \tR12 \t12 \t1.1.8 \t2000 \tNovember, 2000 \tFirst release with bundled Java virtual machine (JVM)\nMATLAB 6.1 \tR12.1 \t1.3.0 \t2001 \tJune, 2001 \t\nMATLAB 6.5 \tR13 \t13 \t1.3.1 \t2002 \tJuly, 2002 \t\nMATLAB 6.5.1 \tR13SP1 \t\t2003 \t\t\nMATLAB 6.5.2 \tR13SP2 \t\t\tLast release for IBM/AIX, Alpha/TRU64, and SGI/IRIX[42]\nMATLAB 7 \tR14 \t14 \t1.4.2 \t2004 \tJune, 2004 \tIntroduced anonymous and nested functions[43]\nMATLAB 7.0.1 \tR14SP1 \t\tOctober, 2004 \t\nMATLAB 7.0.4 \tR14SP2 \t1.5.0 \t2005 \tMarch 7, 2005 \tSupport for memory-mapped files[44]\nMATLAB 7.1 \tR14SP3 \t1.5.0 \tSeptember 1, 2005 \t\nMATLAB 7.2 \tR2006a \t15 \t1.5.0 \t2006 \tMarch 1, 2006 \t\nMATLAB 7.3 \tR2006b \t16 \t1.5.0 \tSeptember 1, 2006 \tHDF5-based MAT-file support\nMATLAB 7.4 \tR2007a \t17 \t1.5.0_07 \t2007 \tMarch 1, 2007 \tNew bsxfun function to apply element-by-element binary operation with singleton expansion enabled[45]\nMATLAB 7.5 \tR2007b \t18 \t1.6.0 \tSeptember 1, 2007 \tLast release for Windows 2000 and PowerPC Mac; License Server support for Windows Vista;[46] new internal format for P-code\nMATLAB 7.6 \tR2008a \t19 \t1.6.0 \t2008 \tMarch 1, 2008 \tMajor enhancements to object-oriented programming abilities with a new class definition syntax,[47] and ability to manage namespaces with packages[48]\nMATLAB 7.7 \tR2008b \t20 \t1.6.0_04 \tOctober 9, 2008 \tNew Map data structure:[49] upgrades to random number generators[50]\nMATLAB 7.8 \tR2009a \t21 \t1.6.0_04 \t2009 \tMarch 6, 2009 \tFirst release for Microsoft 32-bit & 64-bit Windows 7, new external interface to .NET Framework[51]\nMATLAB 7.9 \tR2009b \t22 \t1.6.0_12 \tSeptember 4, 2009 \tFirst release for Intel 64-bit Mac, and last for Solaris SPARC; new use for the tilde operator (~) to ignore arguments in function calls[52][53]\nMATLAB 7.9.1 \tR2009bSP1 \t1.6.0_12 \t2010 \tApril 1, 2010 \tbug fixes.\nMATLAB 7.10 \tR2010a \t23 \t1.6.0_12 \tMarch 5, 2010 \tLast release for Intel 32-bit Mac\nMATLAB 7.11 \tR2010b \t24 \t1.6.0_17 \tSeptember 3, 2010 \tAdd support for enumerations[54]\nMATLAB 7.11.1 \tR2010bSP1 \t1.6.0_17 \t2011 \tMarch 17, 2011 \tbug fixes and updates\nMATLAB 7.11.2 \tR2010bSP2 \t1.6.0_17 \tApril 5, 2012[55] \tbug fixes\nMATLAB 7.12 \tR2011a \t25 \t1.6.0_17 \tApril 8, 2011 \tNew rng function to control random number generation[56][57][58]\nMATLAB 7.13 \tR2011b \t26 \t1.6.0_17 \tSeptember 1, 2011 \tAccess-change parts of variables directly in MAT-files, without loading into memory;[59] increased maximum local workers with Parallel Computing Toolbox from 8 to 12[60]\nMATLAB 7.14 \tR2012a \t27 \t1.6.0_17 \t2012 \tMarch 1, 2012 \t\nMATLAB 8 \tR2012b \t28 \t1.6.0_17 \tSeptember 11, 2012 \tFirst release with Toolstrip interface;[61] MATLAB Apps.[62] redesigned documentation system\nMATLAB 8.1 \tR2013a \t29 \t1.6.0_17 \t2013 \tMarch 7, 2013 \tNew unit testing framework[63]\nMATLAB 8.2 \tR2013b \t30 \t1.7.0_11 \tSeptember 6, 2013[64] \tNew table data type[65]\nMATLAB 8.3 \tR2014a \t31 \t1.7.0_11 \t2014 \tMarch 7, 2014[66] \tSimplified compiler setup for building MEX-files; USB Webcams support in core MATLAB; number of local workers no longer limited to 12 with Parallel Computing Toolbox\nMATLAB 8.4 \tR2014b \t32 \t1.7.0_11 \tOctober 3, 2014 \tNew class-based graphics engine (a.k.a. HG2);[67] tabbing function in GUI;[68] improved user toolbox packaging and help files;[69] new objects for time-date manipulations;[70] Git-Subversion integration in IDE;[71] big data abilities with MapReduce (scalable to Hadoop);[72] new py package for using Python from inside MATLAB, new engine interface to call MATLAB from Python;[73][74] several new and improved functions: webread (RESTful web services with JSON/XML support), tcpclient (socket-based connections), histcounts, histogram, animatedline, and others\nMATLAB 8.5 \tR2015a \t33 \t1.7.0_60 \t2015 \tMarch 5, 2015 \t\nMATLAB 8.6 \tR2015b \t34 \t1.7.0_60 \tSeptember 3, 2015 \t\nMATLAB 9.0 \tR2016a \t35 \t1.7.0_60 \t2016 \tMarch 3, 2016 \t\n\nThe number (or release number) is the version reported by Concurrent License Manager program FLEXlm.\n\nFor a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.[75]\nFile extensions\nMATLAB\n\n.fig \n    MATLAB figure\n.m \n    MATLAB code (function, script, or class)\n.mat \n    MATLAB data (binary file for storing variables)\n.mex... (.mexw32, .mexw64, .mexglx, ...) \n    MATLAB executable MEX-files[76] (platform specific, e.g. \".mexmac\" for the Mac, \".mexglx\" for Linux, etc.[77])\n.p \n    MATLAB content-obscured .m file (P-code[78])\n.mlappinstall \n    MATLAB packaged App Installer[79]\n.mlpkginstall\n    support package installer (add-on for third-party hardware)[80]\n.mltx\n    packaged custom toolbox[81]\n.prj\n    project file used by various solutions (packaged app/toolbox projects, MATLAB Compiler/Coder projects, Simulink projects)\n.rpt\n    report setup file created by MATLAB Report Generator[82]\n\nSimulink\n\n.mdl \n    Simulink Model\n.mdlp \n    Simulink Protected Model\n.slx \n    Simulink Model (SLX format)\n.slxp \n    Simulink Protected Model (SLX format)\n\nSimscape\n\n.ssc \n    Simscape[83] Model\n\nMuPAD\n\n.mn \n    MuPAD Notebook\n.mu \n    MuPAD Code\n.xvc, .xvz \n    MuPAD Graphics\n\nThird-party\n\n.jkt \n    GPU Cache file generated by Jacket for MATLAB (AccelerEyes)\n.mum \n    MATLAB CAPE-OPEN Unit Operation Model File (AmsterCHEM)\n\nEaster eggs\nScreen capture of two easter eggs in MATLAB 3.5.\n\nSeveral easter eggs exist in MATLAB.[84] These include hidden pictures,[85] and jokes. For example, typing in \"spy\" will generate a picture of the spies from Spy vs Spy. \"Spy\" was changed to an image of a dog in recent releases (R2011B). Typing in \"why\" randomly outputs a philosophical answer. Other commands include \"penny\", \"toilet\", \"image\", and \"life\". Not every Easter egg appears in every version of MATLAB.", "skillName": "MATLAB."}
{"id": 120, "category": "Software", "skillText": "General Architecture for Text Engineering or GATE is a Java suite of tools originally developed at the University of Sheffield beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many natural language processing tasks, including information extraction in many languages.[1]\n\nGATE has been compared to NLTK, R and RapidMiner.[2] As well as being widely used in its own right, it forms the basis of the KIM semantic platform.[3]\n\nGATE community and research has been involved in several European research projects including TAO, SEKT, NeOn, Media-Campaign, Musing, Service-Finder, LIRICS and KnowledgeWeb, as well as many other projects.\n\nAs of May 28, 2011, 881 people are on the gate-users mailing list at SourceForge.net, and 111,932 downloads from SourceForge are recorded since the project moved to SourceForge in 2005.[4] The paper \"GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications\"[5] has received over 800 citations in the seven years since publication (according to Google Scholar). Books covering the use of GATE, in addition to the GATE User Guide,[6] include \"Building Search Applications: Lucene, LingPipe, and Gate\", by Manu Konchady,[7] and \"Introduction to Linguistic Annotation and Text Analytics\", by Graham Wilcock.[8]\n\nContents\n\n    1 Features\n    2 GATE Developer\n    3 GATE Mímir\n    4 See also\n    5 References\n\nFeatures\n\nGATE includes an information extraction system called ANNIE (A Nearly-New Information Extraction System) which is a set of modules comprising a tokenizer, a gazetteer, a sentence splitter, a part of speech tagger, a named entities transducer and a coreference tagger. ANNIE can be used as-is to provide basic information extraction functionality, or provide a starting point for more specific tasks.\n\nLanguages currently handled in GATE include English, Chinese, Arabic, Bulgarian, French, German, Hindi, Italian, Cebuano, Romanian, Russian, Danish.\n\nPlugins are included for machine learning with Weka, RASP, MAXENT, SVM Light, as well as a LIBSVM integration and an in-house perceptron implementation, for managing ontologies like WordNet, for querying search engines like Google or Yahoo, for part of speech tagging with Brill or TreeTagger, and many more. Many external plugins are also available, for handling e.g. tweets.[9]\n\nGATE accepts input in various formats, such as TXT, HTML, XML, Doc, PDF documents, and Java Serial, PostgreSQL, Lucene, Oracle Databases with help of RDBMS storage over JDBC.\n\nJAPE transducers are used within GATE to manipulate annotations on text. Documentation is provided in the GATE User Guide.[10] A tutorial has also been written by Press Association Images.[11]\nGATE Developer\nGATE 5 main window.\n\nThe screenshot shows the document viewer used to display a document and its annotations. In pink are <A> hyperlink annotations from an HTML file. The right list is the annotation sets list, and the bottom table is the annotation list. In the center is the annotation editor window.\nGATE Mímir\n\nGenerate vast quantities of information including; natural language text, semantic annotations, and ontological information. Sometimes the data itself is the end product of an application but often the information would be more useful if it could be efficiently searched. GATE Mimir provides support for indexing and searching the linguistic and semantic information generated by such applications and allows for querying the information using arbitrary combinations of text, structural information, and SPARQL.", "skillName": "GATE."}
{"id": 121, "category": "Software", "skillText": "KNIME (pronounced /naɪm/), the Konstanz Information Miner, is an open source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining concept. A graphical user interface allows assembly of nodes for data preprocessing (ETL: Extraction, Transformation, Loading), for modeling and data analysis and visualization.\n\nSince 2006, KNIME has been used in pharmaceutical research,[2] but is also used in other areas like CRM customer data analysis, business intelligence and financial data analysis.\n\nContents\n\n    1 History\n    2 Internals\n    3 License\n    4 See also\n    5 References\n    6 External links\n\nHistory\n\nThe Development of KNIME was started January 2004 by a team of software engineers at University of Konstanz as a proprietary product. The original developer team headed by Michael Berthold came from a company in the Silicon Valley providing software for the pharmaceutical industry. KNIME has been developed from day one using rigorous professional software engineering processes since it was clear from the beginning that it was to be used in large scale enterprises[citation needed]. The initial goal was to create a modular, highly scalable and open data processing platform which allowed for the easy integration of different data loading, processing, transformation, analysis and visual exploration modules without the focus on any particular application area. The platform was intended to be a collaboration and research platform and should also serve as an integration platform for various other data analysis projects[citation needed].\n\nIn 2006 the first version of KNIME was released and several pharmaceutical companies started using KNIME and a number of life science software vendors began integrating their tools into KNIME.[3][4][5][6][7] Later that year, after an article in the German magazine c't,[8] users from a number of other areas[9][10] joined ship. As of 2012, KNIME is in use by over 15,000 actual users (i.e. not counting downloads but users regularly retrieving updates when they become available) not only in the life sciences but also at banks, publishers, car manufacturer, telcos, consulting firms, and various other industries but also at a large number of research groups worldwide.\nA screenshot of KNIME.\nInternals\n\nKNIME allows users to visually create data flows (or pipelines), selectively execute some or all analysis steps, and later inspect the results, models, and interactive views. KNIME is written in Java and based on Eclipse and makes use of its extension mechanism to add plugins providing additional functionality. The core version already includes hundreds of modules for data integration (file I/O, database nodes supporting all common database management systems), data transformation (filter, converter, combiner) as well as the commonly used methods for data analysis and visualization. With the free Report Designer extension, KNIME workflows can be used as data sets to create report templates that can be exported to document formats like doc, ppt, xls, pdf and others. Other capabilities of KNIME are:\n\n    KNIMEs core-architecture allows processing of large data volumes that are only limited by the available hard disk space (most other open source data analysis tools are working in main memory and are therefore limited to the available RAM). E.g. KNIME allows analysis of 300 million customer addresses, 20 million cell images and 10 million molecular structures.\n    Additional plugins allows the integration of methods for Text mining, Image mining, as well as time series analysis.\n    KNIME integrates various other open-source projects, e.g. machine learning algorithms from Weka, the statistics package R project, as well as LIBSVM, JFreeChart, ImageJ, and the Chemistry Development Kit.[11]\n\nKNIME is implemented in Java but also allows for wrappers calling other code in addition to providing nodes that allow to run Java, Python, Perl and other code fragments.\nLicense\nAs of version 2.1, KNIME is released under GPLv3 with an exception that allows others to use the well defined node API to add proprietary extensions.[12] This allows also commercial SW vendors to add wrappers calling their tools from KNIME.", "skillName": "KNIME."}
{"id": 122, "category": "Software", "skillText": "Waffles is a collection of command-line tools for performing machine learning operations developed at Brigham Young University. These tools are written in C++, and are available under the GNU Lesser General Public License.\n\nContents\n\n    1 Description\n    2 Advantages\n    3 Disadvantages\n    4 See also\n    5 References\n\nDescription\n\nThe Waffles machine learning toolkit[1] contains command-line tools for performing various operations related to machine learning, data mining, and predictive modeling. The primary focus of Waffles is to provide tools that are simple to use in scripted experiments or processes. For example, the supervised learning algorithms included in Waffles are all designed to support multi-dimensional labels, classification and regression, automatically impute missing values, and automatically apply necessary filters to transform the data to a type that the algorithm can support, such that arbitrary learning algorithms can be used with arbitrary data sets. Many other machine learning toolkits provide similar functionality, but require the user to explicitly configure data filters and transformations to make it compatible with a particular learning algorithm. The algorithms provided in Waffles also have the ability to automatically tune their own parameters (with the cost of additional computational overhead).\n\nBecause Waffles is designed for script-ability, it deliberately avoids presenting its tools in a graphical environment. It does, however, include a graphical \"wizard\" tool that guides the user to generate a command that will perform a desired task. This wizard does not actually perform the operation, but requires the user to paste the command that it generates into a command terminal or a script. The idea motivating this design is to prevent the user from becoming \"locked in\" to a graphical interface.\n\nAll of the Waffles tools are implemented as thin wrappers around functionality in a C++ class library. This makes it possible to convert scripted processes into native applications with minimal effort.\n\nWaffles was first released as an open source project in 2005. Since that time, it has been developed at Brigham Young University, with a new version having been released approximately every 6–9 months. Waffles is not an acronym—the toolkit was named after the food for historical reasons.\nAdvantages\n\nSome of the advantages of Waffles in contrast with other popular open source machine learning toolkits include:\n\n    Waffles automatically takes care of many issues related to data format in order to simplify its tools.\n    Because it is implemented in C++, many of its algorithms are particularly fast. Also, the lack of dependency on any virtual machine makes it easier to deploy in conjunction with other applications.\n    The functionality included in Waffles is very broad, including algorithms for dimensionality reduction, collaborative filtering, visualization, clustering, supervised learning, optimization, linear algebra, data transformation, image and signal processing, policy learning, and sparse matrix operations.\n\nDisadvantages\n\n    Although Waffles provides significant breadth, it lacks the depth of many toolkits that focus on a particular area of machine learning. The Weka (machine learning) toolkit, for example, provides many more classification algorithms than Waffles provides.\n    Waffles only has a limited graphical interface.", "skillName": "Waffles."}
{"id": 123, "category": "Software", "skillText": "Wolfram Mathematica (sometimes referred to as Mathematica) is a symbolic mathematical computation program, sometimes called a computer algebra program, used in many scientific, engineering, mathematical, and computing fields. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois.[6][7] The Wolfram Language is the programming language used in Mathematica.[8]\n\nContents\n\n    1 Features\n    2 Interface\n    3 High-performance computing\n    4 Deployment\n    5 Connections with other applications\n    6 Computable data\n    7 Version history\n    8 See also\n    9 References\n    10 External links\n\nFeatures\nDini's surface plotted with adjustable parameters\n\nFeatures of Wolfram Mathematica include:[9]\n\n    Elementary and Special mathematical function libraries\n    Support for complex number, arbitrary precision, interval arithmetic and symbolic computation\n    Matrix and data manipulation tools including support for sparse arrays\n    2D and 3D data, function and geo visualization and animation tools\n    Solvers for systems of equations, diophantine equations, ODEs, PDEs, DAEs, DDEs, SDEs and recurrence relations\n    Numeric and symbolic tools for discrete and continuous calculus including continuous and discrete integral transforms\n    Constrained and unconstrained local and global optimization\n    Multivariate statistics libraries including fitting, hypothesis testing, and probability and expectation calculations on over 140 distributions.\n    Support for censored data, temporal data, time-series and unit based data\n    Calculations and simulations on random processes and queues\n    Supervised and unsupervised machine learning tools for data, images and sounds\n    Tools for text mining including regular expressions and semantic analysis\n    Data mining tools such as cluster analysis, sequence alignment and pattern matching\n    Computational geometry in 2D, 3D and higher dimensions\n    Finite element analysis including 2D and 3D adaptive mesh generation\n    Libraries for signal processing including wavelet analysis on sounds, images and data\n    Linear and non-linear Control systems libraries\n    Tools for 2D and 3D image processing[10] and morphological image processing including image recognition\n    Tools for visualizing and analysing directed and undirected graphs\n    Tools for combinatoric problems\n    Number theory function library\n    Tools for financial calculations including bonds, annuities, derivatives, options etc.\n    Group theory and symbolic tensor functions\n    Import and export filters for data, images, video, sound, CAD, GIS,[11] document and biomedical formats\n    Database collection for mathematical, scientific, and socio-economic information and access to WolframAlpha data and computations\n    Technical word processing including formula editing and automated report generating\n    Programming language supporting procedural, functional and object oriented constructs\n    Toolkit for adding user interfaces to calculations and applications\n    Tools for connecting to DLL, SQL, Java, .NET, C++, Fortran, CUDA, OpenCL, and http based systems\n    Tools for parallel programming\n    Using both \"free-form linguistic input\" (a natural language user interface)[12][13] and Wolfram Language in notebook when connected to the Internet\n\nInterface\n\nWolfram Mathematica is split into two parts, the kernel and the front end. The kernel interprets expressions (Wolfram Language code) and returns result expressions.\n\nMathematica is a programming language that has evolved over several years of development\n\nThe front end, designed by Theodore Gray,[14] provides a GUI, which allows the creation and editing of Notebook documents containing program code with prettyprinting, formatted text together with results including typeset mathematics, graphics, GUI components, tables, and sounds. All content and formatting can be generated algorithmically or edited interactively. Most standard word processing capabilities are supported. It includes a spell-checker but does not spell check automatically as you type.\n\nDocuments can be structured using a hierarchy of cells, which allow for outlining and sectioning of a document and support automatic numbering index creation. Documents can be presented in a slideshow environment for presentations. Notebooks and their contents are represented as Mathematica expressions that can be created, modified or analyzed by Mathematica programs. This allows conversion to other formats such as TeX or XML.\n\nThe front end includes development tools such as a debugger, input completion and automatic syntax coloring.\n\nAmong the alternative front ends is the Wolfram Workbench, an Eclipse based IDE, introduced in 2006. It provides project-based code development tools for Mathematica, including revision management, debugging, profiling, and testing.[15] The Mathematica Kernel also includes a command line front end.[16] Other interfaces include JMath,[17] based on GNU readline and MASH[18] which runs self-contained Mathematica programs (with arguments) from the UNIX command line.\n\nWolfram Research has published a series of hands-on starter webcasts that introduce the user interface and the engine.[19]\nHigh-performance computing\n\nIn recent years, the capabilities for high-performance computing have been extended with the introduction of packed arrays (version 4, 1999)[20] and sparse matrices (version 5, 2003),[21] and by adopting the GNU Multi-Precision Library to evaluate high-precision arithmetic.\n\nVersion 5.2 (2005) added automatic multi-threading when computations are performed on multi-core computers.[22] This release included CPU specific optimized libraries. In addition Mathematica is supported by third party specialist acceleration hardware such as ClearSpeed.[23]\n\nIn 2002, gridMathematica was introduced to allow user level parallel programming on heterogeneous clusters and multiprocessor systems[24] and in 2008 parallel computing technology was included in all Mathematica licenses including support for grid technology such as Windows HPC Server 2008, Microsoft Compute Cluster Server and Sun Grid.\n\nSupport for CUDA and OpenCL GPU hardware was added in 2010. Also, since version 8 it can generate C code, which is automatically compiled by a system C compiler, such as GCC or Microsoft Visual Studio.\nDeployment\n\nThere are several ways to deploy applications written in Wolfram Mathematica:\n\n    Mathematica Player Pro is a runtime version of Mathematica that will run any Mathematica application but does not allow editing or creation of the code.[25]\n    A free-of-charge version, Wolfram CDF Player, is provided for running Mathematica programs that have been saved in the Computable Document Format (CDF).[26] It can also view standard Mathematica files, but not run them. It includes plugins for common web browsers on Windows and Macintosh.\n    webMathematica allows a web browser to act as a front end to a remote Mathematica server. It is designed to allow a user written application to be remotely accessed via a browser on any platform. It may not be used to give full access to Mathematica. Due to bandwidth limitations interactive 3D graphics ist not fully supported within a web browser.\n    Wolfram Language code can be converted to C code or to an automatically generated DLL.\n    Wolfram Language code can be run on a Wolfram cloud service as a web-app or as an API\n\nConnections with other applications\n\nCommunication with other applications occurs through a protocol called WSTP \n. It allows communication between the Wolfram Mathematica kernel and front-end, and also provides a general interface between the kernel and other applications. Wolfram Research freely distributes a developer kit for linking applications written in the C programming language to the Mathematica kernel through WSTP. Using J/Link.,[27] a Java program can ask Mathematica to perform computations; likewise, a Mathematica program can load Java classes, manipulate Java objects and perform method calls. Similar functionality is achieved with .NET /Link,[28] but with .NET programs instead of Java programs. Other languages that connect to Mathematica include Haskell,[29] AppleScript,[30] Racket,[31] Visual Basic,[32] Python[33][34] and Clojure.[35]\n\nLinks are available to many mathematical software packages including OpenOffice.org Calc,[36] Microsoft Excel,[37] MATLAB,[38][39][40] R,[41] Sage,[42][43] SINGULAR,[44] Wolfram SystemModeler, and Origin.[45] Mathematical equations can be exchanged with other computational or typesetting software via MathML.\n\nCommunication with SQL databases is achieved through built-in support for JDBC.[46] Mathematica can also install web services from a WSDL description.[47][48] It can access HDFS data via Hadoop.[49]\n\nMathematica can capture real-time data via a link to LabVIEW,[50] from financial data feeds[51] and directly from hardware devices via GPIB (IEEE 488),[52] USB[53] and serial interfaces.[54] It automatically detects and reads from HID devices.\nComputable data\nA stream plot of live weather data\n\nWolfram Mathematica includes collections of curated data provided for use in computations. Mathematica is also integrated with Wolfram Alpha, an online service which provides additional data, some of which is kept updated in real time. Some of the data sets include astronomical, chemical, geopolitical, language, biomedical and weather data, in addition to mathematical data (such as knots and polyhedra).[55]\nVersion history\n\nWolfram Mathematica built on the ideas in Cole and Wolfram's earlier Symbolic Manipulation Program (SMP).[56][57] The name of the program \"Mathematica\" was suggested to Stephen Wolfram by Apple co-founder Steve Jobs although Stephen Wolfram had thought about it earlier and rejected it.[58]\n\nWolfram Research has released the following versions of Mathematica:[59]\n\n    Mathematica 1.0 (June 23, 1988)[60][61][62][63]\n    Mathematica 1.1 (October 31, 1988)\n    Mathematica 1.2 (August 1, 1989)[64][63]\n    Mathematica 2.0 (January 15, 1991)[65][63]\n    Mathematica 2.1 (June 15, 1992)[63]\n    Mathematica 2.2 (June 1, 1993)[63][66]\n    Mathematica 3.0 (September 3, 1996)[67]\n    Mathematica 4.0 (May 19, 1999)[63][68]\n    Mathematica 4.1 (November 2, 2000)[63]\n    Mathematica 4.2 (November 1, 2002)[63]\n    Mathematica 5.0 (June 12, 2003)[63][69]\n    Mathematica 5.1 (October 25, 2004)[63][70]\n    Mathematica 5.2 (June 20, 2005)[63][71]\n    Mathematica 6.0 (May 1, 2007)[72][73]\n    Mathematica 7.0 (November 18, 2008)[74]\n    Mathematica 8.0 (November 15, 2010)\n    Mathematica 9.0 (November 28, 2012)\n    Mathematica 10.0 (July 9, 2014)\n    Mathematica 10.0.2 (December 10, 2014)\n    Mathematica 10.1 (March 30, 2015)[75]\n    Mathematica 10.2 (July 14, 2015)[76]\n    Mathematica 10.3 (October 15, 2015)\n    Mathematica 10.3.1 (December 16, 2015)\n    Mathematica 10.4 (March 2, 2016)\n    Mathematica 10.4.1 (April 18, 2016)", "skillName": "Mathematica."}
{"id": 124, "category": "Software", "skillText": "The Apache Software Foundation /əˈpætʃiː/ (ASF) is an American non-profit corporation (classified as 501(c)(3) in the United States) to support Apache software projects, including the Apache HTTP Server. The ASF was formed from the Apache Group and incorporated in Delaware, U.S., in June 1999.[1][2]\n\nThe Apache Software Foundation is a decentralized community of developers. The software they produce is distributed under the terms of the Apache License and is free and open source software (FOSS). The Apache projects are characterized by a collaborative, consensus-based development process and an open and pragmatic software license. Each project is managed by a self-selected team of technical experts who are active contributors to the project. The ASF is a meritocracy, implying that membership of the foundation is granted only to volunteers who have actively contributed to Apache projects. The ASF is considered a second generation[3] open-source organization, in that commercial support is provided without the risk of platform lock-in.\n\nAmong the ASF's objectives are: to provide legal protection[4] to volunteers working on Apache projects; to prevent the Apache brand name from being used by other organizations without permission.\n\nThe ASF also holds several ApacheCon[5] conferences each year, highlighting Apache projects, related technology.\n\nContents\n\n    1 History\n    2 Projects\n    3 Board of directors\n    4 Financials\n    5 See also\n    6 Notes\n    7 Further reading\n    8 External links\n\nHistory\n\nThe history of the Apache Software Foundation is linked to the Apache HTTP Server, development beginning in February 1995. A group of eight developers started working on enhancing the NCSA HTTPd daemon. They came to be known as the Apache Group. On March 25, 1999, the Apache Software Foundation was formed.[1] The first official meeting of the Apache Software Foundation was held on April 13, 1999, and by general consent that the initial membership list of the Apache Software Foundation, would be: Brian Behlendorf, Ken Coar, Miguel Gonzales, Mark Cox, Lars Eilebrecht, Ralf S. Engelschall, Roy T. Fielding, Dean Gaudet, Ben Hyde, Jim Jagielski, Alexei Kosut, Martin Kraemer, Ben Laurie, Doug MacEachern, Aram Mirzadeh, Sameer Parekh, Cliff Skolnick, Marc Slemko, William (Bill) Stoddard, Paul Sutton, Randy Terbush and Dirk-Willem van Gulik.[6] After a series of additional meetings to elect board members and resolve other legal matters regarding incorporation, the effective incorporation date of the Apache Software Foundation was set to June 1, 1999.[2]\n\nThe name 'Apache' was chosen from respect for the Native American Apache Nation, well known for their superior skills in warfare strategy and their inexhaustible endurance. It also makes a pun on \"a patchy web server\"—a server made from a series of patches—but this was not its origin. The group of developers who released this new software soon started to call themselves the \"Apache Group\".[citation needed]\nProjects\nSee also: List of Apache Software Foundation projects\n\nApache divides its software development activities into separate semi-autonomous areas called \"top-level projects\" (formally known as a \"Project Management Committee\" in the bylaws[7]), some of which have a number of sub-projects. Unlike some other organizations that host FOSS projects, before a project is hosted at Apache it has to be licensed to the ASF with a grant or contributor agreement.[8] In this way, the ASF gains the necessary intellectual property rights for the development and distribution of all its projects.[9]\nBoard of directors\n\nThe ASF board of directors has responsibility for overseeing the ASF's activities and acting as a central point of contact and communication for its projects. The board assigns corporate issues, assigning resources to projects, and manages corporate services, including funds and legal issues. It does not make technical decisions about individual projects; these are made by the individual Project Management Committees. The board is elected annually by members of the foundation and, after the March 2016 Annual Members Meeting, it consists of:[10][11][12]\n\n    Shane Curcuru\n    Bertrand Delacretaz\n    Isabel Drost-Fromm\n    Marvin Humphrey\n    Jim Jagielski\n    Chris Mattmann (Treasurer)\n    Brett Porter (Chairman)\n    Greg Stein (Vice Chairman)\n    Mark Thomas\n\nFinancials\n\nIn the 2010–11 fiscal year, the Foundation took in $539,410, almost entirely from grants and contributions with $12,349 from two ApacheCons. With no employees and 2,663 volunteers, it spent $270,846 on infrastructure, $92,364 on public relations, and $17,891 on two ApacheCons.[13]\nSee also\n\n    Apache Attic\n    Apache Incubator\n\n\n\nApache Incubator is the gateway for open-source projects intended to become fully fledged Apache Software Foundation projects.\n\nThe Incubator project was created in October 2002 to provide an entry path to the Apache Software Foundation for projects and codebases wishing to become part of the Foundation's efforts. All code donations from external organizations and existing external projects wishing to move to Apache must enter through the Incubator.\n\nThe Apache Incubator project serves on the one hand as a temporary container project until the incubating project is accepted and becomes a top-level project of the Apache Software Foundation or becomes subproject of a proper project such as the Jakarta Project or Apache XML. On the other hand, the Incubator project documents how the Foundation works, and how to get things done within its framework. This means documenting process, roles and policies within the Apache Software Foundation and its member projects.\n\n\n\n\nThe mission of the Apache Software Foundation (ASF) is to provide software for the public good. We do this by providing services and support for many like-minded software project communities of individuals who choose to join the ASF.\nWhat is the ASF?\n\nEstablished in 1999, the ASF is a US 501(c)(3) charitable organization, funded by individual donations and corporate sponsors. Our all-volunteer board oversees more than 350 leading Open Source projects, including Apache HTTP Server -- the world's most popular Web server software.\n\nThe ASF provides an established framework for intellectual property and financial contributions that simultaneously limits potential legal exposure for our project committers. Through the ASF's meritocratic process known as \"The Apache Way,\" more than 500 individual Members and 4,500 Committers successfully collaborate to develop freely available enterprise-grade software, benefiting millions of users worldwide: thousands of software solutions are distributed under the Apache License; and the community actively participates in ASF mailing lists, mentoring initiatives, and ApacheCon, the Foundation's official user conference, trainings, and expo.\nHow did the ASF and Apache® projects grow?\n\nFormerly known as the Apache Group, the ASF was incorporated in 1999 as a membership-based, not-for-profit corporation in order to ensure that the Apache projects continue to exist beyond the participation of individual volunteers. Individuals who have demonstrated a commitment to collaborative open-source software development, through sustained participation and contributions within the Foundation's projects, are eligible for membership in the ASF. An individual is awarded membership after nomination and approval by a majority of the existing ASF members. Thus, the ASF is governed by the community it most directly serves -- the people collaborating within its projects.\nHow are the ASF and Apache projects governed?\n\nThe ASF members periodically elect a Board of Directors to manage the organizational affairs of the Foundation, as accorded by the ASF Bylaws. The Board, in turn, appoints a number of officers to oversee the day-to-day operations of the Foundation. A number of public records of our operation are made available to the community. A more detailed explanation of How the ASF works in terms of day to day operations is available, and the Apache Community Development project's goal is to help newcomers learn more about the Apache Software Foundation.\n\nIndividual Apache projects are in turn governed directly by Project Management Committees (PMC) made up of individuals who have shown merit and leadership within those projects. There are detailed descriptions of ASF and project governance models.", "skillName": "Software."}
{"id": 125, "category": "Software", "skillText": "Oracle Data Mining (ODM) is an option of Oracle Corporation's Relational Database Management System (RDBMS) Enterprise Edition (EE). It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.\nOracle Data Mining Developer(s) \tOracle Corporation\nStable release \t11gR2 / September, 2009\nType \tdata mining and analytics\nLicense \tproprietary\nWebsite \tOracle Data Mining \n\nContents\n\n    1 Overview\n    2 History\n    3 Functionality\n    4 Input sources and data preparation\n    5 Graphical user interface: Oracle Data Miner\n    6 PL/SQL and Java interfaces\n    7 SQL scoring functions\n    8 PMML\n    9 Predictive Analytics MS Excel Add-In\n    10 References and further reading\n    11 See also\n    12 References\n    13 External links\n\nOverview\n\nOracle implements a variety of data mining algorithms inside the Oracle relational database. These implementations are integrated right into the Oracle database kernel, and operate natively on data stored in the relational database tables. This eliminates the need for extraction or transfer of data into standalone mining/analytic servers. The relational database platform is leveraged to securely manage models and efficiently execute SQL queries on large volumes of data. The system is organized around a few generic operations providing a general unified interface for data mining functions. These operations include functions to create, apply, test, and manipulate data mining models. Models are created and stored as database objects, and their management is done within the database - similar to tables, views, indexes and other database objects.\n\nIn data mining, the process of using a model to derive predictions or descriptions of behavior that is yet to occur is called \"scoring\". In traditional analytic workbenches, a model built in the analytic engine has to be deployed in a mission-critical system to score new data, or the data is moved from relational tables into the analytical workbench - most workbenches offer proprietary scoring interfaces. ODM simplifies model deployment by offering Oracle SQL functions to score data stored right in the database. This way, the user/application developer can leverage the full power of Oracle SQL - in terms of the ability to pipeline and manipulate the results over several levels, and in terms of parallelizing and partitioning data access for performance.\n\nModels can be created and managed by one of several means. (Oracle Data Miner) is a graphical user interface that steps the user through the process of creating, testing, and applying models (e.g. along the lines of the CRISP-DM methodology). Application and tools developers can embed predictive and descriptive mining capabilities using PL/SQL or Java APIs. Business analysts can quickly experiment with, or demonstrate the power of, predictive analytics using Oracle Spreadsheet Add-In for Predictive Analytics, a dedicated Microsoft Excel adaptor interface. ODM offers a choice of well known machine learning approaches such as Decision Trees, Naive Bayes, Support vector machines, Generalized linear model (GLM) for predictive mining, Association rules, K-means and Orthogonal Partitioning[1][2] Clustering, and Non-negative matrix factorization for descriptive mining. A minimum description length based technique to grade the relative importance of an input mining attributes for a given problem is also provided. Most Oracle Data Mining functions also allow text mining by accepting Text (unstructured data) attributes as input. Users do not need to configure text mining options, this is handled behind the scenes by the Database_options database option.\nHistory\n\nOracle Data Mining was first introduced in 2002 and its releases are named according to the corresponding Oracle database release:\n\n    Oracle Data Mining 9iR2 (9.2.0.1.0 - May 2002)\n    Oracle Data Mining 10gR1 (10.1.0.2.0 - February 2004)\n    Oracle Data Mining 10gR2 (10.2.0.1.0 - July 2005)\n    Oracle Data Mining 11gR1 (11.1 - September 2007)\n    Oracle Data Mining 11gR2 (11.2 - September 2009)\n\nOracle Data Mining is a logical successor of the Darwin data mining toolset developed by Thinking Machines Corporation in the mid-1990s and later distributed by Oracle after its acquisition of Thinking Machines in 1999. However, the product itself is a complete redesign and rewrite from ground-up - while Darwin was a classic GUI-based analytical workbench, ODM offers a data mining development/deployment platform integrated into the Oracle database, along with the Oracle Data Miner GUI.\n\nThe Oracle Data Miner 11gR2 New Workflow GUI was previewed at Oracle Open World 2009. An updated Oracle Data Miner GUI was released in 2012. It is free, and is available as an extension to Oracle SQL Developer 3.1 .\nFunctionality\n\nAs of release 11gR1 Oracle Data Mining contains the following data mining functions:\n\n    Data transformation and model analysis:\n        Data sampling, binning, discretization, and other data transformations.\n        Model exploration, evaluation and analysis.\n    Feature selection (Attribute Importance).\n        Minimum description length (MDL).\n    Classification.\n        Naive Bayes (NB).\n        Generalized linear model (GLM) for Logistic regression.\n        Support Vector Machine (SVM).\n        Decision Trees (DT).\n    Anomaly detection.\n        One-class Support Vector Machine (SVM).\n    Regression\n        Support Vector Machine (SVM).\n        Generalized linear model (GLM) for Multiple regression\n    Clustering:\n        Enhanced k-means (EKM).\n        Orthogonal Partitioning Clustering (O-Cluster).[1][2]\n    Association rule learning:\n        Itemsets and association rules (AM).\n    Feature extraction.\n        Non-negative matrix factorization (NMF).\n    Text and spatial mining:\n        Combined text and non-text columns of input data.\n        Spatial/GIS data.\n\nInput sources and data preparation\n\nMost Oracle Data Mining functions accept as input one relational table or view. Flat data can be combined with transactional data through the use of nested columns, enabling mining of data involving one-to-many relationships (e.g. a star schema). The full functionality of SQL can be used when preparing data for data mining, including dates and spatial data.\n\nOracle Data Mining distinguishes numerical, categorical, and unstructured (text) attributes. The product also provides utilities for data preparation steps prior to model building such as outlier treatment, discretization, normalization and binning (sorting in general speak)\nGraphical user interface: Oracle Data Miner\n\nUsers can access Oracle Data Mining through Oracle Data Miner, a GUI client application that provides access to the data mining functions and structured templates (called Mining Activities) that automatically prescribe the order of operations, perform required data transformations, and set model parameters. The user interface also allows the automated generation of Java and/or SQL code associated with the data-mining activities. The Java Code Generator is an extension to Oracle JDeveloper. An independent interface also exists: the Spreadsheet Add-In for Predictive Analytics which enables access to the Oracle Data Mining Predictive Analytics PL/SQL package from Microsoft Excel.\n\nFrom version 11.2 of the Oracle database, Oracle Data Miner integrates with Oracle SQL Developer.[3]\nPL/SQL and Java interfaces\n\nOracle Data Mining provides a native PL/SQL package (DBMS_DATA_MINING) to create, destroy, describe, apply, test, export and import models. The code below illustrates a typical call to build a classification model:\n\nBEGIN\n  DBMS_DATA_MINING.CREATE_MODEL (\n    model_name          => 'credit_risk_model', \n    function            => DBMS_DATA_MINING.classification, \n    data_table_name     => 'credit_card_data', \n    case_id_column_name => 'customer_id', \n    target_column_name  => 'credit_risk',\n    settings_table_name => 'credit_risk_model_settings');\nEND;\n\nwhere 'credit_risk_model' is the model name, built for the express purpose of classifying future customers' 'credit_risk', based on training data provided in the table 'credit_card_data', each case distinguished by a unique 'customer_id', with the rest of the model parameters specified through the table 'credit_risk_model_settings'.\n\nOracle Data Mining also supports a Java API consistent with the Java Data Mining (JDM) standard for data mining (JSR-73) for enabling integration with web and Java EE applications and to facilitate portability across platforms.\nSQL scoring functions\n\nAs of release 10gR2, Oracle Data Mining contains built-in SQL functions for scoring data mining models. These single-row functions support classification, regression, anomaly detection, clustering, and feature extraction. The code below illustrates a typical usage of a classification model:\n\nSELECT customer_name\n  FROM credit_card_data\n WHERE PREDICTION (credit_risk_model USING *) = 'LOW' AND customer_value = 'HIGH';\n\nPMML\n\nIn Release 11gR2 (11.2.0.2), ODM supports the import of externally created PMML for some of the data mining models. PMML is an XML-based standard for representing data mining models.\nPredictive Analytics MS Excel Add-In\n\nThe PL/SQL package DBMS_PREDICTIVE_ANALYTICS automates the data mining process including data preprocessing, model building and evaluation, and scoring of new data. The PREDICT operation is used for predicting target values classification or regression while EXPLAIN ranks attributes in order of influence in explaining a target column feature selection. The new 11g feature PROFILE finds customer segments and their profiles, given a target attribute. These operations can be used as part of an operational pipeline providing actionable results or displayed for interpretation by end users.\nReferences and further reading\n\n    T. H. Davenport, Competing on Analytics \n    , Harvard Business Review, January 2006.\n    I. Ben-Gal,Outlier detection \n    , In: Maimon O. and Rockach L. (Eds.) Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers,\" Kluwer Academic Publishers, 2005, ISBN 0-387-24435-2.\n    M. M. Campos, P. J. Stengard, and B. L. Milenova, Data-centric Automated Data Mining. In proceedings of the Fourth International Conference on Machine Learning and Applications 2005, 15–17 December 2005. pp8, ISBN 0-7695-2495-8\n    M. F. Hornick, Erik Marcade, and Sunil Venkayala. Java Data Mining: Strategy, Standard, and Practice. Morgan-Kaufmann, 2006, ISBN 0-12-370452-9.\n    B. L. Milenova, J. S. Yarmus, and M. M. Campos. SVM in Oracle database 10g: removing the barriers to widespread adoption of support vector machines. In Proceedings of the 31st international Conference on Very Large Data Bases (Trondheim, Norway, August 30 - September 2, 2005). pp1152–1163, ISBN 1-59593-154-6.\n    B. L. Milenova and M. M. Campos. O-Cluster: scalable clustering of large high dimensional data sets. In proceedings of the 2002 IEEE International Conference on Data Mining: ICDM 2002. pp290–297, ISBN 0-7695-1754-4.\n    P. Tamayo, C. Berger, M. M. Campos, J. S. Yarmus, B. L.Milenova, A. Mozes, M. Taft, M. Hornick, R. Krishnan, S.Thomas, M. Kelly, D. Mukhin, R. Haberstroh, S. Stephens and J. Myczkowski. Oracle Data Mining - Data Mining in the Database Environment. In Part VII of Data Mining and Knowledge Discovery Handbook, Maimon, O.; Rokach, L. (Eds.) 2005, p315-1329, ISBN 0-387-24435-2.\n    Brendan Tierney, Predictive Analytics using Oracle Data Miner: for the data scientist, oracle analyst, oracle developer & DBA, Oracle Press, McGraw Hill, Spring 2014.", "skillName": "Oracle_Data_Mining."}
{"id": 126, "category": "Advertising", "skillText": "Co-marketing (Collaborate marketing) is a marketing practice where two companies cooperate with separate distribution channels, sometimes including profit sharing. It is frequently confused with co-promotion. Also Commensal (symbiotic) marketing is a marketing on which both corporation and a corporation, a corporation and a consumer, country and a country, human and nature can live. The ７Cs Compass Model is a framework of Co-marketing (Commensal marketing or Symbiotic marketing). Also the Co-creative marketing of a company and consumers are contained in the co-marketing.\n\nContents\n\n    1 Commensal (symbiotic) marketing\n    2 Co-creative marketing\n    3 See also\n    4 References\n    5 External links\n\nCommensal (symbiotic) marketing\n\nCommensal (symbiotic) marketing is a marketing on which both corporation and a corporation, a corporation and a consumer, country and a country, human and nature can live. The ７Cs Compass Model[1][2][3][4] by Koichi Shimizu is a framework of Co-marketing (Commensal marketing or Symbiotic marketing).\n\nThe four elements of the ７Cs Compass Model\n\n    A formal approach to this customer-focused marketing mix is known as Four Cs (Commodity, Cost, Channel, Communication) in “７Cs Compass Model. The four Cs Model provides a demand/customer centric version alternative to the well-known four Ps supply side model (product, price, place, promotion) of marketing management:[citation needed]\n            Product → Commodity\n            Price → Cost\n            Place → Channel\n            Promotion → Communication\n    (C2)Commodity – (Original meaning of Latin: Commodus=convenient) : the product for the consumers or citizens. Not product out.\n    (C3)Cost – (Original meaning of Latin: Constare= It makes sacrifices) : producing cost, selling cost, purchasing cost and social cost.\n    (C4)Channel – (Original meaning is a Canal) : Flow of commodity : marketing channels.\n    (C5)Communication – (Original meaning of Latin:Communio=sharing of meaning) : marketing communication : It doesn't promote the sales.\n    (C7)Circumstances – (Needle of compass to Circumstances )\n\nIn addition to the consumer, there are various uncontrollable external environmental factors encircling the companies. Here it can also be explained by the first character of the four directions marked on the compass model:[citation needed]\n\n        N = National and International(Political, legal and ethical)environment\n        W = Weather\n        S = Social and Cultural\n        E = Economic\n\n\nEXHIBIT: Shimizu's 7Cs Compass Model (Courtesy: © Koichi Shimizu, Japan) \nCo-creative marketing\n\nThe co-creation of a company and consumers are contained in the co-marketing. Co-creation is a management initiative, or form of economic strategy, that brings different parties together (for instance, a company and a group of customers), in order to jointly produce a mutually valued outcome.[5]\nSee also\n\n    Advertising\n    Co-creation\n    Marketing\n    Marketing mix", "skillName": "Co-marketing."}
{"id": 127, "category": "Advertising", "skillText": "Marketing mix is a business tool used in marketing and by marketers. The marketing mix is often crucial when determining a product or brand's offer, and is often associated with the four Ps: price, product, promotion, and place.[1] In service marketing, however, the four Ps are expanded to the seven Ps: process, people, physical environment [2] or Seven Ps to address the different nature of services.\n\nIn the 1990s, the concept of four C's was introduced as a more customer-driven replacement of the four P's.[3] There are two theories based on four Cs: Lauterborn's four Cs (consumer, cost, communication, convenience), and Shimizu's four Cs (commodity, cost, communication, channel).\n\nIn 2012, a new four P's theory was proposed with people, processes, programs, and performance.[4]\n\nContents\n\n    1 McCarthy's four Ps (1960)\n    2 Lauterborn's four Cs (1990)\n    3 Shimizu's Four Cs: in the 7Cs Compass Model (1973)\n    4 E. Jerome McCarthy\n    5 Booms and Bitner\n    6 See also\n    7 References\n    8 External links\n\nMcCarthy's four Ps (1960)\n\nThe marketer E. Jerome McCarthy proposed a four Ps classification in 1960, which has since been widely used by marketers.[3]\nCategory \tDefinition\nProduct \tA product is seen as an item that satisfies what a consumer demands. It is a tangible good or an intangible service. Tangible products are those that have an independent physical existence. Typical examples of mass-produced, tangible objects are the motor car and the disposable razor. A less obvious but ubiquitous mass-produced service is a computer operating system.\n\nEvery product is subject to a life-cycle including a growth phase followed by a maturity phase and finally an eventual period of decline as sales fall. Marketers must do careful research on how long the life cycle of the product they are marketing is likely to be and focus their attention on different challenges that arise as the product moves.\n\nThe marketer must also consider the product mix. Marketers can expand the current product mix by increasing a certain product line's depth or by increasing the number of product lines. Marketers should consider how to position the product, how to exploit the brand, how to exploit the company's resources and how to configure the product mix so that each product complements the other. The marketer must also consider product development strategies.[3]\nPrice \tThe amount a customer pays for the product. The price is very important as it determines the company's profit and hence, survival. Adjusting the price has a profound impact on the marketing strategy and, depending on the price elasticity of the product, often it will affect the demand and sales as well. The marketer should set a price that complements the other elements of the marketing mix.[3]\n\nWhen setting a price, the marketer must be aware of the customer perceived value for the product. Three basic pricing strategies are: market skimming pricing, market penetration pricing and neutral pricing. The 'reference value' (where the consumer refers to the prices of competing products) and the 'differential value' (the consumer's view of this product's attributes versus the attributes of other products) must be taken into account.[3]\nPromotion \tAll of the methods of communication that a marketer may use to provide information to different parties about the product. Promotion comprises elements such as: advertising, public relations, sales organisation and sales promotion.[3]\n\nAdvertising covers any communication that is paid for, from cinema commercials, radio and Internet advertisements through to print media and billboards. Public relations is where the communication is not directly paid for and includes press releases, sponsorship deals, exhibitions, conferences, seminars or trade fairs and events.\n\nAfter web 2.0, the capacity of the customers to discuss products that they have bought, making reviews and testimonials related to their experiences, are examples of public relations, as well. This kind of behavior takes the dissemination of the product information over the internet space and creates the phenomenon known as word-of-mouth.Word-of-mouth is any apparently informal communication about the product by ordinary individuals, satisfied customers or people specifically engaged to create grassroots momentum. Sales staff often plays an important role in word of mouth and public relations (see 'product' above).[3]\nDistribution (Place) \tRefers to providing the product at a place which is convenient for consumers to access. Various strategies such as intensive distribution, selective distribution, exclusive distribution and franchising can be used by the marketer to complement the other aspects of the marketing mix.[3][5] The last P is place, the distribution channel which is the location where the delivery the value. The role of the marketing channels is not only focus on the participate in demand satisfaction by offering goods, but also need to stimulate demand through information, creating proximity and promotion by customer (Balasecu, 2014). In other words, distribution channels for the product is a system process. Generally, majority of the product need a retail shop. But place also can be a telephone call center or a website.\n\nThe \"seven Ps\" is a marketing model that adds to the aforementioned four Ps, including \"physical evidence\", \"people\", and \"process\":[6] It is used when the relevant product is a service, not merely a physical good.\nCategory \tDefinition\nPhysical evidence \tThe evidence which shows that a service was performed, such as the delivery packaging for the item delivered by a delivery service, or a scar left by a surgeon. This reminds or reassures the consumer that the service took place, positively or negatively.\nPeople \tThe employees that execute the service, chiefly concerning the manner and skill in which they do so.\nProcess \tThe processes and systems within the organization that affect the execution of its service, such as job queuing or query handling.\nLauterborn's four Cs (1990)\n\nRobert F. Lauterborn proposed a four Cs classification in 1990[7] which is a more consumer-orientated version of the four Ps[8] that attempts to better fit the movement from mass marketing to niche marketing[citation needed]:\nFour Ps \tFour Cs \tDefinition\nProduct\n\t\nConsumer wants and needs\n\tA company will only sell what the consumer specifically wants to buy. So, marketers should study consumer wants and needs in order to attract them one by one with something he/she wants to purchase.[7][9]\nPrice\n\t\nCost\n\tPrice is only a part of the total cost to satisfy a want or a need. The total cost will consider for example the cost of time in acquiring a good or a service, a cost of conscience by consuming that or even a cost of guilt \"for not treating the kids\".[7] It reflects the total cost of ownership. Many factors affect cost, including but not limited to the customer's cost to change or implement the new product or service and the customer's cost for not selecting a competitor's product or service.[10]\nPromotion\n\t\nCommunication\n\tWhile promotion is \"manipulative\" and from the seller, communication is \"cooperative\" and from the buyer[7] with the aim to create a dialogue with the potential customers based on their needs and lifestyles.[11] It represents a broader focus. Communications can include advertising, public relations, personal selling, viral advertising, and any form of communication between the organization and the consumer[citation needed].\nPlace\n\t\nConvenience\n\tIn the era of Internet,[9] catalogues, credit cards and phones people neither need to go anywhere to satisfy a want or a need nor are limited to a few places to satisfy them. Marketers should know how the target market prefers to buy, how to be there and be ubiquitous, in order to guarantee convenience to buy.[7][11] With the rise of Internet and hybrid models of purchasing, Place is becoming less relevant. Convenience takes into account the ease of buying the product, finding the product, finding information about the product, and several other factors[citation needed].\nShimizu's Four Cs: in the 7Cs Compass Model (1973)\n\nAfter Koichi Shimizu proposed a four Cs classification in 1973, it was expanded to the 7Cs Compass Model to provide a more complete picture of the nature of marketing in 1979.[12][13][14][15] It attempts to explain the success or failure of a firm within a market and is somewhat analogous to Michael Porter's diamond model, which tries to explain the success and failure of different countries economically.\n\n    The 7Cs Compass Model comprises:\n\n(C1) Corporation – The core of four Cs is corporation (company and non profit organization). C-O-S (organization, competitor, stakeholder) within the corporation. The company has to think of compliance and accountability as important. The competition in the areas in which the company competes with other firms in its industry.\n\nThe four elements in the 7Cs Compass Model are:\n\nA formal approach to this customer-focused marketing mix is known as \"Four Cs\" (commodity, cost, communication, channel) in the Seven Cs Compass Model. The four Cs model provides a demand/customer centric version alternative to the well-known four Ps supply side model (product, price, promotion, place) of marketing management.[16]\n\n    Product → Commodity\n    Price → Cost\n    Promotion → Communication\n    Place → Channel\n\n\"P\" category (narrow) \t\"C\" category (broad) \t\"C\" definition\nProduct \t(C2) Commodity \t(Latin derivation: commodus=convenient) : Co-creation.It is not \"product out\". The goods and services for the consumers or citizens. Steve Jobs has been making the goods with which people are pleased. It will not become commoditization if a commodity is built starting.\nPrice \t(C3) Cost \t(Latin derivation: constare= It makes sacrifices) : There is not only producing cost and selling cost but purchasing cost and social cost.\nPromotion \t(C4) Communication \t(Latin derivation: communis=sharing of meaning) : marketing communication : Not only promotion but communication is important. Communications can include advertising, sales promotion, public relations, publicity, personal selling, corporate identity, internal communication, SNS, MIS.\nPlace \t(C5) Channel \t(Latin derivation: canal) : marketing channels. Flow of goods.\n\nThe compass of consumers and circumstances (environment) are:\n\n    (C6) Consumer – (Needle of compass to consumer)\n\n    The factors related to consumers can be explained by the first character of four directions marked on the compass model. These can be remembered by the cardinal directions, hence the name compass model:\n\n        N = Needs\n        W = Wants\n        S = Security\n        E = Education:(consumer education)\n\n    (C7) circumstances – (Needle of compass to circumstances )\n\n    In addition to the consumer, there are various uncontrollable external environmental factors encircling the companies. Here it can also be explained by the first character of the four directions marked on the compass model:\n\n        N = National and International (Political, legal and ethical) environment\n        W = Weather\n        S = Social and cultural\n        E = Economic\n\n\nEXHIBIT: Shimizu's 7Cs Compass Model (Courtesy: © Koichi Shimizu, Japan) \n\nThese can also be remembered by the cardinal directions marked on a compass. The 7Cs Compass Model is a framework in co-marketing (symbiotic marketing). It has been criticized for being little more than the four Ps with different points of emphasis. In particular, the seven Cs inclusion of consumers in the marketing mix is criticized, since they are a target of marketing, while the other elements of the marketing mix are tactics. The seven Cs also include numerous strategies for product development, distribution, and pricing, while assuming that consumers want two-way communications with companies.\n\nAn alternative approach has been suggested in a book called 'Service 7' by Australian Author, Peter Bowman. Bowman suggests a values based approach to service marketing activities. Bowman suggests implementing seven service marketing principles which include value, business development, reputation, customer service and service design. Service 7 has been widely distributed within Australia.\nE. Jerome McCarthy\n\nSince the first propose of marketing mix of 12 marketing variables by Neil H. Borden, the marketing mix have developed in 1960s. The idea of marketing mix was widely used to help with a business. A business can succeed with carry out all these process properly of marketing mix.\n\nHowever, it is difficult to a company use 12 marketing variables propose by Mr. Borden. So that E. Jerome McCarthy developed the marketing mix into \"4Ps\". The 4Ps model is known as price, place, promotion and product.\n\nProduct can be the \"quality, features, benefits, style, design, branding, packaging, services, warranties, guarantees, life cycles, investments and returns\".\n\nProduct: this is what the business offers a product or service to the customers. Each of the company want their product appeal to everybody even through some kind of product only appeal to a special group of customers. And all the companies are trying to maximize the customer group that can benefit from their products.[17]\n\nPrice can be \"list pricing, discount pricing, special offer pricing, credit payment or credit terms\".\n\nPrice: price is the total cost to customer to assume the product, but it is not the cash payment from the business to the supplier. This costs also included learning how to use the product and the peripheral costs.[17] Not only the raw material included, and also the machining costs by workers, transports costs.\n\nPlace can be the \"direct or indirect channels to market, geographical distribution, territorial coverage, retail outlet, market location, catalogues, inventory, logistics and order fulfilment\".\n\nPlace: place is the location where a business doing their business. It can be a retail store in a most original way. But nowadays it can mean \"a mail order catalogue, a telephone call centre or a website [17]\". As the development of business, e-business is become more and more popular, and this is exactly the reason why website is treated as a location now.\n\nPromotion can be the \"advertising, external communications with the media, direct selling and sales promotions\".\n\nPromotion: \"Promotion is the marketing communication used to make the offer known to potential customers and persuade them to investigate it further [17]\". In terms of promotion can be propose to promotion mix, which is advertising, public relations, sales promotion and personal selling.\n\nThe 4Ps of marketing mix which is helpful to the business, and businesses are attempting to find a balance in these 4Ps process to approach the success. And the marketing mix is helpful to the business to simplify the present marketing conditions, and then make the adjustment appropriate.\nBooms and Bitner\n\nBooms and Bitner are responsible for the creation of the extended marketing mix, featuring 7P's. E. Jerome McCarthy's original 4P's of product, price, promotion, and place are now joined by people, process and physical evidence.[18][19]\n\nPeople are essential in the marketing of any product or service. In the professional, financial or hospitality service industry, people are not producers, but rather the products themselves.[19] When people are the product, they impact public perception of an organization as much as any tangible consumer goods. From a marketing management perspective, it is important to ensure that employees represent the company in alignment with broader messaging strategies.[20] This is easier to ensure when people feel as though they have been treated fairly and earn wages sufficient enough to support their daily lives.\n\nProcess refers a \"the set of activities that results in delivery of the product benefits\". A process could be a sequential order of tasks that an employee undertakes as a part of their job. It can represent sequential steps taken by a number of various employees while attempting to complete a task. Some people are responsible for managing multiple processes at once. For example, a restaurant manager should monitor the performance of employees, ensuring that processes are followed. (S)he is also expected to supervise while customers are promptly greeted, seated, fed, and led out so that the next customer can begin this process.[20]\n\nPhysical evidence is the lasting proof that the service has happened.[19] In terms of buying a physical product, the physical evidence is the product itself. According to Booms and Bitner's framework, \"physical evidence is the service is delivered and any tangible goods that facilitate the performance and communication of the service.[20] Physical evidence is important to customers because the tangible goods are evidence that the seller has (or has not) provided what the customer was expecting . The more inviting the physical environment that surrounds a product when it is sold, the more people are willing to pay for said good or service. Anyone who does not understand how important the physical environment is in business, need only to compare the price of a 4-star and a 2-star hotel.\nSee also\n\n    E. Jerome McCarthy\n    Advertising\n    Co-creation\n    Marketing\n    Co-marketing", "skillName": "MarketingMix."}
{"id": 128, "category": "Advertising", "skillText": "Advertising is an audio or visual form of marketing communication that employs an openly sponsored, nonpersonal message to promote or sell a product, service or idea.[1]:465 Sponsors of advertising are often businesses who wish to promote their products or services. Advertising is differentiated from public relations in that an advertiser usually pays for and has control over the message. It is differentiated from personal selling in that the message is nonpersonal, i.e., not directed to a particular individual.[1]:661,672 Advertising is communicated through various mass media,[2] including old media such as newspapers, magazines, Television, Radio, outdoor advertising or direct mail; or new media such as search results, blogs, websites or text messages. The actual presentation of the message in a medium is referred to as an advertisement or \"ad\".\n\nCommercial ads often seek to generate increased consumption of their products or services through \"branding,\" which associates a product name or image with certain qualities in the minds of consumers. On the other hand, ads that intend to elicit an immediate sale are known as direct response advertising. Non-commercial advertisers who spend money to advertise items other than a consumer product or service include political parties, interest groups, religious organizations and governmental agencies. Non-profit organizations may use free modes of persuasion, such as a public service announcement. Advertising may also be used to reassure employees or shareholders that a company is viable or successful.\n\nModern advertising was created with the techniques introduced with tobacco advertising in the 1920s, most significantly with the campaigns of Edward Bernays, considered the founder of modern, \"Madison Avenue\" advertising.[3][4]\n\nIn 2015, the world spent an estimate of US$592.43 billion on advertising.[5] Internationally, the largest (\"big four\") advertising conglomerates are Interpublic, Omnicom, Publicis, and WPP.[6]\n\nIn Latin, ad vertere means \"to turn toward\".[7]\n\nContents\n\n    1 History\n        1.1 19th century\n        1.2 20th century\n            1.2.1 On the radio from the 1920s\n            1.2.2 Commercial television in the 1950s\n            1.2.3 Cable television from the 1980s\n            1.2.4 On the Internet from the 1990s\n    2 Classification\n        2.1 Types of media\n        2.2 Purposes\n            2.2.1 Sales promotions and brand loyalty\n    3 Media and advertising approaches\n        3.1 Rise in new media\n        3.2 Niche marketing\n        3.3 Crowdsourcing\n        3.4 Global advertising\n        3.5 Foreign public messaging\n        3.6 Diversification\n        3.7 New technology\n        3.8 Advertising education\n    4 Criticisms\n    5 Regulation\n    6 Theory\n        6.1 Hierarchy-of-effects models\n        6.2 Marketing mix\n        6.3 Advertising research\n        6.4 Semiotics\n    7 Gender effects in the processing of advertising\n    8 See also\n    9 References\n    10 Further reading\n        10.1 History\n    11 External links\n\nHistory\nMain article: History of advertising\nBronze plate for printing an advertisement for the Liu family needle shop at Jinan, Song dynasty China. It is considered the world's earliest identified printed advertising medium.\n\nEgyptians used papyrus to make sales messages and wall posters.[citation needed] Commercial messages and political campaign displays have been found in the ruins of Pompeii and ancient Arabia. Lost and found advertising on papyrus was common in Ancient Greece and Ancient Rome. Wall or rock painting for commercial advertising is another manifestation of an ancient advertising form, which is present to this day in many parts of Asia, Africa, and South America. The tradition of wall painting can be traced back to Indian rock art paintings that date back to 4000 BC.[8]\n\nIn ancient China, the earliest advertising known was oral, as recorded in the Classic of Poetry (11th to 7th centuries BC) of bamboo flutes played to sell candy. Advertisement usually takes in the form of calligraphic signboards and inked papers. A copper printing plate dated back to the Song dynasty used to print posters in the form of a square sheet of paper with a rabbit logo with \"Jinan Liu's Fine Needle Shop\" and \"We buy high-quality steel rods and make fine-quality needles, to be ready for use at home in no time\" written above and below[9] is considered the world's earliest identified printed advertising medium.[10]\n\nIn Europe, as the towns and cities of the Middle Ages began to grow, and the general populace was unable to read, instead of signs that read \"cobbler\", \"miller\", \"tailor\", or \"blacksmith\", images associated with their trade would be used such as a boot, a suit, a hat, a clock, a diamond, a horse shoe, a candle or even a bag of flour. Fruits and vegetables were sold in the city square from the backs of carts and wagons and their proprietors used street callers (town criers) to announce their whereabouts for the convenience of the customers. The first compilation of such advertisements was gathered in \"Les Crieries de Paris\", a thirteenth-century poem by Guillaume de la Villeneuve.[11]\n\nIn the 18th century advertisements started to appear in weekly newspapers in England. These early print advertisements were used mainly to promote books and newspapers, which became increasingly affordable with advances in the printing press; and medicines, which were increasingly sought after as disease ravaged Europe. However, false advertising and so-called \"quack\" advertisements became a problem, which ushered in the regulation of advertising content.\n19th century\nEdo period LEL flyer from 1806 for a traditional medicine called Kinseitan.\n\nThomas J. Barratt from London has been called \"the father of modern advertising\".[12][13][14] Working for the Pears Soap company, Barratt created an effective advertising campaign for the company products, which involved the use of targeted slogans, images and phrases. One of his slogans, \"Good morning. Have you used Pears' soap?\" was famous in its day and into the 20th century.[15][16]\n\nBarratt introduced many of the crucial ideas that lie behind successful advertising and these were widely circulated in his day. He constantly stressed the importance of a strong and exclusive brand image for Pears and of emphasizing the product's availability through saturation campaigns. He also understood the importance of constantly reevaluating the market for changing tastes and mores, stating in 1907 that \"tastes change, fashions change, and the advertiser has to change with them. An idea that was effective a generation ago would fall flat, stale, and unprofitable if presented to the public today. Not that the idea of today is always better than the older idea, but it is different – it hits the present taste.\"[13]\n\nAs the economy expanded across the world during the 19th century, advertising grew alongside. In the United States, the success of this advertising format eventually led to the growth of mail-order advertising.\nGnome-searchtool.svg\n\tThis section's factual accuracy is disputed. Please help to ensure that disputed statements are reliably sourced. See the relevant discussion on the talk page. (May 2016) (Learn how and when to remove this template message)\n\nIn June 1836, French newspaper La Presse was the first to include paid advertising in its pages, allowing it to lower its price, extend its readership and increase its profitability and the formula was soon copied by all titles. Around 1840, Volney B. Palmer established the roots of the modern day advertising agency in Philadelphia. In 1842 Palmer bought large amounts of space in various newspapers at a discounted rate then resold the space at higher rates to advertisers. The actual ad – the copy, layout, and artwork – was still prepared by the company wishing to advertise; in effect, Palmer was a space broker. The situation changed in the late 19th century when the advertising agency of N.W. Ayer & Son was founded. Ayer and Son offered to plan, create, and execute complete advertising campaigns for its customers. By 1900 the advertising agency had become the focal point of creative planning, and advertising was firmly established as a profession.\n\n[17] Around the same time, in France, Charles-Louis Havas extended the services of his news agency, Havas to include advertisement brokerage, making it the first French group to organize. At first, agencies were brokers for advertisement space in newspapers. N. W. Ayer & Son was the first full-service agency to assume responsibility for advertising content. N.W. Ayer opened in 1869, and was located in Philadelphia.[17]\n20th century\nA 1900 advertisement for Pears soap.\nA print advertisement for the 1913 issue of the Encyclopædia Britannica.\n\nAdvertising increased dramatically in the United States as industrialization expanded the supply of manufactured products. In order to profit from this higher rate of production, industry needed to recruit workers as consumers of factory products. It did so through the invention of mass marketing designed to influence the population's economic behavior on a larger scale.[18] In the 1910s and 1920s, advertisers in the U.S. adopted the doctrine that human instincts could be targeted and harnessed – \"sublimated\" into the desire to purchase commodities.[19] Edward Bernays, a nephew of Sigmund Freud, became associated with the method and is sometimes called the founder of modern advertising and public relations.[20]\n\nIn the 1920s, under Secretary of Commerce Herbert Hoover, the American government promoted advertising. Hoover himself delivered an address to the Associated Advertising Clubs of the World in 1925 called 'Advertising Is a Vital Force in Our National Life.\"[21] In October 1929, the head of the U.S. Bureau of Foreign and Domestic Commerce, Julius Klein, stated \"Advertising is the key to world prosperity.\"[22] This was part of the \"unparalleled\" collaboration between business and government in the 1920s, according to a 1933 European economic journal.[23]\n\nThe tobacco companies became major advertisers in order to sell packaged cigarettes.[24] The tobacco companies pioneered the new advertising techniques when they hired Bernays to create positive associations with tobacco smoking.[3][4]\n\nAdvertising was also used as a vehicle for cultural assimilation, encouraging workers to exchange their traditional habits and community structure in favor of a shared \"modern\" lifestyle.[25] An important tool for influencing immigrant workers was the American Association of Foreign Language Newspapers (AAFLN). The AAFLN was primarily an advertising agency but also gained heavily centralized control over much of the immigrant press.[26][27]\n1916 Ladies' Home Journal version of the famous ad by Helen Lansdowne Resor of the J. Walter Thompson Agency\n\nAt the turn of the 20th century, there were few career choices for women in business; however, advertising was one of the few. Since women were responsible for most of the purchasing done in their household, advertisers and agencies recognized the value of women's insight during the creative process. In fact, the first American advertising to use a sexual sell was created by a woman – for a soap product. Although tame by today's standards, the advertisement featured a couple with the message \"A skin you love to touch\".[28]\n\nIn the 1920s psychologists Walter D. Scott and John B. Watson contributed applied psychological theory to the field of advertising. Scott said, \"Man has been called the reasoning animal but he could with greater truthfulness be called the creature of suggestion. He is reasonable, but he is to a greater extent suggestible\".[29] He demonstrated this through his advertising technique of a direct command to the consumer.\nOn the radio from the 1920s\nAdvertisement for a live radio broadcast, sponsored by a milk company, Adohr milk, and published in the Los Angeles Times on May 6, 1930\n\nIn the early 1920s, the first radio stations were established by radio equipment manufacturers and retailers who offered programs in order to sell more radios to consumers. As time passed, many non-profit organizations followed suit in setting up their own radio stations, and included: schools, clubs and civic groups.[30]\n\nWhen the practice of sponsoring programs was popularized, each individual radio program was usually sponsored by a single business in exchange for a brief mention of the business' name at the beginning and end of the sponsored shows. However, radio station owners soon realized they could earn more money by selling sponsorship rights in small time allocations to multiple businesses throughout their radio station's broadcasts, rather than selling the sponsorship rights to single businesses per show.[citation needed]\nCommercial television in the 1950s\n\nIn the early 1950s, the DuMont Television Network began the modern practice of selling advertisement time to multiple sponsors. Previously, DuMont had trouble finding sponsors for many of their programs and compensated by selling smaller blocks of advertising time to several businesses. This eventually became the standard for the commercial television industry in the United States. However, it was still a common practice to have single sponsor shows, such as The United States Steel Hour. In some instances the sponsors exercised great control over the content of the show – up to and including having one's advertising agency actually writing the show.[citation needed] The single sponsor model is much less prevalent now, a notable exception being the Hallmark Hall of Fame.[citation needed]\nCable television from the 1980s\n\nThe late 1980s and early 1990s saw the introduction of cable television and particularly MTV. Pioneering the concept of the music video, MTV ushered in a new type of advertising: the consumer tunes in for the advertising message, rather than it being a by-product or afterthought. As cable and satellite television became increasingly prevalent, specialty channels emerged, including channels entirely devoted to advertising, such as QVC, Home Shopping Network, and ShopTV Canada.\nOn the Internet from the 1990s\nMain article: Online advertising\n\nWith the advent of the ad server, online advertising grew, contributing to the \"dot-com\" boom of the 1990s.[citation needed] Entire corporations operated solely on advertising revenue, offering everything from coupons to free Internet access. At the turn of the 21st century, some websites, including the search engine Google, changed online advertising by personalizing ads based on web browsing behavior. This has led to other similar efforts and an increase in interactive advertising.[citation needed]\n\nThe share of advertising spending relative to GDP has changed little across large changes in media since 1925. In 1925, the main advertising media in America were newspapers, magazines, signs on streetcars, and outdoor posters. Advertising spending as a share of GDP was about 2.9 percent. By 1998, television and radio had become major advertising media. Nonetheless, advertising spending as a share of GDP was slightly lower – about 2.4 percent.[31]\n\nGuerrilla marketing involves unusual approaches such as staged encounters in public places, giveaways of products such as cars that are covered with brand messages, and interactive advertising where the viewer can respond to become part of the advertising message. This type of advertising is unpredictable, which causes consumers to buy the product or idea.[citation needed] This reflects an increasing trend of interactive and \"embedded\" ads, such as via product placement, having consumers vote through text messages, and various campaigns utilizing social network services such as Facebook or Twitter.[citation needed]\n\nThe advertising business model has also been adapted in recent years.[when?] In media for equity, advertising is not sold, but provided to start-up companies in return for equity. If the company grows and is sold, the media companies receive cash for their shares.\n\nDomain name registrants (usually those who register and renew domains as an investment) sometimes \"park\" their domains and allow advertising companies to place ads on their sites in return for per-click payments.[32] These ads are typically driven by pay per click search engines like Google or Yahoo, but ads can sometimes be placed directly on targeted domain names through a domain lease or by making contact with the registrant of a domain name that describes a product.[33] Domain name registrants are generally easy to identify through WHOIS records that are publicly available at registrar websites.[34]\nClassification\nFile:The Impact Of Wikipedia.webmPlay media\nAn advertisement for the Wikimedia Foundation.\nAn advertisement for a diner. Such signs are common on storefronts.\nPaying people to hold signs is one of the oldest forms of advertising, as with this human billboard.\nA bus with an advertisement for GAP in Singapore. Buses and other vehicles are popular media for advertisers.\nMobile Billboard in East Coast Park, Singapore.\nA DBAG Class 101 with UNICEF ads at Ingolstadt main railway station.\nA London Bus, with a film advertisement along its side.\nHot air balloon displays advertising for GEO magazine\n\nAdvertising may be categorized in a variety of ways, including by style, target audience, geographic scope, medium, or purpose.[2]:9–15 For example, in print advertising, classification by style can include display advertising (ads with design elements sold by size) vs. classified advertising (ads without design elements sold by the word or line). Advertising may be local, national or global. An ad campaign may be directed toward consumers or to businesses. The purpose of an ad may be to raise awareness (brand advertising), or to elicit an immediate sale (direct response advertising).\nTypes of media\n\nVirtually any medium can be used for advertising. Commercial advertising media can include wall paintings, billboards, street furniture components, printed flyers and rack cards, radio, cinema and television adverts, web banners, mobile telephone screens, shopping carts, web popups, skywriting, bus stop benches, human billboards and forehead advertising, magazines, newspapers, town criers, sides of buses, banners attached to or sides of airplanes (\"logojets\"), in-flight advertisements on seatback tray tables or overhead storage bins, taxicab doors, roof mounts and passenger screens, musical stage shows, subway platforms and trains, elastic bands on disposable diapers, doors of bathroom stalls, stickers on apples in supermarkets, shopping cart handles (grabertising), the opening section of streaming audio and video, posters, and the backs of event tickets and supermarket receipts. Any place an \"identified\" sponsor pays to deliver their message through a medium is advertising.[citation needed]\nShare of global adspend[35] medium \t2015 \t2018\nTelevision advertisement \t37.7% \t34.8%\nDesktop online advertising \t19.9% \t18.2%\nMobile advertising \t9.2% \t18.4%\nNewspaper#Advertising \t12.8% \t10.1%\nMagazines \t6.5% \t5.3%\nOutdoor advertising \t6.8% \t6.6%\nRadio advertisement \t6.5% \t5.9%\nCinema \t0.6% \t0.7%\n\nTelevision\n    Television advertising is one of the most expensive types of advertising; networks charge large amounts for commercial airtime during popular events. The annual Super Bowl football game in the United States is known as the most prominent advertising event on television - with an audience of over 108 million and studies showing that 50% of those only tuned in to see the advertisements.[36][37] The average cost of a single thirty-second television spot during this game reached US$4 million & a 60-second spot double that figure in 2014.[36] Virtual advertisements may be inserted into regular programming through computer graphics. It is typically inserted into otherwise blank backdrops[38] or used to replace local billboards that are not relevant to the remote broadcast audience.[39] More controversially, virtual billboards may be inserted into the background[40] where none exist in real-life. This technique is especially used in televised sporting events.[41][42] Virtual product placement is also possible.[43][44] An infomercial is a long-format television commercial, typically five minutes or longer. The word \"infomercial\" is a portmanteau of the words \"information\" and \"commercial\". The main objective in an infomercial is to create an impulse purchase, so that the target sees the presentation and then immediately buys the product through the advertised toll-free telephone number or website. Infomercials describe, display, and often demonstrate products and their features, and commonly have testimonials from customers and industry professionals.[citation needed]\n\nRadio\n    Radio advertisements are broadcast as radio waves to the air from a transmitter to an antenna and a thus to a receiving device. Airtime is purchased from a station or network in exchange for airing the commercials. While radio has the limitation of being restricted to sound, proponents of radio advertising often cite this as an advantage. Radio is an expanding medium that can be found on air, and also online. According to Arbitron, radio has approximately 241.6 million weekly listeners, or more than 93 percent of the U.S. population.[citation needed]\n\nOnline\n    Online advertising is a form of promotion that uses the Internet and World Wide Web for the expressed purpose of delivering marketing messages to attract customers. Online ads are delivered by an ad server. Examples of online advertising include contextual ads that appear on search engine results pages, banner ads, in pay per click text ads, rich media ads, Social network advertising, online classified advertising, advertising networks and e-mail marketing, including e-mail spam.[citation needed] A newer form of online advertising is Native Ads; they go in a website's news feed and are supposed to improve user experience by being less intrusive. However, some people argue this practice is deceptive.[45]\n\nDomain names\n    Domain name advertising is most commonly done through pay per click web search engines, however, advertisers often lease space directly on domain names that generically describe their products.[33] When an Internet user visits a website by typing a domain name directly into their web browser, this is known as \"direct navigation\", or \"type in\" web traffic. Although many Internet users search for ideas and products using search engines and mobile phones, a large number of users around the world still use the address bar. They will type a keyword into the address bar such as \"geraniums\" and add \".com\" to the end of it. Sometimes they will do the same with \".org\" or a country-code Top Level Domain (TLD such as \".co.uk\" for the United Kingdom or \".ca\" for Canada). When Internet users type in a generic keyword and add .com or another top-level domain (TLD) ending, it produces a targeted sales lead.[46] Domain name advertising was originally developed by Oingo (later known as Applied Semantics), one of Google's early acquisitions.[47]\n\nProduct placements\n    Covert advertising is when a product or brand is embedded in entertainment and media. For example, in a film, the main character can use an item or other of a definite brand, as in the movie Minority Report, where Tom Cruise's character John Anderton owns a phone with the Nokia logo clearly written in the top corner, or his watch engraved with the Bulgari logo. Another example of advertising in film is in I, Robot, where main character played by Will Smith mentions his Converse shoes several times, calling them \"classics\", because the film is set far in the future. I, Robot and Spaceballs also showcase futuristic cars with the Audi and Mercedes-Benz logos clearly displayed on the front of the vehicles. Cadillac chose to advertise in the movie The Matrix Reloaded, which as a result contained many scenes in which Cadillac cars were used. Similarly, product placement for Omega Watches, Ford, VAIO, BMW and Aston Martin cars are featured in recent James Bond films, most notably Casino Royale. In \"Fantastic Four: Rise of the Silver Surfer\", the main transport vehicle shows a large Dodge logo on the front. Blade Runner includes some of the most obvious product placement; the whole film stops to show a Coca-Cola billboard.[citation needed]\n\nPrint\n    Print advertising describes advertising in a printed medium such as a newspaper, magazine, or trade journal. This encompasses everything from media with a very broad readership base, such as a major national newspaper or magazine, to more narrowly targeted media such as local newspapers and trade journals on very specialized topics. One form of print advertising is classified advertising, which allows private individuals or companies to purchase a small, narrowly targeted ad paid by the word or line. Another form of print advertising is the display ad, which is generally a larger ad with design elements that typically run in an article section of a newspaper.[2]:14\n\nOutdoor\n    Billboards are large structures located in public places which display advertisements to passing pedestrians and motorists. Most often, they are located on main roads with a large amount of passing motor and pedestrian traffic; however, they can be placed in any location with large amounts of viewers, such as on mass transit vehicles and in stations, in shopping malls or office buildings, and in stadiums.[48] The form known as street advertising first came to prominence in the UK by Street Advertising Services to create outdoor advertising on street furniture and pavements. Working with products such as Reverse Graffiti, air dancers and 3D pavement advertising, for getting brand messages out into public spaces.[citation needed] Sheltered outdoor advertising combines outdoor with indoor advertisement by placing large mobile, structures (tents) in public places on temporary bases. The large outer advertising space aims to exert a strong pull on the observer, the product is promoted indoors, where the creative decor can intensify the impression.[citation needed] Mobile billboards are generally vehicle mounted billboards or digital screens. These can be on dedicated vehicles built solely for carrying advertisements along routes preselected by clients, they can also be specially equipped cargo trucks or, in some cases, large banners strewn from planes. The billboards are often lighted; some being backlit, and others employing spotlights. Some billboard displays are static, while others change; for example, continuously or periodically rotating among a set of advertisements. Mobile displays are used for various situations in metropolitan areas throughout the world, including: target advertising, one-day and long-term campaigns, conventions, sporting events, store openings and similar promotional events, and big advertisements from smaller companies.[citation needed]\n\nThe RedEye newspaper advertised to its target market at North Avenue Beach with a sailboat billboard on Lake Michigan.\n\nPoint-of-sale\n    In-store advertising is any advertisement placed in a retail store. It includes placement of a product in visible locations in a store, such as at eye level, at the ends of aisles and near checkout counters (a.k.a. POP – point of purchase display), eye-catching displays promoting a specific product, and advertisements in such places as shopping carts and in-store video displays.[citation needed]\n\nNovelties\n    Advertising printed on small tangible items such as coffee mugs, T-shirts, pens, bags, and such is known as novelty advertising. Some printers specialize in printing novelty items, which can then be distributed directly by the advertiser, or items may be distriubed as part of a cross promotion, such as ads on fast food containers.\n\nCelebrity branding\n    This type of advertising focuses upon using celebrity power, fame, money, popularity to gain recognition for their products and promote specific stores or products. Advertisers often advertise their products, for example, when celebrities share their favorite products or wear clothes by specific brands or designers. Celebrities are often involved in advertising campaigns such as television or print adverts to advertise specific or general products. The use of celebrities to endorse a brand can have its downsides, however; one mistake by a celebrity can be detrimental to the public relations of a brand. For example, following his performance of eight gold medals at the 2008 Olympic Games in Beijing, China, swimmer Michael Phelps' contract with Kellogg's was terminated, as Kellogg's did not want to associate with him after he was photographed smoking marijuana.[citation needed] Celebrities such as Britney Spears have advertised for multiple products including Pepsi, Candies from Kohl's, Twister, NASCAR, and Toyota.[citation needed]\n\nAerial\n    Using aircraft, balloons or airships to create or display advertising media. Skywriting is a notable example.[citation needed]\n\nAn Allegiant Air aircraft in the special Blue Man Group livery.\nPurposes\n\nAdvertising is at the front of delivering the proper message to customers and prospective customers. The purpose of advertising is to convince customers that a company's services or products are the best, enhance the image of the company, point out and create a need for products or services, demonstrate new uses for established products, announce new products and programs, reinforce the salespeople's individual messages, draw customers to the business, and to hold existing customers.[49]\nSales promotions and brand loyalty\n\nSales promotions are another way to advertise. Sales promotions are double purposed because they are used to gather information about what type of customers one draws in and where they are, and to jump start sales. Sales promotions include things like contests and games, sweepstakes, product giveaways, samples coupons, loyalty programs, and discounts. The ultimate goal of sales promotions is to stimulate potential customers to action.[50]\n\nOne way to create brand loyalty is to reward consumers for spending time interacting with the brand.[51][original research?] This method may come in many forms like rewards card, rewards programs and sampling.\nMedia and advertising approaches\n\tThis section possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (April 2012) (Learn how and when to remove this template message)\n\nIncreasingly, other media are overtaking many of the \"traditional\" media such as television, radio and newspaper because of a shift toward the usage of the Internet for news and music as well as devices like digital video recorders (DVRs) such as TiVo.[52]\n\nOnline advertising began with unsolicited bulk e-mail advertising known as \"e-mail spam\". Spam has been a problem for e-mail users since 1978.[53] As new online communication channels became available, advertising followed. The first banner ad appeared on the World Wide Web in 1994.[54] Prices of Web-based advertising space are dependent on the \"relevance\" of the surrounding web content and the traffic that the website receives.[citation needed]\n\nIn online display advertising, display ads generate awareness quickly. Unlike search, which requires someone to be aware of a need, display advertising can drive awareness of something new and without previous knowledge. Display works well for direct response. Display is not only used for generating awareness, it's used for direct response campaigns that link to a landing page with a clear 'call to action'.[citation needed]\n\nAs the mobile phone became a new mass medium in 1998 when the first paid downloadable content appeared on mobile phones in Finland,[55][citation needed] mobile advertising followed, also first launched in Finland in 2000.[citation needed] By 2007 the value of mobile advertising had reached $2 billion and providers such as Admob delivered billions of mobile ads.[citation needed]\n\nMore advanced mobile ads include banner ads, coupons, Multimedia Messaging Service picture and video messages, advergames and various engagement marketing campaigns. A particular feature driving mobile ads is the 2D barcode, which replaces the need to do any typing of web addresses, and uses the camera feature of modern phones to gain immediate access to web content. 83 percent of Japanese mobile phone users already are active users of 2D barcodes.[citation needed]\n\nSome companies have proposed placing messages or corporate logos on the side of booster rockets and the International Space Station.[citation needed]\n\nUnpaid advertising (also called \"publicity advertising\"), can include personal recommendations (\"bring a friend\", \"sell it\"), spreading buzz, or achieving the feat of equating a brand with a common noun (in the United States, \"Xerox\" = \"photocopier\", \"Kleenex\" = tissue, \"Vaseline\" = petroleum jelly, \"Hoover\" = vacuum cleaner, and \"Band-Aid\" = adhesive bandage). However, some companies[which?] oppose the use of their brand name to label an object. Equating a brand with a common noun also risks turning that brand into a generic trademark – turning it into a generic term which means that its legal protection as a trademark is lost.[56]\n\nFrom time to time, The CW Television Network airs short programming breaks called \"Content Wraps\", to advertise one company's product during an entire commercial break. The CW pioneered \"content wraps\" and some products featured were Herbal Essences, Crest, Guitar Hero II, CoverGirl, and recently Toyota.[citation needed]\n\nA new promotion concept has appeared, \"ARvertising\", advertising on Augmented Reality technology.[57]\n\nControversy exists on the effectiveness of subliminal advertising (see mind control), and the pervasiveness of mass messages (see propaganda).\nRise in new media\nUS Newspaper Advertising Revenue\nNewspaper Association of America published data [58]\n\nWith the Internet came many new advertising opportunities. Popup, Flash, banner, Popunder, advergaming, and email advertisements (all of which are often unwanted or spam in the case of email) are now commonplace. Particularly since the rise of \"entertaining\" advertising, some people may like an advertisement enough to wish to watch it later or show a friend.[citation needed] In general, the advertising community has not yet made this easy, although some have used the Internet to widely distribute their ads to anyone willing to see or hear them. In the last three-quarters of 2009 mobile and internet advertising grew by 18% and 9% respectively. Older media advertising saw declines: −10.1% (TV), −11.7% (radio), −14.8% (magazines) and −18.7% (newspapers).[citation needed]\nNiche marketing\n\nAnother significant trend regarding future of advertising is the growing importance of the niche market using niche or targeted ads. Also brought about by the Internet and the theory of The Long Tail, advertisers will have an increasing ability to reach specific audiences. In the past, the most efficient way to deliver a message was to blanket the largest mass market audience possible.[citation needed] However, usage tracking, customer profiles and the growing popularity of niche content brought about by everything from blogs to social networking sites, provide advertisers with audiences that are smaller but much better defined,[citation needed] leading to ads that are more relevant to viewers and more effective for companies' marketing products. Among others, Comcast Spotlight is one such advertiser employing this method in their video on demand menus. These advertisements are targeted to a specific group and can be viewed by anyone wishing to find out more about a particular business or practice, from their home. This causes the viewer to become proactive and actually choose what advertisements they want to view.[59]\n\nGoogle AdSense is an example of niche marketing. Google calculates the primary purpose of a website and adjusts ads accordingly; it uses key words on the page (or even in emails) to find the general ideas of topics disused and places ads that will most likely be clicked on by viewers of the email account or website visitors.[60]\nCrowdsourcing\nMain article: Crowdsourcing\n\nThe concept of crowdsourcing has given way to the trend of user-generated advertisements. User-generated ads are created by people, as opposed to an advertising agency or the company themselves, often resulting from brand sponsored advertising competitions. For the 2007 Super Bowl, the Frito-Lays division of PepsiCo held the Crash the Super Bowl contest, allowing people to create their own Doritos commercial.[61] Chevrolet held a similar competition for their Tahoe line of SUVs.[61] Due to the success of the Doritos user-generated ads in the 2007 Super Bowl, Frito-Lays relaunched the competition for the 2009 and 2010 Super Bowl. The resulting ads were among the most-watched and most-liked Super Bowl ads. In fact, the winning ad that aired in the 2009 Super Bowl was ranked by the USA Today Super Bowl Ad Meter as the top ad for the year while the winning ads that aired in the 2010 Super Bowl were found by Nielsen's BuzzMetrics to be the \"most buzzed-about\".[62][63] Another example of companies using crowdsourcing successfully is the beverage company Jones Soda that encourages consumers to participate in the label design themselves.\n\nThis trend has given rise to several online platforms that host user-generated advertising competitions on behalf of a company. Founded in 2007, Zooppa has launched ad competitions for brands such as Google, Nike, Hershey's, General Mills, Microsoft, NBC Universal, Zinio, and Mini Cooper.[citation needed] Crowdsourced remains controversial, as the long-term impact on the advertising industry is still unclear.[64]\nGlobal advertising\nMain article: Global marketing\n\nAdvertising has gone through five major stages of development: domestic, export, international, multi-national, and global. For global advertisers, there are four, potentially competing, business objectives that must be balanced when developing worldwide advertising: building a brand while speaking with one voice, developing economies of scale in the creative process, maximising local effectiveness of ads, and increasing the company's speed of implementation. Born from the evolutionary stages of global marketing are the three primary and fundamentally different approaches to the development of global advertising executions: exporting executions, producing local executions, and importing ideas that travel.[65]\n\nAdvertising research is key to determining the success of an ad in any country or region. The ability to identify which elements and/or moments of an ad contribute to its success is how economies of scale are maximized. Once one knows what works in an ad, that idea or ideas can be imported by any other market. Market research measures, such as Flow of Attention, Flow of Emotion and branding moments provide insight into what is working in an ad in any country or region because the measures are based on the visual, not verbal, elements of the ad.[66]\nForeign public messaging\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2014) (Learn how and when to remove this template message)\n\tThis section possibly contains original research. Please improve it by verifying the claims made and adding inline citations. Statements consisting only of original research should be removed. (December 2014) (Learn how and when to remove this template message)\nSee also: Soft power and International tourism advertising\n\nForeign governments,[which?] particularly those that own marketable commercial products or services, often promote their interests and positions through the advertising of those goods because the target audience is not only largely unaware of the forum as a vehicle for foreign messaging but also willing to receive the message while in a mental state of absorbing information from advertisements during television commercial breaks, while reading a periodical, or while passing by billboards in public spaces. A prime example of this messaging technique is advertising campaigns to promote international travel. While advertising foreign destinations and services may stem from the typical goal of increasing revenue by drawing more tourism, some travel campaigns carry the additional or alternative intended purpose of promoting good sentiments or improving existing ones among the target audience towards a given nation or region. It is common for advertising promoting foreign countries to be produced and distributed by the tourism ministries of those countries, so these ads often carry political statements and/or depictions of the foreign government's desired international public perception. Additionally, a wide range of foreign airlines and travel-related services which advertise separately from the destinations, themselves, are owned by their respective governments; examples include, though are not limited to, the Emirates airline (Dubai), Singapore Airlines (Singapore), Qatar Airways (Qatar), China Airlines (Taiwan/Republic of China), and Air China (People's Republic of China). By depicting their destinations, airlines, and other services in a favorable and pleasant light, countries market themselves to populations abroad in a manner that could mitigate prior public impressions.\nDiversification\n\nIn the realm of advertising agencies, continued industry diversification has seen observers note that \"big global clients don't need big global agencies any more\".[67] This is reflected by the growth of non-traditional agencies in various global markets, such as Canadian business TAXI and SMART in Australia and has been referred to as \"a revolution in the ad world\".[68]\nNew technology\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2014) (Learn how and when to remove this template message)\n\nThe ability to record shows on digital video recorders (such as TiVo) allow watchers to record the programs for later viewing, enabling them to fast forward through commercials. Additionally, as more seasons of pre-recorded box sets are offered for sale of television programs; fewer people watch the shows on TV. However, the fact that these sets are sold, means the company will receive additional profits from these sets.\n\nTo counter this effect, a variety of strategies have been employed. Many advertisers have opted for product placement on TV shows like Survivor. Other strategies include integrating advertising with internet-connected EPGs, advertising on companion devices (like smartphones and tablets) during the show, and creating TV apps. Additionally, some like brands have opted for social television sponsorship.[69]\nAdvertising education\n\nAdvertising education has become popular with bachelor, master and doctorate degrees becoming available in the emphasis.[citation needed] A surge in advertising interest is typically attributed to the strong relationship advertising plays in cultural and technological changes, such as the advance of online social networking.[citation needed] A unique model for teaching advertising is the student-run advertising agency, where advertising students create campaigns for real companies.[70] Organizations such as the American Advertising Federation establish companies with students to create these campaigns.[citation needed]\nCriticisms\nMain article: Criticism of advertising\n\nWhile advertising can be seen as necessary for economic growth,[22] it is not without social costs. Unsolicited commercial e-mail and other forms of spam have become so prevalent as to have become a major nuisance to users of these services, as well as being a financial burden on internet service providers.[71] Advertising is increasingly invading public spaces, such as schools, which some critics argue is a form of child exploitation.[72]\n\nOne of the most controversial criticisms of advertisement in the present day is that of the predominance of advertising of foods high in sugar, fat, and salt specifically to children. Critics claim that food advertisements targeting children are exploitive and are not sufficiently balanced with proper nutritional education to help children understand the consequences of their food choices. Additionally, children may not understand that they are being sold something, and are therefore more impressionable.[73][better source needed] Michelle Obama has criticized large food companies for advertising unhealthy foods largely towards children and has requested that food companies either limit their advertising to children or advertise foods that are more in line with dietary guidelines.[74] The other criticisms include the change that are brought by those advertisements on the society and also the deceiving ads that are aired and published by the corporations.Cosmetic and health industry are the ones which exploited the highest and created reasons of concern.[75]\nRegulation\nMain article: Advertising regulation\n\nThere have been increasing efforts to protect the public interest by regulating the content and the influence of advertising. Some examples include restrictions for advertising alcohol, tobacco or gambling imposed in many countries, as well as the bans around advertising to children, which exist in parts of Europe. Advertising regulation focuses heavily on the veracity of the claims and as such, there are often tighter restrictions placed around advertisements for food and healthcare products.[76]\n\nThe advertising industries within some countries rely less on laws and more on systems of self-regulation.[77][78][79] Advertisers and the media agree on a code of advertising standards that they attempt to uphold. The general aim of such codes is to ensure that any advertising is 'legal, decent, honest and truthful'. Some self-regulatory organizations are funded by the industry, but remain independent, with the intent of upholding the standards or codes like the Advertising Standards Authority in the UK.[80]\n\nIn the UK, most forms of outdoor advertising such as the display of billboards is regulated by the UK Town and County Planning system. Currently, the display of an advertisement without consent from the Planning Authority is a criminal offense liable to a fine of £2,500 per offense.[81] In the US, many communities believe that many forms of outdoor advertising blight the public realm.[82] As long ago as the 1960s in the US there were attempts to ban billboard advertising in the open countryside.[83] Cities such as São Paulo have introduced an outright ban[84] with London also having specific legislation to control unlawful displays.\n\nSome governments restrict the languages that can be used in advertisements, but advertisers may employ tricks to try avoiding them. In France for instance, advertisers sometimes print English words in bold and French translations in fine print to deal with Article 120 of the 1994 Toubon Law limiting the use of English).[85]\n\nThe advertising of pricing information is another topic of concern for governments. In the United States for instance, it is common for businesses to only mention the existence and amount of applicable taxes at a later stage of a transaction.[86] In Canada and New Zealand, taxes can be listed as separate items, as long as they are quoted up-front.[87][88] In most other countries, the advertised price must include all applicable taxes, enabling customers to easily know how much it will cost them.[89][90][91]\nTheory\nHierarchy-of-effects models\n\nVarious competing models of hierarchies of effects attempt to provide a theoretical underpinning to advertising practice.[92]\n\n    The model of Clow and Baack[93] clarifies the objectives of an advertising campaign and for each individual advertisement. The model postulates six steps a buyer moves through when making a purchase:\n        Awareness\n        Knowledge\n        Liking\n        Preference\n        Conviction\n        Purchase\n    Means-End Theory suggests that an advertisement should contain a message or means that leads the consumer to a desired end-state.\n    Leverage Points aim to move the consumer from understanding a product's benefits to linking those benefits with personal values.\n\nMarketing mix\nMain article: Marketing mix\n\nThe marketing mix was proposed by professor E. Jerome McCarthy in the 1960s.[94] It consists of four basic elements called the \"four Ps\". Product is the first P representing the actual product. Price represents the process of determining the value of a product. Place represents the variables of getting the product to the consumer such as distribution channels, market coverage and movement organization. The last P stands for Promotion which is the process of reaching the target market and convincing them to buy the product.\n\nIn the 1990s, the concept of four Cs was introduced as a more customer-driven replacement of four P's.[95] There are two theories based on four Cs: Lauterborn's four Cs (consumer, cost, communication, convenience) [96] and Shimizu's four Cs (commodity, cost, communication, channel) in the 7Cs Compass Model (Co-marketing). Communications can include advertising, sales promotion, public relations, publicity, personal selling, corporate identity, internal communication, SNS, MIS.[97][98][99][100]\nAdvertising research\nMain article: Advertising research\n\nAdvertising research is a specialized form of research that works to improve the effectiveness and efficiency of advertising. It entails numerous forms of research which employ different methodologies. Advertising research includes pre-testing (also known as copy testing) and post-testing of ads and/or campaigns – pre-testing is done before an ad airs to gauge how well it will perform and post-testing is done after an ad airs to determine the in-market impact of the ad or campaign. Continuous ad tracking and the Communicus System are competing examples of post-testing advertising research types.[101]\nSemiotics\nMain article: Advertising research\n\nMeanings between consumers and marketers depict signs and symbols that are encoded in everyday objects.[102][need quotation to verify] Semiotics is the study of signs and how they are interpreted. Advertising has many hidden signs and meanings[citation needed] within brand names, logos, package designs, print advertisements, and television advertisements. Semiotics aims to study and interpret the message being conveyed in (for example) advertisements. Logos and advertisements can be interpreted at two levels - known as the surface level and the underlying level. The surface level uses signs creatively to create an image or personality for a product.[citation needed] These signs can be images, words, fonts, colors, or slogans. The underlying level is made up of hidden meanings. The combination of images, words, colors, and slogans must be interpreted by the audience or consumer.[103] The \"key to advertising analysis\" is the signifier and the signified. The signifier is the object and the signified is the mental concept.[104] A product has a signifier and a signified. The signifier is the color, brand name, logo design, and technology. The signified has two meanings known as denotative and connotative. The denotative meaning is the meaning of the product. A television's denotative meaning might be that it is high definition. The connotative meaning is the product's deep and hidden meaning. A connotative meaning of a television would be that it is top-of-the-line.[105]\n\nApple's commercials[when?] used a black silhouette of a person that was the age of Apple's target market. They placed the silhouette in front of a blue screen so that the picture behind the silhouette could be constantly changing. However, the one thing that stays the same in these ads is that there is music in the background and the silhouette is listening to that music on a white iPod through white headphones. Through advertising, the white color on a set of earphones now signifies that the music device is an iPod. The white color signifies almost all of Apple's products.[106]\n\nThe semiotics of gender plays a key influence on the way in which signs are interpreted. When considering gender roles in advertising, individuals are influenced by three categories. Certain characteristics of stimuli may enhance or decrease the elaboration of the message (if the product is perceived as feminine or masculine). Second, the characteristics of individuals can affect attention and elaboration of the message (traditional or non-traditional gender role orientation). Lastly, situational factors may be important to influence the elaboration of the message.[107]\n\nThere are two types of marketing communication claims-objective and subjective.[108] Objective claims stem from the extent to which the claim associates the brand with a tangible product or service feature. For instance, a camera may have auto-focus features. Subjective claims convey emotional, subjective, impressions of intangible aspects of a product or service. They are non-physical features of a product or service that cannot be directly perceived, as they have no physical reality. For instance the brochure has a beautiful design.[109] Males tend to respond better to objective marketing-communications claims while females tend to respond better to subjective marketing communications claims.[110]\n\nVoiceovers are commonly used in advertising. Most voiceovers are done by men, with figures of up to 94% having been reported.[111] There have been more female voiceovers in recent years[when?], but mainly for food, household products, and feminine-care products.[112]\nGender effects in the processing of advertising\n\nAccording to a 1977 study by David Statt, females process information comprehensively, while males process information through heuristic devices such as procedures, methods or strategies for solving problems, which could have an effect on how they interpret advertising.[113] According to this study, men prefer to have available and apparent cues to interpret the message where females engage in more creative, associative, imagery-laced interpretation. Later research by a Danish team[114] found that advertising attempts to persuade men to improve their appearance or performance, whereas its approach to women is aimed at transformation toward an impossible ideal of female presentation. Advertising's manipulation of women's aspiration to these ideal types, as they are portrayed in film, in erotic art, in advertising, on stage, music video, and other media exposures, requires at least a conditioned rejection of female reality, and thereby takes on a highly ideological cast. Not everyone agrees: one critic viewed this monologic, gender-specific interpretation of advertising as excessively skewed and politicized.[115]\n\nMore recently, research by Martin (2003) reveals that males and females differ in how they react to advertising depending on their mood at the time of exposure to the ads, and the affective tone of the advertising. When feeling sad, males prefer happy ads to boost their mood. In contrast, females prefer happy ads when they are feeling happy. The television programs in which the ads are embedded are shown to influence a viewer's mood state.[116]\nSee also\n\n    iconBusiness and economics portal \n\n    Advertising in biology\n    Advertisements in schools\n    Advertorial\n    Bibliography of advertising\n    Branded content\n    Co-marketing\n    Comparative advertising\n    Conquesting\n    Copywriting\n    Demo mode\n    Family in advertising\n    Graphic design\n    History of advertising\n    History of advertising in Britain\n    History of Advertising Trust\n    Informative advertising\n    Integrated marketing communications\n    Local advertising\n    Marketing Mix\n    Market overhang\n    Meta-advertising\n    Mobile marketing\n    Museum of Brands, Packaging and Advertising\n    Performance-based advertising\n    Scad (fraud)\n    Senior media creative\n    Shock advertising\n    Television advertisement\n    Tobacco advertising\n    Trade literature\n    Video commerce\n    Viral marketing\n    World Federation of Advertisers", "skillName": "Advertising."}
{"id": 129, "category": "Advertising", "skillText": "Mobile marketing is marketing on or with a mobile device, such as a smart phone.[1] Mobile marketing can provide customers with time and location sensitive, personalized information that promotes goods, services and ideas.[2] In a more theoretical manner, academic Andreas Kaplan defines mobile marketing as \"any marketing activity conducted through a ubiquitous network to which consumers are constantly connected using a personal mobile device\".[3]\n\nContents\n\n    1 SMS marketing\n    2 MMS\n    3 Push notifications\n    4 App-based marketing\n    5 In-game mobile marketing\n    6 QR codes\n    7 Bluetooth\n    8 Proximity systems\n    9 Location-based services\n    10 Ringless voice mail\n    11 User-controlled media\n    12 Privacy concerns in mobile marketing\n    13 Classification of mobile marketing\n    14 References\n\nSMS marketing\n\nMarketing through cellphones' SMS (Short Message Service) became increasingly popular in the early 2000s in Europe and some parts of Asia when businesses started to collect mobile phone numbers and send off wanted (or unwanted) content. On average, SMS messages are read within four minutes, making them highly convertible.\n\nOver the past few years SMS marketing has become a legitimate advertising channel in some parts of the world. This is because unlike email over the public internet, the carriers who police their own networks have set guidelines and best practices for the mobile media industry (including mobile advertising). The IAB (Interactive Advertising Bureau) and the Mobile Marketing Association (MMA), as well, have established guidelines and are evangelizing the use of the mobile channel for marketers. While this has been fruitful in developed regions such as North America, Western Europe and some other countries, mobile SPAM messages (SMS sent to mobile subscribers without a legitimate and explicit opt-in by the subscriber) remain an issue in many other parts of the world, partly due to the carriers selling their member databases to third parties. In India, however, government's efforts of creating National Do Not Call Registry have helped cellphone users to stop SMS advertisements by sending a simple SMS or calling 1909.[4][5]\n\nMobile marketing via SMS has expanded rapidly in Europe and Asia as a new channel to reach the consumer. SMS initially received negative media coverage in many parts of Europe for being a new form of spam as some advertisers purchased lists and sent unsolicited content to consumer's phones; however, as guidelines are put in place by the mobile operators, SMS has become the most popular branch of the Mobile Marketing industry with several 100 million advertising SMS sent out every month in Europe alone.\n\nIn Europe the first cross-carrier SMS shortcode campaign was run by Txtbomb in 2001 for an Island Records release, In North America it was the Labatt Brewing Company in 2002. Over the past few years mobile short codes have been increasingly popular as a new channel to communicate to the mobile consumer. Brands have begun to treat the mobile short code as a mobile domain name allowing the consumer to text message the brand at an event, in store and off any traditional media.\n\nSMS marketing services typically run off a short code, but sending text messages to an email address is another methodology (though this method is not supported by the carriers). Short codes are 5 or 6 digit numbers that have been assigned by all the mobile operators in a given country for the use of brand campaign and other consumer services. Due to the high price of short codes of $500–$1000 a month, many small businesses opt to share a short code in order to reduce monthly costs. The mobile operators vet every short code application before provisioning and monitor the service to make sure it does not diverge from its original service description. Another alternative to sending messages by short code or email is to do so through one's own dedicated phone number.\n\nBesides short codes, inbound SMS can be received on long numbers (international number format, e.g. +44 7624 805000 or US number format,[6] e.g. 757 772 8555), which can be used in place of short codes or premium-rated short messages for SMS reception in several applications, such as product promotions and campaigns. Long numbers are internationally available, as well as enabling businesses to have their own number, rather than short codes which are usually shared across a number of brands. Additionally, long numbers are non-premium inbound numbers.\n\nOne key criterion for provisioning is that the consumer opts into the service. The mobile operators demand a double opt in from the consumer and the ability for the consumer to opt out of the service at any time by sending the word STOP via SMS. These guidelines are established in the CTIA Playbook and the MMA Consumer Best Practices Guidelines[7] which are followed by all mobile marketers in the United States. In Canada, opt in will be mandatory once the Fighting Internet and Wireless Spam Act comes in force in mid-2012.\nMMS\n\nMMS mobile marketing can contain a timed slideshow of images, text, audio and video. This mobile content is delivered via MMS (Multimedia Message Service). Nearly all new phones produced with a color screen are capable of sending and receiving standard MMS message. Brands are able to both send (mobile terminated) and receive (mobile originated) rich content through MMS A2P (application-to-person) mobile networks to mobile subscribers. In some networks, brands are also able to sponsor messages that are sent P2P (person-to-person).\n\nGood examples of mobile-originated MMS marketing campaigns are Motorola's ongoing campaigns at House of Blues venues, where the brand allows the consumer to send their mobile photos to the LED board in real-time as well as blog their images online.\nPush notifications\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2013) (Learn how and when to remove this template message)\n\nPush notifications were first introduced to smartphones by Apple with the Push Notification Service in 2009.[8] For Android devices, Google developed Android Cloud to Messaging or C2DM in 2010. Google replaced this service with Google Cloud Messaging in 2013.[9] Commonly referred to as GCM, Google Cloud Messaging served as C2DM's successor, making improvements to authentication and delivery, new API endpoints and messaging parameters, and the removal of limitations on API send-rates and message sizes. It is a message that pops up on a mobile device. It is the delivery of information from a software application to a computing device without any request from the client or the user. They look like SMS notifications but they are reached only the users who installed the app. The specifications vary for iOS and android users. SMS and push notifications can be part of a well-developed inbound mobile marketing strategy.\n\nAccording to mobile marketing company Leanplum, Android sees open rates twice as high as those on iOS. Android sees open rates of 3.48 percent for push notification, versus iOS which has open rates of 1.77 percent.[10]\nApp-based marketing\n\nWith the increasingly widespread use of smartphones, app usage has also greatly increased. Therefore, mobile marketers have increasingly taken advantage of smartphone apps as a marketing resource. Marketers will aim to increase the visibility of an app in a store, which will in turn help in getting more downloads. By optimizing the placement of the app usage, marketers can ensure a significant number of increases in download. This allows for direct engagement, payment, and targeted advertising.[11]\n\nThere is a lot of competition[12] in this field as well. However, just like other services, it is not easy anymore to rule the mobile application market.\n\nThe current wave of progression and growth highly depends upon the wise use of technology and Mobile App Development is one such technology that is benefiting various companies in order to maximize their profits. In the past couple of years the usage of mobile phones has increased at an astonishing rate. Most of the companies have slowly but surely acknowledged the potential that Mobile App possess in order to increase the interaction between a company and its target customers. While planning to invest in Mobile App for your business you must consider all the aspects of it. Always choose a reliable and experienced company to develop customized apps for your business as this would help to increase the chances of success of that App and minimize the chances of any technical glitches that might crop up. You must keep in mind that an App can prove to be a very beneficial element to promote your business on a vast level only if it is developed by efficient service provider.\nIn-game mobile marketing\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2016) (Learn how and when to remove this template message)\n\nThere are essentially three major trends in mobile gaming right now: interactive real-time 3D games, massive multi-player games and social networking games. This means a trend towards more complex and more sophisticated, richer game play. On the other side, there are the so-called casual games, i.e. games that are very simple and very easy to play. Most mobile games today are such casual games and this will probably stay so for quite a while to come.\n\nBrands are now delivering promotional messages within mobile games or sponsoring entire games to drive consumer engagement. This is known as mobile advergaming or ad-funded mobile game.\n\nIn in-game mobile marketing, advertisers pay to have their name or products featured in the mobile games. For instance, racing games can feature real cars made by Ford or Chevy. Advertisers have been both creative and aggressive in their attempts to integrate ads organically in the mobile games.\n\nAlthough investment in mobile marketing strategies like advergaming is slightly more expensive than what is intended for a mobile app, a good strategy can make the brand derive a substantial revenue. Games that use advergaming make the users remember better the brand involved. This memorization increases virality of the content so that the users tend to recommend them to their friends and acquaintances, and share them via social networks.[13]\nQR codes\n\nQR codes allow a customer to visit a web page address by scanning a 2D image with their phone's camera, instead of manually entering a URL. The resultant URLs typically include tracking features which would be unwieldy if typed by the customer. Originally approved as an ISS standard in 1997, Denso-Wave first developed the standard for tracking automobile parts in Japan.[14]\n\nQR codes have been growing in popularity in Asia and Europe, but have been slow to be adopted in North America.[15][16] Some high-profile QR campaigns in the United States have included billboards by Calvin Klein in Times Square, QR codes for every SKU in Home Depot and Best Buy stores, and a scavenger hunt promoting Starbucks and Lady Gaga.\n\nApple Passbook (application), implemented as a native app for iOS6, has employed QR codes as one of the ways that the iPhone (or iPod Touch) users can take a real world action. i.e. scan the Barcode on their Passbook Pass. In addition to QR codes, the Passbook (application) also supports PDF417 and Aztec 2D Barcodes\nBluetooth\n\nBluetooth technology is a global wireless standard enabling, convenient, secure connectivity for an expanding range of devices and services. Created by Ericsson in 1994, Bluetooth wireless technology was originally conceived as a wireless alternative to RS-232 data cables.[17]\n\nThe use of Bluetooth gained traction around 2003 and a few companies in Europe have started establishing successful businesses. Most of these businesses offer \"hotspot\" systems which consist of some kind of content-management system with a Bluetooth distribution function. This technology has the advantages that it is permission-based, has higher transfer speeds and is a radio-based technology and thus can neither be metered nor billed. The likely earliest device built for mobile marketing via Bluetooth was the context tag of the AmbieSense project (2001-2004). More recently Tata Motors conducted one of the biggest Bluetooth marketing campaigns in India for its brand the Sumo Grande and more of such activities have happened for brands like Walt Disney promoting their movie High School Musical.\nProximity systems\n\nMobile marketing via proximity systems, or proximity marketing, relies on GSM 03.41 which defines the Short Message Service - Cell Broadcast.[18] SMS-CB allows messages (such as advertising or public information) to be broadcast to all mobile users in a specified geographical area. In the Philippines, GSM-based proximity broadcast systems are used by select Government Agencies for information dissemination on Government-run community-based programs to take advantage of its reach and popularity (Philippines has the world's highest traffic of SMS). It is also used for commercial service known as Proxima SMS. Bluewater, a super-regional shopping centre in the UK, has a GSM based system supplied by NTL to help its GSM coverage for calls, it also allows each customer with a mobile phone to be tracked though the centre which shops they go into and for how long. The system enables special offer texts to be sent to the phone. For example, a retailer could send a mobile text message to those customers in their database who have opted-in, who happen to be walking in a mall. That message could say \"Save 50% in the next 5 minutes only when you purchase from our store.\" Snacks company, Mondelez International, makers of Cadbury and Oreo products has committed to exploring proximity-based messaging citing significant gains in point-of-purchase influence.[19]\nLocation-based services\n\nLocation-based services (LBS) are offered by some cell phone networks as a way to send custom advertising and other information to cell-phone subscribers based on their current location. The cell-phone service provider gets the location from a GPS chip built into the phone, or using radiolocation and trilateration based on the signal-strength of the closest cell-phone towers (for phones without GPS features). In the United Kingdom, which launched location-based services in 2003, networks do not use trilateration; LBS uses a single base station, with a \"radius\" of inaccuracy, to determine a phone's location.\n\nSome location-based services work without GPS tracking technique, instead transmitting content between devices peer-to-peer.\nRingless voice mail\n\nThe advancement of mobile technologies has allowed the ability to leave a voice mail message on a mobile phone without ringing the line. The technology was pioneered by VoAPP, which used the technology in conjunction with live operators as a debt collection service. The FCC has ruled that the technology is compliant with all regulations.[20] CPL expanded on the existing technology to allow for a completely automated process including the replacement of live operators with pre recorded messages.[21] By optimizing the technology, marketers can utilize the process to increase engagement of their product or service.\nUser-controlled media\n\nMobile marketing differs from most other forms of marketing communication in that it is often user (consumer) initiated (mobile originated, or MO) message, and requires the express consent of the consumer to receive future communications. A call delivered from a server (business) to a user (consumer) is called a mobile terminated (MT) message. This infrastructure points to a trend set by mobile marketing of consumer controlled marketing communications.[22]\n\nDue to the demands for more user controlled media, mobile messaging infrastructure providers have responded by developing architectures that offer applications to operators with more freedom for the users, as opposed to the network-controlled media. Along with these advances to user-controlled Mobile Messaging 2.0, blog events throughout the world have been implemented in order to launch popularity in the latest advances in mobile technology. In June 2007, Airwide Solutions became the official sponsor for the Mobile Messaging 2.0 blog that provides the opinions of many through the discussion of mobility with freedom.[23]\n\nGPS plays an important role in location-based marketing.[citation needed]\nPrivacy concerns in mobile marketing\n\nMobile advertising has become more and more popular. However, some mobile advertising is sent without a required permission from the consumer causing privacy violations. It should be understood that irrespective of how well advertising messages are designed and how many additional possibilities they provide, if consumers do not have confidence that their privacy will be protected, this will hinder their widespread deployment.[24] But if the messages originate from a source where the user is enrolled in a relationship/loyalty program, privacy is not considered violated and even interruptions can generate goodwill.[25]\n\nThe privacy issue became even more salient as it was before with the arrival of mobile data networks. A number of important new concerns emerged mainly stemming from the fact that mobile devices are intimately personal[26] and are always with the user, and four major concerns can be identified: mobile spam, personal identification, location information and wireless security.[27] Aggregate presence of mobile phone users could be tracked in a privacy-preserving fashion.[28]\nClassification of mobile marketing\n\nKaplan categorizes mobile marketing along the degree of consumer knowledge and the trigger of communication into four groups: strangers, groupies, victims, and patrons. Consumer knowledge can be high or low and according to its degree organizations can customize their messages to each individual user, similar to the idea of one-to-one marketing. Regarding the trigger of communication, Kaplan differentiates between push communication, initiated by the organization, and pull communication, initiated by the consumer. Within the first group (low knowledge/push), organizations broadcast a general message to a large number of mobile users. Given that the organization cannot know which customers have ultimately been reached by the message, this group is referred to as “strangers”. Within the second group (low knowledge/pull), customers opt to receive information but do not identify themselves when doing so. The organizations therefore does not know which specific clients it is dealing with exactly, which is why this cohort is called “groupies”. In the third group (high knowledge/push) referred to as “victims”, organizations know their customers and can send them messages and information without first asking permission. The last group (high knowledge/pull), the “patrons” covers situations where customers actively give permission to be contacted and provide personal information about themselves, which allows for one-to-one communication without running the risk of annoying them.[29]", "skillName": "Mobile_marketing."}
{"id": 130, "category": "Advertising", "skillText": "Interactive advertising uses online or offline interactive media to communicate with consumers and to promote products, brands, services, and public service announcements, corporate or political groups.\n\nIn the inaugural issue of the Journal of Interactive Advertising,[1] editors Li and Leckenby (2000) defined interactive advertising as the \"paid and unpaid presentation and promotion of products, services and ideas by an identified sponsor through mediated means involving mutual action between consumers and producers.\" This is most commonly performed through the internet; often through the use of an ad server that can deliver a variety of interactive advertising units.\n\nContents\n\n    1 Interactive advertising objectives\n    2 Advantages of interactive advertising\n    3 Disadvantages of interactive advertising\n    4 Elements of interactive advertising\n    5 User generated/controlled aspects\n    6 Advertiser controlled aspects\n    7 See also\n    8 References\n\nInteractive advertising objectives\n\nThe goals of interactive advertising are usually akin to the traditional objectives of advertising, i.e. to sell a product. This in turn means that many of the traditional elements of advertising impact and effectiveness remain relevant, even within the scope of interactive media. However, according to the Journal of Interactive Advertising 2001, interactive advertising also has some properties that expand the range of potential objectives and that improve advertising effectiveness. Interactive advertising also has the potential to decrease the losses associated with poorly coordinated advertising, to reduce the difficulties commonly encountered in clearly communicating an advertising message and to help overcome new product hurdles.\nAdvantages of interactive advertising\n\n    Interactive advertising allows consumers to interpret advertisements in unique ways and understandings, and sheds light on the increasing significance of the consumer’s role in determining the value of marketing campaigns in modern society.[2]\n    Interactive advertising encourages consumers to actively engage in the marketing communications themselves to input feedback, neglect irrelevant elements, and absorb content that appeals to them.\n    As consumerism becomes more prominent within the global economy and social interactions become more significant in establishing healthy long-term relationships with consumers, interactive advertising also grows in importance because it triggers greater motivation for social interaction between potential consumers and suppliers.[3]\n\nDisadvantages of interactive advertising\n\n    Whilst interactive advertising may be highly appealing to a prepared audience, it is difficult, costly and time-consuming to prepare, especially for target markets that have yet to be properly identified and analyzed.\n    Interactive advertising has greater benefits in industries where creativity helps to capture the attention of buyers. In some markets however, there is little room for this, and excessive use of creativity can become a form of noise that disrupts the conveying of intended messages to consumers[3]\n\nElements of interactive advertising\n\nThere are many different facets to interactive advertising, including varying methods and types. Using many different types of cognitive tools and advert presentations, organizations can enhance the impact of their campaigns with this type of advertising. According to Thorson (1996), all advertisements can be classified into one of five basic categories, including: product/service, public service announcement, issue, corporate and political. Advert types also interact with the user's motives to influence outcomes, or consumer responses, reinforcing the need for Interactive Advertising as a means of persuading potential consumers and target audiences.\n\nUsing the Internet as the main medium for interactive advertising to study the methods, types and outcomes, we can then sound out the different user or advertiser controlled aspects.\nUser generated/controlled aspects\n\nFunctions, Internet motives and mode are the main factors of user controlled aspects. In fact, a number of researchers and practitioners argue that consumers have more control on the Internet than do advertisers (Roehm & Haugtvedt, 1999). Some have gone so far as to argue that interactive marketing and advertising techniques will not work unless practitioners \"step into the shoes\" of and approach the Internet from the consumer's vantage point (Cross & Smith, 1997).\nAdvertiser controlled aspects\n\nVarious aspects of Internet advertising are under the control of the advertiser. Most of these variables include structural elements, such as ad types, formats and features. This does not mean that consumers never control the structure of the interactive ads. Banner Ads, sponsorship, hyperlinks and non-carrier websites are examples of advertiser controlled interactive advertising.\nSee also\n\n    Augmented Reality Advertising\n    Digital marketing\n    Advergaming\n    Immersive advertising\n    In-game advertising\n    Advertainment\n    View-through rate", "skillName": "Interactive_advertising."}
{"id": 131, "category": "Advertising", "skillText": "Meta-advertising refers to a hybrid form of advertising, where the advertiser advertises for an advertisement.[1][2] It can also be used for advertisements about advertising agencies.[3]\n\nContents\n\n    1 Advertisements about Advertisements\n    2 Advertising for Advertising Agencies\n    3 See also\n    4 References\n\nAdvertisements about Advertisements\n\nThe most common definition of meta-advertising is an ad about an ad. This form of advertising is popular with Super Bowl advertising. Super Bowl ads and spots cost far more than regular ads.[4] The Super Bowl ads are highly anticipated. This often leads the companies to air ads encouraging viewers to watch the companies' upcoming Super Bowl ads, a form of meta-advertising.\n\nAdvertising about advertisements is a form of viral advertising, whereby advertisers seek to garner attention for their ad and therefore product.[5][6]\n\nOther examples include advertisements in one form of media, advertising for an ad in another medium. This could include a radio ad saying \"Look in your Sunday paper for a free coupon.\"\nAdvertising for Advertising Agencies\n\nThe term meta-advertising can also refer to advertisers advertising for themselves. This could include an advertisement for an ad agency.[7]\n\nMeta-advertising can also include ads which advertise for advertising. This is common with billboards, such as a billboard that says \"A thousand people will pass by this billboard today. To advertise here call...\"\nSee also\n\n    Self-reference\n    Meta-\n    Meta-reference", "skillName": "Meta-advertising."}
{"id": 132, "category": "Advertising", "skillText": "A television advertisement (variously called a television commercial, commercial or ad in American English, and known in British English as an advert) is a span of television programming produced and paid for by an organization, which conveys a message, typically to market a product or service. Advertising revenue provides a significant portion of the funding for most privately owned television networks. The vast majority of television advertisements today consist of brief advertising spots, ranging in length from a few seconds to several minutes (as well as program-length infomercials). Advertisements of this sort have been used to promote a wide variety of goods, services and ideas since the beginning of television.\nTelevision was still in its experimental phase in 1928, but the medium's potential to sell goods was already predicted.\n\nThe effects of television advertising upon the viewing public (and the effects of mass media in general) have been the subject of philosophical discourse by such luminaries as Marshall McLuhan. The viewership of television programming, as measured by companies such as Nielsen Media Research, is often used as a metric for television advertisement placement, and consequently, for the rates charged to advertisers to air within a given network, television program, or time of day (called a \"daypart\").\n\nIn many countries, including the United States, television campaign advertisements are considered indispensable for a political campaign. In other countries, such as France, political advertising on television is heavily restricted,[1] while some countries, such as Norway, completely ban political advertisements.\n\nThe first official, paid television advertisement was broadcast in the United States on July 1, 1941 over New York station WNBT (now WNBC) before a baseball game between the Brooklyn Dodgers and Philadelphia Phillies. The announcement for Bulova watches, for which the company paid anywhere from $4.00 to $9.00 (reports vary), displayed a WNBT test pattern modified to look like a clock with the hands showing the time. The Bulova logo, with the phrase \"Bulova Watch Time\", was shown in the lower right-hand quadrant of the test pattern while the second hand swept around the dial for one minute.[2][3] The first TV ad broadcast in the UK was on ITV on 22 September 1955, advertising Gibbs SR toothpaste. The first TV ad broadcast in Asia was on Nippon Television in Tokyo on August 28, 1953, advertising Seikosha (now Seiko), which also displayed a clock with the current time.[4]\n\nContents\n\n    1 General background\n        1.1 Characteristics\n    2 TV advertisements by country\n        2.1 United States of America\n            2.1.1 Popularity\n            2.1.2 Restrictions\n            2.1.3 Advertisements as programming\n        2.2 Europe\n            2.2.1 United Kingdom\n            2.2.2 Germany\n            2.2.3 Greece\n            2.2.4 France\n            2.2.5 Ireland\n            2.2.6 Finland\n            2.2.7 Russia\n            2.2.8 Denmark\n            2.2.9 Norway\n            2.2.10 Croatia\n            2.2.11 Poland\n        2.3 Asia-Pacific\n            2.3.1 India\n            2.3.2 Malaysia\n            2.3.3 The Philippines\n            2.3.4 Australia\n            2.3.5 New Zealand\n            2.3.6 South Korea\n            2.3.7 Other SEA countries and the rest of the Asia-Pacific region\n        2.4 Latin America\n            2.4.1 Argentina\n    3 Use of popular music\n    4 Future of TV advertisements\n        4.1 Digital television recorders and advertisement skipping\n        4.2 Product placement\n        4.3 Overlay advertisements\n        4.4 Google in the TV advertising business\n        4.5 Interactive advertisements\n        4.6 Shorter commercial breaks\n        4.7 Competition from internet\n    5 See also\n    6 References\n    7 External links\n\nGeneral background\n\nTelevision advertising involves two main tasks: creating a television advertisement that meets broadcast standards and then, placing the advertisement on television via a targeted air time media buy that reaches the desired customer.\n\nTo accomplish the first step means different things in different parts of the world depending on the regulation in place. In the UK for example, clearance must be given by the body Clearcast. Another example is Venezuela where clearance is governed by a body called CNAC.[5] The clearance provides guarantee to the broadcasters that the content of the advertisement meets legal guidelines. Because of this, special extended clearance sometimes applies to food and medical products as well as gambling advertisements.\n\nThe second is the process of TV Advertising Delivery and usually incorporates the involvement of a Post-Production House, a Media agency, Advertising Distribution Specialists and the end-goal, the broadcasters.\nCharacteristics\n\nIt is important to choose a television production company and advertising agency with pertinent expertise in these two areas, and it is preferable to choose an agency that both produces advertisements and places air time, because expertise in broadcast quality production and broadcast standards is vital to gaining the advertisement's acceptance by the networks. After the advent of cheap video software and consumer cameras, numerous individuals have offered video production services on the internet. Video production companies that do not regularly place TV advertisements on the air often have their productions rejected by networks for technical or content issues, due to their inexperience with creating broadcast-ready content.\n\nMany television advertisements feature songs or melodies (\"jingles\") or slogans designed to be striking and memorable, which may remain in the minds of television viewers long after the span of the advertising campaign. Some of these ad jingles or catch-phrases may take on lives of their own, spawning gags that appear in films, television shows, magazines, comics, or literature. These long-lasting advertising elements may be said to have taken a place in the pop culture history of the demographic to whom they appeared. An example is the enduring phrase, \"Winston tastes good like a cigarette should\", from the eighteen-year advertising campaign for Winston cigarettes from the 1950s to the 1970s. Variations of this dialogue and direct references to it appeared as long as two decades after the advertising campaign expired. Another example is \"Where's the Beef?\", which grew so popular it was used in the 1984 presidential election by Walter Mondale. Another popular catch-phrase is \"I've fallen and I can't get up\", which still appears occasionally, over two decades after its first use. Some advertising agency executives have originated more than one enduring slogan, such as Mary Wells Lawrence, who is responsible for such famous slogans as \"Raise your hand if you're Sure\", \"I♥New York\" and \"Trust the Midas touch.\"\n\nAdvertising agencies often use humor as a tool in their creative marketing campaigns. Many psychological studies have attempted to demonstrate the effects of humor and their relationship to empowering advertising persuasion.\n\nAnimation is often used in advertisements. The pictures can vary from hand-drawn traditional animation to computer animation. By using animated characters, an advertisement may have a certain appeal that is difficult to achieve with actors or mere product displays. Animation also protects the advertisement from changes in fashion that would date it. For this reason, an animated advertisement (or a series of such advertisements) can be very long-running, several decades in many instances. Notable examples are the series of advertisements for Kellogg's cereals, starring Snap, Crackle and Pop and also Tony the Tiger. The animation is often combined with real actors. Animated advertisements can achieve lasting popularity. In any popular vote for the most memorable television advertisements in the UK (such as on ITV[6] or Channel 4[7]) the top positions in the list invariably include animations, such as the classic Smash and Creature Comforts advertisements.\n\nOther long-running advertising campaigns catch people by surprise, even tricking the viewer, such as the Energizer Bunny advertisement series. It started in the late 1980s as a simple comparison advertisement, where a room full of battery-operated bunnies was seen pounding their drums, all slowing down except one, with the Energizer battery. Years later, a revised version of this seminal advertisement had the Energizer bunny escaping the stage and moving on (according to the announcer, he \"keeps going and going and going...\"). This was followed by what appeared to be another advertisement: viewers were oblivious to the fact that the following \"advertisement\" was actually a parody of other well-known advertisements until the Energizer bunny suddenly intrudes on the situation, with the announcer saying \"Still going...\" (the Energizer Battery Company's way of emphasizing that their battery lasts longer than other leading batteries). This ad campaign lasted for nearly fifteen years. The Energizer Bunny series has itself been imitated by others, via a Coors Light Beer advertisement, in motion pictures, and by current advertisements by GEICO Insurance.\nTV advertisements by country\nUnited States of America\n\nIn the United States, the Nielsen ratings system measures audience viewership of television programs, and provides a way for television broadcasters to determine how popular their television shows are, so that they can decide what rates to charge advertisers for air time.\nFile:Theater commercial, electric refrigerator, 1926.oggPlay media\nTheater commercial about electric refrigerator, in 1926\nFile:Around the Corner (1937) 24fps selection.webmPlay media\nAround the Corner (1937)\n\nFor each hour in a broadcast day, advertisements take up a fairly consistent proportion of the time. Commercial breaks have become longer. In the 1960s a typical hour-long American show would run for 51 minutes excluding advertisements. Today, a similar program would only be 42 minutes long; a typical 30-minute block of time now includes 22 minutes of programming and eight minutes of advertisements - six minutes for national advertising and two minutes for local.[8]\n\nA television broadcast of the 101-minute film The Wizard of Oz (1939) for instance, could, in the early to mid-1960s, take two hours even with advertisements. Today, a telecast of the same film would last between 2½ and three hours, including advertisements. Because of this, it is common practice to edit films to fit within the time allotted.\n\nOver the course of 10 hours, American viewers will be shown approximately three hours of advertisements, twice what they would have seen in the 1960s. If a 1960s show is rerun today, the content may be edited by nine minutes to make room for the extra advertisements.\n\nIn the 1950s and 1960s, the average advertisement's length was one minute. As the years passed, the average length shrank to 30 seconds (and sometimes 10 seconds), but more advertisements are now shown during the break.\n\nTV advertisements were coded for identification by broadcasters via an ISCI code. As of March 31, 2014, Ad-ID has been mandated as the standard method of identification for TV advertisements.[9]\n\nA special case of TV advertisements are rare or one time events known as Mega Event Advertising.\nPopularity\n\nIn the United States, the TV advertisement is generally considered the most effective mass-market advertising format, and this is reflected by the high prices TV networks charge for commercial broadcasting airtime during popular TV events. The annual Super Bowl American football game is known as much for its commercial advertisements as for the game itself, and the average cost of a single 30-second TV spot during this game (seen by 100 million viewers) has reached US$4.5 million (as of February 2013).\n\nIt has been suggested that, in general, television executives believe that advertisers covet the 18-49 age demographic and that older viewers are of almost no interest to most advertisers due to their unwillingness to change their buying habits.[10] Products intended for older consumers, such as certain health products and insurance, are advertised regularly on television, generally during programming that appeals to older adults.\n\nThe number of viewers within the target demographic is more important to ad revenues than total viewers. According to Advertising Age, during the 2007-08 season, Grey's Anatomy was able to charge $419,000 per advertisement, compared to only $248,000 for an advertisement during CSI, despite CSI having almost five million more viewers on average.[11] Due to its demographic strength, Friends was able to charge almost three times as much for an advertisement as Murder, She Wrote, even though the two series had similar total viewer numbers during the seasons they were on the air together.[10] Broadcast networks are concerned by the increasing use of DVRs by young viewers, resulting in aging of the live viewing audience and consequently, lower advertising rates.[12] TV advertisers may also target certain audiences of the population such as certain races, and people of a certain income level or gender.[10] In recent years, shows that tend to target young women tend to be more profitable for advertisements than shows targeted to younger men. This is due to the fact that younger men are watching TV less than their female counterparts.[13]\n\nIn the United Kingdom, television advertising is considerably cheaper than in the United States of America. The current record for an advertising slot on British terrestrial television is quoted at being £250,000 for a 30-second slot during the 2010 series of Britain's Got Talent. However while British TV advertising is cheaper, this is only to be expected as the United Kingdom has a much lower population (63 million) compared to the US (310 million). So if the £250,000 figure is adjusted into American terms, the population being 5 times bigger and the exchange rate, this figure would become $2 million, much closer to the US Super Bowl figure.[14]\n\nBecause a single television advertisement can be broadcast repeatedly over the course of weeks, months, and even years (the Tootsie Roll company has been broadcasting a famous advertisement that asks \"How many licks does it take to get to the tootsie center of a Tootsie Pop?\" for over three decades), television advertisement production studios often spend very large sums of money in the production of a single thirty-second television spot. This significant expenditure has resulted in a number of high-quality advertisements with high production values, the latest in special effects technology, the most popular personalities, and the best music. A number of television advertisements are so elaborately produced that they can be considered miniature thirty-second movies; indeed, many film directors have directed television advertisements both as a way to gain exposure and to earn a paycheck. One of film director Ridley Scott's most famous cinematic moments was a television advertisement he directed for the Apple Macintosh computer, that was broadcast in 1984. Although this advertisement was broadcast only once (aside from occasional appearances in television advertisement compilation specials and one 1 a.m. airing in Idaho a month before the Super Bowl so that the advertisement could be submitted for the 1983 Clio Awards), it has become famous and well-known, to the point where it is considered a classic television moment.[15]\n\nDespite the popularity of some advertisements, many consider them to be an annoyance for a number of reasons. The main reason may be that the sound volume of advertisements tends to be higher (and in some cases much higher) than that of regular programming. The United States Congress passed a bill on September 30, 2010, called the CALM Act, to reduce the sound volume of advertisements, and loudness rules set by the FCC are effective as of December 13, 2012.[16] In the UK, the Broadcast Committee of Advertising Practice has a similar regulation. The increasing number of advertisements, as well as overplaying of the same advertisement, are secondary annoyance factors. A third might be that television is currently the main medium to advertise, prompting advertising campaigns by everyone from cell-phone companies, political campaigns, fast food restaurants, to local businesses, and small businesses, prompting longer commercial breaks. Another reason is that advertisements often cut into certain parts in the regular programming that are either climaxes of the plot or a major turning point in the show, which many people find exciting or entertaining to watch.\n\nFrom a cognitive standpoint, the core reason people find advertisements annoying is that the advertisement's offer is not of interest at that moment, or the presentation is unclear. A typical viewer has seen enough advertisements to anticipate that most advertisements will be bothersome, prompting the viewer to be selective in their viewing. Conversely, if an advertisement concerns a topic important to the viewer (such as an advertisement for debt relief shown to a viewer who has received a late notice in the mail), or has entertainment value beyond the basic message (such as the classic humorous spots for Wendy's \"Where's the beef?\" campaign), then viewers tend to stay with the advertisement, perhaps even looking forward to viewing it again.[citation needed]\nRestrictions\nTelevision commercial in 1948\n\nBeginning on January 2, 1971, advertisements featuring cigarettes were banned from American TV. Advertisements for alcohol products are allowed, but the consumption of any alcohol product is not allowed in a television advertisement (for example, an actor in a beer commercial cannot be shown actually drinking the beer). The Federal Trade Commission and the Federal Communications Commission have laid out regulations for television advertising, outlining restrictions on certain products, content, and claims, in addition to mandating minimum technical standards.[17] Additional content standards are set by individual television broadcast entities to accommodate local laws, community standards, and their particular audience demographic; these broadcast outlets examine each incoming advertisement through a process known as \"clearance.\"\nAdvertisements as programming\n\nSince the 1960s, media critics have claimed that the boundaries between \"programming\" and \"advertisements\" have been eroded to the point where the line is blurred nearly as much as it was during the beginnings of the medium, when almost all individual television shows were sponsored entirely by a single corporation (the model which was carried over from old-time network radio). For much of the 1970s, 1980s, and 1990s, the FCC imposed a rule requiring networks that broadcast programming on Saturday morning and Sunday nights at 7 PM/6 PM Central to air bumpers (\"We'll return after these messages...\", \"...now back to our programming\" and variations thereof) to help younger audiences distinguish programs from advertisements. The only programs that were exempt from this rule were news shows and information shows relating to news (such as 60 Minutes). Conditions on children's programming have relaxed to an extent since the period of the 1970s and 1980s.\nEurope\n\nIn many European countries television advertisements appear in longer, but less frequent advertising breaks. For example, instead of 3 minutes every 8 minutes, there might be around 6 minutes every half-hour. European Union legislation limits the time taken by commercial breaks to 12 minutes per hour (20%), with a minimum segment length of 20 or 30 minutes, depending on the program content.[18] Live imported telecasts with shorter segments, such as U.S. sporting events, replace the original ads with promotional material. Advertising broadcast time can vary within the EU and other countries and between networks depending on local policy. Unlike in the United States, in Europe the advertising agency name may appear at the beginning or at the end of the advert.\nUnited Kingdom\n\nIn the UK, the BBC is funded by a licence fee and does not screen adverts. On the commercial channels, the amount of airtime allowed by the UK broadcasting regulator Ofcom for advertising is an overall average of 7 minutes per hour, with limits of 12 minutes for any particular clock hour (8 minutes per hour between 6pm and 11pm). With 42-minute American exports to Britain, such as Lost, being given a one-hour slot, nearly one third of the slot is taken up by adverts or trailers for other programs. Live imported television programs such as WWE Raw show promotional material in place of U.S. advert breaks. Infomercials (known as \"admags\") were originally a feature of the regional commercial ITV stations from launch in 1955 but were banned in 1963.\n\nThe first advert to be shown in the UK was an advert for S.R. Toothpaste on September 22, 1955 on the ITV network (its first day).[19]\n\nOn 1 July 2000, TV broadcasters began requiring commercials to be delivered to them in widescreen, an event referred to as C-Day in industry promotion of the change.\n\nIn 2008, Ofcom announced a review of television advertising and teleshopping regulations, with a view to possibly changing their code, Rules on the Amount and Distribution of Advertising (RADA), which regulates the duration, frequency and restriction of adverts on television.\n\nTelevision advertising specialist, Nick Illston, of advertising-buying agency Pace Media,[14] states that ITV's £250,000 asking price for a 30-second slot during the 2010 series of Britain's Got Talent is currently the most expensive advertising slot on television.[14]\nGermany\n\nAs in Britain, in Germany, public television stations own a major share of the market. Their programming is funded by a licence fee as well as advertisements on specific hours of the day (20 minutes per day; not after 8 p.m.), except on Sundays and holidays. Private stations are allowed to show up to 12 minutes of advertisements per hour with a minimum of 20 minutes of programming in between interruptions.\nGreece\n\nIn Greece, where most television stations and channels do not remove their logo before a commercial break, it is mandatory for public and commercial TV stations to separate the advertisements from the rest of the program with each station's ident. Subliminal advertising, tobacco and medication advertisements are not allowed. Most programs (series, documentaries, etc.) can have one commercial break every 20 minutes. Advertisements must not exceed the 15% of the broadcasting day or 20% if there are direct advertisements for buying or renting products (like informercials). A commercial break must not exceed the length of 4 minutes with the exceptions of movies/telefilms with the length exceeding the 45 minutes, which can have one commercial break for 9 minutes and of the movies/telefilms that exceed the length of 110 minutes which can have more than one commercial break every 45 minutes for 9 minutes.[20]\nFrance\n\nThe Conseil supérieur de l'audiovisuel allows up to 9 minutes of advertising per hour on average in a day. Private channels can only broadcast one commercial break if the show is less than an hour and two commercial breaks if the show is more than an hour. For public channels, advertising is forbidden after 8 p.m.\nIreland\nFilming a movie ad\n\nIn Ireland, the Broadcasting Authority of Ireland determines the number of adverts on commercial and community TV stations, while the Minister for Communications, Energy and Natural Resources is responsible for the advertising limits of Public Service Broadcasters on TV and Radio, and commercial radio advertising is governed through statue.\n\nUp to 2010 commercial broadcasters where permitted a maximum of 15% advertising time vs. overall broadcast time.[21] In 2010 TV3 asked for an increase in advertising minutes due to the high level of competition from advertising opt-outs coming into Ireland via satellite and cable pay providers, and also the number of live ITV programmes carried by TV3 (such as The X Factor). The BAI agreed to increasing the number of ads on commercial TV broadcasts to bring Irish commercial TV channels into line with UK commercial TV channels.[22] The Independent Broadcasters of Ireland also lobbied for a similar increase, but the regulator was unable to increase their advertising minutes as these are written into Statue, meaning that any change is the responsibility of the members of the national parliament (The House of the Oireachtas).\n\nCommercial Broadcasters may not exceed 18% of their broadcast day on advertising and they may only have a maximum of 12 minutes of adverts per hour, except for children's programming where advertising may not exceed 10 minutes per hour[23] (commercial TV broadcasters include TV3 and Setanta). Irish community TV channels rarely show advertising; however they are permitted to show 6 minutes of advertising per hour.\n\nRTÉ TV and Radio carry a maximum of 9 minutes per hour of advertising, but only an average of 6 minutes of advertising per hour, and the same rules apply to TG4: this amounts to 10% of total broadcast time. On television, RTÉ cannot carry advertising on RTÉ News Now and RTÉjr; they do not carry advertising on programming for the under 6s, RnaG or their Digital Radio services.\nFinland\n\nIn Finland, there are two mainstream non-commercial channels run by the state owned broadcasting company Yle, that run advertisements only on very infrequent occasions, such as important sport events. The three main commercial channels MTV3, Sub (a subsidiary of MTV3), and Nelonen, all run their advertisements during breaks approximately every 15 minutes. Since digital TV has been introduced, the number of TV channels has grown, with Yle and the main broadcasters all adding new channels (including some subscription channels). Analogue broadcasts ceased in August 2007 and the nation's TV services are now exclusively digital. A typical break lasts about 4 minutes. The length of individual advertisements can vary from a few seconds (7, 10 and 15 are common), and they are currently rarely over one minute in length. Many advertisements of supranational companies are commonly dubbed from English language advertisements. Although Swedish is the other official language of Finland, the advertisements do not feature Swedish subtitles nor are any Swedish language advertisements shown with the infrequent exception of some political advertisements at the time of elections.\nRussia\n\nThe Russian advertising break consists of 2 parts: federal adverts and regional adverts. The duration for each is 4 minutes and 15 minutes per hour respectively. Like Greece, Russia is one of the few countries where the television channel's logo is retained during the commercial break. In Russia, tobacco advertising is prohibited, and in 2013, this was followed suit by alcohol and medication advertising.\nDenmark\n\nThe Danish DR-channels are funded by a television licence, so they do not show any advertisements. The other Danish television network, TV2 shows advertisements only in blocks between the programs. These can take from 2 minutes to 10 minutes depending on the time to the next show. In Denmark, commercial breaks are strictly prohibited and advertising targeted to children is restricted. Channels like Kanal 5 and TV3 are allowed to interrupt programs, as these channels are being broadcast via satellite from the United Kingdom.\nNorway\n\nTelevision advertising is regulated by the Broadcasting Act. Public service broadcaster NRK is licence-funded, and cannot carry advertising, however sponsorship of programs (typically sport events) is allowed. Commercial channels may interrupt television programmes with ads, but certain conditions apply. The maximum amount of advertising is 15% per hour (09 min). The nation's second-most watched channel - TV2 - is funded by television advertisements. Norwegian-language programs broadcast from abroad - such as TV3 Norway broadcasting from London in the Norwegian language - must comply to British regulations. Political TV advertisements are strictly prohibited, as are all advertisements for tobacco and alcohol. TV advertisements are monitored by the Norwegian Media Authority (Medietilsynet).\nCroatia\n\nIn Croatia, the commercial broadcasters may show up to 12 minutes of advertising per hour, while the public broadcaster HRT, being partially funded by a license fee, shows less advertising and usually does not interrupt series and films. Tobacco advertising is banned but alcohol and non-prescription medications can be advertised on TV. Political advertisements are allowed only in the short official campaign period before an election and only in separate blocks from other advertisements.\nPoland\n\nIn Poland, national public broadcaster Telewizja Polska is partially funded by a license fee, advertising breaks can be broadcast only between shows, but for live sports broadasting which contain breaks under the rules of the playing and other events contains breaks can also be interrupted, while almost all commercial broadcasters (except e.g. Canal+) can interrupt shows for advertising broadcasting. Tobacco and alcohol drink advertising are banned respectively by Health Protection Against the Consequences of Tobacco Use and Tobacco Articles Act of November 9, 1995 and Education in Sobriety and Counteracting Alcoholism Act of October 26, 1982 but beer advertising is restricted and can be shown between 8:00 PM and 6:00 AM as well as unhealthy food cannot be broadcast since January 1, 2015 on children's channel such as MiniMini+, TVP ABC and Nickelodeon and on children's show by agreement.\nAsia-Pacific\nIndia\n\nIn India, the Code for Self-Regulation of Advertising in India, established by the Advertising Standards Council of India (ASCI), is applicable on Television Commercials (TVCs). If consumers see an advertisement which they consider misleading or offensive, they can write to ASCI.[24]\nMalaysia\n\nIn Malaysia, a government department monitors television advertisements rather than a private institution. Almost all television stations and channels in the country, whether government-owned, private or pay television channels, broadcast advertisements, with television channel logos removed before the start of the commercial break.\n\nThere are usually two commercial breaks in a half-hour program and three commercial breaks in an hour-long program, with the exception of news programs. Terrestrial television can only broadcast advertisements during the program that was currently aired except before announcing the breaking of fast in the month of Ramadan, although advertisements may be included during CJ Wow Shop on Media Prima television channels such as NTV7 and TV9 as of June 2016.\n\nBecause Malaysian television advertisements were controlled by the government department, the Malaysian TV advertising permit number appears during the first few seconds of any advertisement. The abbreviation at the first part of the permit number which refers to the ministry which handles the TV advertising permit number always changes sometime after the Malaysian government made a new portfolio of ministers and name changes to some ministries after the general election. The usage of advertising code began in 1995, with KP/YYYY/XXXX of which KP means Ministry of Information, YYYY is the year the advertisement produced and the XXXX is the number of the advert permit. This was replaced by KPKK (Ministry of Information, Communications and Culture) and KKMM (Ministry of Communications and Multimedia of Malaysia) in 2009 and 2013, respectively. The advertising code used on channels other than RTM may or may not be used. Other advertising permits includes the KKLIU (Ministry of Health, the Medicine Advertising Authority) for medicinal advertisements, which was used before 1995 and the JIRP (Pesticide Advertising Department) for pesticide advertisements. All pesticide advertisements must show the word \"INI ADALAH IKLAN RACUN PEROSAK\" (This is a pesticide advertisement) and JIRP advertising code at the beginning of the advertisement and the words \"BACALAH LABEL KELUARAN SEBELUM MENGGUNAKANNYA\" (Read the label before use) at the end of the advertisement. It was also used in advertisements on newspapers and magazines.\n\nAstro is also known to delay incoming satellite feeds by two to five minutes (some channels delay by one hour) from the actual time of the start and the end of program (for example a program aired at 1:30 pm will be started at 1:33 pm) with broadcasting advertisements during in-between programs for its purpose of commercial replacement, as government laws forbid advertisements produced from overseas, with exceptions such as advertisement for movies (since release dates may include more than one country).\n\nLiquor advertisements shown after 10:00 pm during non-Malay programs have been banned in the country since 1995, while cigarette advertisements have been banned from showing cigarette packaging since 1995, and have been banned entirely since 2003. Fast-food advertisements during children's programs were also banned in 2007. There are also restrictions on Malaysian television advertisements such as advertisements for 18-rated films, women's care products and unhealthy foods not being allowed to be broadcast during children's programs, and lottery advertising, which is prohibited during Malay programs. Lingerie advertisements are prohibited in Malaysian television, but allowed in non-Malay magazines published in Malaysia.\nThe Philippines\n\nPhilippines advertising industry is self-regulated. The Philippine advertising industry adopted an Advertising Code of Ethics essentially to promote efficiency in processing applications and resolution cases and avoid costly litigation in regular courts. The earliest Advertising Code of Ethics dates back to the Philippine Board of Advertising (PBA) established in 1974. In 1989, the PBA was renamed Advertising Board of the Philippines (ADBOARD) and was mandated by the Implementing Rules and Regulations of R.A. 7394 or The Consumer Protection Act to ensure that all advertising materials conform to its Code of Ethics. The ADBOARD Advertising Content & Regulations Committee (ACRC) had been the main implementing arm of advertising self-regulation in the Philippines until March 31, 2008 when the Ad Standards Council (ASC) took over this function.[25] The Association of Broadcasters of the Philippines, a self-regulatory organization representing most television and radio broadcasters in the country, limit advertising to 18 minutes per hour, a move taken to help \"promote public interest.\"[26][27]\n\nIn this case some TV broadcasters have to remove TV logos (like ABS-CBN, TV5 & GMA) before starting a commercial break. Also, all commercial content and advertisements will start (right after for most TV shows will break) before or after TV program announcements/etc. Some TV commercials to be aired requires ASC clearance if the content has trivial facts and testimonies.\n\nSince television was introduced to the Philippines in 1953, they used imported TV advertisements until 1960, the same way in which they used billboard advertisements during the American period. In the 1960, P&G paved their way to start the first local TV advertisement.\n\nIn 1966, when the Philippine TV turned from black-and-white to color, Colgate-Palmolive was the first to advertise in color.\n\nSome ads have the following notice indicated at the during or at the end of the commercial depending on type of products.\n\n    \"Drink Moderately/Responsibly.\" for alcoholic beverages.\n    \"Member: BancNet, PDIC Maximum deposit insurance for each depositor P 500,000, PANA Truth in Advertising\" for banks\n    \"GOVERNMENT WARNING: CIGARETTE SMOKING IS DANGEROUS TO YOUR HEALTH.\" (formerly \"Government Warning: Cigarette smoking is hazardous to health\"[28] from January 1, 1983 to November 30, 1990) for tobacco products (radio and television advertising of cigarette and hand-rolling tobaccos is banned since July, 2008 in accordance of Section 22 of Republic Act 9,211 or the Tobacco Regulation Act of 2003)[29]\n    \"MAHALAGANG PAALALA: ANG (name of product) AY HINDI GAMOT AT HINDI DAPAT GAMITING PANGGAMOT SA ANUMANG URI NG SAKIT.\" (formerly \"NO APPROVED THERAPEUTIC CLAIMS\") for food, dietary, fiber and herbal supplements.\n    \"If symptoms persist, consult your doctor.\" for over-the-counter medicines.\n    \"This is a paid advertisement.\" for political ads.\n    \"Political advertisement paid for and by (name), (address).\" also for political ads.\n    \"The use of milk supplements must only be upon advice of a health professional\". (formerly \"Breastmilk is the best for babies up to 2 years of age and beyond.\") for infant formulas. Used in conjunction with the DOH-IAC permit number.\n    “This product is not intended for babies 6 months of age and below”. for food/infant device that are targeted to 6 months old and above.\n    \"Per (DTI-FTEB SPD/DOH-FDA CDRR/CFRR/DA-NMIS) Permit Number (number of permit), Series of (year)\" for promos.\n\nAustralia\n\nSimilar to the European Union, advertising on Australian commercial television is restricted to a certain amount in a 24-hour period, but there are no restrictions on how much advertising may appear in any particular hour.[30] Australian television has one of the highest proportions of advertising content in the world. In primetime there can be 18 minutes or more of advertising (inc. news updates) per hour. Furthermore, product advertisements with informational content are labeled \"public service announcements\" and not included in the time restrictions; similarly with \"this program brought to you by...\" announcements, and station identifications. Consequently, there may be less than 40 minutes of actual program time per hour. Foreign, older television programs and movies are noticeably cut; comedy shows often return from an ad break into laughter, for instance. Australia is also one of the few countries in the world where advertisements may appear prior to, and over the top of, the closing credits of a program. There are some restrictions on television advertising in Australia, such as a complete ban on advertising for cigarettes, as well as advertising during programs intended for young children.[citation needed] The ABC, the nation's public broadcaster, broadcasts no external advertisements, but between programs will broadcast promotions for its own programs and merchandise, although is restricted to approximately five minutes per hour. SBS had similar restrictions on advertising until 2005 when it began airing external advertisements between programs, and later, during programs, as per the commercial networks, although still limited to 5 minutes per hour with the first advertisement being half-hourly news updates. Back in 1978, major commercial networks has the longest ever ad break run with the record 53 minutes of advertising.\nNew Zealand\n\nAll major New Zealand television channels, both state-owned and private, screen advertisements. Adverts on average taking up 15 minutes of each hour; there are usually two advert breaks in a half-hour program, and four advert breaks in an hour-long program.\n\nTelevision advertising began in New Zealand in April 1961, ten months after the first official television broadcast, and was initially allowed only on Tuesdays, Thursdays and Saturdays.[31] Today, television adverts are only prohibited on Good Friday, Easter Sunday and Christmas Day The Advertising Standards Authority[32] is responsible for advertisement compliance, and deals with advertisement complaints (except for election advertising, in which the Broadcasting Standards Authority is responsible).\nSouth Korea\n\nIn South Korea, a television advertisement is called a CF, an abbreviation for \"commercial film\".[33][34][35][36][37]\n\nUnder the current rules, terrestrial channels cannot take in-program commercial breaks. So, the advertisements are usually put between the intro and the start of a program, and between the end credits and the end of the program. Terrestrial channels often divide some longer-length films like The Ten Commandments into parts and consider each part as an individual program. Terrestrial channels can take commercial breaks during breaks in action during sporting events.\n\nPay-television channels can take in-program commercial breaks, although some pay TV channels schedule advertisements in the same way that terrestrial channels do. Regulations for advertisements on terrestrial channels are more strict than those for pay channels. Non-South Korean channels are not subject to these regulations. Tobacco advertisements are prohibited.\nOther SEA countries and the rest of the Asia-Pacific region\n\nDepending on its style, system and other factors while in the process of broadcasting commercial breaks and TV advertisements, some officials (like government, associations, institutions, etc.) regulate and monitor the assets and broadcasting TV advertisements based on international standards.\nLatin America\nArgentina\n\nSince September 2010, all Argentine television channels (including cable channels operated from the country itself), are required to separate advertising from the rest of the programming using bumpers with the text \"Espacio publicitario\" (\"Advertising space\"). As of June 2016, this usage is no longer compulsory.\n\nCommercial advertising is limited to 12 minutes per hour. In-program advertising is allowed, but counted toward the 12-minute quota. That means that if a 60-minute show has 2 minutes of in-program advertising, the commercial breaks have to be limited to 10 minutes for that specific hour, otherwise the station might face a fine.\n\nFrom March 2014, advertisements shown in Argentinian television began to use the word \"Aviso publicitario de producción nacional\" (Advertisement of domestic production) and \"Aviso publicitario de producción extranjera\" (Advertisement of foreign production) for local and foreign advertisements, respectively. Also, advertisements for alcoholic products had the warning \"Beber con moderación. Prohibida su venta a menores de 18 años\" (Drink in moderation. Do not sell to persons under 18). The warning is not used in alcohol advertising before 1997.\nUse of popular music\n\nPrior to the 1970s, music in television advertisements was generally limited to jingles and incidental music; on some occasions lyrics to a popular song would be changed to create a theme song or a jingle for a particular product. An example of this is found on the recent popular Gocompare.com advert that utilises \"Over There\", the 1917 song popular with United States soldiers in both world wars and written by George M. Cohan during World War I. In 1971 the converse occurred when a song written for a Coca-Cola advertisement was re-recorded as the pop single \"I'd Like to Teach the World to Sing\" by the New Seekers, and became a hit. Additionally songwriter Paul Williams composed a piece for a Crocker Bank commercial which he lengthened and The Carpenters recorded as \"We've Only Just Begun\". Some pop and rock songs were re-recorded by cover bands for use in advertisements, but the cost of licensing original recordings for this purpose remained prohibitive in certain countries (including the U.S.) until the late 1980s.[citation needed]\n\nThe use of previously recorded popular songs in American television advertisements began in earnest in 1985 when Burger King used the original recording of Aretha Franklin's song \"Freeway of Love\" in a television advertisement for the restaurant. This also occurred in 1987 when Nike used the original recording of The Beatles' song \"Revolution\" in an advertisement for athletic shoes. Since then, many classic popular songs have been used in similar fashion. Songs can be used to concretely illustrate a point about the product being sold (such as Bob Seger's \"Like a Rock\" used for Chevy trucks), but more often are simply used to associate the good feelings listeners had for the song to the product on display. In some cases the original meaning of the song can be totally irrelevant or even completely opposite to the implication of the use in advertising; for example Iggy Pop's \"Lust for Life\", a song about heroin addiction, has been used to advertise Royal Caribbean International, a cruise ship line. Music-licensing agreements with major artists, especially those that had not previously allowed their recordings to be used for this purpose, such as Microsoft's use of \"Start Me Up\" by the Rolling Stones and Apple Inc.'s use of U2's \"Vertigo\" became a source of publicity in themselves.\n\nIn early instances, songs were often used over the objections of the original artists,[citation needed] who had lost control of their music publishing, the music of Beatles being perhaps the most well-known case; more recently artists have actively solicited use of their music in advertisements and songs have gained popularity and sales after being used in advertisements. A famous case is Levi's company, which has used several one hit wonders in their advertisements (songs such as \"Inside\", \"Spaceman\", and \"Flat Beat\"). In 2010, research conducted by PRS for Music revealed that \"Light & Day\" by The Polyphonic Spree is the most performed song in UK TV advertising.[38]\n\nSometimes a controversial reaction has followed the use of some particular song on an advertisement. Often the trouble has been that people do not like the idea of using songs that promote values important for them in advertisements. For example, Sly and the Family Stone's anti-racism song, \"Everyday People\", was used in a car advertisement, which angered some people.[who?][citation needed]\n\nGeneric scores for advertisements often feature clarinets, saxophones, or various strings (such as the acoustic/electric guitars and violins) as the primary instruments.\n\nIn the late 1990s and early 2000s, electronica music was increasingly used as background scores for television advertisements, initially for automobiles,[39] and later for other technological and business products such as computers and financial services. Television advertising has become a popular outlet for new artists to gain an audience for their work, with some advertisements displaying artist and song information onscreen at the beginning or end.\nFuture of TV advertisements\nDigital television recorders and advertisement skipping\nThough advertisements for cigarettes are banned in many countries, such advertising could still be seen in the sponsorship of events such as auto racing.\n\nAfter the video cassette recorder (VCR) became popular in the 1980s, the television industry began studying the impact of users fast forwarding through commercials. Advertising agencies fought the trend by making them more entertaining.[40] The introduction of digital video recorders (also known as digital television recorders or DTRs), such as TiVo, and services like Sky+, Dish Network and Astro MAX, which allow the recording of television programs into a hard drive, also enabled viewers to fast-forward or automatically skip through advertisements of recorded programs.\n\nThere is speculation that television advertisements are threatened by digital video recorders as viewers choose not to watch them. However evidence from the UK shows that this is so far not the case. At the end of 2008 22 percent of UK households had a DTR. The majority of these households had Sky+ and data from these homes (collected via the SkyView[41] panel of more than 33,000) shows that, once a household gets a DTR, they watch 17 percent more television. 82 percent of their viewing is to normal, linear, broadcast TV without fast-forwarding the ads. In the 18 percent of TV viewing that is time-shifted (i.e. not watched as live broadcast), viewers still watch 30 percent of the ads at normal speed. Overall, the extra viewing encouraged by owning a DTR results in viewers watching 2 percent more ads at normal speed than they did before the DTR was installed.\n\nThe SkyView evidence is reinforced by studies on actual DTR behaviour by the Broadcasters' Audience Research Board (BARB) and the London Business School.\nProduct placement\n\nOther forms of TV advertising include product placement advertising in the TV shows themselves. For example, Extreme Makeover: Home Edition advertises Sears, Kenmore, and Home Depot by specifically using products from these companies, and some sports events like the Sprint Cup of NASCAR are named after sponsors, and race cars are frequently covered in advertisements. Many major sporting venues in North America are named for commercial companies, dating back as far as Wrigley Field. Television programs delivered through new mediums such as streaming online video also bring different opportunities to the traditional methods of generating revenue from television advertising.\nOverlay advertisements\n\nAnother type of advertisement shown increasingly, mostly for advertising TV shows on the same channel, is an ad overlay at the bottom of the TV screen, which blocks out some of the picture. \"Banners\", or \"Logo Bugs\", as they are called, are referred to by media companies as Secondary Events (2E). This is done in much the same way as a severe weather warning is done, only these happen more frequently. They may sometimes take up only 5 to 10 percent of the screen, but in the extreme, they can take up as much as 25 percent of the viewing area. Subtitles that are part of the program content can be completely obscured by banners. Some even make noise or move across the screen. One example is the 2E ads for Three Moons Over Milford, which was broadcast in the months before the TV show's premiere. A video taking up approximately 25 percent of the bottom-left portion of the screen would show a comet impacting into the moon with an accompanying explosion, during another television program.\nGoogle in the TV advertising business\n\nIn 2007, Google Executive Chairman Eric Schmidt, then CEO, announced plans to enter the television advertising business, despite its lack of any internal video production or network placement capability or expertise. Initial industry speculation was that they would use a business strategy similar to their scheme for radio ad sales, primarily the acquisition of operations system support provider dMarc. Google announced the shutdown of its TV ads product in 2012.[42] According to the Google corporate blog, Google abandoned its radio advertising sales feature and its radio advertisers in February 2009. Google sold its radio buying assets in August 2009. Google's radio effort was widely considered a failure,[43] as was their foray into print advertising sales.[44] These two traditional media schemes were among a number of Google's media and technology experiments, which have yielded mixed results.[45]\nInteractive advertisements\n\nOnline video directories are an emerging form of interactive advertising, which help in recalling and responding to advertising produced primarily for television. These directories also have the potential to offer other value-added services, such as response sheets and click-to-call, which enhance the scope of the interaction with the brand.\nShorter commercial breaks\n\nDuring the 2008-09 TV season, Fox experimented with a new strategy, which the network dubbed \"Remote-Free TV\". Episodes of Fringe and Dollhouse contained approximately ten minutes of advertisements, four to six minutes fewer than other hour-long programs. Fox stated that shorter commercial breaks keep viewers more engaged and improve brand recall for advertisers, as well as reducing channel surfing and fast-forwarding past the advertisements. However, the strategy was not as successful as the network had hoped and it is unclear whether it will be continued in the future.[46]\nCompetition from internet\n\nThe growing popularity of the internet[47] continues to draw audiences away from advertisers populating just the television platform.\nSee also\n\n    Advertising\n    Advertising Adstock\n    Commercial bumper\n    Clio Awards\n    Interactive advertising\n    International Standardized Commercial Identifier (ISCI)\n    Marketing\n        Brand\n        Thinkbox\n    Political TV advertising\n        Attack ad\n    Radio commercial\n    Product placement\n        Promo (media)\n    Promotion (marketing)\n    Public service announcement\n    Sponsor (commercial)\n    Television license\n    Two Cs in a K\n    Upfront\n    FAST marketing", "skillName": "Television_advertisement."}
{"id": 133, "category": "Advertising", "skillText": "Viral marketing, viral advertising, or marketing buzz are buzzwords referring to marketing techniques that use pre-existing social networking services and other technologies to try to produce increases in brand awareness or to achieve other marketing objectives (such as product sales) through self-replicating viral processes, analogous to the spread of viruses or computer viruses (cf. Internet memes and memetics). It can be delivered by word of mouth or enhanced by the network effects of the Internet and mobile networks.[1] Viral advertising is personal and, while coming from an identified sponsor, it does not mean businesses pay for its distribution.[2] Most of the well-known viral ads circulating online are ads paid by a sponsor company, launched either on their own platform (company webpage or social media profile) or on social media websites such as YouTube.[3] Consumers receive the page link from a social media network or copy the entire ad from a website and pass it along through e-mail or posting it on a blog, webpage or social media profile. Viral marketing may take the form of video clips, interactive Flash games, advergames, ebooks, brandable software, images, text messages, email messages, or web pages. The most commonly utilized transmission vehicles for viral messages include: pass-along based, incentive based, trendy based, and undercover based. However, the creative nature of viral marketing enables an \"endless amount of potential forms and vehicles the messages can utilize for transmission\", including mobile devices.[4]\n\nThe ultimate goal of marketers interested in creating successful viral marketing programs is to create viral messages that appeal to individuals with high social networking potential (SNP) and that have a high probability of being presented and spread by these individuals and their competitors in their communications with others in a short period of time.[5]\n\nThe term \"Viral marketing\" has also been used pejoratively to refer to stealth marketing campaigns—marketing strategies that advertise a product to people without them knowing they are being marketed to.[6]\n\nContents\n\n    1 History\n    2 Methods and metrics\n    3 Metrics\n    4 Methods\n    5 Social networking\n    6 Notable examples\n    7 See also\n    8 References\n\nHistory\n\nThe emergence of \"viral marketing,\" as an approach to advertisement, has been tied to the popularization of the notion that ideas spread like viruses. The field that developed around this notion, memetics, peaked in popularity in the 1990s.[7] As this then began to influence marketing gurus, it took on a life of its own in that new context.\n\nThere is debate on the origination and the popularization of the specific term viral marketing, though some of the earliest uses of the current term are attributed to the Harvard Business School graduate Tim Draper and faculty member Jeffrey Rayport. The term was later popularized by Rayport in the 1996 Fast Company article \"The Virus of Marketing,\"[8] and Tim Draper and Steve Jurvetson of the venture capital firm Draper Fisher Jurvetson in 1997 to describe Hotmail's practice of appending advertising to outgoing mail from their users.[9] An earlier attestation of the term is found in PC User magazine in 1989, but with a somewhat differing meaning.[10][11]\n\nAmong the first to write about viral marketing on the Internet was the media critic Doug Rushkoff.[12] The assumption is that if such an advertisement reaches a \"susceptible\" user, that user becomes \"infected\" (i.e., accepts the idea) and shares the idea with others \"infecting them,\" in the viral analogy's terms. As long as each infected user shares the idea with more than one susceptible user on average (i.e., the basic reproductive rate is greater than one—the standard in epidemiology for qualifying something as an epidemic), the number of infected users grows according to an exponential curve. Of course, the marketing campaign may be successful even if the message spreads more slowly, if this user-to-user sharing is sustained by other forms of marketing communications, such as public relations or advertising.[citation needed]\n\nBob Gerstley was among the first to write about algorithms designed to identify people with high \"social networking potential.\"[13] Gerstley employed SNP algorithms in quantitative marketing research. In 2004, the concept of the alpha user was coined to indicate that it had now become possible to identify the focal members of any viral campaign, the \"hubs\" who were most influential. Alpha users could be targeted for advertising purposes most accurately in mobile phone networks, due to their personal nature.[citation needed]\n\nIn early 2013 the first ever Viral Summit was held in Las Vegas. It attempted to identify similar trends in viral marketing methods for various media.\nMethods and metrics\nAmbox current red.svg\n\tThis section's factual accuracy may be compromised due to out-of-date information. Please update this article to reflect recent events or newly available information. (January 2013)\n\nAccording to marketing professors Andreas Kaplan and Michael Haenlein, to make viral marketing work, three basic criteria must be met, i.e., giving the right message to the right messengers in the right environment:[14]\n\n    Messenger: Three specific types of messengers are required to ensure the transformation of an ordinary message into a viral one: market mavens, social hubs, and salespeople. Market mavens are individuals who are continuously 'on the pulse' of things (information specialists); they are usually among the first to get exposed to the message and who transmit it to their immediate social network. Social hubs are people with an exceptionally large number of social connections; they often know hundreds of different people and have the ability to serve as connectors or bridges between different subcultures. Salespeople might be needed who receive the message from the market maven, amplify it by making it more relevant and persuasive, and then transmit it to the social hub for further distribution. Market mavens may not be particularly convincing in transmitting the information.\n    Message: Only messages that are both memorable and sufficiently interesting to be passed on to others have the potential to spur a viral marketing phenomenon. Making a message more memorable and interesting or simply more infectious, is often not a matter of major changes but minor adjustments. It should be unique and engaging with a main idea that motivates the recipient to share it widely with friends – a \"must-see\" element.[15]\n    Environment: The environment is crucial in the rise of successful viral marketing – small changes in the environment lead to huge results, and people are much more sensitive to environment. The timing and context of the campaign launch must be right.\n\nWhereas Kaplan, Haenlein and others reduce the role of marketers to crafting the initial viral message and seeding it, futurist and sales and marketing analyst Marc Feldman, who conducted IMT Strategies' viral marketing study in 2001,[citation needed] carves a different role for marketers which pushes the 'art' of viral marketing much closer to 'science.'[16]\nMetrics\n\nTo clarify and organize the information related to potential measures of viral campaigns, the key measurement possibilities should be considered in relation to the objectives formulated for the viral campaign. In this sense, some of the key cognitive outcomes of viral marketing activities can include measures such as the number of views, clicks, and hits for specific content, as well as the number of shares in social media, such as likes on Facebook or retweets on Twitter, which demonstrate that consumers processed the information received through the marketing message. Measures such as the number of reviews for a product or the number of members for a campaign webpage quantify the number of individuals who have acknowledged the information provided by marketers. Besides statistics that are related to online traffic, surveys can assess the degree of product or brand knowledge, though this type of measurement is more complicated and requires more resources.[17][18]\n\nRelated to consumers’ attitudes toward a brand or even toward the marketing communication, different online and social media statistics, including the number of likes and shares within a social network, can be used. The number of reviews for a certain brand or product and the quality assessed by users are indicators of attitudes. Classical measures of consumer attitude toward the brand can be gathered through surveys of consumers. Behavioral measures are very important because changes in consumers’ behavior and buying decisions are what marketers hope to see through viral campaigns. There are numerous indicators that can be used in this context as a function of marketers’ objectives. Some of them include the most known online and social media statistics such as number and quality of shares, views, product reviews, and comments. Consumers’ brand engagement can be measured through the K-factor, the number of followers, friends, registered users, and time spent on the website. Indicators that are more bottom-line oriented focus on consumers’ actions after acknowledging the marketing content, including the number of requests for information, samples, or test-drives. Nevertheless, responses to actual call-to-action messages are important, including the conversion rate. Consumers’ behavior is expected to lead to contributions to the bottom line of the company, meaning increase in sales, both in quantity and financial amount. However, when quantifying changes in sales, managers need to consider other factors that could potentially affect sales besides the viral marketing activities. Besides positive effects on sales, the use of viral marketing is expected to bring significant reductions in marketing costs and expenses.[19][20]\nMethods\n\nViral marketing often involves and utilizes:\n\n    Customer participation and polling services\n    Industry-specific organization contributions\n    Web search engines and blogs\n    Mobile smartphone integration\n    Multiple forms of print and direct marketing\n    Target marketing web services\n    Search engine optimization (SEO)\n    Social media optimization (SMO)\n    Television and radio\n\nViral target marketing is based on three important principles:[21]\n\n    Social profile gathering\n    Proximity market analysis\n    Real-time key word density analysis\n\nBy applying these three important disciplines to an advertising model, a VMS company is able to match a client with their targeted customers at a cost effective advantage.\n\nThe Internet makes it possible for a campaign to go viral very fast; it can, so to speak, make a brand famous overnight. However, the Internet and social media technologies themselves do not make a brand viral; they just enable people to share content to other people faster. Therefore, it is generally agreed that a campaign must typically follow a certain set of guidelines in order to potentially be successful:[22]\n\n    It must be appealing to most of the audience.\n    It must be worth sharing with friends and family.\n    A large platform, e.g. YouTube or Facebook must be used.[23]\n    An initial boost to gain attention is used, e.g. seeding, buying views, or sharing to Facebook fans.\n    The content is of good quality.\n\nSocial networking\n\nThe growth of social networks significantly contributed to the effectiveness of viral marketing.[24] As of 2009, two thirds of the world's Internet population visits a social networking service or blog site at least every week.[25] Facebook alone has over 1 billion active users.[26] In 2009, time spent visiting social media sites began to exceed time spent emailing.[27] A 2010 study found that 52% of people who view news online forward it on through social networks, email, or posts.[28]\nNotable examples\n\tThis article may contain excessive, poor, irrelevant, or self-sourcing examples. Please improve the article by adding more descriptive text and removing less pertinent examples. See Wikipedia's guide to writing better articles for further suggestions. (November 2011)\n\nEarly in its existence, the television show Mystery Science Theater 3000 had limited distribution. The producers encouraged viewers to make copies of the show on video tapes and give them to friends in order to expand viewership and increase demand for the fledgling Comedy Channel network. During this period the closing credits included the words \"Keep circulating the tapes!\"[29]\n\nBetween 1996–1997, Hotmail was one of the first internet businesses to become extremely successful utilizing viral marketing techniques by inserting the tagline \"Get your free e-mail at Hotmail\" at the bottom of every e-mail sent out by its users. Hotmail was able to sign up 12 million users in 18 months.[30] At the time, this was historically the fastest growth of any user based media company.[31] By the time Hotmail reached \"66 million users\", the company was establishing \"270,000 new accounts each day\".[31]\n\nIn 2000, Slate.com described TiVo's unpublicized gambit of giving free systems to web-savvy enthusiasts to create \"viral\" word of mouth, pointing out that a viral campaign differs from a publicity stunt.[32]\n\nBurger King has used several marketing campaigns. Its The Subservient Chicken campaign, running from 2004 until 2007, was an example of viral or word-of-mouth marketing.[33]\n\nThe Blendtec viral video series Will It Blend? debuted in 2006. In the show, Tom Dickson, Blendtec founder and CEO, attempts to blend various unusual items in order to show off the power of his blender. Will it Blend? has been nominated for the 2007 YouTube award for Best Series, winner of .Net Magazine's 2007 Viral Video campaign of the year and winner of the Bronze level Clio Award for Viral Video in 2008.[34] In 2010, Blendtec claimed the top spot on the AdAge list of \"Top 10 Viral Ads of All Time.\"[35] The Will It Blend page on YouTube currently shows over 200 million video views.[36]\n\nIn 2007, World Wrestling Entertainment promoted the return of Chris Jericho with a viral marketing campaign using 15-second cryptic binary code videos. The videos contained hidden messages and biblical links related to Jericho, although speculation existed throughout WWE fans over whom the campaign targeted.[37][38] The text \"Save Us\" and \"2nd Coming\" were most prominent in the videos. The campaign spread throughout the internet with numerous websites, though no longer operational, featuring hidden messages and biblical links to further hint at Jericho's return.[39][40]\n\nIn 2007, Portuguese football club Sporting Portugal integrated a viral feature in their campaign for season seats. In their website, a video required the user to input his name and phone number before playback started, which then featured the coach Paulo Bento and the players waiting at the locker room while he makes a phone call to the user telling him that they just can't start the season until the user buys his season ticket.[41]\n\nThe Big Word Project, launched in 2008, aimed to redefine the Oxford English Dictionary by allowing people to submit their website as the definition of their chosen word. The project, created to fund two Masters students' educations, attracted the attention of bloggers worldwide, and was featured on Daring Fireball and Wired Magazine.[42]\n\nBetween December 2009 and March 2010 a series of seven videos were posted to YouTube under the name \"iamamiwhoami\" leading to speculation that they were a marketing campaign for a musician. In March 2010, an anonymous package was sent to an MTV journalist claiming to contain a code which if cracked would give the identity of the artist.[43] The seventh video, entitled 'y', appears to feature the Swedish singer Jonna Lee.[44][45][46][47]\n\nOn July 14, 2010, Old Spice launched the fastest growing online viral video campaign ever, garnering 6.7 million views after 24 hours, ballooning over 23 million views after 36 hours.[48] Old Spice's agency created a bathroom set in Portland, OR and had their TV commercial star, Isaiah Mustafa, reply to 186 online comments and questions from websites like Twitter, Facebook, Reddit, Digg, YouTube and others. The campaign ran for 3 days.[49]\n\nCompanies may also be able to use a viral video that they did not create for marketing purposes. A notable example is the viral video \"The Extreme Diet Coke & Mentos Experiments\" created by Fritz Grobe and Stephen Voltz of EepyBird. After the initial success of the video, Mentos were quick to offer their support. They shipped EepyBird thousands of mints for their experiments. Coke were slower to get involved.[50]\n\nOn March 6, 2012, Dollar Shave Club launched their online video campaign. In the first 48hrs of their video debuting on YouTube they had over 12,000 people signing up for the service. The video cost just $4500 to make and as of November 2015 has had more than 21 million views. The video was considered as one of the best viral marketing campaigns[51] of 2012 and won \"Best Out-of-Nowhere Video Campaign\" at the 2012 AdAge Viral Video Awards.\nSee also\n\n    Clickbait\n    Guerrilla marketing\n    Internet marketing\n    K-factor (marketing)\n    Marketing buzz\n    Seeding agency\n    Viral (disambiguation)\n    Viral video\n    Visual marketing\n    Growth hacking\n    Social media", "skillName": "Viral_marketing."}
{"id": 134, "category": "Advertising", "skillText": "Video Commerce, Video e-Commerce or eCommerce Video is the practice of using video content to promote, sell and support commercial products or services on the Internet. The video can be downloaded and played or streamed to the viewer. Either way, the video often contains clickable links which can open up a web page or a transaction process.\n\nThe end goal is to convert a shopper into a customer1, but conversion is not the only metric as View Through Rate (VTR) is a common measurement2. Some merchants realize additional benefit such as Search Engine Optimization (SEO)3. A typical video commerce application would involve a video which contains a number of clickable objects so that the viewer can click on any of those objects for further information or to purchase them. However, the clickable object may not always be within the video itself, but part of the Flash or HTML5 player used to play back the video.\n\nThe term vCommerce (video commerce) was first used by ShopNBC (a.k.a. ValueVision - VVTV) in the summer of 2007 as part of the launch of ShopNBC.TV. The strategy combined the use of leveraging web video content for higher conversion on eCommerce websites. The term was quickly adopted in the industry including the start of the Video Commerce Consortium (http://video-commerce.org \n). ShopNBC was also the first home shopping network to utilize live (and archived) webcasts (web-only live video streams which sold products just like TV shopping).\n\nVideo commerce can take place over any Internet-enabled communications device – a Personal Computer, a laptop, a PDA, a mobile phone, or a smart phone.", "skillName": "Video_commerce."}
{"id": 135, "category": "Machine_Learning", "skillText": "Unsupervised learning\n\nUnsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.\n\nUnsupervised learning is closely related to the problem of density estimation in statistics.[1] However unsupervised learning also encompasses many other techniques that seek to summarize and explain key features of the data.\n\nApproaches to unsupervised learning include:\n\nclustering\nk-means\nmixture models\nhierarchical clustering,[2]\nanomaly detection\nNeural Networks\nHebbian Learning\nApproaches for learning latent variable models such as\nExpectation�maximization algorithm (EM)\nMethod of moments\nBlind signal separation techniques, e.g.,\nPrincipal component analysis,\nIndependent component analysis,\nNon-negative matrix factorization,\nSingular value decomposition.[3]\nContents  [hide]\n1\tUnsupervised Learning in Neural Networks\n2\tMethod of moments\n3\tSee also\n4\tNotes\n5\tFurther reading\nUnsupervised Learning in Neural Networks\nThe classical example of unsupervised learning in the study of both natural and artificial neural networks is subsumed by Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version exists that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n\nAmong neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are also used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing. The first version of ART was \"ART1\", developed by Carpenter and Grossberg (1988).[4]\n\nMethod of moments\nOne of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\n\nIn particular, the method of moments is shown to be effective in learning the parameters of latent variable models.[5] Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.[5]\n\nThe Expectation�maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. Alternatively, for the method of moments, the global convergence is guaranteed under some conditions.[5]\n\nSee also\nCluster analysis\nAnomaly detection\nExpectation�maximization algorithm\nGenerative topographic map\nMultivariate analysis\nRadial basis function network\nHebbian Theory", "skillName": "Unsupervised_learning."}
{"id": 136, "category": "Machine_Learning", "skillText": "Statistics\nStatistics is the study of the collection, analysis, interpretation, presentation, and organization of data.[1] In applying statistics to, e.g., a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1]\n\nSome popular definitions are:\n\nMerriam-Webster dictionary defines statistics as \"classified facts representing the conditions of a people in a state � especially the facts that can be stated in numbers or any other tabular or classified arrangement[2]\".\nStatistician Sir Arthur Lyon Bowley defines statistics as \"Numerical statements of facts in any department of inquiry placed in relation to each other[3]\".\nWhen census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\n\nTwo main statistical methodologies are used in data analysis: descriptive statistics, which summarizes data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draws conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[4] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\n\nA standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and a synthetic data drawn from idealized model. An hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\").[5] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]\n\nMeasurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\nStatistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.\n\n1\tScope\n1.1\tMathematical statistics\n2\tOverview\n3\tData collection\n3.1\tSampling\n3.2\tExperimental and observational studies\n4\tTypes of data\n5\tTerminology and theory of inferential statistics\n5.1\tStatistics, estimators and pivotal quantities\n5.2\tNull hypothesis and alternative hypothesis\n5.3\tError\n5.4\tInterval estimation\n5.5\tSignificance\n5.6\tExamples\n6\tMisuse of statistics\n6.1\tMisinterpretation: correlation\n7\tHistory of statistical science\n8\tApplications\n8.1\tApplied statistics, theoretical statistics and mathematical statistics\n8.2\tMachine learning and data mining\n8.3\tStatistics in society\n8.4\tStatistical computing\n8.5\tStatistics applied to mathematics or the arts\n9\tSpecialized disciplines\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nScope\nStatistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data,[6] or as a branch of mathematics.[7] Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.[8][9]\n\nMathematical statistics\nMain article: Mathematical statistics\nMathematical statistics is the application of mathematics to statistics, which was originally conceived as the science of the state � the collection and analysis of facts about a country: its economy, land, military, population, and so forth. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[10][11]\n\nOverview\nIn applying statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as \"all persons living in a country\" or \"every atom composing a crystal\".\n\nIdeally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).\n\nWhen a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.\n\nData collection\nSampling\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting the use of data through statistical models. To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\n\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction�inductively inferring from samples to the parameters of a larger or total population.\n\nExperimental and observational studies\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data � like natural experiments and observational studies[12] � for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\n\nExperiments\nThe basic steps of a statistical experiment are:\n\nPlanning the research, including finding the number of replicates of the study, using the following information: preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.\nDesign of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.\nPerforming the experiment following the experimental protocol and analyzing the data following the experimental protocol.\nFurther examining the data set in secondary analyses, to suggest new hypotheses for future study.\nDocumenting and presenting the results of the study.\nExperiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[13]\n\nObservational study\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a case-control study, and then look for the number of cases of lung cancer in each group.\n\nTypes of data\nMain articles: Statistical data type and Levels of measurement\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\n\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\n\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977)[14] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[15] described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998),[16] van den Berg (1991).[17]\n\nThe issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer\" (Hand, 2004, p. 82).[18]\n\nTerminology and theory of inferential statistics\nStatistics, estimators and pivotal quantities\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[19] The population being examined is described by a probability distribution that may have unknown parameters.\n\nA statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.\n\nConsider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\n\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\n\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\n\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\n\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\n\nNull hypothesis and alternative hypothesis\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[20][21]\n\nThe best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\n\nWhat statisticians call an alternative hypothesis is simply an hypothesis that contradicts the null hypothesis.\n\nError\nWorking from a null hypothesis, two basic forms of error are recognized:\n\nType I errors where the null hypothesis is falsely rejected giving a \"false positive\".\nType II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed giving a \"false negative\".\nStandard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\n\nA statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\n\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\n\n\nA least squares fit: in red the points to be fitted, in blue the fitted line.\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\n\nMeasurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.[22]\n\nInterval estimation\nMain article: Interval estimation\n\nConfidence intervals: the red line is true value for the mean in this example, the blue lines are random confidence intervals for 100 realizations.\nMost studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\n\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\n\nSignificance\nMain article: Statistical significance\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\n\n\nIn this graph the black line is probability distribution for the test statistic, the critical region is the set of values to the right of the observed data point (observed value of the test statistic) and the p-value is represented by the green area.\nThe standard approach[19] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\n\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\n\nWhile in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.\n\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\n\nA difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.\nFallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.[23]\nRejecting the null hypothesis does not automatically prove the alternative hypothesis.\nAs everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.\nExamples\nSome well-known statistical tests and procedures are:\n\nAnalysis of variance (ANOVA)\nChi-squared test\nCorrelation\nFactor analysis\nMann�Whitney U\nMean square weighted deviation (MSWD)\nPearson product-moment correlation coefficient\nRegression analysis\nSpearman's rank correlation coefficient\nStudent's t-test\nTime series analysis\nConjoint Analysis\nMisuse of statistics\nMain article: Misuse of statistics\nMisuse of statistics can produce subtle, but serious errors in description and interpretation�subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\n\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data�which measures the extent to which a trend could be caused by random variation in the sample�may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\n\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[24] A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics[24] outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[25]\n\nWays to avoid misuse of statistics include using proper diagrams and avoiding bias.[26] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[27] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[26] Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[27] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[28] According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"[29]\n\nTo assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[30]\n\nWho says so? (Does he/she have an axe to grind?)\nHow does he/she know? (Does he/she have the resources to know the facts?)\nWhat�s missing? (Does he/she give us a complete picture?)\nDid someone change the subject? (Does he/she offer us the right answer to the wrong problem?)\nDoes it make sense? (Is his/her conclusion logical and consistent with what we already know?)\n\nThe confounding variable problem: X and Y may be correlated, not because there is causal relationship between them, but because both depend on a third variable Z. Z is called a confounding factor.\nMisinterpretation: correlation\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)\n\nHistory of statistical science\n\nGerolamo Cardano, the earliest pioneer on the mathematics of probability.\nMain articles: History of statistics and Founders of statistics\nStatistical methods date back at least to the 5th century BC.\n\nSome scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[31] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n\nIts mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel.[32] The method of least squares was first described by Adrien-Marie Legendre in 1805.\n\n\nKarl Pearson, a founder of mathematical statistics.\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages.[33] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics � height, weight, eyelash length among others.[34] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[35] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[36] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[37]\n\nRonald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".[38][39]\n\nThe second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance, which was the first to use the statistical term, variance, his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[40][41][42][43] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[44] In his 1930 book The Genetical Theory of Natural Selection he applied statistics to various biological concepts such as Fisher's principle[45]). Nevertheless, A. W. F. Edwards has remarked that it is \"probably the most celebrated argument in evolutionary biology\".[45] (about the sex ratio), the Fisherian runaway,[46][47][48][49][50][51] a concept in sexual selection about a positive feedback runaway affect found in evolution.\n\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[52]\n\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.[53]\n\nApplications\nApplied statistics, theoretical statistics and mathematical statistics\n\"Applied statistics\" comprises descriptive statistics and the application of inferential statistics.[54][55] Theoretical statistics concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\n\nMachine learning and data mining\nThere are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis.\n\nStatistics in society\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.\n\nStatistical computing\n\ngretl, an example of an open source statistical package\nMain article: Computational statistics\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\n\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available.\n\nStatistics applied to mathematics or the arts\nTraditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.\n\nIn number theory, scatter plots of data generated by a distribution function may be transformed with familiar tools used in statistics to reveal underlying patterns, which may then lead to hypotheses.\nMethods of statistics including predictive methods in forecasting are combined with chaos theory and fractal geometry to create video works that are considered to have great beauty.[citation needed]\nThe process art of Jackson Pollock relied on artistic experiments whereby underlying distributions in nature were artistically revealed.[citation needed] With the advent of computers, statistical methods were applied to formalize such distribution-driven natural processes to make and analyze moving video art.[citation needed]\nMethods of statistics may be used predicatively in performance art, as in a card trick based on a Markov process that only works some of the time, the occasion of which can be predicted using statistical methodology.\nStatistics can be used to predicatively create art, as in the statistical or stochastic music invented by Iannis Xenakis, where the music is performance-specific. Though this type of artistry does not always come out as expected, it does behave in ways that are predictable and tunable using statistics.\nSpecialized disciplines\nMain article: List of fields of application of statistics\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n\nActuarial science (assesses risk in the insurance and finance industries)\nApplied information economics\nAstrostatistics (statistical evaluation of astronomical data)\nBiostatistics\nBusiness statistics\nChemometrics (for analysis of data from chemistry)\nData mining (applying statistics and pattern recognition to discover knowledge from data)\nData science\nDemography\nEconometrics (statistical analysis of economic data)\nEnergy statistics\nEngineering statistics\nEpidemiology (statistical analysis of disease)\nGeography and Geographic Information Systems, specifically in Spatial analysis\nImage processing\nMedical Statistics\nPsychological statistics\nReliability engineering\nSocial statistics\nStatistical Mechanics\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\n\nBootstrap / Jackknife resampling\nMultivariate statistics\nStatistical classification\nStructured data analysis (statistics)\nStructural equation modelling\nSurvey methodology\nSurvival analysis\nStatistics in various sports, particularly baseball - known as Sabermetrics - and cricket\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.\n\nSee also\nLibrary resources about\nStatistics\nResources in your library\nMain article: Outline of statistics\nAbundance estimation\nData science\nGlossary of probability and statistics\nList of academic statistical associations\nList of important publications in statistics\nList of national and international statistical services\nList of statistical packages (software)\nList of statistics articles\nList of university statistical consulting centers\nNotation in probability and statistics\nFoundations and major areas of statistics\nFoundations of statistics\nList of statisticians\nOfficial statistics\nMultivariate analysis of variance", "skillName": "Statistics."}
{"id": 137, "category": "Machine_Learning", "skillText": "Supervised learning\nSupervised learning is the machine learning task of inferring a function from labeled training data.[1] The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias).\n\nThe parallel task in human and animal psychology is often referred to as concept learning.\n\nIn order to solve a given problem of supervised learning, one has to perform the following steps:\n\nDetermine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, or an entire line of handwriting.\nGather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\nDetermine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\nDetermine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support vector machines or decision trees.\nComplete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.\nEvaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.\nA wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\n\nThere are four major issues to consider in supervised learning:\n\nBias-variance tradeoff\nMain article: Bias-variance dilemma\nA first issue is the tradeoff between bias and variance.[2] Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input {\\displaystyle x} x if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for {\\displaystyle x} x. A learning algorithm has high variance for a particular input {\\displaystyle x} x if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[3] Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\n\nFunction complexity and amount of training data\nThe second issue is the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function). If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be learnable from a very large amount of training data and using a \"flexible\" learning algorithm with low bias and high variance. Good learning algorithms therefore automatically adjust the bias/variance tradeoff based on the amount of data available and the apparent complexity of the function to be learned.\n\nDimensionality of the input space\nA third issue is the dimensionality of the input space. If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features. This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, high input dimensionality typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\n\nNoise in the output values\nA fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation that part of the target function that cannot be modeled \"corrupts\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\n\nIn practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[4][5]\n\nOther factors to consider\nOther factors to consider when choosing and applying a learning algorithm include the following:\n\nHeterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including Support Vector Machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data.\nRedundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization.\nPresence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, Support Vector Machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.\nWhen considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\n\nThe most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k-nearest neighbor algorithm, and Neural Networks (Multilayer perceptron).\n\nHow supervised learning algorithms work\nGiven a set of {\\displaystyle N} N training examples of the form {\\displaystyle \\{(x_{1},y_{1}),...,(x_{N},\\;y_{N})\\}} \\{(x_1, y_1), ..., (x_N,\\; y_N)\\} such that {\\displaystyle x_{i}} x_{i} is the feature vector of the i-th example and {\\displaystyle y_{i}} y_{i} is its label (i.e., class), a learning algorithm seeks a function {\\displaystyle g:X\\to Y} g: X \\to Y, where {\\displaystyle X} X is the input space and {\\displaystyle Y} Y is the output space. The function {\\displaystyle g} g is an element of some space of possible functions {\\displaystyle G} G, usually called the hypothesis space. It is sometimes convenient to represent {\\displaystyle g} g using a scoring function {\\displaystyle f:X\\times Y\\to {\\mathbb {R}}} f: X \\times Y \\to \\Bbb{R} such that {\\displaystyle g} g is defined as returning the {\\displaystyle y} y value that gives the highest score: {\\displaystyle g(x)=\\arg \\max _{y}\\;f(x,y)} g(x) = \\arg \\max_y \\; f(x,y). Let {\\displaystyle F} F denote the space of scoring functions.\n\nAlthough {\\displaystyle G} G and {\\displaystyle F} F can be any space of functions, many learning algorithms are probabilistic models where {\\displaystyle g} g takes the form of a conditional probability model {\\displaystyle g(x)=P(y|x)} g(x) =\nP(y|x), or {\\displaystyle f} f takes the form of a joint probability model {\\displaystyle f(x,y)=P(x,y)} f(x,y) = P(x,y). For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\n\nThere are two basic approaches to choosing {\\displaystyle f} f or {\\displaystyle g} g: empirical risk minimization and structural risk minimization.[6] Empirical risk minimization seeks the function that best fits the training data. Structural risk minimize includes a penalty function that controls the bias/variance tradeoff.\n\nIn both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, {\\displaystyle (x_{i},\\;y_{i})} (x_i, \\;y_i). In order to measure how well a function fits the training data, a loss function {\\displaystyle L:Y\\times Y\\to {\\mathbb {R}}^{\\geq 0}} L: Y \\times Y \\to\n\\Bbb{R}^{\\ge 0} is defined. For training example {\\displaystyle (x_{i},\\;y_{i})} (x_i,\\;y_i), the loss of predicting the value {\\displaystyle {\\hat {y}}} {\\hat {y}} is {\\displaystyle L(y_{i},{\\hat {y}})} L(y_i,\\hat{y}).\n\nThe risk {\\displaystyle R(g)} R(g) of function {\\displaystyle g} g is defined as the expected loss of {\\displaystyle g} g. This can be estimated from the training data as\n\n{\\displaystyle R_{emp}(g)={\\frac {1}{N}}\\sum _{i}L(y_{i},g(x_{i}))} R_{emp}(g) = \\frac{1}{N} \\sum_i L(y_i, g(x_i)).\nEmpirical risk minimization\nMain article: Empirical risk minimization\nIn empirical risk minimization, the supervised learning algorithm seeks the function {\\displaystyle g} g that minimizes {\\displaystyle R(g)} R(g). Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find {\\displaystyle g} g.\n\nWhen {\\displaystyle g} g is a conditional probability distribution {\\displaystyle P(y|x)} P(y|x) and the loss function is the negative log likelihood: {\\displaystyle L(y,{\\hat {y}})=-\\log P(y|x)} L(y, \\hat{y}) = -\\log P(y | x), then empirical risk minimization is equivalent to maximum likelihood estimation.\n\nWhen {\\displaystyle G} G contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting.\n\nStructural risk minimization\nStructural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.\n\nA wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function {\\displaystyle g} g is a linear function of the form\n\n{\\displaystyle g(x)=\\sum _{j=1}^{d}\\beta _{j}x_{j}}  g(x) = \\sum_{j=1}^d \\beta_j x_j.\nA popular regularization penalty is {\\displaystyle \\sum _{j}\\beta _{j}^{2}} \\sum_j \\beta_j^2, which is the squared Euclidean norm of the weights, also known as the {\\displaystyle L_{2}} L_{2} norm. Other norms include the {\\displaystyle L_{1}} L_{1} norm, {\\displaystyle \\sum _{j}|\\beta _{j}|} \\sum_j |\\beta_j|, and the {\\displaystyle L_{0}} L_{0} norm, which is the number of non-zero {\\displaystyle \\beta _{j}} \\beta _{j}s. The penalty will be denoted by {\\displaystyle C(g)} C(g).\n\nThe supervised learning optimization problem is to find the function {\\displaystyle g} g that minimizes\n\n{\\displaystyle J(g)=R_{emp}(g)+\\lambda C(g).}  J(g) = R_{emp}(g) + \\lambda C(g).\nThe parameter {\\displaystyle \\lambda } \\lambda  controls the bias-variance tradeoff. When {\\displaystyle \\lambda =0} \\lambda =0, this gives empirical risk minimization with low bias and high variance. When {\\displaystyle \\lambda } \\lambda  is large, the learning algorithm will have high bias and low variance. The value of {\\displaystyle \\lambda } \\lambda  can be chosen empirically via cross validation.\n\nThe complexity penalty has a Bayesian interpretation as the negative log prior probability of {\\displaystyle g} g, {\\displaystyle -\\log P(g)} -\\log P(g), in which case {\\displaystyle J(g)} J(g) is the posterior probabability of {\\displaystyle g} g.\n\nGenerative training\nThe training methods described above are discriminative training methods, because they seek to find a function {\\displaystyle g} g that discriminates well between the different output values (see discriminative model). For the special case where {\\displaystyle f(x,y)=P(x,y)} f(x,y) = P(x,y) is a joint probability distribution and the loss function is the negative log likelihood {\\displaystyle -\\sum _{i}\\log P(x_{i},y_{i}),} - \\sum_i \\log P(x_i, y_i), a risk minimization algorithm is said to perform generative training, because {\\displaystyle f} f can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\n\nGeneralizations of supervised learning\nThere are several ways in which the standard supervised learning problem can be generalized:\n\nSemi-supervised learning: In this setting, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled.\nActive learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning.\nStructured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended.\nLearning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended.\nApproaches and algorithms\nAnalytical learning\nArtificial neural network\nBackpropagation\nBoosting (meta-algorithm)\nBayesian statistics\nCase-based reasoning\nDecision tree learning\nInductive logic programming\nGaussian process regression\nGroup method of data handling\nKernel estimators\nLearning Automata\nMinimum message length (decision trees, decision graphs, etc.)\nMultilinear subspace learning\nNaive bayes classifier\nNearest Neighbor Algorithm\nProbably approximately correct learning (PAC) learning\nRipple down rules, a knowledge acquisition methodology\nSymbolic machine learning algorithms\nSubsymbolic machine learning algorithms\nSupport vector machines\nMinimum Complexity Machines (MCM)\nRandom Forests\nEnsembles of Classifiers\nOrdinal classification\nData Pre-processing\nHandling imbalanced datasets\nStatistical relational learning\nProaftn, a multicriteria classification algorithm\nApplications\nBioinformatics\nCheminformatics\nQuantitative structure�activity relationship\nDatabase marketing\nHandwriting recognition\nInformation retrieval\nLearning to rank\nObject recognition in computer vision\nOptical character recognition\nSpam detection\nPattern recognition\nSpeech recognition\nSupervised learning is a special case of Downward causation in biological systems\nGeneral issues\nComputational learning theory\nInductive bias\nOverfitting (machine learning)\n(Uncalibrated) Class membership probabilities\nUnsupervised learning\nVersion spaces", "skillName": "Supervised_Learning."}
{"id": 138, "category": "Machine_Learning", "skillText": "Supervised learning\nSupervised learning is the machine learning task of inferring a function from labeled training data.[1] The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias).\n\nThe parallel task in human and animal psychology is often referred to as concept learning.\n\nIn order to solve a given problem of supervised learning, one has to perform the following steps:\n\nDetermine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, or an entire line of handwriting.\nGather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\nDetermine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\nDetermine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support vector machines or decision trees.\nComplete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.\nEvaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.\nA wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\n\nThere are four major issues to consider in supervised learning:\n\nBias-variance tradeoff\nMain article: Bias-variance dilemma\nA first issue is the tradeoff between bias and variance.[2] Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input {\\displaystyle x} x if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for {\\displaystyle x} x. A learning algorithm has high variance for a particular input {\\displaystyle x} x if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[3] Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\n\nFunction complexity and amount of training data\nThe second issue is the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function). If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be learnable from a very large amount of training data and using a \"flexible\" learning algorithm with low bias and high variance. Good learning algorithms therefore automatically adjust the bias/variance tradeoff based on the amount of data available and the apparent complexity of the function to be learned.\n\nDimensionality of the input space\nA third issue is the dimensionality of the input space. If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features. This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, high input dimensionality typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\n\nNoise in the output values\nA fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation that part of the target function that cannot be modeled \"corrupts\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\n\nIn practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[4][5]\n\nOther factors to consider\nOther factors to consider when choosing and applying a learning algorithm include the following:\n\nHeterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including Support Vector Machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data.\nRedundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form of regularization.\nPresence of interactions and non-linearities. If each of the features makes an independent contribution to the output, then algorithms based on linear functions (e.g., linear regression, logistic regression, Support Vector Machines, naive Bayes) and distance functions (e.g., nearest neighbor methods, support vector machines with Gaussian kernels) generally perform well. However, if there are complex interactions among features, then algorithms such as decision trees and neural networks work better, because they are specifically designed to discover these interactions. Linear methods can also be applied, but the engineer must manually specify the interactions when using them.\nWhen considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\n\nThe most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k-nearest neighbor algorithm, and Neural Networks (Multilayer perceptron).\n\nHow supervised learning algorithms work\nGiven a set of {\\displaystyle N} N training examples of the form {\\displaystyle \\{(x_{1},y_{1}),...,(x_{N},\\;y_{N})\\}} \\{(x_1, y_1), ..., (x_N,\\; y_N)\\} such that {\\displaystyle x_{i}} x_{i} is the feature vector of the i-th example and {\\displaystyle y_{i}} y_{i} is its label (i.e., class), a learning algorithm seeks a function {\\displaystyle g:X\\to Y} g: X \\to Y, where {\\displaystyle X} X is the input space and {\\displaystyle Y} Y is the output space. The function {\\displaystyle g} g is an element of some space of possible functions {\\displaystyle G} G, usually called the hypothesis space. It is sometimes convenient to represent {\\displaystyle g} g using a scoring function {\\displaystyle f:X\\times Y\\to {\\mathbb {R}}} f: X \\times Y \\to \\Bbb{R} such that {\\displaystyle g} g is defined as returning the {\\displaystyle y} y value that gives the highest score: {\\displaystyle g(x)=\\arg \\max _{y}\\;f(x,y)} g(x) = \\arg \\max_y \\; f(x,y). Let {\\displaystyle F} F denote the space of scoring functions.\n\nAlthough {\\displaystyle G} G and {\\displaystyle F} F can be any space of functions, many learning algorithms are probabilistic models where {\\displaystyle g} g takes the form of a conditional probability model {\\displaystyle g(x)=P(y|x)} g(x) =\nP(y|x), or {\\displaystyle f} f takes the form of a joint probability model {\\displaystyle f(x,y)=P(x,y)} f(x,y) = P(x,y). For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\n\nThere are two basic approaches to choosing {\\displaystyle f} f or {\\displaystyle g} g: empirical risk minimization and structural risk minimization.[6] Empirical risk minimization seeks the function that best fits the training data. Structural risk minimize includes a penalty function that controls the bias/variance tradeoff.\n\nIn both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, {\\displaystyle (x_{i},\\;y_{i})} (x_i, \\;y_i). In order to measure how well a function fits the training data, a loss function {\\displaystyle L:Y\\times Y\\to {\\mathbb {R}}^{\\geq 0}} L: Y \\times Y \\to\n\\Bbb{R}^{\\ge 0} is defined. For training example {\\displaystyle (x_{i},\\;y_{i})} (x_i,\\;y_i), the loss of predicting the value {\\displaystyle {\\hat {y}}} {\\hat {y}} is {\\displaystyle L(y_{i},{\\hat {y}})} L(y_i,\\hat{y}).\n\nThe risk {\\displaystyle R(g)} R(g) of function {\\displaystyle g} g is defined as the expected loss of {\\displaystyle g} g. This can be estimated from the training data as\n\n{\\displaystyle R_{emp}(g)={\\frac {1}{N}}\\sum _{i}L(y_{i},g(x_{i}))} R_{emp}(g) = \\frac{1}{N} \\sum_i L(y_i, g(x_i)).\nEmpirical risk minimization\nMain article: Empirical risk minimization\nIn empirical risk minimization, the supervised learning algorithm seeks the function {\\displaystyle g} g that minimizes {\\displaystyle R(g)} R(g). Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find {\\displaystyle g} g.\n\nWhen {\\displaystyle g} g is a conditional probability distribution {\\displaystyle P(y|x)} P(y|x) and the loss function is the negative log likelihood: {\\displaystyle L(y,{\\hat {y}})=-\\log P(y|x)} L(y, \\hat{y}) = -\\log P(y | x), then empirical risk minimization is equivalent to maximum likelihood estimation.\n\nWhen {\\displaystyle G} G contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting.\n\nStructural risk minimization\nStructural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.\n\nA wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function {\\displaystyle g} g is a linear function of the form\n\n{\\displaystyle g(x)=\\sum _{j=1}^{d}\\beta _{j}x_{j}}  g(x) = \\sum_{j=1}^d \\beta_j x_j.\nA popular regularization penalty is {\\displaystyle \\sum _{j}\\beta _{j}^{2}} \\sum_j \\beta_j^2, which is the squared Euclidean norm of the weights, also known as the {\\displaystyle L_{2}} L_{2} norm. Other norms include the {\\displaystyle L_{1}} L_{1} norm, {\\displaystyle \\sum _{j}|\\beta _{j}|} \\sum_j |\\beta_j|, and the {\\displaystyle L_{0}} L_{0} norm, which is the number of non-zero {\\displaystyle \\beta _{j}} \\beta _{j}s. The penalty will be denoted by {\\displaystyle C(g)} C(g).\n\nThe supervised learning optimization problem is to find the function {\\displaystyle g} g that minimizes\n\n{\\displaystyle J(g)=R_{emp}(g)+\\lambda C(g).}  J(g) = R_{emp}(g) + \\lambda C(g).\nThe parameter {\\displaystyle \\lambda } \\lambda  controls the bias-variance tradeoff. When {\\displaystyle \\lambda =0} \\lambda =0, this gives empirical risk minimization with low bias and high variance. When {\\displaystyle \\lambda } \\lambda  is large, the learning algorithm will have high bias and low variance. The value of {\\displaystyle \\lambda } \\lambda  can be chosen empirically via cross validation.\n\nThe complexity penalty has a Bayesian interpretation as the negative log prior probability of {\\displaystyle g} g, {\\displaystyle -\\log P(g)} -\\log P(g), in which case {\\displaystyle J(g)} J(g) is the posterior probabability of {\\displaystyle g} g.\n\nGenerative training\nThe training methods described above are discriminative training methods, because they seek to find a function {\\displaystyle g} g that discriminates well between the different output values (see discriminative model). For the special case where {\\displaystyle f(x,y)=P(x,y)} f(x,y) = P(x,y) is a joint probability distribution and the loss function is the negative log likelihood {\\displaystyle -\\sum _{i}\\log P(x_{i},y_{i}),} - \\sum_i \\log P(x_i, y_i), a risk minimization algorithm is said to perform generative training, because {\\displaystyle f} f can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\n\nGeneralizations of supervised learning\nThere are several ways in which the standard supervised learning problem can be generalized:\n\nSemi-supervised learning: In this setting, the desired output values are provided only for a subset of the training data. The remaining data is unlabeled.\nActive learning: Instead of assuming that all of the training examples are given at the start, active learning algorithms interactively collect new examples, typically by making queries to a human user. Often, the queries are based on unlabeled data, which is a scenario that combines semi-supervised learning with active learning.\nStructured prediction: When the desired output value is a complex object, such as a parse tree or a labeled graph, then standard methods must be extended.\nLearning to rank: When the input is a set of objects and the desired output is a ranking of those objects, then again the standard methods must be extended.\nApproaches and algorithms\nAnalytical learning\nArtificial neural network\nBackpropagation\nBoosting (meta-algorithm)\nBayesian statistics\nCase-based reasoning\nDecision tree learning\nInductive logic programming\nGaussian process regression\nGroup method of data handling\nKernel estimators\nLearning Automata\nMinimum message length (decision trees, decision graphs, etc.)\nMultilinear subspace learning\nNaive bayes classifier\nNearest Neighbor Algorithm\nProbably approximately correct learning (PAC) learning\nRipple down rules, a knowledge acquisition methodology\nSymbolic machine learning algorithms\nSubsymbolic machine learning algorithms\nSupport vector machines\nMinimum Complexity Machines (MCM)\nRandom Forests\nEnsembles of Classifiers\nOrdinal classification\nData Pre-processing\nHandling imbalanced datasets\nStatistical relational learning\nProaftn, a multicriteria classification algorithm\nApplications\nBioinformatics\nCheminformatics\nQuantitative structure�activity relationship\nDatabase marketing\nHandwriting recognition\nInformation retrieval\nLearning to rank\nObject recognition in computer vision\nOptical character recognition\nSpam detection\nPattern recognition\nSpeech recognition\nSupervised learning is a special case of Downward causation in biological systems\nGeneral issues\nComputational learning theory\nInductive bias\nOverfitting (machine learning)\n(Uncalibrated) Class membership probabilities\nUnsupervised learning\nVersion spaces", "skillName": "Supervised Learning."}
{"id": 139, "category": "Machine_Learning", "skillText": "Data mining\n\nData mining is an interdisciplinary subfield of computer science.[1][2][3] It is the computational process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems.[1] The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.[1] Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[4]\n\nThe term is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[5] It also is a buzzword[6] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[7] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[8] Often the more general terms (large scale) data analysis and analytics � or, when referring to actual methods, artificial intelligence and machine learning � are more appropriate.\n\nThe actual data mining task is the automatic or semi-automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.\n\nThe related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.\n\n1\tEtymology\n2\tBackground\n3\tProcess\n3.1\tPre-processing\n3.2\tData mining\n3.3\tResults validation\n4\tResearch\n5\tStandards\n6\tNotable uses\n7\tPrivacy concerns and ethics\n7.1\tSituation in Europe\n7.2\tSituation in the United States\n8\tCopyright Law\n8.1\tSituation in Europe\n8.2\tSituation in the United States\n9\tSoftware\n9.1\tFree open-source data mining software and applications\n9.2\tProprietary data-mining software and applications\n9.3\tMarketplace surveys\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nMachine learning\nEtymology\nIn the 1960s, statisticians used terms like \"Data Fishing\" or \"Data Dredging\" to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"Data Mining\" appeared around 1990 in the database community. For a short time in 1980s, a phrase \"database mining\"�, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[9] researchers consequently turned to \"data mining\". Other terms used include Data Archaeology, Information Harvesting, Information Discovery, Knowledge Extraction, etc. Gregory Piatetsky-Shapiro coined the term \"Knowledge Discovery in Databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and Machine Learning Community. However, the term data mining became more popular in the business and press communities.[10] Currently, Data Mining and Knowledge Discovery are used interchangeably. Since about 2007, \"Predictive Analytics\" and since 2011, \"Data Science\" terms were also used to describe this field.\n\nIn the Academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding Editor-in-Chief. Later he started the SIGKDDD Newsletter SIGKDD Explorations.[11] The KDD International conference became the primary highest quality conference in Data Mining with an acceptance rate of research paper submissions below 18%. The Journal Data Mining and Knowledge Discovery is the primary research journal of the field.\n\nBackground\nThe manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology has dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns[12] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever larger data sets.\n\nProcess\nThe Knowledge Discovery in Databases (KDD) process is commonly defined with the stages:\n\n(1) Selection\n(2) Pre-processing\n(3) Transformation\n(4) Data Mining\n(5) Interpretation/Evaluation.[4]\nIt exists, however, in many variations on this theme, such as the Cross Industry Standard Process for Data Mining (CRISP-DM) which defines six phases:\n\n(1) Business Understanding\n(2) Data Understanding\n(3) Data Preparation\n(4) Modeling\n(5) Evaluation\n(6) Deployment\nor a simplified process such as (1) pre-processing, (2) data mining, and (3) results validation.\n\nPolls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[13] The only other data mining standard named in these polls was SEMMA. However, 3�4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[14][15] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[16]\n\nPre-processing\nBefore data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.\n\nData mining\nData mining involves six common classes of tasks:[4]\n\nAnomaly detection (Outlier/change/deviation detection) � The identification of unusual data records, that might be interesting or data errors that require further investigation.\nAssociation rule learning (Dependency modelling) � Searches for relationships between variables. For example, a supermarket might gather data on customer purchasing habits. Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis.\nClustering � is the task of discovering groups and structures in the data that are in some way or another \"similar\", without using known structures in the data.\nClassification � is the task of generalizing known structure to apply to new data. For example, an e-mail program might attempt to classify an e-mail as \"legitimate\" or as \"spam\".\nRegression � attempts to find a function which models the data with the least error.\nSummarization � providing a more compact representation of the data set, including visualization and report generation.\nResults validation\n\nAn example of data produced by data dredging through a bot operated by statistician Tyler Viglen, apparently showing a close link between the best word winning a spelling bee competition and the number of people in the United States killed by venomous spiders. The similarity in trends is obviously a coincidence.\nData mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening.[citation needed]\n\nWiki letter w.svg\nThis section is missing information about non-classification tasks in data mining. It only covers machine learning. Please expand the section to include this information. Further details may exist on the talk page. (September 2011)\nThe final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.\n\nIf the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.\n\nResearch\nThe premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[17][18] Since 1989 this ACM SIG has hosted an annual international conference and published its proceedings,[19] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".[20]\n\nComputer science conferences on data mining include:\n\nCIKM Conference � ACM Conference on Information and Knowledge Management\nDMIN Conference � International Conference on Data Mining\nDMKD Conference � Research Issues on Data Mining and Knowledge Discovery\nDSAA Conference � IEEE International Conference on Data Science and Advanced Analytics\nECDM Conference � European Conference on Data Mining\nECML-PKDD Conference � European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases\nEDM Conference � International Conference on Educational Data Mining\nINFOCOM Conference � IEEE INFOCOM\nICDM Conference � IEEE International Conference on Data Mining\nKDD Conference � ACM SIGKDD Conference on Knowledge Discovery and Data Mining\nMLDM Conference � Machine Learning and Data Mining in Pattern Recognition\nPAKDD Conference � The annual Pacific-Asia Conference on Knowledge Discovery and Data Mining\nPAW Conference � Predictive Analytics World\nSDM Conference � SIAM International Conference on Data Mining (SIAM)\nSSTD Symposium � Symposium on Spatial and Temporal Databases\nWSDM Conference � ACM Conference on Web Search and Data Mining\nData mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases\n\nStandards\nThere have been some efforts to define standards for the data mining process, for example the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006, but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.\n\nFor exchanging the extracted models � in particular for use in predictive analytics � the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[21]\n\nNotable uses\nMain article: Examples of data mining\nSee also: Category:Applied data mining.\nData mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.\n\nPrivacy concerns and ethics\nWhile the term \"data mining\" itself has no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[22]\n\nThe ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[23] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[24][25]\n\nData mining requires data preparation which can uncover information or patterns which may compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[26] This is not data mining per se, but a result of the preparation of data before � and for the purposes of � the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[27][28][29]\n\nIt is recommended that an individual is made aware of the following before data are collected:[26]\n\nthe purpose of the data collection and any (known) data mining projects;\nhow the data will be used;\nwho will be able to mine the data and use the data and their derivatives;\nthe status of security surrounding access to the data;\nhow collected data can be updated.\nData may also be modified so as to become anonymous, so that individuals may not readily be identified.[26] However, even \"de-identified\"/\"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[30]\n\nThe inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial, emotional, or bodily harm to the indicated individual. In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies.[31]\n\nSituation in Europe\nEurope has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.-E.U. Safe Harbor Principles currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's Global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement have failed.[citation needed]\n\nSituation in the United States\nIn the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week', \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is undermined by the complexity of consent forms that are required of patients and participants, which approach a level of incomprehensibility to average individuals.\"[32] This underscores the necessity for data anonymity in data aggregation and mining practices.\n\nU.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. Use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.\n\nCopyright Law\nSituation in Europe\nDue to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. Where a database is pure data in Europe there is likely to be no copyright, but database rights may exist so data mining becomes subject to regulations by the Database Directive. On the recommendation of the Hargreaves review this led to the UK government to amend its copyright law in 2014[33] to allow content mining as a limitation and exception. Only the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[34] The focus on the solution to this legal issue being licences and not limitations and exceptions led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[35]\n\nSituation in the United States\nBy contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that content mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one being text and data mining.[36]\n\nSoftware\nSee also: Category:Data mining and machine learning software.\nFree open-source data mining software and applications\nThe following applications are available under free/open source licenses. Public access to application sourcecode is also available.\n\nCarrot2: Text and search results clustering framework.\nChemicalize.org: A chemical structure miner and web search engine.\nELKI: A university research project with advanced cluster analysis and outlier detection methods written in the Java language.\nGATE: a natural language processing and language engineering tool.\nKNIME: The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\nMassive Online Analysis (MOA): a real-time big data stream mining with concept drift tool in the Java programming language.\nML-Flex: A software package that enables users to integrate with third-party machine-learning packages written in any programming language, execute classification analyses in parallel across multiple computing nodes, and produce HTML reports of classification results.\nMLPACK library: a collection of ready-to-use machine learning algorithms written in the C++ language.\nNLTK (Natural Language Toolkit): A suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python language.\nOpenNN: Open neural networks library.\nOrange: A component-based data mining and machine learning software suite written in the Python language.\nR: A programming language and software environment for statistical computing, data mining, and graphics. It is part of the GNU Project.\nSCaViS: Java cross-platform data analysis framework developed at Argonne National Laboratory.\nscikit-learn is an open source machine learning library for the Python programming language\nSenticNet API: A semantic and affective resource for opinion mining and sentiment analysis.\nTorch: An open source deep learning library for the Lua programming language and scientific computing framework with wide support for machine learning algorithms.\nUIMA: The UIMA (Unstructured Information Management Architecture) is a component framework for analyzing unstructured content such as text, audio and video � originally developed by IBM.\nWeka: A suite of machine learning software applications written in the Java programming language.\nProprietary data-mining software and applications\nThe following applications are available under proprietary licenses.\n\nAngoss KnowledgeSTUDIO: data mining tool provided by Angoss.\nClarabridge: enterprise class text analytics solution.\nHP Vertica Analytics Platform: data mining software provided by HP.\nIBM SPSS Modeler: data mining software provided by IBM.\nKXEN Modeler: data mining tool provided by KXEN.\nLIONsolver: an integrated software application for data mining, business intelligence, and modeling that implements the Learning and Intelligent OptimizatioN (LION) approach.\nMegaputer Intelligence: data and text mining software is called PolyAnalyst.\nMicrosoft Analysis Services: data mining software provided by Microsoft.\nNetOwl: suite of multilingual text and entity analytics products that enable data mining.\nOpenText� Big Data Analytics: Visual Data Mining & Predictive Analysis by Open Text Corporation\nOracle Data Mining: data mining software by Oracle.\nPSeven: platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining provided by DATADVANCE.\nQlucore Omics Explorer: data mining software provided by Qlucore.\nRapidMiner: An environment for machine learning and data mining experiments.\nSAS Enterprise Miner: data mining software provided by the SAS Institute.\nSTATISTICA Data Miner: data mining software provided by StatSoft.\nTanagra: A visualisation-oriented data mining software, also for teaching.\nMarketplace surveys\nSeveral researchers and organizations have conducted reviews of data mining tools and surveys of data miners. These identify some of the strengths and weaknesses of the software packages. They also provide an overview of the behaviors, preferences and views of data miners. Some of these reports include:\n\nHurwitz Victory Index: Report for Advanced Analytics as a market research assessment tool, it highlights both the diverse uses for advanced analytics technology and the vendors who make those applications possible.Recent-research\n2011 Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery[37]\nRexer Analytics Data Miner Surveys (2007�2013)[38]\nForrester Research 2010 Predictive Analytics and Data Mining Solutions report[39]\nGartner 2008 \"Magic Quadrant\" report[40]\nRobert A. Nisbet's 2006 Three Part Series of articles \"Data Mining Tools: Which One is Best For CRM?\"[41]\nHaughton et al.'s 2003 Review of Data Mining Software Packages in The American Statistician[42]\nGoebel & Gruenwald 1999 \"A Survey of Data Mining a Knowledge Discovery Software Tools\" in SIGKDD Explorations[43]\nSee also\nMethods\nAnomaly/outlier/change detection\nAssociation rule learning\nClassification\nCluster analysis\nDecision tree\nFactor analysis\nGenetic algorithms\nIntention mining\nMultilinear subspace learning\nNeural networks\nRegression analysis\nSequence mining\nStructured data analysis\nSupport vector machines\nText mining\nAgent mining\nApplication domains\nAnalytics\nBehavior informatics\nBig Data\nBioinformatics\nBusiness intelligence\nData analysis\nData warehouse\nDecision support system\nDomain driven data mining\nDrug discovery\nExploratory data analysis\nPredictive analytics\nWeb mining\nApplication examples\nSee also: Category:Applied data mining.\nCustomer analytics\nData mining in agriculture\nData mining in meteorology\nEducational data mining\nNational Security Agency\nPolice-enforced ANPR in the UK\nQuantitative structure�activity relationship\nSurveillance / Mass surveillance (e.g., Stellar Wind)\nRelated topics\nData mining is about analyzing data; for information about extracting information out of data, see:\n\nData integration\nData transformation\nElectronic discovery\nInformation extraction\nInformation integration\nNamed-entity recognition\nProfiling (information science)\nWeb scraping", "skillName": "Data_mining."}
{"id": 140, "category": "Machine_Learning", "skillText": "Cluster analysis\nMachine learning and\ndata mining\nKernel Machine.svg\nProblems[show]\nSupervised learning\n(classification � regression)\n[show]\nClustering[show]\nDimensionality reduction[show]\nStructured prediction[show]\nAnomaly detection[show]\nNeural nets[show]\nTheory[show]\nMachine learning venues[show]\nPortal icon Machine learning portal\n\nThe result of a cluster analysis shown as the coloring of the squares into three clusters.\nCluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.\n\nCluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances among the cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.\n\nBesides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek �?t??? \"grape\") and typological analysis. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest. This often leads to misunderstandings between researchers coming from the fields of data mining and machine learning[citation needed], since they use the same terms and often the same algorithms, but have different goals.\n\nCluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and Robert Tryon in 1939[1][2] and famously used by Cattell beginning in 1943[3] for trait theory classification in personality psychology.\n\n1\tDefinition\n2\tAlgorithms\n2.1\tConnectivity-based clustering (hierarchical clustering)\n2.2\tCentroid-based clustering\n2.3\tDistribution-based clustering\n2.4\tDensity-based clustering\n2.5\tRecent developments\n2.6\tOther methods\n3\tEvaluation and assessment\n3.1\tInternal evaluation\n3.2\tExternal evaluation\n4\tApplications\n5\tSee also\n5.1\tSpecialized types of cluster analysis\n5.2\tTechniques used in cluster analysis\n5.3\tData projection and preprocessing\n5.4\tOther\n6\tReferences\n7\tExternal links\nDefinition\nThe notion of a \"cluster\" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms.[4] There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these \"cluster models\" is key to understanding the differences between the various algorithms. Typical cluster models include:\n\nConnectivity models: for example, hierarchical clustering builds models based on distance connectivity.\nCentroid models: for example, the k-means algorithm represents each cluster by a single mean vector.\nDistribution models: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the Expectation-maximization algorithm.\nDensity models: for example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space.\nSubspace models: in Biclustering (also known as Co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.\nGroup models: some algorithms do not provide a refined model for their results and just provide the grouping information.\nGraph-based models: a clique, that is, a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. Relaxations of the complete connectivity requirement (a fraction of the edges can be missing) are known as quasi-cliques, as in the HCS clustering algorithm.\nA \"clustering\" is essentially a set of such clusters, usually containing all objects in the data set. Additionally, it may specify the relationship of the clusters to each other, for example, a hierarchy of clusters embedded in each other. Clusterings can be roughly distinguished as:\n\nhard clustering: each object belongs to a cluster or not\nsoft clustering (also: fuzzy clustering): each object belongs to each cluster to a certain degree (for example, a likelihood of belonging to the cluster)\nThere are also finer distinctions possible, for example:\n\nstrict partitioning clustering: here each object belongs to exactly one cluster\nstrict partitioning clustering with outliers: objects can also belong to no cluster, and are considered outliers.\noverlapping clustering (also: alternative clustering, multi-view clustering): while usually a hard clustering, objects may belong to more than one cluster.\nhierarchical clustering: objects that belong to a child cluster also belong to the parent cluster\nsubspace clustering: while an overlapping clustering, within a uniquely defined subspace, clusters are not expected to overlap.\nAlgorithms\nMain category: Data clustering algorithms\nClustering algorithms can be categorized based on their cluster model, as listed above. The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms. Not all provide models for their clusters and can thus not easily be categorized. An overview of algorithms explained in Wikipedia can be found in the list of statistics algorithms.\n\nThere is no objectively \"correct\" clustering algorithm, but as it was noted, \"clustering is in the eye of the beholder.\"[4] The most appropriate clustering algorithm for a particular problem often needs to be chosen experimentally, unless there is a mathematical reason to prefer one cluster model over another. It should be noted that an algorithm that is designed for one kind of model has no chance on a data set that contains a radically different kind of model.[4] For example, k-means cannot find non-convex clusters.[4]\n\nConnectivity-based clustering (hierarchical clustering)\nMain article: Hierarchical clustering\nConnectivity based clustering, also known as hierarchical clustering, is based on the core idea of objects being more related to nearby objects than to objects farther away. These algorithms connect \"objects\" to form \"clusters\" based on their distance. A cluster can be described largely by the maximum distance needed to connect parts of the cluster. At different distances, different clusters will form, which can be represented using a dendrogram, which explains where the common name \"hierarchical clustering\" comes from: these algorithms do not provide a single partitioning of the data set, but instead provide an extensive hierarchy of clusters that merge with each other at certain distances. In a dendrogram, the y-axis marks the distance at which the clusters merge, while the objects are placed along the x-axis such that the clusters don't mix.\n\nConnectivity based clustering is a whole family of methods that differ by the way distances are computed. Apart from the usual choice of distance functions, the user also needs to decide on the linkage criterion (since a cluster consists of multiple objects, there are multiple candidates to compute the distance to) to use. Popular choices are known as single-linkage clustering (the minimum of object distances), complete linkage clustering (the maximum of object distances) or UPGMA (\"Unweighted Pair Group Method with Arithmetic Mean\", also known as average linkage clustering). Furthermore, hierarchical clustering can be agglomerative (starting with single elements and aggregating them into clusters) or divisive (starting with the complete data set and dividing it into partitions).\n\nThese methods will not produce a unique partitioning of the data set, but a hierarchy from which the user still needs to choose appropriate clusters. They are not very robust towards outliers, which will either show up as additional clusters or even cause other clusters to merge (known as \"chaining phenomenon\", in particular with single-linkage clustering). In the general case, the complexity is {\\displaystyle {\\mathcal {O}}(n^{3})} {\\mathcal {O}}(n^{3}) for agglomerative clustering and {\\displaystyle {\\mathcal {O}}(2^{n-1})} {\\mathcal {O}}(2^{n-1}) for divisive clustering,[5] which makes them too slow for large data sets. For some special cases, optimal efficient methods (of complexity {\\displaystyle {\\mathcal {O}}(n^{2})} {\\mathcal {O}}(n^{2})) are known: SLINK[6] for single-linkage and CLINK[7] for complete-linkage clustering. In the data mining community these methods are recognized as a theoretical foundation of cluster analysis, but often considered obsolete[citation needed]. They did however provide inspiration for many later methods such as density based clustering.\n\nLinkage clustering examples\n\nSingle-linkage on Gaussian data. At 35 clusters, the biggest cluster starts fragmenting into smaller parts, while before it was still connected to the second largest due to the single-link effect.\n\n\nSingle-linkage on density-based clusters. 20 clusters extracted, most of which contain single elements, since linkage clustering does not have a notion of \"noise\".\nCentroid-based clustering\nMain article: k-means clustering\nIn centroid-based clustering, clusters are represented by a central vector, which may not necessarily be a member of the data set. When the number of clusters is fixed to k, k-means clustering gives a formal definition as an optimization problem: find the {\\displaystyle k} k cluster centers and assign the objects to the nearest cluster center, such that the squared distances from the cluster are minimized.\n\nThe optimization problem itself is known to be NP-hard, and thus the common approach is to search only for approximate solutions. A particularly well known approximative method is Lloyd's algorithm,[8] often actually referred to as \"k-means algorithm\". It does however only find a local optimum, and is commonly run multiple times with different random initializations. Variations of k-means often include such optimizations as choosing the best of multiple runs, but also restricting the centroids to members of the data set (k-medoids), choosing medians (k-medians clustering), choosing the initial centers less randomly (K-means++) or allowing a fuzzy cluster assignment (Fuzzy c-means).\n\nMost k-means-type algorithms require the number of clusters - {\\displaystyle k} k - to be specified in advance, which is considered to be one of the biggest drawbacks of these algorithms. Furthermore, the algorithms prefer clusters of approximately similar size, as they will always assign an object to the nearest centroid. This often leads to incorrectly cut borders in between of clusters (which is not surprising, as the algorithm optimized cluster centers, not cluster borders).\n\nK-means has a number of interesting theoretical properties. First, it partitions the data space into a structure known as a Voronoi diagram. Second, it is conceptually close to nearest neighbor classification, and as such is popular in machine learning. Third, it can be seen as a variation of model based classification, and Lloyd's algorithm as a variation of the Expectation-maximization algorithm for this model discussed below.\n\nk-Means clustering examples\n\nK-means separates data into Voronoi-cells, which assumes equal-sized clusters (not adequate here)\n\n\nK-means cannot represent density-based clusters\nDistribution-based clustering\nThe clustering model most closely related to statistics is based on distribution models. Clusters can then easily be defined as objects belonging most likely to the same distribution. A convenient property of this approach is that this closely resembles the way artificial data sets are generated: by sampling random objects from a distribution.\n\nWhile the theoretical foundation of these methods is excellent, they suffer from one key problem known as overfitting, unless constraints are put on the model complexity. A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult.\n\nOne prominent method is known as Gaussian mixture models (using the expectation-maximization algorithm). Here, the data set is usually modelled with a fixed (to avoid overfitting) number of Gaussian distributions that are initialized randomly and whose parameters are iteratively optimized to fit better to the data set. This will converge to a local optimum, so multiple runs may produce different results. In order to obtain a hard clustering, objects are often then assigned to the Gaussian distribution they most likely belong to; for soft clusterings, this is not necessary.\n\nDistribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes. However, these algorithms put an extra burden on the user: for many real data sets, there may be no concisely defined mathematical model (e.g. assuming Gaussian distributions is a rather strong assumption on the data).\n\nExpectation-Maximization (EM) clustering examples\n\nOn Gaussian-distributed data, EM works well, since it uses Gaussians for modelling clusters\n\n\nDensity-based clusters cannot be modeled using Gaussian distributions\nDensity-based clustering\nIn density-based clustering,[9] clusters are defined as areas of higher density than the remainder of the data set. Objects in these sparse areas - that are required to separate clusters - are usually considered to be noise and border points.\n\nThe most popular[10] density based clustering method is DBSCAN.[11] In contrast to many newer methods, it features a well-defined cluster model called \"density-reachability\". Similar to linkage based clustering, it is based on connecting points within certain distance thresholds. However, it only connects points that satisfy a density criterion, in the original variant defined as a minimum number of other objects within this radius. A cluster consists of all density-connected objects (which can form a cluster of an arbitrary shape, in contrast to many other methods) plus all objects that are within these objects' range. Another interesting property of DBSCAN is that its complexity is fairly low - it requires a linear number of range queries on the database - and that it will discover essentially the same results (it is deterministic for core and noise points, but not for border points) in each run, therefore there is no need to run it multiple times. OPTICS[12] is a generalization of DBSCAN that removes the need to choose an appropriate value for the range parameter {\\displaystyle \\varepsilon } \\varepsilon , and produces a hierarchical result related to that of linkage clustering. DeLi-Clu,[13] Density-Link-Clustering combines ideas from single-linkage clustering and OPTICS, eliminating the {\\displaystyle \\varepsilon } \\varepsilon  parameter entirely and offering performance improvements over OPTICS by using an R-tree index.\n\nThe key drawback of DBSCAN and OPTICS is that they expect some kind of density drop to detect cluster borders. Moreover, they cannot detect intrinsic cluster structures which are prevalent in the majority of real life data. A variation of DBSCAN, EnDBSCAN,[14] efficiently detects such kinds of structures. On data sets with, for example, overlapping Gaussian distributions - a common use case in artificial data - the cluster borders produced by these algorithms will often look arbitrary, because the cluster density decreases continuously. On a data set consisting of mixtures of Gaussians, these algorithms are nearly always outperformed by methods such as EM clustering that are able to precisely model this kind of data.\n\nMean-shift is a clustering approach where each object is moved to the densest area in its vicinity, based on kernel density estimation. Eventually, objects converge to local maxima of density. Similar to k-means clustering, these \"density attractors\" can serve as representatives for the data set, but mean-shift can detect arbitrary-shaped clusters similar to DBSCAN. Due to the expensive iterative procedure and density estimation, mean-shift is usually slower than DBSCAN or k-Means.\n\nDensity-based clustering examples\n\nDensity-based clustering with DBSCAN.\n\n\nDBSCAN assumes clusters of similar density, and may have problems separating nearby clusters\n\n\nOPTICS is a DBSCAN variant that handles different densities much better\nRecent developments\nIn recent years considerable effort has been put into improving the performance of existing algorithms.[15][16] Among them are CLARANS (Ng and Han, 1994),[17] and BIRCH (Zhang et al., 1996).[18] With the recent need to process larger and larger data sets (also known as big data), the willingness to trade semantic meaning of the generated clusters for performance has been increasing. This led to the development of pre-clustering methods such as canopy clustering, which can process huge data sets efficiently, but the resulting \"clusters\" are merely a rough pre-partitioning of the data set to then analyze the partitions with existing slower methods such as k-means clustering. Various other approaches to clustering have been tried such as seed based clustering.[19]\n\nFor high-dimensional data, many of the existing methods fail due to the curse of dimensionality, which renders particular distance functions problematic in high-dimensional spaces. This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated (\"correlated\") subspace clusters that can be modeled by giving a correlation of their attributes.[20] Examples for such clustering algorithms are CLIQUE[21] and SUBCLU.[22]\n\nIdeas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adopted to subspace clustering (HiSC,[23] hierarchical subspace clustering and DiSH[24]) and correlation clustering (HiCO,[25] hierarchical correlation clustering, 4C[26] using \"correlation connectivity\" and ERiC[27] exploring hierarchical density-based correlation clusters).\n\nSeveral different clustering systems based on mutual information have been proposed. One is Marina Meila's variation of information metric;[28] another provides hierarchical clustering.[29] Using genetic algorithms, a wide range of different fit-functions can be optimized, including mutual information.[30] Also message passing algorithms, a recent development in Computer Science and Statistical Physics, has led to the creation of new types of clustering algorithms.[31]\n\nOther methods\nBasic sequential algorithmic scheme (BSAS)\nEvaluation and assessment\nEvaluation of clustering results sometimes is referred to as cluster validation.\n\nThere have been several suggestions for a measure of similarity between two clusterings. Such a measure can be used to compare how well different data clustering algorithms perform on a set of data. These measures are usually tied to the type of criterion being considered in assessing the quality of a clustering method.\n\nInternal evaluation\nSee also: Determining the number of clusters in a data set\nWhen a clustering result is evaluated based on the data that was clustered itself, this is called internal evaluation. These methods usually assign the best score to the algorithm that produces clusters with high similarity within a cluster and low similarity between clusters. One drawback of using internal criteria in cluster evaluation is that high scores on an internal measure do not necessarily result in effective information retrieval applications.[32] Additionally, this evaluation is biased towards algorithms that use the same cluster model. For example, k-Means clustering naturally optimizes object distances, and a distance-based internal criterion will likely overrate the resulting clustering.\n\nTherefore, the internal evaluation measures are best suited to get some insight into situations where one algorithm performs better than another, but this shall not imply that one algorithm produces more valid results than another.[4] Validity as measured by such an index depends on the claim that this kind of structure exists in the data set. An algorithm designed for some kind of models has no chance if the data set contains a radically different set of models, or if the evaluation measures a radically different criterion.[4] For example, k-means clustering can only find convex clusters, and many evaluation indexes assume convex clusters. On a data set with non-convex clusters neither the use of k-means, nor of an evaluation criterion that assumes convexity, is sound.\n\nThe following methods can be used to assess the quality of clustering algorithms based on internal criterion:\n\nDavies�Bouldin index\nThe Davies�Bouldin index can be calculated by the following formula:\n{\\displaystyle DB={\\frac {1}{n}}\\sum _{i=1}^{n}\\max _{j\\neq i}\\left({\\frac {\\sigma _{i}+\\sigma _{j}}{d(c_{i},c_{j})}}\\right)} DB={\\frac {1}{n}}\\sum _{i=1}^{n}\\max _{j\\neq i}\\left({\\frac {\\sigma _{i}+\\sigma _{j}}{d(c_{i},c_{j})}}\\right)\nwhere n is the number of clusters, {\\displaystyle c_{x}} c_{x} is the centroid of cluster {\\displaystyle x} x, {\\displaystyle \\sigma _{x}} \\sigma _{x} is the average distance of all elements in cluster {\\displaystyle x} x to centroid {\\displaystyle c_{x}} c_{x}, and {\\displaystyle d(c_{i},c_{j})} d(c_{i},c_{j}) is the distance between centroids {\\displaystyle c_{i}} c_{i} and {\\displaystyle c_{j}} c_{j}. Since algorithms that produce clusters with low intra-cluster distances (high intra-cluster similarity) and high inter-cluster distances (low inter-cluster similarity) will have a low Davies�Bouldin index, the clustering algorithm that produces a collection of clusters with the smallest Davies�Bouldin index is considered the best algorithm based on this criterion.\nDunn index\nThe Dunn index aims to identify dense and well-separated clusters. It is defined as the ratio between the minimal inter-cluster distance to maximal intra-cluster distance. For each cluster partition, the Dunn index can be calculated by the following formula:[33]\n{\\displaystyle D={\\frac {\\min _{1\\leq i<j\\leq n}d(i,j)}{\\max _{1\\leq k\\leq n}d^{\\prime }(k)}}\\,,} D={\\frac {\\min _{1\\leq i<j\\leq n}d(i,j)}{\\max _{1\\leq k\\leq n}d^{\\prime }(k)}}\\,,\nwhere d(i,j) represents the distance between clusters i and j, and d '(k) measures the intra-cluster distance of cluster k. The inter-cluster distance d(i,j) between two clusters may be any number of distance measures, such as the distance between the centroids of the clusters. Similarly, the intra-cluster distance d '(k) may be measured in a variety ways, such as the maximal distance between any pair of elements in cluster k. Since internal criterion seek clusters with high intra-cluster similarity and low inter-cluster similarity, algorithms that produce clusters with high Dunn index are more desirable.\nSilhouette coefficient\nThe silhouette coefficient contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered, objects with a low value may be outliers. This index works well with k-means clustering, and is also used to determine the optimal number of clusters.\nExternal evaluation\nIn external evaluation, clustering results are evaluated based on data that was not used for clustering, such as known class labels and external benchmarks. Such benchmarks consist of a set of pre-classified items, and these sets are often created by (expert) humans. Thus, the benchmark sets can be thought of as a gold standard for evaluation. These types of evaluation methods measure how close the clustering is to the predetermined benchmark classes. However, it has recently been discussed whether this is adequate for real data, or only on synthetic data sets with a factual ground truth, since classes can contain internal structure, the attributes present may not allow separation of clusters or the classes may contain anomalies.[34] Additionally, from a knowledge discovery point of view, the reproduction of known knowledge may not necessarily be the intended result.[34] In the special scenario of constrained clustering, where meta information (such as class labels) is used already in the clustering process, the hold-out of information for evaluation purposes is non-trivial.[35]\n\nA number of measures are adapted from variants used to evaluate classification tasks. In place of counting the number of times a class was correctly assigned to a single data point (known as true positives), such pair counting metrics assess whether each pair of data points that is truly in the same cluster is predicted to be in the same cluster.\n\nSome of the measures of quality of a cluster algorithm using external criterion include:\n\nRand measure (William M. Rand 1971)[36]\nThe Rand index computes how similar the clusters (returned by the clustering algorithm) are to the benchmark classifications. One can also view the Rand index as a measure of the percentage of correct decisions made by the algorithm. It can be computed using the following formula:\n{\\displaystyle RI={\\frac {TP+TN}{TP+FP+FN+TN}}} RI={\\frac {TP+TN}{TP+FP+FN+TN}}\nwhere {\\displaystyle TP} TP is the number of true positives, {\\displaystyle TN} TN is the number of true negatives, {\\displaystyle FP} FP is the number of false positives, and {\\displaystyle FN} FN is the number of false negatives. One issue with the Rand index is that false positives and false negatives are equally weighted. This may be an undesirable characteristic for some clustering applications. The F-measure addresses this concern, as does the chance-corrected adjusted Rand index.\nF-measure\nThe F-measure can be used to balance the contribution of false negatives by weighting recall through a parameter {\\displaystyle \\beta \\geq 0} \\beta \\geq 0. Let precision and recall be defined as follows:\n{\\displaystyle P={\\frac {TP}{TP+FP}}} P={\\frac {TP}{TP+FP}}\n{\\displaystyle R={\\frac {TP}{TP+FN}}} R={\\frac {TP}{TP+FN}}\nwhere {\\displaystyle P} P is the precision rate and {\\displaystyle R} R is the recall rate. We can calculate the F-measure by using the following formula:[32]\n{\\displaystyle F_{\\beta }={\\frac {(\\beta ^{2}+1)\\cdot P\\cdot R}{\\beta ^{2}\\cdot P+R}}} F_{\\beta }={\\frac {(\\beta ^{2}+1)\\cdot P\\cdot R}{\\beta ^{2}\\cdot P+R}}\nNotice that when {\\displaystyle \\beta =0} \\beta =0, {\\displaystyle F_{0}=P} F_{0}=P. In other words, recall has no impact on the F-measure when {\\displaystyle \\beta =0} \\beta =0, and increasing {\\displaystyle \\beta } \\beta  allocates an increasing amount of weight to recall in the final F-measure.\nJaccard index\nThe Jaccard index is used to quantify the similarity between two datasets. The Jaccard index takes on a value between 0 and 1. An index of 1 means that the two dataset are identical, and an index of 0 indicates that the datasets have no common elements. The Jaccard index is defined by the following formula:\n{\\displaystyle J(A,B)={\\frac {|A\\cap B|}{|A\\cup B|}}={\\frac {TP}{TP+FP+FN}}} J(A,B)={\\frac {|A\\cap B|}{|A\\cup B|}}={\\frac {TP}{TP+FP+FN}}\nThis is simply the number of unique elements common to both sets divided by the total number of unique elements in both sets.\nFowlkes�Mallows index (E. B. Fowlkes & C. L. Mallows 1983)[37]\nThe Fowlkes-Mallows index computes the similarity between the clusters returned by the clustering algorithm and the benchmark classifications. The higher the value of the Fowlkes-Mallows index the more similar the clusters and the benchmark classifications are. It can be computed using the following formula:\n{\\displaystyle FM={\\sqrt {{\\frac {TP}{TP+FP}}\\cdot {\\frac {TP}{TP+FN}}}}} FM={\\sqrt {{\\frac {TP}{TP+FP}}\\cdot {\\frac {TP}{TP+FN}}}}\nwhere {\\displaystyle TP} TP is the number of true positives, {\\displaystyle FP} FP is the number of false positives, and {\\displaystyle FN} FN is the number of false negatives. The {\\displaystyle FM} FM index is the geometric mean of the precision and recall {\\displaystyle P} P and {\\displaystyle R} R, while the F-measure is their harmonic mean.[38] Moreover, precision and recall are also known as Wallace's indices {\\displaystyle B^{I}} B^{I} and {\\displaystyle B^{II}} B^{II}.[39]\nThe Mutual Information is an information theoretic measure of how much information is shared between a clustering and a ground-truth classification that can detect a non-linear similarity between two clusterings. Adjusted mutual information is the corrected-for-chance variant of this that has a reduced bias for varying cluster numbers.\nConfusion matrix\nA confusion matrix can be used to quickly visualize the results of a classification (or clustering) algorithm. It shows how different a cluster is from the gold standard cluster.\nApplications\nBiology, computational biology and bioinformatics\nPlant and animal ecology\ncluster analysis is used to describe and to make spatial and temporal comparisons of communities (assemblages) of organisms in heterogeneous environments; it is also used in plant systematics to generate artificial phylogenies or clusters of organisms (individuals) at the species, genus or higher level that share a number of attributes\nTranscriptomics\nclustering is used to build groups of genes with related expression patterns (also known as coexpressed genes) as in HCS clustering algorithm . Often such groups contain functionally related proteins, such as enzymes for a specific pathway, or genes that are co-regulated. High throughput experiments using expressed sequence tags (ESTs) or DNA microarrays can be a powerful tool for genome annotation, a general aspect of genomics.\nSequence analysis\nclustering is used to group homologous sequences into gene families. This is a very important concept in bioinformatics, and evolutionary biology in general. See evolution by gene duplication.\nHigh-throughput genotyping platforms\nclustering algorithms are used to automatically assign genotypes.\nHuman genetic clustering\nThe similarity of genetic data is used in clustering to infer population structures.\nMedicine\nMedical imaging\nOn PET scans, cluster analysis can be used to differentiate between different types of tissue and blood in a three-dimensional image. In this application, actual position does not matter, but the voxel intensity is considered as a vector, with a dimension for each image that was taken over time. This technique allows, for example, accurate measurement of the rate a radioactive tracer is delivered to the area of interest, without a separate sampling of arterial blood, an intrusive technique that is most common today.\nAnalysis of antimicrobial activity\nCluster analysis can be used to analyse patterns of antibiotic resistance, to classify antimicrobial compounds according to their mechanism of action, to classify antibiotics according to their antibacterial activity.\nIMRT segmentation\nClustering can be used to divide a fluence map into distinct regions for conversion into deliverable fields in MLC-based Radiation Therapy.\nBusiness and marketing\nMarket research\nCluster analysis is widely used in market research when working with multivariate data from surveys and test panels. Market researchers use cluster analysis to partition the general population of consumers into market segments and to better understand the relationships between different groups of consumers/potential customers, and for use in market segmentation, Product positioning, New product development and Selecting test markets.\nGrouping of shopping items\nClustering can be used to group all the shopping items available on the web into a set of unique products. For example, all the items on eBay can be grouped into unique products. (eBay doesn't have the concept of a SKU)\nWorld wide web\nSocial network analysis\nIn the study of social networks, clustering may be used to recognize communities within large groups of people.\nSearch result grouping\nIn the process of intelligent grouping of the files and websites, clustering may be used to create a more relevant set of search results compared to normal search engines like Google. There are currently a number of web based clustering tools such as Clusty.\nSlippy map optimization\nFlickr's map of photos and other map sites use clustering to reduce the number of markers on a map. This makes it both faster and reduces the amount of visual clutter.\nComputer science\nSoftware evolution\nClustering is useful in software evolution as it helps to reduce legacy properties in code by reforming functionality that has become dispersed. It is a form of restructuring and hence is a way of direct preventative maintenance.\nImage segmentation\nClustering can be used to divide a digital image into distinct regions for border detection or object recognition.[40]\nEvolutionary algorithms\nClustering may be used to identify different niches within the population of an evolutionary algorithm so that reproductive opportunity can be distributed more evenly amongst the evolving species or subspecies.\nRecommender systems\nRecommender systems are designed to recommend new items based on a user's tastes. They sometimes use clustering algorithms to predict a user's preferences based on the preferences of other users in the user's cluster.\nMarkov chain Monte Carlo methods\nClustering is often utilized to locate and characterize extrema in the target distribution.\nAnomaly detection\nAnomalies/outliers are typically - be it explicitly or implicitly - defined with respect to clustering structure in data.\nSocial science\nCrime analysis\nCluster analysis can be used to identify areas where there are greater incidences of particular types of crime. By identifying these distinct areas or \"hot spots\" where a similar crime has happened over a period of time, it is possible to manage law enforcement resources more effectively.\nEducational data mining\nCluster analysis is for example used to identify groups of schools or students with similar properties.\nTypologies\nFrom poll data, projects such as those undertaken by the Pew Research Center use cluster analysis to discern typologies of opinions, habits, and demographics that may be useful in politics and marketing.\nOthers\nField robotics\nClustering algorithms are used for robotic situational awareness to track objects and detect outliers in sensor data.[41]\nMathematical chemistry\nTo find structural similarity, etc., for example, 3000 chemical compounds were clustered in the space of 90 topological indices.[42]\nClimatology\nTo find weather regimes or preferred sea level pressure atmospheric patterns.[43]\nPetroleum geology\nCluster analysis is used to reconstruct missing bottom hole core data or missing log curves in order to evaluate reservoir properties.\nPhysical geography\nThe clustering of chemical properties in different sample locations.\nSee also\n\tWikimedia Commons has media related to Cluster analysis.\nSpecialized types of cluster analysis\nBalanced clustering\nClustering high-dimensional data\nConceptual clustering\nConsensus clustering\nConstrained clustering\nData stream clustering\nHCS clustering\nSequence clustering\nSpectral clustering\nTechniques used in cluster analysis\nArtificial neural network (ANN)\nNearest neighbor search\nNeighbourhood components analysis\nLatent class analysis\nData projection and preprocessing\nDimension reduction\nPrincipal component analysis\nMultidimensional scaling\nOthe\nCluster-weighted modeling\nCurse of dimensionality\nDetermining the number of clusters in a data set\nParallel coordinates\nStructured data analysis", "skillName": "Clustering."}
{"id": 141, "category": "Machine_Learning", "skillText": "Machine learning\nMachine Learning (journal).\nMachine learning and\ndata mining\nKernel Machine.svg\nProblems\nSupervised learning\n(classification � regression)\nClustering\nDimensionality reduction\nStructured prediction\nAnomaly detection\nNeural nets\nTheory\nMachine learning venues\nPortal icon Machine learning portal\nMachine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] In 1959, Arthur Samuel defined machine learning as a \"Field of study that gives computers the ability to learn without being explicitly programmed\".[2] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[3] Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs,[4]:2 rather than following strictly static program instructions.\n\nMachine learning is closely related to (and often overlaps with) computational statistics; a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is unfeasible. Example applications include spam filtering, optical character recognition (OCR),[5] search engines and computer vision. Machine learning is sometimes conflated with data mining,[6] where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning.[4]:vii[7]\n\nWithin the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction. These analytical models allow researchers, data scientists, engineers, and analysts to \"produce reliable, repeatable decisions and results\" and uncover \"hidden insights\" through learning from historical relationships and trends in the data.[8]\n\n1\tOverview\n1.1\tTypes of problems and tasks\n2\tHistory and relationships to other fields\n2.1\tRelation to statistics\n3\tTheory\n4\tApproaches\n4.1\tDecision tree learning\n4.2\tAssociation rule learning\n4.3\tArtificial neural networks\n4.4\tDeep Learning\n4.5\tInductive logic programming\n4.6\tSupport vector machines\n4.7\tClustering\n4.8\tBayesian networks\n4.9\tReinforcement learning\n4.10\tRepresentation learning\n4.11\tSimilarity and metric learning\n4.12\tSparse dictionary learning\n4.13\tGenetic algorithms\n5\tApplications\n6\tEthics\n7\tSoftware\n7.1\tOpen-source software\n7.2\tCommercial software with open-source editions\n7.3\tCommercial software\n8\tJournals\n9\tConferences\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nOverview\nTom M. Mitchell provided a widely quoted, more formal definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\"[9] This definition is notable for its defining machine learning in fundamentally operational rather than cognitive terms, thus following Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\" that the question \"Can machines think?\" be replaced with the question \"Can machines do what we (as thinking entities) can do?\"[10]\n\nTypes of problems and tasks\n\nMachine learning tasks are typically classified into three broad categories, depending on the nature of the learning \"signal\" or \"feedback\" available to a learning system. These are[11]\n\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle), without a teacher explicitly telling it whether it has come close to its goal. Another example is learning to play a game by playing against an opponent.[4]:3\nBetween supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. Transduction is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing.\n\n\nA support vector machine is a classifier that divides its input space into two regions, separated by a linear boundary. Here, it has learned to distinguish black and white circles.\nAmong other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers, and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.\n\nAnother categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[4]:3\n\nIn classification, inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes. This is typically tackled in a supervised way. Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are \"spam\" and \"not spam\".\nIn regression, also a supervised problem, the outputs are continuous rather than discrete.\nIn clustering, a set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.\nDensity estimation finds the distribution of inputs in some space.\nDimensionality reduction simplifies inputs by mapping them into a lower-dimensional space. Topic modeling is a related problem, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.\nHistory and relationships to other fields\nSee also: Timeline of machine learning\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.[11]:488\n\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[11]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[12] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[11]:708�710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[11]:25\n\nMachine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[12] It also benefited from the increasing availability of digitized information, and the possibility to distribute that via the Internet.\n\nMachine learning and data mining often employ the same methods and overlap significantly. They can be roughly distinguished as follows:\n\nMachine learning focuses on prediction, based on known properties learned from the training data.\nData mining focuses on the discovery of (previously) unknown properties in the data. This is the analysis step of Knowledge Discovery in Databases.\nThe two areas overlap in many ways: data mining uses many machine learning methods, but often with a slightly different goal in mind. On the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in Knowledge Discovery and Data Mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n\nMachine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[13]\n\nRelation to statistics\nMachine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[14] He also suggested the term data science as a placeholder to call the overall field.[14]\n\nLeo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[15] wherein 'algorithmic model' means more or less the machine learning algorithms like Random forest.\n\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[16]\n\nTheory\nMain article: Computational learning theory\nA core objective of a learner is to generalize from its experience.[17][18] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias�variance decomposition is one way to quantify generalization error.\n\nHow well a model, trained with existing examples, predicts the output for unknown instances is called generalization. For best generalization, complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, we've underfitted. Then, we increase the complexity, the training error decreases. But if our hypothesis is too complex, we've overfitted. After then, we should find the hypothesis that has the minimum training error.[19]\n\nIn addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\nThere are many similarities between machine learning theory and statistical inference, although they use different terms.\n\nApproaches\nMain article: List of machine learning algorithms\nDecision tree learning\nMain article: Decision tree learning\nDecision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value.\n\nAssociation rule learning\nMain article: Association rule learning\nAssociation rule learning is a method for discovering interesting relations between variables in large databases.\n\nArtificial neural networks\nMain article: Artificial neural network\nAn artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is inspired by the structure and functional aspects of biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.\n\nDeep Learning\nMain article: Deep learning\nFalling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of Deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[20]\n\nInductive logic programming\nMain article: Inductive logic programming\nInductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.\n\nSupport vector machines\nMain article: Support vector machines\nSupport vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.\n\nClustering\nMain article: Cluster analysis\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.\n\nBayesian networks\nMain article: Bayesian network\nA Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.\n\nReinforcement learning\nMain article: Reinforcement learning\nReinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.\n\nRepresentation learning\nMain article: Representation learning\nSeveral learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing to reconstruct the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.\n\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[21] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[22]\n\nSimilarity and metric learning\nMain article: Similarity learning\nIn this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.\n\nSparse dictionary learning\nMain article: Sparse dictionary learning\nIn this method, a datum is represented as a linear combination of basis functions, and the coefficients are assumed to be sparse. Let x be a d-dimensional datum, D be a d by n matrix, where each column of D represents a basis function. r is the coefficient to represent x using D. Mathematically, sparse dictionary learning means solving {\\displaystyle x\\approx Dr} {\\displaystyle x\\approx Dr} where r is sparse. Generally speaking, n is assumed to be larger than d to allow the freedom for a sparse representation.\n\nLearning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[23] A popular heuristic method for sparse dictionary learning is K-SVD.\n\nSparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[24]\n\nGenetic algorithms\nMain article: Genetic algorithm\nA genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[25][26] Vice versa, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[27]\n\nApplications\nApplications for machine learning include:\n\nAdaptive websites\nAffective computing\nBioinformatics\nBrain-machine interfaces\nCheminformatics\nClassifying DNA sequences\nComputational anatomy\nComputer vision, including object recognition\nDetecting credit card fraud\nGame playing[28]\nInformation retrieval\nInternet fraud detection\nMarketing\nMachine perception\nMedical diagnosis\nNatural language processing[29]\nOptimization and metaheuristic\nOnline advertising\nRecommender systems\nRobot locomotion\nSearch engines\nSentiment analysis (or opinion mining)\nSequence mining\nSoftware engineering\nSpeech and handwriting recognition\nStock market analysis\nStructural health monitoring\nSyntactic pattern recognition\nEconomics\nIn 2006, the online movie company Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[30] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[31]\n\nIn 2010 The Wall Street Journal wrote about money management firm Rebellion Research's use of machine learning to predict economic movements. The article describes Rebellion Research's prediction of the financial crisis and economic recovery.[32]\n\nIn 2014 it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[33]\n\nEthics\nMachine Learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use, thus digitizing cultural prejudices such as institutional racism and classism. [34] Responsible collection of data thus is a critical part of machine learning. See Machine ethics for additional information.\n\nSoftware\nSoftware suites containing a variety of machine learning algorithms include the following:\n\nOpen-source software\nCaffe\ndlib\nELKI\nEncog\nGNU Octave\nH2O\nMahout\nMallet (software project)\nmlpy\nMLPACK\nMOA (Massive Online Analysis)\nND4J with Deeplearning4j\nNuPIC\nOpenAI\nOpenCV\nOpenNN\nOrange\nR\nscikit-learn\nscikit-image\nShogun\nTensorFlow\nTorch (machine learning)\nSpark\nYooreeka\nWeka\nCommercial software with open-source editions\nKNIME\nRapidMiner\nCommercial software\nAngoss KnowledgeSTUDIO\nAyasdi\nDatabricks\nGoogle Prediction API\nIBM SPSS Modeler\nKXEN Modeler\nLIONsolver\nMathematica\nMATLAB\nMicrosoft Azure Machine Learning\nNeural Designer\nNeuroSolutions\nOracle Data Mining\nRCASE\nSAS Enterprise Miner\nSTATISTICA Data Miner\nJournals\nJournal of Machine Learning Research\nMachine Learning\nNeural Computation\nConferences\nConference on Neural Information Processing Systems\nInternational Conference on Machine Learning\nInternational Conference on Learning Representations\nPortal icon\tArtificial intelligence portal\nPortal icon\tMachine learning portal\nAdaptive control\nAdversarial machine learning\nAutomatic reasoning\nBayesian structural time series\nBig data\nCache language model\nCognitive model\nCognitive science\nComputational intelligence\nComputational neuroscience\nData science\nEthics of artificial intelligence\nExistential risk from advanced artificial intelligence\nExplanation-based learning\nGlossary of artificial intelligence\nImportant publications in machine learning\nList of machine learning algorithms\nList of datasets for machine learning research\nSimilarity learning\nSpike-and-slab variable selection", "skillName": "Machine learning."}
{"id": 142, "category": "Machine_Learning", "skillText": "Information retrieval\nInformation retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on or on full-text (or other content-based) indexing.\n\nAutomated information retrieval systems are used to reduce what has been called \"information overload\". Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.\n1\tOverview\n2\tHistory\n3\tModel types\n3.1\tFirst dimension: mathematical basis\n3.2\tSecond dimension: properties of the model\n4\tPerformance and correctness measures\n4.1\tPrecision\n4.2\tRecall\n4.3\tFall-out\n4.4\tF-score / F-measure\n4.5\tAverage precision\n4.6\tPrecision at K\n4.7\tR-Precision\n4.8\tMean average precision\n4.9\tDiscounted cumulative gain\n4.10\tOther measures\n4.11\tVisualization\n5\tTimeline\n6\tAwards in the field\n7\tSee also\n8\tReferences\n9\tFurther reading\n10\tExternal links\nOverview\nAn information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.\n\nAn object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.[1]\n\nDepending on the application the data objects may be, for example, text documents, images,[2] audio,[3] mind maps[4] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\n\nMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.[5]\n\nHistory\n�\tthere is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute\t�\n�  J. E. Holmstrom, 1948\nThe idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945.[6] It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film.[7] The first description of a computer searching for information was described by Holmstrom in 1948,[8] detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents).[6] Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\n\nIn 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.\n\nModel types\n\nCategorization of IR-models (translated from German entry, original source Dominik Kuropka).\nFor effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n\nFirst dimension: mathematical basis\nSet-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:\nStandard Boolean model\nExtended Boolean model\nFuzzy retrieval\nAlgebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.\nVector space model\nGeneralized vector space model\n(Enhanced) Topic-based Vector Space Model\nExtended Boolean model\nLatent semantic indexing a.k.a. latent semantic analysis\nProbabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes' theorem are often used in these models.\nBinary Independence Model\nProbabilistic relevance model on which is based the okapi (BM25) relevance function\nUncertain inference\nLanguage models\nDivergence-from-randomness model\nLatent Dirichlet allocation\nFeature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.\nSecond dimension: properties of the model\nModels without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.\nModels with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.\nModels with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)\nPerformance and correctness measures\nFurther information: Evaluation measures (information retrieval)\nThe evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.\n\nVirtually all modern evaluation metrics (e.g., mean average precision, discounted cumulative gain) are designed for ranked retrieval without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.[citation needed]\n\nThe mathematical symbols used in the formulas below mean:\n\n{\\displaystyle X\\cap Y} X\\cap Y - Intersection - in this case, specifying the documents in both sets X and Y\n{\\displaystyle |X|} |X| - Cardinality - in this case, the number of documents in set X\n{\\displaystyle \\int } \\int  - Integral\n{\\displaystyle \\sum } \\sum  - Summation\n{\\displaystyle \\Delta } \\Delta  - Symmetric difference\nPrecision\nMain article: Precision and recall\nPrecision is the fraction of the documents retrieved that are relevant to the user's information need.\n\n{\\displaystyle {\\mbox{precision}}={\\frac {|\\{{\\mbox{relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{retrieved documents}}\\}|}}}  \\mbox{precision}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{retrieved documents}\\}|}\nIn binary classification, precision is analogous to positive predictive value. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called precision at n or P@n.\n\nNote that the meaning and usage of \"precision\" in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics.\n\nRecall\nMain article: Precision and recall\nRecall is the fraction of the documents that are relevant to the query that are successfully retrieved.\n\n{\\displaystyle {\\mbox{recall}}={\\frac {|\\{{\\mbox{relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{relevant documents}}\\}|}}} \\mbox{recall}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{relevant documents}\\}|}\nIn binary classification, recall is often called sensitivity. So it can be looked at as the probability that a relevant document is retrieved by the query.\n\nIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\n\nFall-out\nThe proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:\n\n{\\displaystyle {\\mbox{fall-out}}={\\frac {|\\{{\\mbox{non-relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{non-relevant documents}}\\}|}}}  \\mbox{fall-out}=\\frac{|\\{\\mbox{non-relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{non-relevant documents}\\}|}\nIn binary classification, fall-out is closely related to specificity and is equal to {\\displaystyle (1-{\\mbox{specificity}})} (1-\\mbox{specificity}). It can be looked at as the probability that a non-relevant document is retrieved by the query.\n\nIt is trivial to achieve fall-out of 0% by returning zero documents in response to any query.\n\nF-score / F-measure\nMain article: F-score\nThe weighted harmonic mean of precision and recall, the traditional F-measure or balanced F-score is:\n\n{\\displaystyle F={\\frac {2\\cdot \\mathrm {precision} \\cdot \\mathrm {recall} }{(\\mathrm {precision} +\\mathrm {recall} )}}} F={\\frac  {2\\cdot {\\mathrm  {precision}}\\cdot {\\mathrm  {recall}}}{({\\mathrm  {precision}}+{\\mathrm  {recall}})}}\nThis is also known as the {\\displaystyle F_{1}} F_{1} measure, because recall and precision are evenly weighted.\n\nThe general formula for non-negative real {\\displaystyle \\beta } \\beta  is:\n\n{\\displaystyle F_{\\beta }={\\frac {(1+\\beta ^{2})\\cdot (\\mathrm {precision} \\cdot \\mathrm {recall} )}{(\\beta ^{2}\\cdot \\mathrm {precision} +\\mathrm {recall} )}}\\,} F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\mathrm{precision} \\cdot \\mathrm{recall})}{(\\beta^2 \\cdot \\mathrm{precision} + \\mathrm{recall})}\\,\nTwo other commonly used F measures are the {\\displaystyle F_{2}} F_{2} measure, which weights recall twice as much as precision, and the {\\displaystyle F_{0.5}} F_{0.5} measure, which weights precision twice as much as recall.\n\nThe F-measure was derived by van Rijsbergen (1979) so that {\\displaystyle F_{\\beta }} F_{\\beta } \"measures the effectiveness of retrieval with respect to a user who attaches {\\displaystyle \\beta } \\beta  times as much importance to recall as precision\". It is based on van Rijsbergen's effectiveness measure {\\displaystyle E=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}} E=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}. Their relationship is:\n\n{\\displaystyle F_{\\beta }=1-E} F_{\\beta }=1-E where {\\displaystyle \\alpha ={\\frac {1}{1+\\beta ^{2}}}} \\alpha ={\\frac {1}{1+\\beta ^{2}}}\nF-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.[citation needed]\n\nAverage precision\nPrecision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision {\\displaystyle p(r)} p(r) as a function of recall {\\displaystyle r} r. Average precision computes the average value of {\\displaystyle p(r)} p(r) over the interval from {\\displaystyle r=0} r=0 to {\\displaystyle r=1} r=1:[9]\n\n{\\displaystyle \\operatorname {AveP} =\\int _{0}^{1}p(r)dr} \\operatorname{AveP} = \\int_0^1 p(r)dr\nThat is the area under the precision-recall curve. This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:\n\n{\\displaystyle \\operatorname {AveP} =\\sum _{k=1}^{n}P(k)\\Delta r(k)} \\operatorname{AveP} = \\sum_{k=1}^n P(k) \\Delta r(k)\nwhere {\\displaystyle k} k is the rank in the sequence of retrieved documents, {\\displaystyle n} n is the number of retrieved documents, {\\displaystyle P(k)} P(k) is the precision at cut-off {\\displaystyle k} k in the list, and {\\displaystyle \\Delta r(k)} \\Delta r(k) is the change in recall from items {\\displaystyle k-1} k-1 to {\\displaystyle k} k.[9]\n\nThis finite sum is equivalent to:\n\n{\\displaystyle \\operatorname {AveP} ={\\frac {\\sum _{k=1}^{n}(P(k)\\times \\operatorname {rel} (k))}{\\mbox{number of relevant documents}}}\\!}  \\operatorname{AveP} = \\frac{\\sum_{k=1}^n (P(k) \\times \\operatorname{rel}(k))}{\\mbox{number of relevant documents}} \\!\nwhere {\\displaystyle \\operatorname {rel} (k)} \\operatorname{rel}(k) is an indicator function equaling 1 if the item at rank {\\displaystyle k} k is a relevant document, zero otherwise.[10] Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.\n\nSome authors choose to interpolate the {\\displaystyle p(r)} p(r) function to reduce the impact of \"wiggles\" in the curve.[11][12] For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:[11][12]\n\n{\\displaystyle \\operatorname {AveP} ={\\frac {1}{11}}\\sum _{r\\in \\{0,0.1,\\ldots ,1.0\\}}p_{\\operatorname {interp} }(r)} \\operatorname{AveP} = \\frac{1}{11} \\sum_{r \\in \\{0, 0.1, \\ldots, 1.0\\}} p_{\\operatorname{interp}}(r)\nwhere {\\displaystyle p_{\\operatorname {interp} }(r)} p_{\\operatorname{interp}}(r) is an interpolated precision that takes the maximum precision over all recalls greater than {\\displaystyle r} r:\n\n{\\displaystyle p_{\\operatorname {interp} }(r)=\\operatorname {max} _{{\\tilde {r}}:{\\tilde {r}}\\geq r}p({\\tilde {r}})} p_{\\operatorname{interp}}(r) = \\operatorname{max}_{\\tilde{r}:\\tilde{r} \\geq r} p(\\tilde{r}).\nAn alternative is to derive an analytical {\\displaystyle p(r)} p(r) function by assuming a particular parametric distribution for the underlying decision values. For example, a binormal precision-recall curve can be obtained by assuming decision values in both classes to follow a Gaussian distribution.[13]\n\nPrecision at K\nFor modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. Precision at k documents (P@k) is still a useful metric (e.g., P@10 or \"Precision at 10\" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.[citation needed] Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.[14] It easier to score manually since only the top k results need to be examined to determine if they are relevant or not.\n\nR-Precision\nR-precision requires knowing all documents that are relevant to a query. The number of relevant documents, {\\displaystyle R} R, is used as the cutoff for calculation, and this varies from query to query. For example, if there are 15 documents relevant to \"red\" in a corpus (R=15), R-precision for \"red\" looks at the top 15 documents returned, counts the number that are relevant {\\displaystyle r} r turns that into a relevancy fraction: {\\displaystyle r/R=r/15} r/R=r/15.[15]\n\nPrecision is equal to recall at the R-th position.[14]\n\nEmpirically, this measure is often highly correlated to mean average precision.[14]\n\nMean average precision\nMean average precision for a set of queries is the mean of the average precision scores for each query.\n\n{\\displaystyle \\operatorname {MAP} ={\\frac {\\sum _{q=1}^{Q}\\operatorname {AveP(q)} }{Q}}\\!}  \\operatorname{MAP} = \\frac{\\sum_{q=1}^Q \\operatorname{AveP(q)}}{Q} \\!\nwhere Q is the number of queries.\n\nDiscounted cumulative gain\nMain article: Discounted cumulative gain\nDCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n\nThe DCG accumulated at a particular rank position {\\displaystyle p} p is defined as:\n\n{\\displaystyle \\mathrm {DCG_{p}} =rel_{1}+\\sum _{i=2}^{p}{\\frac {rel_{i}}{\\log _{2}i}}.}  \\mathrm{DCG_{p}} = rel_{1} + \\sum_{i=2}^{p} \\frac{rel_{i}}{\\log_{2}i}.\nSince result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p ( {\\displaystyle IDCG_{p}} IDCG_p), which normalizes the score:\n\n{\\displaystyle \\mathrm {nDCG_{p}} ={\\frac {DCG_{p}}{IDCG{p}}}.}  \\mathrm{nDCG_{p}} = \\frac{DCG_{p}}{IDCG{p}}.\nThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the {\\displaystyle DCG_{p}} DCG_p will be the same as the {\\displaystyle IDCG_{p}} IDCG_p producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\n\nOther measures\nTerminology and derivations\nfrom a confusion matrix\ntrue positive (TP)\neqv. with hit\ntrue negative (TN)\neqv. with correct rejection\nfalse positive (FP)\neqv. with false alarm, Type I error\nfalse negative (FN)\neqv. with miss, Type II error\nsensitivity or true positive rate (TPR)\neqv. with hit rate, recall\n{\\displaystyle {\\mathit {TPR}}={\\frac {\\mathit {TP}}{P}}={\\frac {\\mathit {TP}}{{\\mathit {TP}}+{\\mathit {FN}}}}} {\\mathit {TPR}}={\\frac {\\mathit {TP}}{P}}={\\frac {\\mathit {TP}}{{\\mathit {TP}}+{\\mathit {FN}}}}\nspecificity (SPC) or true negative rate (TNR)\n{\\displaystyle {\\mathit {SPC}}={\\frac {\\mathit {TN}}{N}}={\\frac {\\mathit {TN}}{{\\mathit {FP}}+{\\mathit {TN}}}}} {\\mathit {SPC}}={\\frac {\\mathit {TN}}{N}}={\\frac {\\mathit {TN}}{{\\mathit {FP}}+{\\mathit {TN}}}}\nprecision or positive predictive value (PPV)\n{\\displaystyle {\\mathit {PPV}}={\\frac {\\mathit {TP}}{{\\mathit {TP}}+{\\mathit {FP}}}}} {\\mathit {PPV}}={\\frac {\\mathit {TP}}{{\\mathit {TP}}+{\\mathit {FP}}}}\nnegative predictive value (NPV)\n{\\displaystyle {\\mathit {NPV}}={\\frac {\\mathit {TN}}{{\\mathit {TN}}+{\\mathit {FN}}}}} {\\mathit {NPV}}={\\frac {\\mathit {TN}}{{\\mathit {TN}}+{\\mathit {FN}}}}\nfall-out or false positive rate (FPR)\n{\\displaystyle {\\mathit {FPR}}={\\frac {\\mathit {FP}}{N}}={\\frac {\\mathit {FP}}{{\\mathit {FP}}+{\\mathit {TN}}}}=1-{\\mathit {SPC}}} {\\mathit {FPR}}={\\frac {\\mathit {FP}}{N}}={\\frac {\\mathit {FP}}{{\\mathit {FP}}+{\\mathit {TN}}}}=1-{\\mathit {SPC}}\nfalse discovery rate (FDR)\n{\\displaystyle {\\mathit {FDR}}={\\frac {\\mathit {FP}}{{\\mathit {FP}}+{\\mathit {TP}}}}=1-{\\mathit {PPV}}} {\\mathit {FDR}}={\\frac {\\mathit {FP}}{{\\mathit {FP}}+{\\mathit {TP}}}}=1-{\\mathit {PPV}}\nmiss rate or false negative rate (FNR)\n{\\displaystyle {\\mathit {FNR}}={\\frac {\\mathit {FN}}{P}}={\\frac {\\mathit {FN}}{{\\mathit {FN}}+{\\mathit {TP}}}}} {\\mathit {FNR}}={\\frac {\\mathit {FN}}{P}}={\\frac {\\mathit {FN}}{{\\mathit {FN}}+{\\mathit {TP}}}}\naccuracy (ACC)\n{\\displaystyle {\\mathit {ACC}}={\\frac {{\\mathit {TP}}+{\\mathit {TN}}}{P+N}}} {\\mathit {ACC}}={\\frac {{\\mathit {TP}}+{\\mathit {TN}}}{P+N}}\nF1 score\nis the harmonic mean of precision and sensitivity\n{\\displaystyle {\\mathit {F1}}={\\frac {2{\\mathit {TP}}}{2{\\mathit {TP}}+{\\mathit {FP}}+{\\mathit {FN}}}}} {\\mathit {F1}}={\\frac {2{\\mathit {TP}}}{2{\\mathit {TP}}+{\\mathit {FP}}+{\\mathit {FN}}}}\nMatthews correlation coefficient (MCC)\n{\\displaystyle {\\frac {TP\\times TN-FP\\times FN}{\\sqrt {(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}}} {\\frac {TP\\times TN-FP\\times FN}{\\sqrt {(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}}\nInformedness = Sensitivity + Specificity - 1\nMarkedness = Precision + NPV - 1\nSources: Fawcett (2006) and Powers (2011).[16][17]\nMean reciprocal rank\nSpearman's rank correlation coefficient\nbpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents[15]\nGMAP - geometric mean of (per-topic) average precision[15]\nMeasures based on marginal relevance and document diversity - see Relevance (information retrieval) � Problems and alternatives\nVisualization\nVisualizations of information retrieval performance include:\n\nGraphs which chart precision on one axis and recall on the other[15]\nHistograms of average precision over various topics[15]\nReceiver operating characteristic (ROC curve)\nConfusion matrix\nTimeline\nBefore the 1900s\n1801: Joseph Marie Jacquard invents the Jacquard loom, the first machine to use punched cards to control a sequence of operations.\n1880s: Herman Hollerith invents an electro-mechanical data tabulator using punch cards as a machine readable medium.\n1890 Hollerith cards, keypunches and tabulators used to process the 1890 US Census data.\n1920s-1930s\nEmanuel Goldberg submits patents for his \"Statistical Machine� a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.\n1940s�1950s\nlate 1940s: The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.\n1945: Vannevar Bush's As We May Think appeared in Atlantic Monthly.\n1947: Hans Peter Luhn (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.\n1950s: Growing concern in the US for a \"science gap\" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent et al.) and the invention of citation indexing (Eugene Garfield).\n1950: The term \"information retrieval\" was coined by Calvin Mooers.[18]\n1951: Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at MIT.[19]\n1955: Allen Kent joined Case Western Reserve University, and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed \"framework\" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.[20]\n1958: International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: Proceedings of the International Conference on Scientific Information, 1958 (National Academy of Sciences, Washington, DC, 1959)\n1959: Hans Peter Luhn published \"Auto-encoding of documents for information retrieval.\"\n1960s:\nearly 1960s: Gerard Salton began work on IR at Harvard, later moved to Cornell.\n1960: Melvin Earl Maron and John Lary Kuhns[21] published \"On relevance, probabilistic indexing, and information retrieval\" in the Journal of the ACM 7(3):216�244, July 1960.\n1962:\nCyril W. Cleverdon published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, \"Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems\". Cranfield Collection of Aeronautics, Cranfield, England, 1962.\nKent published Information Analysis and Retrieval.\n1963:\nWeinberg report \"Science, Government and Information\" gave a full articulation of the idea of a \"crisis of scientific information.\" The report was named after Dr. Alvin Weinberg.\nJoseph Becker and Robert M. Hayes published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. Information storage and retrieval: tools, elements, theories. New York, Wiley (1963).\n1964:\nKaren Sp�rck Jones finished her thesis at Cambridge, Synonymy and Semantic Classification, and continued work on computational linguistics as it applies to IR.\nThe National Bureau of Standards sponsored a symposium titled \"Statistical Association Methods for Mechanized Documentation.\" Several highly significant papers, including G. Salton's first published reference (we believe) to the SMART system.\nmid-1960s:\nNational Library of Medicine developed MEDLARS Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.\nProject Intrex at MIT.\n1965: J. C. R. Licklider published Libraries of the Future.\n1966: Don Swanson was involved in studies at University of Chicago on Requirements for Future Catalogs.\nlate 1960s: F. Wilfrid Lancaster completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.\n1968:\nGerard Salton published Automatic Information Organization and Retrieval.\nJohn W. Sammon, Jr.'s RADC Tech report \"Some Mathematics of Information Storage and Retrieval...\" outlined the vector model.\n1969: Sammon's \"A nonlinear mapping for data structure analysis\" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.\n1970s\nearly 1970s:\nFirst online systems�NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.\nTheodor Nelson promoting concept of hypertext, published Computer Lib/Dream Machines.\n1971: Nicholas Jardine and Cornelis J. van Rijsbergen published \"The use of hierarchic clustering in information retrieval\", which articulated the \"cluster hypothesis.\"[22]\n1975: Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:\nA Theory of Indexing (Society for Industrial and Applied Mathematics)\nA Theory of Term Importance in Automatic Text Analysis (JASIS v. 26)\nA Vector Space Model for Automatic Indexing (CACM 18:11)\n1978: The First ACM SIGIR conference.\n1979: C. J. van Rijsbergen published Information Retrieval (Butterworths). Heavy emphasis on probabilistic models.\n1979: Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.[23]\n1980s\n1980: First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.\n1982: Nicholas J. Belkin, Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.\n1983: Salton (and Michael J. McGill) published Introduction to Modern Information Retrieval (McGraw-Hill), with heavy emphasis on vector models.\n1985: David Blair and Bill Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System\nmid-1980s: Efforts to develop end-user versions of commercial IR systems.\n1985�1993: Key papers on and experimental systems for visualization interfaces.\nWork by Donald B. Crouch, Robert R. Korfhage, Matthew Chalmers, Anselm Spoerri and others.\n1989: First World Wide Web proposals by Tim Berners-Lee at CERN.\n1990s\n1992: First TREC conference.\n1997: Publication of Korfhage's Information Storage and Retrieval[24] with emphasis on visualization and multi-reference point systems.\nlate 1990s: Web search engines implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.\nAwards in the field\nTony Kent Strix award\nGerard Salton Award\nSee also\nAdversarial information retrieval\nCollaborative information seeking\nControlled vocabulary\nCross-language information retrieval\nData mining\nEuropean Summer School in Information Retrieval\nHuman�computer information retrieval (HCIR)\nInformation extraction\nInformation Retrieval Facility\nKnowledge visualization\nMultimedia Information Retrieval\nPersonal information management\nRelevance (Information Retrieval)\nRelevance feedback\nRocchio Classification\nSearch index\nSocial information seeking\nSpecial Interest Group on Information Retrieval\nSubject indexing\nTemporal information retrieval\ntf-idf\nXML-Retrieval", "skillName": "Information_Retrieval."}
{"id": 143, "category": "Machine_Learning", "skillText": "Categorization\nInformation science\nGeneral aspects\nInformation access � Information architecture\nInformation management\nInformation retrieval\nInformation seeking � Information society\nKnowledge organization � Ontology � Taxonomy\nPhilosophy of information\nScience, technology and society\nRelated fields and sub-fields\nBibliometrics � Categorization\nCensorship � Classification\nComputer data storage � Cultural studies\nData modeling � Informatics\nInformation technology\nIntellectual freedom\nIntellectual property � Memory\nLibrary and information science\nPreservation � Privacy\nQuantum information science\nPortal icon Information science portal\nMachine learning\nCategorization is the process in which ideas and objects are recognized, differentiated, and understood.[1] Categorization implies that objects are grouped into categories, usually for some specific purpose. Ideally, a category illuminates a relationship between the subjects and objects of knowledge. Categorization is fundamental in language, prediction, inference, decision making and in all kinds of environmental interaction. It is indicated that categorization plays a major role in computer programming.[2]\n\nThere are many categorization theories and techniques. In a broader historical view, however, three general approaches to categorization may be identified:\n\nClassical categorization\nConceptual clustering\nPrototype theory\n\n1\tThe classical view\n2\tConceptual clustering\n3\tPrototype theory\n4\tMiscategorization\n5\tSee also\n6\tReferences\n7\tExternal links\nThe classical view\nMain article: Categories (Aristotle)\nClassical categorization first appears in the context of Western Philosophy in the work of Plato, who, in his Statesman dialogue, introduces the approach of grouping objects based on their similar properties. This approach was further explored and systematized by Aristotle in his Categories treatise, where he analyzes the differences between classes and objects. Aristotle also applied intensively the classical categorization scheme in his approach to the classification of living beings (which uses the technique of applying successive narrowing questions such as \"Is it an animal or vegetable?\", \"How many feet does it have?\", \"Does it have fur or feathers?\", \"Can it fly?\"...), establishing this way the basis for natural taxonomy.\n\nThe classical Aristotelian view claims that categories are discrete entities characterized by a set of properties which are shared by their members. In analytic philosophy, these properties are assumed to establish the conditions which are both necessary and sufficient conditions to capture meaning.\n\nAccording to the classical view, categories should be clearly defined, mutually exclusive and collectively exhaustive. This way, any entity of the given classification universe belongs unequivocally to one, and only one, of the proposed categories.\n\nConceptual clustering\nMain article: Conceptual clustering\nConceptual clustering is a modern variation of the classical approach, and derives from attempts to explain how knowledge is represented. In this approach, classes (clusters or entities) are generated by first formulating their conceptual descriptions and then classifying the entities according to the descriptions.\n\nConceptual clustering developed mainly during the 1980s, as a machine paradigm for unsupervised learning. It is distinguished from ordinary data clustering by generating a concept description for each generated category.\n\nCategorization tasks in which category labels are provided to the learner for certain objects are referred to as supervised classification, supervised learning, or concept learning. Categorization tasks in which no labels are supplied are referred to as unsupervised classification, unsupervised learning, or data clustering. The task of supervised classification involves extracting information from the labeled examples that allows accurate prediction of class labels of future examples. This may involve the abstraction of a rule or concept relating observed object features to category labels, or it may not involve abstraction (e.g., exemplar models). The task of clustering involves recognizing inherent structure in a data set and grouping objects together by similarity into classes. It is thus a process of generating a classification structure.\n\nConceptual clustering is closely related to fuzzy set theory, in which objects may belong to one or more groups, in varying degrees of fitness.\n\nPrototype theory\nMain article: Prototype theory\nSince the research by Eleanor Rosch and George Lakoff in the 1970s, categorization can also be viewed as the process of grouping things based on prototypes�the idea of necessary and sufficient conditions is almost never met in categories of naturally occurring things. It has also been suggested that categorization based on prototypes is the basis for human development, and that this learning relies on learning about the world via embodiment.\n\nA cognitive approach accepts that natural categories are graded (they tend to be fuzzy at their boundaries) and inconsistent in the status of their constituent members.\n\nSystems of categories are not objectively \"out there\" in the world but are rooted in people's experience. Conceptual categories are not identical for different cultures, or indeed, for every individual in the same culture.\n\nCategories form part of a hierarchical structure when applied to such subjects as taxonomy in biological classification: higher level: life-form level, middle level: generic or genus level, and lower level: the species level. These can be distinguished by certain traits that put an item in its distinctive category. But even these can be arbitrary and are subject to revision.\n\nCategories at the middle level are perceptually and conceptually the more salient. The generic level of a category tends to elicit the most responses and richest images and seems to be the psychologically basic level. Typical taxonomies in zoology for example exhibit categorization at the embodied level, with similarities leading to formulation of \"higher\" categories, and differences leading to differentiation within categories.\n\nMiscategorization\nMiscategorization can be a logical fallacy in which diverse and dissimilar objects, concepts, entities, etc. are grouped together based upon illogical common denominators, or common denominators that virtually any concept, object or entity have in common. A common way miscategorization occurs is through an over-categorization of concepts, objects or entities, and then miscategorization based upon over-similar variables that virtually all things have in common.\n\nSee also\n\nThis \"see also\" section may contain an excessive number of suggestions. Please ensure that only the most relevant suggestions are given and that they are not red links, and consider integrating suggestions into the article itself. (October 2013)\nLumpers and splitters\nArtificial neural network\nCategory learning\nCategorical perception\nClassification in machine learning\nFamily resemblance\nFuzzy concept\nLanguage acquisition\nLibrary classification\nMachine learning\nMulti-label classification\nNatural kind\nOntology\nPattern recognition\nPerceptual learning\nSemantics\nSocrates\nSortal\nStructuralism\nSymbol grounding\nTaxonomy (general)", "skillName": "Categorization."}
{"id": 144, "category": "Machine_Learning", "skillText": "Machine learning\nMachine Learning (journal).\nMachine learning and\ndata mining\nKernel Machine.svg\nProblems\nSupervised learning\n(classification � regression)\nClustering\nDimensionality reduction\nStructured prediction\nAnomaly detection\nNeural nets\nTheory\nMachine learning venues\nPortal icon Machine learning portal\nMachine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] In 1959, Arthur Samuel defined machine learning as a \"Field of study that gives computers the ability to learn without being explicitly programmed\".[2] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[3] Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs,[4]:2 rather than following strictly static program instructions.\n\nMachine learning is closely related to (and often overlaps with) computational statistics; a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is unfeasible. Example applications include spam filtering, optical character recognition (OCR),[5] search engines and computer vision. Machine learning is sometimes conflated with data mining,[6] where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning.[4]:vii[7]\n\nWithin the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction. These analytical models allow researchers, data scientists, engineers, and analysts to \"produce reliable, repeatable decisions and results\" and uncover \"hidden insights\" through learning from historical relationships and trends in the data.[8]\n\n1\tOverview\n1.1\tTypes of problems and tasks\n2\tHistory and relationships to other fields\n2.1\tRelation to statistics\n3\tTheory\n4\tApproaches\n4.1\tDecision tree learning\n4.2\tAssociation rule learning\n4.3\tArtificial neural networks\n4.4\tDeep Learning\n4.5\tInductive logic programming\n4.6\tSupport vector machines\n4.7\tClustering\n4.8\tBayesian networks\n4.9\tReinforcement learning\n4.10\tRepresentation learning\n4.11\tSimilarity and metric learning\n4.12\tSparse dictionary learning\n4.13\tGenetic algorithms\n5\tApplications\n6\tEthics\n7\tSoftware\n7.1\tOpen-source software\n7.2\tCommercial software with open-source editions\n7.3\tCommercial software\n8\tJournals\n9\tConferences\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nOverview\nTom M. Mitchell provided a widely quoted, more formal definition: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\"[9] This definition is notable for its defining machine learning in fundamentally operational rather than cognitive terms, thus following Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\" that the question \"Can machines think?\" be replaced with the question \"Can machines do what we (as thinking entities) can do?\"[10]\n\nTypes of problems and tasks\n\nMachine learning tasks are typically classified into three broad categories, depending on the nature of the learning \"signal\" or \"feedback\" available to a learning system. These are[11]\n\nSupervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\nUnsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\nReinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle), without a teacher explicitly telling it whether it has come close to its goal. Another example is learning to play a game by playing against an opponent.[4]:3\nBetween supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. Transduction is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing.\n\n\nA support vector machine is a classifier that divides its input space into two regions, separated by a linear boundary. Here, it has learned to distinguish black and white circles.\nAmong other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers, and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.\n\nAnother categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[4]:3\n\nIn classification, inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes. This is typically tackled in a supervised way. Spam filtering is an example of classification, where the inputs are email (or other) messages and the classes are \"spam\" and \"not spam\".\nIn regression, also a supervised problem, the outputs are continuous rather than discrete.\nIn clustering, a set of inputs is to be divided into groups. Unlike in classification, the groups are not known beforehand, making this typically an unsupervised task.\nDensity estimation finds the distribution of inputs in some space.\nDimensionality reduction simplifies inputs by mapping them into a lower-dimensional space. Topic modeling is a related problem, where a program is given a list of human language documents and is tasked to find out which documents cover similar topics.\nHistory and relationships to other fields\nSee also: Timeline of machine learning\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.[11]:488\n\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[11]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[12] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[11]:708�710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[11]:25\n\nMachine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[12] It also benefited from the increasing availability of digitized information, and the possibility to distribute that via the Internet.\n\nMachine learning and data mining often employ the same methods and overlap significantly. They can be roughly distinguished as follows:\n\nMachine learning focuses on prediction, based on known properties learned from the training data.\nData mining focuses on the discovery of (previously) unknown properties in the data. This is the analysis step of Knowledge Discovery in Databases.\nThe two areas overlap in many ways: data mining uses many machine learning methods, but often with a slightly different goal in mind. On the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in Knowledge Discovery and Data Mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n\nMachine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[13]\n\nRelation to statistics\nMachine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[14] He also suggested the term data science as a placeholder to call the overall field.[14]\n\nLeo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[15] wherein 'algorithmic model' means more or less the machine learning algorithms like Random forest.\n\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[16]\n\nTheory\nMain article: Computational learning theory\nA core objective of a learner is to generalize from its experience.[17][18] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias�variance decomposition is one way to quantify generalization error.\n\nHow well a model, trained with existing examples, predicts the output for unknown instances is called generalization. For best generalization, complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, we've underfitted. Then, we increase the complexity, the training error decreases. But if our hypothesis is too complex, we've overfitted. After then, we should find the hypothesis that has the minimum training error.[19]\n\nIn addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\nThere are many similarities between machine learning theory and statistical inference, although they use different terms.\n\nApproaches\nMain article: List of machine learning algorithms\nDecision tree learning\nMain article: Decision tree learning\nDecision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value.\n\nAssociation rule learning\nMain article: Association rule learning\nAssociation rule learning is a method for discovering interesting relations between variables in large databases.\n\nArtificial neural networks\nMain article: Artificial neural network\nAn artificial neural network (ANN) learning algorithm, usually called \"neural network\" (NN), is a learning algorithm that is inspired by the structure and functional aspects of biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.\n\nDeep Learning\nMain article: Deep learning\nFalling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of Deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[20]\n\nInductive logic programming\nMain article: Inductive logic programming\nInductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.\n\nSupport vector machines\nMain article: Support vector machines\nSupport vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.\n\nClustering\nMain article: Cluster analysis\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.\n\nBayesian networks\nMain article: Bayesian network\nA Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.\n\nReinforcement learning\nMain article: Reinforcement learning\nReinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.\n\nRepresentation learning\nMain article: Representation learning\nSeveral learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing to reconstruct the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.\n\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[21] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[22]\n\nSimilarity and metric learning\nMain article: Similarity learning\nIn this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.\n\nSparse dictionary learning\nMain article: Sparse dictionary learning\nIn this method, a datum is represented as a linear combination of basis functions, and the coefficients are assumed to be sparse. Let x be a d-dimensional datum, D be a d by n matrix, where each column of D represents a basis function. r is the coefficient to represent x using D. Mathematically, sparse dictionary learning means solving {\\displaystyle x\\approx Dr} {\\displaystyle x\\approx Dr} where r is sparse. Generally speaking, n is assumed to be larger than d to allow the freedom for a sparse representation.\n\nLearning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[23] A popular heuristic method for sparse dictionary learning is K-SVD.\n\nSparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[24]\n\nGenetic algorithms\nMain article: Genetic algorithm\nA genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[25][26] Vice versa, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[27]\n\nApplications\nApplications for machine learning include:\n\nAdaptive websites\nAffective computing\nBioinformatics\nBrain-machine interfaces\nCheminformatics\nClassifying DNA sequences\nComputational anatomy\nComputer vision, including object recognition\nDetecting credit card fraud\nGame playing[28]\nInformation retrieval\nInternet fraud detection\nMarketing\nMachine perception\nMedical diagnosis\nNatural language processing[29]\nOptimization and metaheuristic\nOnline advertising\nRecommender systems\nRobot locomotion\nSearch engines\nSentiment analysis (or opinion mining)\nSequence mining\nSoftware engineering\nSpeech and handwriting recognition\nStock market analysis\nStructural health monitoring\nSyntactic pattern recognition\nEconomics\nIn 2006, the online movie company Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[30] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[31]\n\nIn 2010 The Wall Street Journal wrote about money management firm Rebellion Research's use of machine learning to predict economic movements. The article describes Rebellion Research's prediction of the financial crisis and economic recovery.[32]\n\nIn 2014 it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[33]\n\nEthics\nMachine Learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use, thus digitizing cultural prejudices such as institutional racism and classism. [34] Responsible collection of data thus is a critical part of machine learning. See Machine ethics for additional information.\n\nSoftware\nSoftware suites containing a variety of machine learning algorithms include the following:\n\nOpen-source software\nCaffe\ndlib\nELKI\nEncog\nGNU Octave\nH2O\nMahout\nMallet (software project)\nmlpy\nMLPACK\nMOA (Massive Online Analysis)\nND4J with Deeplearning4j\nNuPIC\nOpenAI\nOpenCV\nOpenNN\nOrange\nR\nscikit-learn\nscikit-image\nShogun\nTensorFlow\nTorch (machine learning)\nSpark\nYooreeka\nWeka\nCommercial software with open-source editions\nKNIME\nRapidMiner\nCommercial software\nAngoss KnowledgeSTUDIO\nAyasdi\nDatabricks\nGoogle Prediction API\nIBM SPSS Modeler\nKXEN Modeler\nLIONsolver\nMathematica\nMATLAB\nMicrosoft Azure Machine Learning\nNeural Designer\nNeuroSolutions\nOracle Data Mining\nRCASE\nSAS Enterprise Miner\nSTATISTICA Data Miner\nJournals\nJournal of Machine Learning Research\nMachine Learning\nNeural Computation\nConferences\nConference on Neural Information Processing Systems\nInternational Conference on Machine Learning\nInternational Conference on Learning Representations\nPortal icon\tArtificial intelligence portal\nPortal icon\tMachine learning portal\nAdaptive control\nAdversarial machine learning\nAutomatic reasoning\nBayesian structural time series\nBig data\nCache language model\nCognitive model\nCognitive science\nComputational intelligence\nComputational neuroscience\nData science\nEthics of artificial intelligence\nExistential risk from advanced artificial intelligence\nExplanation-based learning\nGlossary of artificial intelligence\nImportant publications in machine learning\nList of machine learning algorithms\nList of datasets for machine learning research\nSimilarity learning\nSpike-and-slab variable selection", "skillName": "Machine_learning."}
{"id": 145, "category": "Machine_Learning", "skillText": "Text mining\nText mining, also referred to as text data mining, roughly equivalent to text analytics, refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n\nText analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.\n\nA typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.\n\n1\tText mining and text analytics\n2\tHistory\n3\tText analysis processes\n4\tApplications\n4.1\tSecurity applications\n4.2\tBiomedical applications\n4.3\tSoftware applications\n4.4\tOnline media applications\n4.5\tMarketing applications\n4.6\tSentiment analysis\n4.7\tAcademic applications\n4.8\tDigital Humanities and Computational Sociology\n5\tSoftware\n6\tIntellectual Property Law and Text Mining\n6.1\tSituation in Europe\n6.2\tSituation in United States\n7\tImplications\n8\tSee also\n9\tNotes\n10\tReferences\n11\tExternal links\nText mining and text analytics\nThe term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation.[1] The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of \"text mining\"[2] in 2004 to describe \"text analytics.\"[3] The latter term is now used more frequently in business settings while \"text mining\" is used in some of the earliest application areas, dating to the 1980s,[4] notably life-sciences research and government intelligence.\n\nThe term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text.[5] These techniques and processes discover and present knowledge � facts, business rules, and relationships � that is otherwise locked in textual form, impenetrable to automated processing.\n\nHistory\nLabor-intensive manual text mining approaches first surfaced in the mid-1980s,[6] but technological advances have enabled the field to advance during the past decade. Text mining is an interdisciplinary field that draws on information retrieval, data mining, machine learning, statistics, and computational linguistics. As most information (common estimates say over 80%)[5] is currently stored as text, text mining is believed to have a high commercial potential value. Increasing interest is being paid to multilingual data mining: the ability to gain information across languages and cluster similar items from different linguistic sources according to their meaning.\n\nThe challenge of exploiting the large proportion of enterprise information that originates in \"unstructured\" form has been recognized for decades.[7] It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:\n\n\"...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points.\"\n\nYet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases. This is not surprising: text in \"unstructured\" documents is hard to process. The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:[8]\n\nFor almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.\n\nHearst's 1999 statement of need fairly well describes the state of text analytics technology and practice a decade later.\n\nText analysis processes\nSubtasks � components of a larger text-analytics effort � typically include:\n\nInformation retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.\nAlthough some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.[citation needed]\nNamed entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on. Disambiguation � the use of contextual clues � may be required to decide where, for instance, \"Ford\" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.\nRecognition of Pattern Identified Entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.\nCoreference: identification of noun phrases and other terms that refer to the same object.\nRelationship, fact, and event Extraction: identification of associations among entities and other information in text\nSentiment analysis involves discerning subjective (as opposed to factual) material and extracting various forms of attitudinal information: sentiment, opinion, mood, and emotion. Text analytics techniques are helpful in analyzing, sentiment at the entity, concept, or topic level and in distinguishing opinion holder and opinion object.[9]\nQuantitative text analysis is a set of techniques stemming from the social sciences where either a human judge or a computer extracts semantic or grammatical relationships between words in order to find out the meaning or stylistic patterns of, usually, a casual personal text for the purpose of psychological profiling etc.[10]\nApplications\nThe technology is now broadly applied for a wide variety of government, research, and business needs. Applications can be sorted into a number of categories by analysis type or by business function. Using this approach to classifying solutions, application categories include:\n\nEnterprise Business Intelligence/Data Mining, Competitive Intelligence\nE-Discovery, Records Management\nNational Security/Intelligence\nScientific discovery, especially Life Sciences\nSentiment Analysis Tools, Listening Platforms\nNatural Language/Semantic Toolkit or Service\nPublishing\nAutomated ad placement\nSearch/Information Access\nSocial media monitoring\nSecurity applications\nMany text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes.[11] It is also involved in the study of text encryption/decryption.\n\nBiomedical applications\nMain article: Biomedical text mining\nA range of text mining applications in the biomedical literature has been described.[12]\n\nOne online text mining application in the biomedical literature is PubGene that combines biomedical text mining with network visualization as an Internet service.[13][14] TPX is a concept-assisted search and navigation tool for biomedical literature analyses[15] - it runs on PubMed/PMC and can be configured, on request, to run on local literature repositories too.\n\nGoPubMed is a knowledge-based search engine for biomedical texts.\n\nSoftware applications\nText mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results. Within public sector much effort has been concentrated on creating software for tracking and monitoring terrorist activities.[16]\n\nOnline media applications\nText mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site \"stickiness\" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.\n\nMarketing applications\nText mining is starting to be used in marketing as well, more specifically in analytical customer relationship management.[17] Coussement and Van den Poel (2008)[18][19] apply it to improve predictive analytics models for customer churn (customer attrition).[18]\n\nSentiment analysis\nSentiment analysis may involve analysis of movie reviews for estimating how favorable a review is for a movie.[20] Such an analysis may need a labeled data set or labeling of the affectivity of words. Resources for affectivity of words and concepts have been made for WordNet[21] and ConceptNet,[22] respectively.\n\nText has been used to detect emotions in the related area of affective computing.[23] Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.\n\nAcademic applications\nThe issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within text without removing publisher barriers to public access.\n\nAcademic institutions have also become involved in the text mining initiative:\n\nThe National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world. NaCTeM is operated by the University of Manchester[24] in close collaboration with the Tsujii Lab,[25] University of Tokyo.[26] NaCTeM provides customised tools, research facilities and offers advice to the academic community. They are funded by the Joint Information Systems Committee (JISC) and two of the UK Research Councils (EPSRC & BBSRC). With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences.\nIn the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis.\nDigital Humanities and Computational Sociology\nThe automatic analysis of vast textual corpora has created the possibility for scholars to analyse millions of documents in multiple languages with very limited manual intervention. Key enabling technologies have been Parsing, Machine Translation, Topic categorization, Machine Learning.\n\n\nNarrative network of US Elections 2012[27]\nThe automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data. The resulting networks, which can contain thousands of nodes, are then analysed by using tools from Network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes.[28] This automates the approach introduced by Quantitative Narrative Analysis,[29] whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.[27]\n\nContent analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a \"big data\" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. [30] [31] [32] [33] The analysis of readability, gender bias and topic bias was demonstrated in [34] showing how different topics have different gender biases and levels of readability; the possibility to detect mood shifts in a vast population by analysing Twitter content was demonstrated as well.[35]\n\nSoftware\nText mining computer programs are available from many commercial and open source companies and sources. See List of text mining software.\n\nIntellectual Property Law and Text Mining\nSituation in Europe\nDue to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. In the UK in 2014, on the recommendation of the Hargreaves review the government amended copyright law[36] to allow text mining as a limitation and exception. Only the second country in the world to do so after Japan, which introduced a mining specific exception in 2009. However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions.\n\nThe European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[37] The focus on the solution to this legal issue being licences and not limitations and exceptions to copyright law led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[38]\n\nSituation in United States\nBy contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one such use being text and data mining.[39]\n\nImplications\nUntil recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events. For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence. In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment.\n\nSee also\nFull text search\nConcept Mining\nWeb mining, a task that may involve text mining (e.g. first find appropriate web pages by classifying crawled web pages, then extract the desired information from the text content of these pages considered relevant).\nSequential pattern mining: String and Sequence Mining\nNews analytics\nMarket sentiment\nNamed entity recognition\nName resolution (semantics and text extraction)\nRecord linkage\nw-shingling\nList of text mining software", "skillName": "Text mining."}
{"id": 146, "category": "Machine_Learning", "skillText": "Information retrieval\nInformation retrieval (IR) is the activity of obtaining information resources relevant to an information need from a collection of information resources. Searches can be based on or on full-text (or other content-based) indexing.\n\nAutomated information retrieval systems are used to reduce what has been called \"information overload\". Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.\n1\tOverview\n2\tHistory\n3\tModel types\n3.1\tFirst dimension: mathematical basis\n3.2\tSecond dimension: properties of the model\n4\tPerformance and correctness measures\n4.1\tPrecision\n4.2\tRecall\n4.3\tFall-out\n4.4\tF-score / F-measure\n4.5\tAverage precision\n4.6\tPrecision at K\n4.7\tR-Precision\n4.8\tMean average precision\n4.9\tDiscounted cumulative gain\n4.10\tOther measures\n4.11\tVisualization\n5\tTimeline\n6\tAwards in the field\n7\tSee also\n8\tReferences\n9\tFurther reading\n10\tExternal links\nOverview\nAn information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.\n\nAn object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.[1]\n\nDepending on the application the data objects may be, for example, text documents, images,[2] audio,[3] mind maps[4] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\n\nMost IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.[5]\n\nHistory\n�\tthere is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute\t�\n�  J. E. Holmstrom, 1948\nThe idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945.[6] It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film.[7] The first description of a computer searching for information was described by Holmstrom in 1948,[8] detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents).[6] Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\n\nIn 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.\n\nModel types\n\nCategorization of IR-models (translated from German entry, original source Dominik Kuropka).\nFor effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n\nFirst dimension: mathematical basis\nSet-theoretic models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:\nStandard Boolean model\nExtended Boolean model\nFuzzy retrieval\nAlgebraic models represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.\nVector space model\nGeneralized vector space model\n(Enhanced) Topic-based Vector Space Model\nExtended Boolean model\nLatent semantic indexing a.k.a. latent semantic analysis\nProbabilistic models treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the Bayes' theorem are often used in these models.\nBinary Independence Model\nProbabilistic relevance model on which is based the okapi (BM25) relevance function\nUncertain inference\nLanguage models\nDivergence-from-randomness model\nLatent Dirichlet allocation\nFeature-based retrieval models view documents as vectors of values of feature functions (or just features) and seek the best way to combine these features into a single relevance score, typically by learning to rank methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.\nSecond dimension: properties of the model\nModels without term-interdependencies treat different terms/words as independent. This fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables.\nModels with immanent term interdependencies allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by dimensional reduction) from the co-occurrence of those terms in the whole set of documents.\nModels with transcendent term interdependencies allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)\nPerformance and correctness measures\nFurther information: Evaluation measures (information retrieval)\nThe evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.\n\nVirtually all modern evaluation metrics (e.g., mean average precision, discounted cumulative gain) are designed for ranked retrieval without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.[citation needed]\n\nThe mathematical symbols used in the formulas below mean:\n\n{\\displaystyle X\\cap Y} X\\cap Y - Intersection - in this case, specifying the documents in both sets X and Y\n{\\displaystyle |X|} |X| - Cardinality - in this case, the number of documents in set X\n{\\displaystyle \\int } \\int  - Integral\n{\\displaystyle \\sum } \\sum  - Summation\n{\\displaystyle \\Delta } \\Delta  - Symmetric difference\nPrecision\nMain article: Precision and recall\nPrecision is the fraction of the documents retrieved that are relevant to the user's information need.\n\n{\\displaystyle {\\mbox{precision}}={\\frac {|\\{{\\mbox{relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{retrieved documents}}\\}|}}}  \\mbox{precision}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{retrieved documents}\\}|}\nIn binary classification, precision is analogous to positive predictive value. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called precision at n or P@n.\n\nNote that the meaning and usage of \"precision\" in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics.\n\nRecall\nMain article: Precision and recall\nRecall is the fraction of the documents that are relevant to the query that are successfully retrieved.\n\n{\\displaystyle {\\mbox{recall}}={\\frac {|\\{{\\mbox{relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{relevant documents}}\\}|}}} \\mbox{recall}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{relevant documents}\\}|}\nIn binary classification, recall is often called sensitivity. So it can be looked at as the probability that a relevant document is retrieved by the query.\n\nIt is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.\n\nFall-out\nThe proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:\n\n{\\displaystyle {\\mbox{fall-out}}={\\frac {|\\{{\\mbox{non-relevant documents}}\\}\\cap \\{{\\mbox{retrieved documents}}\\}|}{|\\{{\\mbox{non-relevant documents}}\\}|}}}  \\mbox{fall-out}=\\frac{|\\{\\mbox{non-relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{non-relevant documents}\\}|}\nIn binary classification, fall-out is closely related to specificity and is equal to {\\displaystyle (1-{\\mbox{specificity}})} (1-\\mbox{specificity}). It can be looked at as the probability that a non-relevant document is retrieved by the query.\n\nIt is trivial to achieve fall-out of 0% by returning zero documents in response to any query.\n\nF-score / F-measure\nMain article: F-score\nThe weighted harmonic mean of precision and recall, the traditional F-measure or balanced F-score is:\n\n{\\displaystyle F={\\frac {2\\cdot \\mathrm {precision} \\cdot \\mathrm {recall} }{(\\mathrm {precision} +\\mathrm {recall} )}}} F={\\frac  {2\\cdot {\\mathrm  {precision}}\\cdot {\\mathrm  {recall}}}{({\\mathrm  {precision}}+{\\mathrm  {recall}})}}\nThis is also known as the {\\displaystyle F_{1}} F_{1} measure, because recall and precision are evenly weighted.\n\nThe general formula for non-negative real {\\displaystyle \\beta } \\beta  is:\n\n{\\displaystyle F_{\\beta }={\\frac {(1+\\beta ^{2})\\cdot (\\mathrm {precision} \\cdot \\mathrm {recall} )}{(\\beta ^{2}\\cdot \\mathrm {precision} +\\mathrm {recall} )}}\\,} F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\mathrm{precision} \\cdot \\mathrm{recall})}{(\\beta^2 \\cdot \\mathrm{precision} + \\mathrm{recall})}\\,\nTwo other commonly used F measures are the {\\displaystyle F_{2}} F_{2} measure, which weights recall twice as much as precision, and the {\\displaystyle F_{0.5}} F_{0.5} measure, which weights precision twice as much as recall.\n\nThe F-measure was derived by van Rijsbergen (1979) so that {\\displaystyle F_{\\beta }} F_{\\beta } \"measures the effectiveness of retrieval with respect to a user who attaches {\\displaystyle \\beta } \\beta  times as much importance to recall as precision\". It is based on van Rijsbergen's effectiveness measure {\\displaystyle E=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}} E=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}. Their relationship is:\n\n{\\displaystyle F_{\\beta }=1-E} F_{\\beta }=1-E where {\\displaystyle \\alpha ={\\frac {1}{1+\\beta ^{2}}}} \\alpha ={\\frac {1}{1+\\beta ^{2}}}\nF-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.[citation needed]\n\nAverage precision\nPrecision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision {\\displaystyle p(r)} p(r) as a function of recall {\\displaystyle r} r. Average precision computes the average value of {\\displaystyle p(r)} p(r) over the interval from {\\displaystyle r=0} r=0 to {\\displaystyle r=1} r=1:[9]\n\n{\\displaystyle \\operatorname {AveP} =\\int _{0}^{1}p(r)dr} \\operatorname{AveP} = \\int_0^1 p(r)dr\nThat is the area under the precision-recall curve. This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:\n\n{\\displaystyle \\operatorname {AveP} =\\sum _{k=1}^{n}P(k)\\Delta r(k)} \\operatorname{AveP} = \\sum_{k=1}^n P(k) \\Delta r(k)\nwhere {\\displaystyle k} k is the rank in the sequence of retrieved documents, {\\displaystyle n} n is the number of retrieved documents, {\\displaystyle P(k)} P(k) is the precision at cut-off {\\displaystyle k} k in the list, and {\\displaystyle \\Delta r(k)} \\Delta r(k) is the change in recall from items {\\displaystyle k-1} k-1 to {\\displaystyle k} k.[9]\n\nThis finite sum is equivalent to:\n\n{\\displaystyle \\operatorname {AveP} ={\\frac {\\sum _{k=1}^{n}(P(k)\\times \\operatorname {rel} (k))}{\\mbox{number of relevant documents}}}\\!}  \\operatorname{AveP} = \\frac{\\sum_{k=1}^n (P(k) \\times \\operatorname{rel}(k))}{\\mbox{number of relevant documents}} \\!\nwhere {\\displaystyle \\operatorname {rel} (k)} \\operatorname{rel}(k) is an indicator function equaling 1 if the item at rank {\\displaystyle k} k is a relevant document, zero otherwise.[10] Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.\n\nSome authors choose to interpolate the {\\displaystyle p(r)} p(r) function to reduce the impact of \"wiggles\" in the curve.[11][12] For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:[11][12]\n\n{\\displaystyle \\operatorname {AveP} ={\\frac {1}{11}}\\sum _{r\\in \\{0,0.1,\\ldots ,1.0\\}}p_{\\operatorname {interp} }(r)} \\operatorname{AveP} = \\frac{1}{11} \\sum_{r \\in \\{0, 0.1, \\ldots, 1.0\\}} p_{\\operatorname{interp}}(r)\nwhere {\\displaystyle p_{\\operatorname {interp} }(r)} p_{\\operatorname{interp}}(r) is an interpolated precision that takes the maximum precision over all recalls greater than {\\displaystyle r} r:\n\n{\\displaystyle p_{\\operatorname {interp} }(r)=\\operatorname {max} _{{\\tilde {r}}:{\\tilde {r}}\\geq r}p({\\tilde {r}})} p_{\\operatorname{interp}}(r) = \\operatorname{max}_{\\tilde{r}:\\tilde{r} \\geq r} p(\\tilde{r}).\nAn alternative is to derive an analytical {\\displaystyle p(r)} p(r) function by assuming a particular parametric distribution for the underlying decision values. For example, a binormal precision-recall curve can be obtained by assuming decision values in both classes to follow a Gaussian distribution.[13]\n\nPrecision at K\nFor modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. Precision at k documents (P@k) is still a useful metric (e.g., P@10 or \"Precision at 10\" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.[citation needed] Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.[14] It easier to score manually since only the top k results need to be examined to determine if they are relevant or not.\n\nR-Precision\nR-precision requires knowing all documents that are relevant to a query. The number of relevant documents, {\\displaystyle R} R, is used as the cutoff for calculation, and this varies from query to query. For example, if there are 15 documents relevant to \"red\" in a corpus (R=15), R-precision for \"red\" looks at the top 15 documents returned, counts the number that are relevant {\\displaystyle r} r turns that into a relevancy fraction: {\\displaystyle r/R=r/15} r/R=r/15.[15]\n\nPrecision is equal to recall at the R-th position.[14]\n\nEmpirically, this measure is often highly correlated to mean average precision.[14]\n\nMean average precision\nMean average precision for a set of queries is the mean of the average precision scores for each query.\n\n{\\displaystyle \\operatorname {MAP} ={\\frac {\\sum _{q=1}^{Q}\\operatorname {AveP(q)} }{Q}}\\!}  \\operatorname{MAP} = \\frac{\\sum_{q=1}^Q \\operatorname{AveP(q)}}{Q} \\!\nwhere Q is the number of queries.\n\nDiscounted cumulative gain\nMain article: Discounted cumulative gain\nDCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n\nThe DCG accumulated at a particular rank position {\\displaystyle p} p is defined as:\n\n{\\displaystyle \\mathrm {DCG_{p}} =rel_{1}+\\sum _{i=2}^{p}{\\frac {rel_{i}}{\\log _{2}i}}.}  \\mathrm{DCG_{p}} = rel_{1} + \\sum_{i=2}^{p} \\frac{rel_{i}}{\\log_{2}i}.\nSince result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p ( {\\displaystyle IDCG_{p}} IDCG_p), which normalizes the score:\n\n{\\displaystyle \\mathrm {nDCG_{p}} ={\\frac {DCG_{p}}{IDCG{p}}}.}  \\mathrm{nDCG_{p}} = \\frac{DCG_{p}}{IDCG{p}}.\nThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the {\\displaystyle DCG_{p}} DCG_p will be the same as the {\\displaystyle IDCG_{p}} IDCG_p producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\n\nOther measures\nTerminology and derivations\nfrom a confusion matrix\ntrue positive (TP)\neqv. with hit\ntrue negative (TN)\neqv. with correct rejection\nfalse positive (FP)\neqv. with false alarm, Type I error\nfalse negative (FN)\neqv. with miss, Type II error\nsensitivity or true positive rate (TPR)\neqv. with hit rate, recall\n{\\displaystyle {\\mathit {TPR}}={\\frac {\\mathit {TP}}{P}}={\\frac {\\mathit {TP}}{{\\mathit {TP}}+{\\mathit {FN}}}}} {\\mathit {TPR}}={\\frac {\\mathit {TP}}{P}}={\\frac {\\mathit {TP}}{{\\mathit {TP}}+{\\mathit {FN}}}}\nspecificity (SPC) or true negative rate (TNR)\n{\\displaystyle {\\mathit {SPC}}={\\frac {\\mathit {TN}}{N}}={\\frac {\\mathit {TN}}{{\\mathit {FP}}+{\\mathit {TN}}}}} {\\mathit {SPC}}={\\frac {\\mathit {TN}}{N}}={\\frac {\\mathit {TN}}{{\\mathit {FP}}+{\\mathit {TN}}}}\nprecision or positive predictive value (PPV)\n{\\displaystyle {\\mathit {PPV}}={\\frac {\\mathit {TP}}{{\\mathit {TP}}+{\\mathit {FP}}}}} {\\mathit {PPV}}={\\frac {\\mathit {TP}}{{\\mathit {TP}}+{\\mathit {FP}}}}\nnegative predictive value (NPV)\n{\\displaystyle {\\mathit {NPV}}={\\frac {\\mathit {TN}}{{\\mathit {TN}}+{\\mathit {FN}}}}} {\\mathit {NPV}}={\\frac {\\mathit {TN}}{{\\mathit {TN}}+{\\mathit {FN}}}}\nfall-out or false positive rate (FPR)\n{\\displaystyle {\\mathit {FPR}}={\\frac {\\mathit {FP}}{N}}={\\frac {\\mathit {FP}}{{\\mathit {FP}}+{\\mathit {TN}}}}=1-{\\mathit {SPC}}} {\\mathit {FPR}}={\\frac {\\mathit {FP}}{N}}={\\frac {\\mathit {FP}}{{\\mathit {FP}}+{\\mathit {TN}}}}=1-{\\mathit {SPC}}\nfalse discovery rate (FDR)\n{\\displaystyle {\\mathit {FDR}}={\\frac {\\mathit {FP}}{{\\mathit {FP}}+{\\mathit {TP}}}}=1-{\\mathit {PPV}}} {\\mathit {FDR}}={\\frac {\\mathit {FP}}{{\\mathit {FP}}+{\\mathit {TP}}}}=1-{\\mathit {PPV}}\nmiss rate or false negative rate (FNR)\n{\\displaystyle {\\mathit {FNR}}={\\frac {\\mathit {FN}}{P}}={\\frac {\\mathit {FN}}{{\\mathit {FN}}+{\\mathit {TP}}}}} {\\mathit {FNR}}={\\frac {\\mathit {FN}}{P}}={\\frac {\\mathit {FN}}{{\\mathit {FN}}+{\\mathit {TP}}}}\naccuracy (ACC)\n{\\displaystyle {\\mathit {ACC}}={\\frac {{\\mathit {TP}}+{\\mathit {TN}}}{P+N}}} {\\mathit {ACC}}={\\frac {{\\mathit {TP}}+{\\mathit {TN}}}{P+N}}\nF1 score\nis the harmonic mean of precision and sensitivity\n{\\displaystyle {\\mathit {F1}}={\\frac {2{\\mathit {TP}}}{2{\\mathit {TP}}+{\\mathit {FP}}+{\\mathit {FN}}}}} {\\mathit {F1}}={\\frac {2{\\mathit {TP}}}{2{\\mathit {TP}}+{\\mathit {FP}}+{\\mathit {FN}}}}\nMatthews correlation coefficient (MCC)\n{\\displaystyle {\\frac {TP\\times TN-FP\\times FN}{\\sqrt {(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}}} {\\frac {TP\\times TN-FP\\times FN}{\\sqrt {(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}}\nInformedness = Sensitivity + Specificity - 1\nMarkedness = Precision + NPV - 1\nSources: Fawcett (2006) and Powers (2011).[16][17]\nMean reciprocal rank\nSpearman's rank correlation coefficient\nbpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents[15]\nGMAP - geometric mean of (per-topic) average precision[15]\nMeasures based on marginal relevance and document diversity - see Relevance (information retrieval) � Problems and alternatives\nVisualization\nVisualizations of information retrieval performance include:\n\nGraphs which chart precision on one axis and recall on the other[15]\nHistograms of average precision over various topics[15]\nReceiver operating characteristic (ROC curve)\nConfusion matrix\nTimeline\nBefore the 1900s\n1801: Joseph Marie Jacquard invents the Jacquard loom, the first machine to use punched cards to control a sequence of operations.\n1880s: Herman Hollerith invents an electro-mechanical data tabulator using punch cards as a machine readable medium.\n1890 Hollerith cards, keypunches and tabulators used to process the 1890 US Census data.\n1920s-1930s\nEmanuel Goldberg submits patents for his \"Statistical Machine� a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.\n1940s�1950s\nlate 1940s: The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.\n1945: Vannevar Bush's As We May Think appeared in Atlantic Monthly.\n1947: Hans Peter Luhn (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.\n1950s: Growing concern in the US for a \"science gap\" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent et al.) and the invention of citation indexing (Eugene Garfield).\n1950: The term \"information retrieval\" was coined by Calvin Mooers.[18]\n1951: Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at MIT.[19]\n1955: Allen Kent joined Case Western Reserve University, and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed \"framework\" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.[20]\n1958: International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: Proceedings of the International Conference on Scientific Information, 1958 (National Academy of Sciences, Washington, DC, 1959)\n1959: Hans Peter Luhn published \"Auto-encoding of documents for information retrieval.\"\n1960s:\nearly 1960s: Gerard Salton began work on IR at Harvard, later moved to Cornell.\n1960: Melvin Earl Maron and John Lary Kuhns[21] published \"On relevance, probabilistic indexing, and information retrieval\" in the Journal of the ACM 7(3):216�244, July 1960.\n1962:\nCyril W. Cleverdon published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, \"Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems\". Cranfield Collection of Aeronautics, Cranfield, England, 1962.\nKent published Information Analysis and Retrieval.\n1963:\nWeinberg report \"Science, Government and Information\" gave a full articulation of the idea of a \"crisis of scientific information.\" The report was named after Dr. Alvin Weinberg.\nJoseph Becker and Robert M. Hayes published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. Information storage and retrieval: tools, elements, theories. New York, Wiley (1963).\n1964:\nKaren Sp�rck Jones finished her thesis at Cambridge, Synonymy and Semantic Classification, and continued work on computational linguistics as it applies to IR.\nThe National Bureau of Standards sponsored a symposium titled \"Statistical Association Methods for Mechanized Documentation.\" Several highly significant papers, including G. Salton's first published reference (we believe) to the SMART system.\nmid-1960s:\nNational Library of Medicine developed MEDLARS Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.\nProject Intrex at MIT.\n1965: J. C. R. Licklider published Libraries of the Future.\n1966: Don Swanson was involved in studies at University of Chicago on Requirements for Future Catalogs.\nlate 1960s: F. Wilfrid Lancaster completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.\n1968:\nGerard Salton published Automatic Information Organization and Retrieval.\nJohn W. Sammon, Jr.'s RADC Tech report \"Some Mathematics of Information Storage and Retrieval...\" outlined the vector model.\n1969: Sammon's \"A nonlinear mapping for data structure analysis\" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.\n1970s\nearly 1970s:\nFirst online systems�NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.\nTheodor Nelson promoting concept of hypertext, published Computer Lib/Dream Machines.\n1971: Nicholas Jardine and Cornelis J. van Rijsbergen published \"The use of hierarchic clustering in information retrieval\", which articulated the \"cluster hypothesis.\"[22]\n1975: Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:\nA Theory of Indexing (Society for Industrial and Applied Mathematics)\nA Theory of Term Importance in Automatic Text Analysis (JASIS v. 26)\nA Vector Space Model for Automatic Indexing (CACM 18:11)\n1978: The First ACM SIGIR conference.\n1979: C. J. van Rijsbergen published Information Retrieval (Butterworths). Heavy emphasis on probabilistic models.\n1979: Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.[23]\n1980s\n1980: First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.\n1982: Nicholas J. Belkin, Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.\n1983: Salton (and Michael J. McGill) published Introduction to Modern Information Retrieval (McGraw-Hill), with heavy emphasis on vector models.\n1985: David Blair and Bill Maron publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System\nmid-1980s: Efforts to develop end-user versions of commercial IR systems.\n1985�1993: Key papers on and experimental systems for visualization interfaces.\nWork by Donald B. Crouch, Robert R. Korfhage, Matthew Chalmers, Anselm Spoerri and others.\n1989: First World Wide Web proposals by Tim Berners-Lee at CERN.\n1990s\n1992: First TREC conference.\n1997: Publication of Korfhage's Information Storage and Retrieval[24] with emphasis on visualization and multi-reference point systems.\nlate 1990s: Web search engines implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.\nAwards in the field\nTony Kent Strix award\nGerard Salton Award\nSee also\nAdversarial information retrieval\nCollaborative information seeking\nControlled vocabulary\nCross-language information retrieval\nData mining\nEuropean Summer School in Information Retrieval\nHuman�computer information retrieval (HCIR)\nInformation extraction\nInformation Retrieval Facility\nKnowledge visualization\nMultimedia Information Retrieval\nPersonal information management\nRelevance (Information Retrieval)\nRelevance feedback\nRocchio Classification\nSearch index\nSocial information seeking\nSpecial Interest Group on Information Retrieval\nSubject indexing\nTemporal information retrieval\ntf-idf\nXML-Retrieval", "skillName": "Information Retrieval."}
{"id": 147, "category": "Machine_Learning", "skillText": "Predictive analytics\nPredictive analytics encompasses a variety of statistical techniques from predictive modeling, machine learning, and data mining that analyze current and historical facts to make predictions about future or otherwise unknown events.[1][2]\n\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions.[3]\n\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\n\nPredictive analytics is used in actuarial science,[4] marketing,[5] financial services,[6] insurance, telecommunications,[7] retail,[8] travel,[9] healthcare,[10] child protection,[11][12] pharmaceuticals,[13] capacity planning[citation needed] and other fields.\n\nOne of the most well known applications is credit scoring,[1] which is used throughout financial services. Scoring models process a customer's credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.\n\n1\tDefinition\n2\tTypes\n2.1\tPredictive models\n2.2\tDescriptive models\n2.3\tDecision models\n3\tApplications\n3.1\tAnalytical customer relationship management (CRM)\n3.2\tChild protection\n3.3\tClinical decision support systems\n3.4\tCollection analytics\n3.5\tCross-sell\n3.6\tCustomer retention\n3.7\tDirect marketing\n3.8\tFraud detection\n3.9\tPortfolio, product or economy-level prediction\n3.10\tRisk management\n3.11\tUnderwriting\n4\tTechnology and big data influences\n5\tAnalytical Techniques\n5.1\tRegression techniques\n5.1.1\tLinear regression model\n5.1.2\tDiscrete choice models\n5.1.3\tLogistic regression\n5.1.4\tMultinomial logistic regression\n5.1.5\tProbit regression\n5.1.6\tLogit versus probit\n5.1.7\tTime series models\n5.1.8\tSurvival or duration analysis\n5.1.9\tClassification and regression trees (CART)\n5.1.10\tMultivariate adaptive regression splines\n5.2\tMachine learning techniques\n5.2.1\tNeural networks\n5.2.2\tMultilayer Perceptron (MLP)\n5.2.3\tRadial basis functions\n5.2.4\tSupport vector machines\n5.2.5\tNa�ve Bayes\n5.2.6\tk-nearest neighbours\n5.2.7\tGeospatial predictive modeling\n6\tTools\n6.1\tPMML\n7\tCriticism\n8\tSee also\n9\tReferences\n10\tFurther reading\nDefinition\nPredictive analytics is an area of data mining that deals with extracting information from data and using it to predict trends and behavior patterns. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs.[14] The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.\n\nPredictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analytics�Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\"[15] In the future industrial systems the value of Predictive Analytics is to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization. Furthermore, the converted data can be used for closed-loop product life cycle improvement[16] which is the vision of Industrial Internet Consortium.\n\nTypes\nGenerally, the term predictive analytics is used to mean predictive modeling, \"scoring\" data with predictive models, and forecasting. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.\n\nPredictive models\nPredictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, or fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.\n\nThe available sample units with known attributes and known performances is referred to as the �training sample.� The units in other samples, with known attributes but unknown performances, are referred to as �out of [training] sample� units. The out of sample bear no chronological relation to the training sample units. For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.\n\nDescriptive models\nDescriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.\n\nDecision models\nDecision models describe the relationship between all the elements of a decision � the known data (including results of predictive models), the decision, and the forecast results of the decision � in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.\n\nApplications\nAlthough predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.\n\nAnalytical customer relationship management (CRM)\nAnalytical Customer Relationship Management is a frequent commercial application of Predictive Analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base. They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones. Analytical Customer Relationship Management can be applied throughout the customers lifecycle (acquisition, relationship growth, retention, and win-back). Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements.\n\nChild protection\nOver the last 5 years, some Child Welfare Agencies have started using predictive analytics to flag high risk cases.[17] The approach has been called \"innovative\" by the Commission to Eliminate Child Abuse and Neglect Fatalities (CECANF),[18] and in Hillsborough County, FL, where the Lead Child Welfare Agency uses a predictive modeling tool called Eckerd Rapid Safety Feedback�, there have been no abuse-related child deaths in the target population as of this writing.[19]\n\nClinical decision support systems\nExperts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: \"Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care.\"[citation needed]\n\nCollection analytics\nMany portfolios have a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.\n\nCross-sell\nOften corporate organizations collect and maintain abundant data (e.g. customer records, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers.[2] This directly leads to higher profitability per customer and stronger customer relationships.\n\nCustomer retention\nWith the number of competing services available, businesses need to focus efforts on maintaining continuous consumer satisfaction, rewarding consumer loyalty and minimizing customer attrition. In addition, small increases in customer retention have been shown to increase profits disproportionately. One study concluded that a 5% increase in customer retention rates will increase profits by 25% to 95%.[20] Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer's decision is almost zero. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer�s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon.[7] An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.\n\nDirect marketing\nWhen marketing consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The goal of predictive analytics is typically to lower the cost per order or cost per action.\n\nFraud detection\nFraud is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent transactions (both offline and online), identity thefts and false insurance claims. These problems plague firms of all sizes in many industries. Some examples of likely victims are credit card issuers, insurance companies,[21] retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the \"bads\" and reduce a business's exposure to fraud.\n\nPredictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. Mark Nigrini developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.[22]\n\nThe Internal Revenue Service (IRS) of the United States also uses predictive analytics to mine tax returns and identify tax fraud.[21]\n\nRecent[when?] advancements in technology have also introduced predictive behavior analysis for web fraud detection. This type of solution utilizes heuristics in order to study normal web user behavior and detect anomalies indicating fraud attempts.\n\nPortfolio, product or economy-level prediction\nOften the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.[23][24]\n\nRisk management\nWhen employing risk management techniques, the results are always to predict and benefit from a future scenario. The Capital asset pricing model (CAP-M) \"predicts\" the best portfolio to maximize return, Probabilistic Risk Assessment (PRA)--when combined with mini-Delphi Techniques and statistical approaches yields accurate forecasts and RiskAoA is a stand-alone predictive tool.[25] These are three examples of approaches that can extend from project to market, and from near to long term. Underwriting (see below) and other business approaches identify risk management as a predictive method.\n\nUnderwriting\nMany businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.[4] Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.\n\nTechnology and big data influences\nBig data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID, sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Big Data is the core of most predictive analytic services offered by IT organizations.[26] Thanks to technological advances in computer hardware � faster CPUs, cheaper memory, and MPP architectures � and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights.[21] Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed [27] [28]\n\nAnalytical Techniques\nThe approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.\n\nRegression techniques\nRegression models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there are a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below.\n\nLinear regression model\nThe linear regression model analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.\n\nThe goal of regression is to select the parameters of the model so as to minimize the sum of the squared residuals. This is referred to as ordinary least squares (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the Gauss-Markov assumptions are satisfied.\n\nOnce the model has been estimated we would be interested to know if the predictor variables belong in the model � i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model�s coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R� statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is \"explained\" (accounted for) by variation in the independent variables.\n\nDiscrete choice models\nMultivariate regression (above) is generally used when the response variable is continuous and has an unbounded range. Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are logistic regression, multinomial logit and probit models. Logistic regression and probit models are used when the dependent variable is binary.\n\nLogistic regression\nFor more details on this topic, see logistic regression.\nIn a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (See Allison's Logistic Regression for more information on the theory of Logistic Regression).\n\nThe Wald and likelihood-ratio test are used to test the statistical significance of each coefficient b in the model (analogous to the t tests used in OLS regression; see above). A test assessing the goodness-of-fit of a classification model is the \"percentage correctly predicted\".\n\nMultinomial logistic regression\nAn extension of the binary logit model to cases where the dependent variable has more than 2 categories is the multinomial logit model. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as Random multinomial logit.\n\nProbit regression\nProbit models offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.\n\nA good way to understand the key difference between probit and logit models is to assume that the dependent variable is driven by a latent variable z, which is a sum of a linear combination of explanatory variables and a random noise term.\n\nWe do not observe z but instead observe y which takes the value 0 (when z < 0) or 1 (otherwise). In the logit model we assume that the random noise term follows a logistic distribution with mean zero. In the probit model we assume that it follows a normal distribution with mean zero. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.\n\nLogit versus probit\nThe Probit model has been around longer than the logit model. They behave similarly, except that the logistic distribution tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model.\n\nPractical reasons for choosing the probit model over the logistic model would be:\n\nThere is a strong belief that the underlying distribution is normal\nThe actual event is not a binary outcome (e.g., bankruptcy status) but a proportion (e.g., proportion of population at different debt levels).\nTime series models\nTime series models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result, standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.\n\nTime series models estimate difference equations containing stochastic components. Two commonly used forms of these models are autoregressive models (AR) and moving-average (MA) models. The Box�Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model which is the cornerstone of stationary time series analysis. ARIMA (autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series. Box and Jenkins suggest differencing a non stationary time series to obtain a stationary series to which an ARMA model can be applied. Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.\n\nBox and Jenkins proposed a three-stage methodology which includes: model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions. In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.\n\nIn recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH (autoregressive conditional heteroskedasticity) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.\n\nSurvival or duration analysis\nSurvival analysis is another name for time to event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).\n\nCensoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple linear regression. The normal distribution, being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.\n\nThe assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.\n\nAn important concept in survival analysis is the hazard rate, defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.\n\nMost models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function. A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. All these distributions are for a non-negative random variable.\n\nDuration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are Kaplan-Meier and Cox proportional hazard model (non parametric).\n\nClassification and regression trees (CART)\nMain article: decision tree learning\nGlobally-optimal classification tree analysis (GO-CTA) (also called hierarchical optimal discriminant analysis) is a generalization of optimal discriminant analysis that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.\n\nClassification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.\n\nDecision trees are formed by a collection of rules based on variables in the modeling data set:\n\nRules based on variables' values are selected to get the best split to differentiate observations based on the dependent variable\nOnce a rule is selected and splits a node into two, the same process is applied to each \"child\" node (i.e. it is a recursive procedure)\nSplitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.)\nEach branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.\n\nA very popular method for predictive analytics is Leo Breiman's Random forests.\n\nMultivariate adaptive regression splines\nMultivariate adaptive regression splines (MARS) is a non-parametric technique that builds flexible models by fitting piecewise linear regressions.\n\nAn important concept associated with regression splines is that of a knot. Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.\n\nIn multivariate and adaptive regression splines, basis functions are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables. Multivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.\n\nMultivariate and adaptive regression spline approach deliberately overfits the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.\n\nMachine learning techniques\nMachine learning, a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including medical diagnostics, credit card fraud detection, face and speech recognition and analysis of the stock market. In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human cognition and learn from training examples to predict future events.\n\nA brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell (1997).\n\nNeural networks\nNeural networks are nonlinear sophisticated modeling techniques that are able to model complex functions. They can be applied to problems of prediction, classification or control in a wide spectrum of fields such as finance, cognitive psychology/neuroscience, medicine, engineering, and physics.\n\nNeural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training in neural networks used by different networks, supervised and unsupervised training, reinforcement learning, with supervised being the most common one.\n\nSome examples of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer perceptrons, Kohonen networks, Hopfield networks, etc.\n\nMultilayer Perceptron (MLP)\nThe Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of nonlinearly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The backpropagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule).\n\nRadial basis functions\nA radial basis function (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of neural networks where they are used as a replacement for the sigmoidal transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the feed-forward networks such as the multilayer perceptron.\n\nSupport vector machines\nSupport Vector Machines (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. They are learning machines that are used to perform binary classifications and regression estimations. They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc.\n\nNa�ve Bayes\nNa�ve Bayes based on Bayes conditional probability rule is used for performing classification tasks. Na�ve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the problem of �curse of dimensionality� i.e. when the number of predictors is very high.\n\nk-nearest neighbours\nThe nearest neighbour algorithm (KNN) belongs to the class of pattern recognition statistical methods. The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values. A new sample is classified by calculating the distance to the nearest neighbouring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours; (2) the decision rule used to derive a classification from the k-nearest neighbours; and (3) the number of neighbours used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e.: as the size of the training set increases, if the observations are independent and identically distributed (i.i.d.), regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error. See Devroy et al.\n\nGeospatial predictive modeling\nConceptually, geospatial predictive modeling is rooted in the principle that the occurrences of events being modeled are limited in distribution. Occurrences of events are neither uniform nor random in distribution � there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence.\n\nTools\nHistorically, using predictive analytics tools�as well as understanding the results they delivered�required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists[citation needed]. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model.[29] Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems[citation needed], so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results.[2] For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.[30]\n\nThere are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.\n\nNotable open source predictive analytic tools include:\n\nApache Mahout\nGNU Octave\nKNIME\nOpenNN\nOrange\nR\nRiskAoA\nscikit-learn\nWeka\nNotable commercial predictive analytic tools include:\n\nAlpine Data Labs\nAngoss KnowledgeSTUDIO\nBIRT Analytics\nIBM SPSS Statistics and IBM SPSS Modeler\nKXEN Modeler\nMathematica\nMATLAB\nMinitab\nLabVIEW[31]\nNeural Designer\nOracle Advanced Analytics\nPervasive\nPredixion Software\nRapidMiner\nRCASE\nRevolution Analytics\nSAP\nSAS and SAS Enterprise Miner\nSTATA\nStatgraphics\nSTATISTICA\nTeleRetail\nTIBCO\nBeside these software packages, specific tools have also been developed for industrial applications. For example, Watchdog Agent Toolbox has been developed and optimized for predictive analysis in prognostics and health management applications and is available for MATLAB and LABVIEW[32][33]\n\nThe most popular commercial predictive analytics software packages according to the Rexer Analytics Survey for 2013 are IBM SPSS Modeler, SAS Enterprise Miner, and Dell Statistica <http://www.rexeranalytics.com/Data-Miner-Survey-2013-Intro.html>.\n\nPMML\nIn an attempt to provide a standard language for expressing predictive models, the Predictive Model Markup Language (PMML) has been proposed. Such an XML-based language provides a way for the different tools to define predictive models and to share these between PMML compliant applications. PMML 4.0 was released in June, 2009.\n\nCriticism\nThere are plenty of skeptics when it comes to computers and algorithms abilities to predict the future, including Gary King, a professor from Harvard University and the director of the Institute for Quantitative Social Science. [34] People are influenced by their environment in innumerable ways. Trying to understand what people will do next assumes that all the influential variables can be known and measured accurately. \"People's environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn't as useful as it seemed before.\" [35]\n\nSee also\nCriminal Reduction Utilising Statistical History\nData mining\nLearning analytics\nOdds algorithm\nPattern recognition\nPrescriptive analytics\nPredictive modeling\nRiskAoA a predictive tool for discriminating future decisions.", "skillName": "Predictive Analytics."}
{"id": 148, "category": "Machine_Learning", "skillText": "Text mining\nText mining, also referred to as text data mining, roughly equivalent to text analytics, refers to the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n\nText analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.\n\nA typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.\n\n1\tText mining and text analytics\n2\tHistory\n3\tText analysis processes\n4\tApplications\n4.1\tSecurity applications\n4.2\tBiomedical applications\n4.3\tSoftware applications\n4.4\tOnline media applications\n4.5\tMarketing applications\n4.6\tSentiment analysis\n4.7\tAcademic applications\n4.8\tDigital Humanities and Computational Sociology\n5\tSoftware\n6\tIntellectual Property Law and Text Mining\n6.1\tSituation in Europe\n6.2\tSituation in United States\n7\tImplications\n8\tSee also\n9\tNotes\n10\tReferences\n11\tExternal links\nText mining and text analytics\nThe term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation.[1] The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of \"text mining\"[2] in 2004 to describe \"text analytics.\"[3] The latter term is now used more frequently in business settings while \"text mining\" is used in some of the earliest application areas, dating to the 1980s,[4] notably life-sciences research and government intelligence.\n\nThe term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text.[5] These techniques and processes discover and present knowledge � facts, business rules, and relationships � that is otherwise locked in textual form, impenetrable to automated processing.\n\nHistory\nLabor-intensive manual text mining approaches first surfaced in the mid-1980s,[6] but technological advances have enabled the field to advance during the past decade. Text mining is an interdisciplinary field that draws on information retrieval, data mining, machine learning, statistics, and computational linguistics. As most information (common estimates say over 80%)[5] is currently stored as text, text mining is believed to have a high commercial potential value. Increasing interest is being paid to multilingual data mining: the ability to gain information across languages and cluster similar items from different linguistic sources according to their meaning.\n\nThe challenge of exploiting the large proportion of enterprise information that originates in \"unstructured\" form has been recognized for decades.[7] It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:\n\n\"...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points.\"\n\nYet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases. This is not surprising: text in \"unstructured\" documents is hard to process. The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:[8]\n\nFor almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.\n\nHearst's 1999 statement of need fairly well describes the state of text analytics technology and practice a decade later.\n\nText analysis processes\nSubtasks � components of a larger text-analytics effort � typically include:\n\nInformation retrieval or identification of a corpus is a preparatory step: collecting or identifying a set of textual materials, on the Web or held in a file system, database, or content corpus manager, for analysis.\nAlthough some text analytics systems apply exclusively advanced statistical methods, many others apply more extensive natural language processing, such as part of speech tagging, syntactic parsing, and other types of linguistic analysis.[citation needed]\nNamed entity recognition is the use of gazetteers or statistical techniques to identify named text features: people, organizations, place names, stock ticker symbols, certain abbreviations, and so on. Disambiguation � the use of contextual clues � may be required to decide where, for instance, \"Ford\" can refer to a former U.S. president, a vehicle manufacturer, a movie star, a river crossing, or some other entity.\nRecognition of Pattern Identified Entities: Features such as telephone numbers, e-mail addresses, quantities (with units) can be discerned via regular expression or other pattern matches.\nCoreference: identification of noun phrases and other terms that refer to the same object.\nRelationship, fact, and event Extraction: identification of associations among entities and other information in text\nSentiment analysis involves discerning subjective (as opposed to factual) material and extracting various forms of attitudinal information: sentiment, opinion, mood, and emotion. Text analytics techniques are helpful in analyzing, sentiment at the entity, concept, or topic level and in distinguishing opinion holder and opinion object.[9]\nQuantitative text analysis is a set of techniques stemming from the social sciences where either a human judge or a computer extracts semantic or grammatical relationships between words in order to find out the meaning or stylistic patterns of, usually, a casual personal text for the purpose of psychological profiling etc.[10]\nApplications\nThe technology is now broadly applied for a wide variety of government, research, and business needs. Applications can be sorted into a number of categories by analysis type or by business function. Using this approach to classifying solutions, application categories include:\n\nEnterprise Business Intelligence/Data Mining, Competitive Intelligence\nE-Discovery, Records Management\nNational Security/Intelligence\nScientific discovery, especially Life Sciences\nSentiment Analysis Tools, Listening Platforms\nNatural Language/Semantic Toolkit or Service\nPublishing\nAutomated ad placement\nSearch/Information Access\nSocial media monitoring\nSecurity applications\nMany text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes.[11] It is also involved in the study of text encryption/decryption.\n\nBiomedical applications\nMain article: Biomedical text mining\nA range of text mining applications in the biomedical literature has been described.[12]\n\nOne online text mining application in the biomedical literature is PubGene that combines biomedical text mining with network visualization as an Internet service.[13][14] TPX is a concept-assisted search and navigation tool for biomedical literature analyses[15] - it runs on PubMed/PMC and can be configured, on request, to run on local literature repositories too.\n\nGoPubMed is a knowledge-based search engine for biomedical texts.\n\nSoftware applications\nText mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results. Within public sector much effort has been concentrated on creating software for tracking and monitoring terrorist activities.[16]\n\nOnline media applications\nText mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site \"stickiness\" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.\n\nMarketing applications\nText mining is starting to be used in marketing as well, more specifically in analytical customer relationship management.[17] Coussement and Van den Poel (2008)[18][19] apply it to improve predictive analytics models for customer churn (customer attrition).[18]\n\nSentiment analysis\nSentiment analysis may involve analysis of movie reviews for estimating how favorable a review is for a movie.[20] Such an analysis may need a labeled data set or labeling of the affectivity of words. Resources for affectivity of words and concepts have been made for WordNet[21] and ConceptNet,[22] respectively.\n\nText has been used to detect emotions in the related area of affective computing.[23] Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.\n\nAcademic applications\nThe issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within text without removing publisher barriers to public access.\n\nAcademic institutions have also become involved in the text mining initiative:\n\nThe National Centre for Text Mining (NaCTeM), is the first publicly funded text mining centre in the world. NaCTeM is operated by the University of Manchester[24] in close collaboration with the Tsujii Lab,[25] University of Tokyo.[26] NaCTeM provides customised tools, research facilities and offers advice to the academic community. They are funded by the Joint Information Systems Committee (JISC) and two of the UK Research Councils (EPSRC & BBSRC). With an initial focus on text mining in the biological and biomedical sciences, research has since expanded into the areas of social sciences.\nIn the United States, the School of Information at University of California, Berkeley is developing a program called BioText to assist biology researchers in text mining and analysis.\nDigital Humanities and Computational Sociology\nThe automatic analysis of vast textual corpora has created the possibility for scholars to analyse millions of documents in multiple languages with very limited manual intervention. Key enabling technologies have been Parsing, Machine Translation, Topic categorization, Machine Learning.\n\n\nNarrative network of US Elections 2012[27]\nThe automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale, turning textual data into network data. The resulting networks, which can contain thousands of nodes, are then analysed by using tools from Network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes.[28] This automates the approach introduced by Quantitative Narrative Analysis,[29] whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.[27]\n\nContent analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a \"big data\" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. [30] [31] [32] [33] The analysis of readability, gender bias and topic bias was demonstrated in [34] showing how different topics have different gender biases and levels of readability; the possibility to detect mood shifts in a vast population by analysing Twitter content was demonstrated as well.[35]\n\nSoftware\nText mining computer programs are available from many commercial and open source companies and sources. See List of text mining software.\n\nIntellectual Property Law and Text Mining\nSituation in Europe\nDue to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. In the UK in 2014, on the recommendation of the Hargreaves review the government amended copyright law[36] to allow text mining as a limitation and exception. Only the second country in the world to do so after Japan, which introduced a mining specific exception in 2009. However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions.\n\nThe European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[37] The focus on the solution to this legal issue being licences and not limitations and exceptions to copyright law led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[38]\n\nSituation in United States\nBy contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one such use being text and data mining.[39]\n\nImplications\nUntil recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events. For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence. In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment.\n\nSee also\nFull text search\nConcept Mining\nWeb mining, a task that may involve text mining (e.g. first find appropriate web pages by classifying crawled web pages, then extract the desired information from the text content of these pages considered relevant).\nSequential pattern mining: String and Sequence Mining\nNews analytics\nMarket sentiment\nNamed entity recognition\nName resolution (semantics and text extraction)\nRecord linkage\nw-shingling\nList of text mining software", "skillName": "Text_mining."}
{"id": 149, "category": "Machine_Learning", "skillText": "Unsupervised learning\n\nUnsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.\n\nUnsupervised learning is closely related to the problem of density estimation in statistics.[1] However unsupervised learning also encompasses many other techniques that seek to summarize and explain key features of the data.\n\nApproaches to unsupervised learning include:\n\nclustering\nk-means\nmixture models\nhierarchical clustering,[2]\nanomaly detection\nNeural Networks\nHebbian Learning\nApproaches for learning latent variable models such as\nExpectation�maximization algorithm (EM)\nMethod of moments\nBlind signal separation techniques, e.g.,\nPrincipal component analysis,\nIndependent component analysis,\nNon-negative matrix factorization,\nSingular value decomposition.[3]\nContents  [hide]\n1\tUnsupervised Learning in Neural Networks\n2\tMethod of moments\n3\tSee also\n4\tNotes\n5\tFurther reading\nUnsupervised Learning in Neural Networks\nThe classical example of unsupervised learning in the study of both natural and artificial neural networks is subsumed by Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version exists that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n\nAmong neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are also used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing. The first version of ART was \"ART1\", developed by Carpenter and Grossberg (1988).[4]\n\nMethod of moments\nOne of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\n\nIn particular, the method of moments is shown to be effective in learning the parameters of latent variable models.[5] Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.[5]\n\nThe Expectation�maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. Alternatively, for the method of moments, the global convergence is guaranteed under some conditions.[5]\n\nSee also\nCluster analysis\nAnomaly detection\nExpectation�maximization algorithm\nGenerative topographic map\nMultivariate analysis\nRadial basis function network\nHebbian Theory", "skillName": "Unsupervised learning."}
{"id": 150, "category": "Machine_Learning", "skillText": "Data mining\n\nData mining is an interdisciplinary subfield of computer science.[1][2][3] It is the computational process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems.[1] The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.[1] Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[4]\n\nThe term is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[5] It also is a buzzword[6] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java[7] (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons.[8] Often the more general terms (large scale) data analysis and analytics � or, when referring to actual methods, artificial intelligence and machine learning � are more appropriate.\n\nThe actual data mining task is the automatic or semi-automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.\n\nThe related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.\n\n1\tEtymology\n2\tBackground\n3\tProcess\n3.1\tPre-processing\n3.2\tData mining\n3.3\tResults validation\n4\tResearch\n5\tStandards\n6\tNotable uses\n7\tPrivacy concerns and ethics\n7.1\tSituation in Europe\n7.2\tSituation in the United States\n8\tCopyright Law\n8.1\tSituation in Europe\n8.2\tSituation in the United States\n9\tSoftware\n9.1\tFree open-source data mining software and applications\n9.2\tProprietary data-mining software and applications\n9.3\tMarketplace surveys\n10\tSee also\n11\tReferences\n12\tFurther reading\n13\tExternal links\nMachine learning\nEtymology\nIn the 1960s, statisticians used terms like \"Data Fishing\" or \"Data Dredging\" to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"Data Mining\" appeared around 1990 in the database community. For a short time in 1980s, a phrase \"database mining\"�, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation;[9] researchers consequently turned to \"data mining\". Other terms used include Data Archaeology, Information Harvesting, Information Discovery, Knowledge Extraction, etc. Gregory Piatetsky-Shapiro coined the term \"Knowledge Discovery in Databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and Machine Learning Community. However, the term data mining became more popular in the business and press communities.[10] Currently, Data Mining and Knowledge Discovery are used interchangeably. Since about 2007, \"Predictive Analytics\" and since 2011, \"Data Science\" terms were also used to describe this field.\n\nIn the Academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding Editor-in-Chief. Later he started the SIGKDDD Newsletter SIGKDD Explorations.[11] The KDD International conference became the primary highest quality conference in Data Mining with an acceptance rate of research paper submissions below 18%. The Journal Data Mining and Knowledge Discovery is the primary research journal of the field.\n\nBackground\nThe manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology has dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns[12] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever larger data sets.\n\nProcess\nThe Knowledge Discovery in Databases (KDD) process is commonly defined with the stages:\n\n(1) Selection\n(2) Pre-processing\n(3) Transformation\n(4) Data Mining\n(5) Interpretation/Evaluation.[4]\nIt exists, however, in many variations on this theme, such as the Cross Industry Standard Process for Data Mining (CRISP-DM) which defines six phases:\n\n(1) Business Understanding\n(2) Data Understanding\n(3) Data Preparation\n(4) Modeling\n(5) Evaluation\n(6) Deployment\nor a simplified process such as (1) pre-processing, (2) data mining, and (3) results validation.\n\nPolls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[13] The only other data mining standard named in these polls was SEMMA. However, 3�4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[14][15] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[16]\n\nPre-processing\nBefore data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.\n\nData mining\nData mining involves six common classes of tasks:[4]\n\nAnomaly detection (Outlier/change/deviation detection) � The identification of unusual data records, that might be interesting or data errors that require further investigation.\nAssociation rule learning (Dependency modelling) � Searches for relationships between variables. For example, a supermarket might gather data on customer purchasing habits. Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis.\nClustering � is the task of discovering groups and structures in the data that are in some way or another \"similar\", without using known structures in the data.\nClassification � is the task of generalizing known structure to apply to new data. For example, an e-mail program might attempt to classify an e-mail as \"legitimate\" or as \"spam\".\nRegression � attempts to find a function which models the data with the least error.\nSummarization � providing a more compact representation of the data set, including visualization and report generation.\nResults validation\n\nAn example of data produced by data dredging through a bot operated by statistician Tyler Viglen, apparently showing a close link between the best word winning a spelling bee competition and the number of people in the United States killed by venomous spiders. The similarity in trends is obviously a coincidence.\nData mining can unintentionally be misused, and can then produce results which appear to be significant; but which do not actually predict future behaviour and cannot be reproduced on a new sample of data and bear little use. Often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split - when applicable at all - may not be sufficient to prevent this from happening.[citation needed]\n\nWiki letter w.svg\nThis section is missing information about non-classification tasks in data mining. It only covers machine learning. Please expand the section to include this information. Further details may exist on the talk page. (September 2011)\nThe final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily valid. It is common for the data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" emails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used to evaluate the algorithm, such as ROC curves.\n\nIf the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.\n\nResearch\nThe premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[17][18] Since 1989 this ACM SIG has hosted an annual international conference and published its proceedings,[19] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".[20]\n\nComputer science conferences on data mining include:\n\nCIKM Conference � ACM Conference on Information and Knowledge Management\nDMIN Conference � International Conference on Data Mining\nDMKD Conference � Research Issues on Data Mining and Knowledge Discovery\nDSAA Conference � IEEE International Conference on Data Science and Advanced Analytics\nECDM Conference � European Conference on Data Mining\nECML-PKDD Conference � European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases\nEDM Conference � International Conference on Educational Data Mining\nINFOCOM Conference � IEEE INFOCOM\nICDM Conference � IEEE International Conference on Data Mining\nKDD Conference � ACM SIGKDD Conference on Knowledge Discovery and Data Mining\nMLDM Conference � Machine Learning and Data Mining in Pattern Recognition\nPAKDD Conference � The annual Pacific-Asia Conference on Knowledge Discovery and Data Mining\nPAW Conference � Predictive Analytics World\nSDM Conference � SIAM International Conference on Data Mining (SIAM)\nSSTD Symposium � Symposium on Spatial and Temporal Databases\nWSDM Conference � ACM Conference on Web Search and Data Mining\nData mining topics are also present on many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases\n\nStandards\nThere have been some efforts to define standards for the data mining process, for example the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006, but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.\n\nFor exchanging the extracted models � in particular for use in predictive analytics � the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[21]\n\nNotable uses\nMain article: Examples of data mining\nSee also: Category:Applied data mining.\nData mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.\n\nPrivacy concerns and ethics\nWhile the term \"data mining\" itself has no ethical implications, it is often associated with the mining of information in relation to peoples' behavior (ethical and otherwise).[22]\n\nThe ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[23] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[24][25]\n\nData mining requires data preparation which can uncover information or patterns which may compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[26] This is not data mining per se, but a result of the preparation of data before � and for the purposes of � the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[27][28][29]\n\nIt is recommended that an individual is made aware of the following before data are collected:[26]\n\nthe purpose of the data collection and any (known) data mining projects;\nhow the data will be used;\nwho will be able to mine the data and use the data and their derivatives;\nthe status of security surrounding access to the data;\nhow collected data can be updated.\nData may also be modified so as to become anonymous, so that individuals may not readily be identified.[26] However, even \"de-identified\"/\"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[30]\n\nThe inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial, emotional, or bodily harm to the indicated individual. In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling prescription information to data mining companies who in turn provided the data to pharmaceutical companies.[31]\n\nSituation in Europe\nEurope has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.-E.U. Safe Harbor Principles currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's Global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement have failed.[citation needed]\n\nSituation in the United States\nIn the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week', \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is undermined by the complexity of consent forms that are required of patients and participants, which approach a level of incomprehensibility to average individuals.\"[32] This underscores the necessity for data anonymity in data aggregation and mining practices.\n\nU.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. Use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.\n\nCopyright Law\nSituation in Europe\nDue to a lack of flexibilities in European copyright and database law, the mining of in-copyright works such as web mining without the permission of the copyright owner is not legal. Where a database is pure data in Europe there is likely to be no copyright, but database rights may exist so data mining becomes subject to regulations by the Database Directive. On the recommendation of the Hargreaves review this led to the UK government to amend its copyright law in 2014[33] to allow content mining as a limitation and exception. Only the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions. The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[34] The focus on the solution to this legal issue being licences and not limitations and exceptions led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[35]\n\nSituation in the United States\nBy contrast to Europe, the flexible nature of US copyright law, and in particular fair use means that content mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea is viewed as being legal. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed - one being text and data mining.[36]\n\nSoftware\nSee also: Category:Data mining and machine learning software.\nFree open-source data mining software and applications\nThe following applications are available under free/open source licenses. Public access to application sourcecode is also available.\n\nCarrot2: Text and search results clustering framework.\nChemicalize.org: A chemical structure miner and web search engine.\nELKI: A university research project with advanced cluster analysis and outlier detection methods written in the Java language.\nGATE: a natural language processing and language engineering tool.\nKNIME: The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\nMassive Online Analysis (MOA): a real-time big data stream mining with concept drift tool in the Java programming language.\nML-Flex: A software package that enables users to integrate with third-party machine-learning packages written in any programming language, execute classification analyses in parallel across multiple computing nodes, and produce HTML reports of classification results.\nMLPACK library: a collection of ready-to-use machine learning algorithms written in the C++ language.\nNLTK (Natural Language Toolkit): A suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python language.\nOpenNN: Open neural networks library.\nOrange: A component-based data mining and machine learning software suite written in the Python language.\nR: A programming language and software environment for statistical computing, data mining, and graphics. It is part of the GNU Project.\nSCaViS: Java cross-platform data analysis framework developed at Argonne National Laboratory.\nscikit-learn is an open source machine learning library for the Python programming language\nSenticNet API: A semantic and affective resource for opinion mining and sentiment analysis.\nTorch: An open source deep learning library for the Lua programming language and scientific computing framework with wide support for machine learning algorithms.\nUIMA: The UIMA (Unstructured Information Management Architecture) is a component framework for analyzing unstructured content such as text, audio and video � originally developed by IBM.\nWeka: A suite of machine learning software applications written in the Java programming language.\nProprietary data-mining software and applications\nThe following applications are available under proprietary licenses.\n\nAngoss KnowledgeSTUDIO: data mining tool provided by Angoss.\nClarabridge: enterprise class text analytics solution.\nHP Vertica Analytics Platform: data mining software provided by HP.\nIBM SPSS Modeler: data mining software provided by IBM.\nKXEN Modeler: data mining tool provided by KXEN.\nLIONsolver: an integrated software application for data mining, business intelligence, and modeling that implements the Learning and Intelligent OptimizatioN (LION) approach.\nMegaputer Intelligence: data and text mining software is called PolyAnalyst.\nMicrosoft Analysis Services: data mining software provided by Microsoft.\nNetOwl: suite of multilingual text and entity analytics products that enable data mining.\nOpenText� Big Data Analytics: Visual Data Mining & Predictive Analysis by Open Text Corporation\nOracle Data Mining: data mining software by Oracle.\nPSeven: platform for automation of engineering simulation and analysis, multidisciplinary optimization and data mining provided by DATADVANCE.\nQlucore Omics Explorer: data mining software provided by Qlucore.\nRapidMiner: An environment for machine learning and data mining experiments.\nSAS Enterprise Miner: data mining software provided by the SAS Institute.\nSTATISTICA Data Miner: data mining software provided by StatSoft.\nTanagra: A visualisation-oriented data mining software, also for teaching.\nMarketplace surveys\nSeveral researchers and organizations have conducted reviews of data mining tools and surveys of data miners. These identify some of the strengths and weaknesses of the software packages. They also provide an overview of the behaviors, preferences and views of data miners. Some of these reports include:\n\nHurwitz Victory Index: Report for Advanced Analytics as a market research assessment tool, it highlights both the diverse uses for advanced analytics technology and the vendors who make those applications possible.Recent-research\n2011 Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery[37]\nRexer Analytics Data Miner Surveys (2007�2013)[38]\nForrester Research 2010 Predictive Analytics and Data Mining Solutions report[39]\nGartner 2008 \"Magic Quadrant\" report[40]\nRobert A. Nisbet's 2006 Three Part Series of articles \"Data Mining Tools: Which One is Best For CRM?\"[41]\nHaughton et al.'s 2003 Review of Data Mining Software Packages in The American Statistician[42]\nGoebel & Gruenwald 1999 \"A Survey of Data Mining a Knowledge Discovery Software Tools\" in SIGKDD Explorations[43]\nSee also\nMethods\nAnomaly/outlier/change detection\nAssociation rule learning\nClassification\nCluster analysis\nDecision tree\nFactor analysis\nGenetic algorithms\nIntention mining\nMultilinear subspace learning\nNeural networks\nRegression analysis\nSequence mining\nStructured data analysis\nSupport vector machines\nText mining\nAgent mining\nApplication domains\nAnalytics\nBehavior informatics\nBig Data\nBioinformatics\nBusiness intelligence\nData analysis\nData warehouse\nDecision support system\nDomain driven data mining\nDrug discovery\nExploratory data analysis\nPredictive analytics\nWeb mining\nApplication examples\nSee also: Category:Applied data mining.\nCustomer analytics\nData mining in agriculture\nData mining in meteorology\nEducational data mining\nNational Security Agency\nPolice-enforced ANPR in the UK\nQuantitative structure�activity relationship\nSurveillance / Mass surveillance (e.g., Stellar Wind)\nRelated topics\nData mining is about analyzing data; for information about extracting information out of data, see:\n\nData integration\nData transformation\nElectronic discovery\nInformation extraction\nInformation integration\nNamed-entity recognition\nProfiling (information science)\nWeb scraping", "skillName": "Data mining."}
{"id": 151, "category": "Machine_Learning", "skillText": "Statistical model\n statistical model is a class of mathematical model, which embodies a set of assumptions concerning the generation of some sample data, and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.\n\nThe assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.\n\nA statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, \"a model is a formal representation of a theory\" (Herman Ad�r quoting Kenneth Bollen).[1]\n\nAll statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.\n\n\n1\tFormal definition\n2\tAn example\n3\tGeneral remarks\n4\tDimension of a model\n5\tNested models\n6\tComparing models\n7\tSee also\n8\tNotes\n9\tReferences\n10\tFurther reading\nFormal definition\nIn mathematical terms, a statistical model is usually thought of as a pair ( {\\displaystyle S,{\\mathcal {P}}} S,{\\mathcal  {P}}), where {\\displaystyle S} S is the set of possible observations, i.e. the sample space, and {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} is a set of probability distributions on {\\displaystyle S} S.[2]\n\nThe intuition behind this definition is as follows. It is assumed that there is a \"true\" probability distribution that generates the observed data. We choose {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} to represent a set (of distributions) which contains a distribution that adequately approximates the true distribution. Note that we do not require that {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} contains the true distribution, and in practice that is rarely the case. Indeed, as Burnham & Anderson state, \"A model is a simplification or approximation of reality and hence will not reflect all of reality\"[3]�whence the saying \"all models are wrong\".\n\nThe set {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} is almost always parameterized: {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}} {\\mathcal  {P}}=\\{P_{{\\theta }}:\\theta \\in \\Theta \\}. The set {\\displaystyle \\Theta } \\Theta  defines the parameters of the model. A parameterization is generally required to have distinct parameter values give rise to distinct distributions, i.e. {\\displaystyle P_{\\theta _{1}}=P_{\\theta _{2}}\\Rightarrow \\theta _{1}=\\theta _{2}} P_{{\\theta _{1}}}=P_{{\\theta _{2}}}\\Rightarrow \\theta _{1}=\\theta _{2} must hold (in other words, it must be injective). A parameterization that meets the condition is said to be identifiable.[2]\n\nAn example\nHeight and age are each probabilistically distributed over humans. They are stochastically related: when we know that a person is of age 10, this influences the chance of the person being 6 feet tall. We could formalize that relationship in a linear regression model with the following form: heighti = b0 + b1agei + ei, where b0 is the intercept, b1 is a parameter that age is multiplied by to get a prediction of height, e is the error term, and i identifies the person. This implies that height is predicted by age, with some error.\n\nAn admissible model must be consistent with all the data points. Thus, the straight line (heighti = b0 + b1agei) is not a model of the data. The line cannot be a model, unless it exactly fits all the data points�i.e. all the data points lie perfectly on a straight line. The error term, ei, must be included in the model, so that the model is consistent with all the data points.\n\nTo do statistical inference, we would first need to assume some probability distributions for the ei. For instance, we might assume that the ei distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.\n\nWe can formally specify the model in the form ( {\\displaystyle S,{\\mathcal {P}}} S,{\\mathcal  {P}}) as follows. The sample space, {\\displaystyle S} S, of our model comprises the set of all possible pairs (age, height). Each possible value of {\\displaystyle \\theta } \\theta  = (b0, b1, s2) determines a distribution on {\\displaystyle S} S; denote that distribution by {\\displaystyle P_{\\theta }} P_{{\\theta }}. If {\\displaystyle \\Theta } \\Theta  is the set of all possible values of {\\displaystyle \\theta } \\theta , then {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}} {\\mathcal  {P}}=\\{P_{{\\theta }}:\\theta \\in \\Theta \\}. (The parameterization is identifiable, and this is easy to check.)\n\nIn this example, the model is determined by (1) specifying {\\displaystyle S} S and (2) making some assumptions relevant to {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}}. There are two assumptions: that height can be approximated by a linear function of age; that errors in the approximation are distributed as i.i.d. Gaussian. The assumptions are sufficient to specify {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}}�as they are required to do.\n\nGeneral remarks\nA statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, e is a stochastic variable; without that variable, the model would be deterministic.\n\nStatistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).\n\nThere are three purposes for a statistical model, according to Konishi & Kitagawa.[4]\n\nPredictions\nExtraction of information\nDescription of stochastic structures\nDimension of a model\nSuppose that we have a statistical model ( {\\displaystyle S,{\\mathcal {P}}} S,{\\mathcal  {P}}) with {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}} {\\mathcal  {P}}=\\{P_{{\\theta }}:\\theta \\in \\Theta \\}. The model is said to be parametric if {\\displaystyle \\Theta } \\Theta  has a finite dimension. In notation, we write that {\\displaystyle \\Theta \\subseteq \\mathbb {R} ^{d}} \\Theta \\subseteq {\\mathbb  {R}}^{d} where d is a positive integer ( {\\displaystyle \\mathbb {R} } \\mathbb {R} denotes the real numbers; other sets can be used, in principle). Here, d is called the dimension of the model.\n\nAs an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that\n\n{\\displaystyle {\\mathcal {P}}=\\{P_{\\mu ,\\sigma }(x)\\equiv {\\frac {1}{{\\sqrt {2\\pi }}\\sigma }}\\exp \\left(-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}\\right):\\mu \\in \\mathbb {R} ,\\sigma >0\\}} {\\mathcal  {P}}=\\{P_{{\\mu ,\\sigma }}(x)\\equiv {\\frac  {1}{{\\sqrt  {2\\pi }}\\sigma }}\\exp \\left(-{\\frac  {(x-\\mu )^{2}}{2\\sigma ^{2}}}\\right):\\mu \\in {\\mathbb  {R}},\\sigma >0\\}.\nIn this example, the dimension, d, equals 2.\n\nAs another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)\n\nA statistical model is nonparametric if the parameter set {\\displaystyle \\Theta } \\Theta  is infinite dimensional. A statistical model is semiparametric if it has both finite-dimensional and infinite-dimensional parameters. Formally, if d is the dimension of {\\displaystyle \\Theta } \\Theta  and n is the number of samples, both semiparametric and nonparemtric models have {\\displaystyle d\\rightarrow \\infty } d \\rightarrow \\infty as {\\displaystyle n\\rightarrow \\infty } n\\rightarrow \\infty . If {\\displaystyle d/n\\rightarrow 0} d/n\\rightarrow 0 as {\\displaystyle n\\rightarrow \\infty } n\\rightarrow \\infty , then the model is semiparametric; otherwise, the model is nonparametric.\n\nParametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, \"These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies\".[5]\n\nNested models\nTwo statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. For example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions.\n\nIn that example, the first model has a higher dimension than the second model (the zero-mean model has dimension 1). Such is usually, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.\n\nComparing models\nMain article: Model selection\nIt is assumed that there is a \"true\" probability distribution that generates the observed data. The main goal of model selection is to make statements about which elements of {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} are most likely to adequately approximate the true distribution.\n\nModels can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include R2, Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.\n\nKonishi & Kitagawa state: \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models.\"[6] Relatedly, Sir David Cox has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\".[7]\n\nDeterministic system\nEconometric model\nGraphical model\nIdentifiability\nRegression analysis\nScientific modelling\nStatistical inference\nStatistical theory\nStochastic process\nSystem identification", "skillName": "Statistical method."}
{"id": 152, "category": "Machine_Learning", "skillText": "Classification\nClassification is a general process related to categorization, the process in which ideas and objects are recognized, differentiated, and understood.\n\nA classification system is an approach to accomplishing classification.\n\nClassification may refer specifically to:\nMachine learning\nContents  \n1\tMathematics\n2\tMedia\n3\tScience\n4\tBusiness, organizations, and economics\n5\tOther uses\n6\tOrganizations involved in classification\n7\tSee also\n8\tReferences\nMathematics\nStatistical classification, identifying to which of a set of categories a new observation belongs, on the basis of a training set of data\nMathematical classification, a collection of sets which can be unambiguously defined by a property that all its members share\nClassification theorems in mathematics\nAttribute-value system, a basic knowledge representation framework\nMedia\nDocument classification, a problem in library science, information science and computer science\nLibrary classification, system of coding, assorting and organizing library materials according to their subject\nClassified information, sensitive information to which access is restricted by law or regulation to particular classes of people\nMotion picture rating system, for film classification\nClassification (literature), a figure of speech linking a proper noun to a common noun using the or other articles\nScience\nScientific classification (disambiguation)\nChemical classification\nTaxonomic classification, also known as classification of species\nCladistics, an approach using similarities\nBiological classification of organisms\nMedical classification, the process of transforming descriptions of medical diagnoses and procedures into universal medical code numbers\nBusiness, organizations, and economics\nClassification of customers, for marketing (as in Master data management) or for profitability (e.g. by Activity-Based Costing)\nStandard Industrial Classification, economic activities\nJob classification, as in job analysis\nOther uses\nCivil service classification, personnel grades in government\nClassification society, a non-governmental organization that establishes and maintains technical standards for the construction and operation of ships and offshore structures\nProduct classification\nLocomotive classification\nClassification of wine\nAn industrial process such as mechanical screening for sorting materials by size, shape, and density, etc\nClassification of swords\nOrganizations involved in classification\nInternational Society for Knowledge Organization\n\nClass (disambiguation)\nData classification (disambiguation)\nClassified (disambiguation)\nClassifier (disambiguation)\nTaxonomy (disambiguation)", "skillName": "Classification."}
{"id": 153, "category": "Machine_Learning", "skillText": "Predictive analytics\nPredictive analytics encompasses a variety of statistical techniques from predictive modeling, machine learning, and data mining that analyze current and historical facts to make predictions about future or otherwise unknown events.[1][2]\n\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions.[3]\n\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\n\nPredictive analytics is used in actuarial science,[4] marketing,[5] financial services,[6] insurance, telecommunications,[7] retail,[8] travel,[9] healthcare,[10] child protection,[11][12] pharmaceuticals,[13] capacity planning[citation needed] and other fields.\n\nOne of the most well known applications is credit scoring,[1] which is used throughout financial services. Scoring models process a customer's credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.\n\n1\tDefinition\n2\tTypes\n2.1\tPredictive models\n2.2\tDescriptive models\n2.3\tDecision models\n3\tApplications\n3.1\tAnalytical customer relationship management (CRM)\n3.2\tChild protection\n3.3\tClinical decision support systems\n3.4\tCollection analytics\n3.5\tCross-sell\n3.6\tCustomer retention\n3.7\tDirect marketing\n3.8\tFraud detection\n3.9\tPortfolio, product or economy-level prediction\n3.10\tRisk management\n3.11\tUnderwriting\n4\tTechnology and big data influences\n5\tAnalytical Techniques\n5.1\tRegression techniques\n5.1.1\tLinear regression model\n5.1.2\tDiscrete choice models\n5.1.3\tLogistic regression\n5.1.4\tMultinomial logistic regression\n5.1.5\tProbit regression\n5.1.6\tLogit versus probit\n5.1.7\tTime series models\n5.1.8\tSurvival or duration analysis\n5.1.9\tClassification and regression trees (CART)\n5.1.10\tMultivariate adaptive regression splines\n5.2\tMachine learning techniques\n5.2.1\tNeural networks\n5.2.2\tMultilayer Perceptron (MLP)\n5.2.3\tRadial basis functions\n5.2.4\tSupport vector machines\n5.2.5\tNa�ve Bayes\n5.2.6\tk-nearest neighbours\n5.2.7\tGeospatial predictive modeling\n6\tTools\n6.1\tPMML\n7\tCriticism\n8\tSee also\n9\tReferences\n10\tFurther reading\nDefinition\nPredictive analytics is an area of data mining that deals with extracting information from data and using it to predict trends and behavior patterns. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs.[14] The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.\n\nPredictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analytics�Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\"[15] In the future industrial systems the value of Predictive Analytics is to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization. Furthermore, the converted data can be used for closed-loop product life cycle improvement[16] which is the vision of Industrial Internet Consortium.\n\nTypes\nGenerally, the term predictive analytics is used to mean predictive modeling, \"scoring\" data with predictive models, and forecasting. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.\n\nPredictive models\nPredictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, or fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.\n\nThe available sample units with known attributes and known performances is referred to as the �training sample.� The units in other samples, with known attributes but unknown performances, are referred to as �out of [training] sample� units. The out of sample bear no chronological relation to the training sample units. For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.\n\nDescriptive models\nDescriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.\n\nDecision models\nDecision models describe the relationship between all the elements of a decision � the known data (including results of predictive models), the decision, and the forecast results of the decision � in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.\n\nApplications\nAlthough predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.\n\nAnalytical customer relationship management (CRM)\nAnalytical Customer Relationship Management is a frequent commercial application of Predictive Analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base. They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones. Analytical Customer Relationship Management can be applied throughout the customers lifecycle (acquisition, relationship growth, retention, and win-back). Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements.\n\nChild protection\nOver the last 5 years, some Child Welfare Agencies have started using predictive analytics to flag high risk cases.[17] The approach has been called \"innovative\" by the Commission to Eliminate Child Abuse and Neglect Fatalities (CECANF),[18] and in Hillsborough County, FL, where the Lead Child Welfare Agency uses a predictive modeling tool called Eckerd Rapid Safety Feedback�, there have been no abuse-related child deaths in the target population as of this writing.[19]\n\nClinical decision support systems\nExperts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: \"Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care.\"[citation needed]\n\nCollection analytics\nMany portfolios have a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.\n\nCross-sell\nOften corporate organizations collect and maintain abundant data (e.g. customer records, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers.[2] This directly leads to higher profitability per customer and stronger customer relationships.\n\nCustomer retention\nWith the number of competing services available, businesses need to focus efforts on maintaining continuous consumer satisfaction, rewarding consumer loyalty and minimizing customer attrition. In addition, small increases in customer retention have been shown to increase profits disproportionately. One study concluded that a 5% increase in customer retention rates will increase profits by 25% to 95%.[20] Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer's decision is almost zero. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer�s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon.[7] An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.\n\nDirect marketing\nWhen marketing consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The goal of predictive analytics is typically to lower the cost per order or cost per action.\n\nFraud detection\nFraud is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent transactions (both offline and online), identity thefts and false insurance claims. These problems plague firms of all sizes in many industries. Some examples of likely victims are credit card issuers, insurance companies,[21] retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the \"bads\" and reduce a business's exposure to fraud.\n\nPredictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. Mark Nigrini developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.[22]\n\nThe Internal Revenue Service (IRS) of the United States also uses predictive analytics to mine tax returns and identify tax fraud.[21]\n\nRecent[when?] advancements in technology have also introduced predictive behavior analysis for web fraud detection. This type of solution utilizes heuristics in order to study normal web user behavior and detect anomalies indicating fraud attempts.\n\nPortfolio, product or economy-level prediction\nOften the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.[23][24]\n\nRisk management\nWhen employing risk management techniques, the results are always to predict and benefit from a future scenario. The Capital asset pricing model (CAP-M) \"predicts\" the best portfolio to maximize return, Probabilistic Risk Assessment (PRA)--when combined with mini-Delphi Techniques and statistical approaches yields accurate forecasts and RiskAoA is a stand-alone predictive tool.[25] These are three examples of approaches that can extend from project to market, and from near to long term. Underwriting (see below) and other business approaches identify risk management as a predictive method.\n\nUnderwriting\nMany businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.[4] Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.\n\nTechnology and big data influences\nBig data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID, sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Big Data is the core of most predictive analytic services offered by IT organizations.[26] Thanks to technological advances in computer hardware � faster CPUs, cheaper memory, and MPP architectures � and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights.[21] Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed [27] [28]\n\nAnalytical Techniques\nThe approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.\n\nRegression techniques\nRegression models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there are a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below.\n\nLinear regression model\nThe linear regression model analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.\n\nThe goal of regression is to select the parameters of the model so as to minimize the sum of the squared residuals. This is referred to as ordinary least squares (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the Gauss-Markov assumptions are satisfied.\n\nOnce the model has been estimated we would be interested to know if the predictor variables belong in the model � i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model�s coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R� statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is \"explained\" (accounted for) by variation in the independent variables.\n\nDiscrete choice models\nMultivariate regression (above) is generally used when the response variable is continuous and has an unbounded range. Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are logistic regression, multinomial logit and probit models. Logistic regression and probit models are used when the dependent variable is binary.\n\nLogistic regression\nFor more details on this topic, see logistic regression.\nIn a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (See Allison's Logistic Regression for more information on the theory of Logistic Regression).\n\nThe Wald and likelihood-ratio test are used to test the statistical significance of each coefficient b in the model (analogous to the t tests used in OLS regression; see above). A test assessing the goodness-of-fit of a classification model is the \"percentage correctly predicted\".\n\nMultinomial logistic regression\nAn extension of the binary logit model to cases where the dependent variable has more than 2 categories is the multinomial logit model. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as Random multinomial logit.\n\nProbit regression\nProbit models offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.\n\nA good way to understand the key difference between probit and logit models is to assume that the dependent variable is driven by a latent variable z, which is a sum of a linear combination of explanatory variables and a random noise term.\n\nWe do not observe z but instead observe y which takes the value 0 (when z < 0) or 1 (otherwise). In the logit model we assume that the random noise term follows a logistic distribution with mean zero. In the probit model we assume that it follows a normal distribution with mean zero. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.\n\nLogit versus probit\nThe Probit model has been around longer than the logit model. They behave similarly, except that the logistic distribution tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model.\n\nPractical reasons for choosing the probit model over the logistic model would be:\n\nThere is a strong belief that the underlying distribution is normal\nThe actual event is not a binary outcome (e.g., bankruptcy status) but a proportion (e.g., proportion of population at different debt levels).\nTime series models\nTime series models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result, standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.\n\nTime series models estimate difference equations containing stochastic components. Two commonly used forms of these models are autoregressive models (AR) and moving-average (MA) models. The Box�Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model which is the cornerstone of stationary time series analysis. ARIMA (autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series. Box and Jenkins suggest differencing a non stationary time series to obtain a stationary series to which an ARMA model can be applied. Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.\n\nBox and Jenkins proposed a three-stage methodology which includes: model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions. In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.\n\nIn recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH (autoregressive conditional heteroskedasticity) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.\n\nSurvival or duration analysis\nSurvival analysis is another name for time to event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).\n\nCensoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple linear regression. The normal distribution, being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.\n\nThe assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.\n\nAn important concept in survival analysis is the hazard rate, defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.\n\nMost models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function. A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. All these distributions are for a non-negative random variable.\n\nDuration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are Kaplan-Meier and Cox proportional hazard model (non parametric).\n\nClassification and regression trees (CART)\nMain article: decision tree learning\nGlobally-optimal classification tree analysis (GO-CTA) (also called hierarchical optimal discriminant analysis) is a generalization of optimal discriminant analysis that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.\n\nClassification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.\n\nDecision trees are formed by a collection of rules based on variables in the modeling data set:\n\nRules based on variables' values are selected to get the best split to differentiate observations based on the dependent variable\nOnce a rule is selected and splits a node into two, the same process is applied to each \"child\" node (i.e. it is a recursive procedure)\nSplitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.)\nEach branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.\n\nA very popular method for predictive analytics is Leo Breiman's Random forests.\n\nMultivariate adaptive regression splines\nMultivariate adaptive regression splines (MARS) is a non-parametric technique that builds flexible models by fitting piecewise linear regressions.\n\nAn important concept associated with regression splines is that of a knot. Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.\n\nIn multivariate and adaptive regression splines, basis functions are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables. Multivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.\n\nMultivariate and adaptive regression spline approach deliberately overfits the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.\n\nMachine learning techniques\nMachine learning, a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including medical diagnostics, credit card fraud detection, face and speech recognition and analysis of the stock market. In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human cognition and learn from training examples to predict future events.\n\nA brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell (1997).\n\nNeural networks\nNeural networks are nonlinear sophisticated modeling techniques that are able to model complex functions. They can be applied to problems of prediction, classification or control in a wide spectrum of fields such as finance, cognitive psychology/neuroscience, medicine, engineering, and physics.\n\nNeural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training in neural networks used by different networks, supervised and unsupervised training, reinforcement learning, with supervised being the most common one.\n\nSome examples of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer perceptrons, Kohonen networks, Hopfield networks, etc.\n\nMultilayer Perceptron (MLP)\nThe Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of nonlinearly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The backpropagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule).\n\nRadial basis functions\nA radial basis function (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of neural networks where they are used as a replacement for the sigmoidal transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the feed-forward networks such as the multilayer perceptron.\n\nSupport vector machines\nSupport Vector Machines (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. They are learning machines that are used to perform binary classifications and regression estimations. They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc.\n\nNa�ve Bayes\nNa�ve Bayes based on Bayes conditional probability rule is used for performing classification tasks. Na�ve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the problem of �curse of dimensionality� i.e. when the number of predictors is very high.\n\nk-nearest neighbours\nThe nearest neighbour algorithm (KNN) belongs to the class of pattern recognition statistical methods. The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values. A new sample is classified by calculating the distance to the nearest neighbouring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours; (2) the decision rule used to derive a classification from the k-nearest neighbours; and (3) the number of neighbours used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e.: as the size of the training set increases, if the observations are independent and identically distributed (i.i.d.), regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error. See Devroy et al.\n\nGeospatial predictive modeling\nConceptually, geospatial predictive modeling is rooted in the principle that the occurrences of events being modeled are limited in distribution. Occurrences of events are neither uniform nor random in distribution � there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence.\n\nTools\nHistorically, using predictive analytics tools�as well as understanding the results they delivered�required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists[citation needed]. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model.[29] Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems[citation needed], so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results.[2] For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.[30]\n\nThere are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.\n\nNotable open source predictive analytic tools include:\n\nApache Mahout\nGNU Octave\nKNIME\nOpenNN\nOrange\nR\nRiskAoA\nscikit-learn\nWeka\nNotable commercial predictive analytic tools include:\n\nAlpine Data Labs\nAngoss KnowledgeSTUDIO\nBIRT Analytics\nIBM SPSS Statistics and IBM SPSS Modeler\nKXEN Modeler\nMathematica\nMATLAB\nMinitab\nLabVIEW[31]\nNeural Designer\nOracle Advanced Analytics\nPervasive\nPredixion Software\nRapidMiner\nRCASE\nRevolution Analytics\nSAP\nSAS and SAS Enterprise Miner\nSTATA\nStatgraphics\nSTATISTICA\nTeleRetail\nTIBCO\nBeside these software packages, specific tools have also been developed for industrial applications. For example, Watchdog Agent Toolbox has been developed and optimized for predictive analysis in prognostics and health management applications and is available for MATLAB and LABVIEW[32][33]\n\nThe most popular commercial predictive analytics software packages according to the Rexer Analytics Survey for 2013 are IBM SPSS Modeler, SAS Enterprise Miner, and Dell Statistica <http://www.rexeranalytics.com/Data-Miner-Survey-2013-Intro.html>.\n\nPMML\nIn an attempt to provide a standard language for expressing predictive models, the Predictive Model Markup Language (PMML) has been proposed. Such an XML-based language provides a way for the different tools to define predictive models and to share these between PMML compliant applications. PMML 4.0 was released in June, 2009.\n\nCriticism\nThere are plenty of skeptics when it comes to computers and algorithms abilities to predict the future, including Gary King, a professor from Harvard University and the director of the Institute for Quantitative Social Science. [34] People are influenced by their environment in innumerable ways. Trying to understand what people will do next assumes that all the influential variables can be known and measured accurately. \"People's environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn't as useful as it seemed before.\" [35]\n\nSee also\nCriminal Reduction Utilising Statistical History\nData mining\nLearning analytics\nOdds algorithm\nPattern recognition\nPrescriptive analytics\nPredictive modeling\nRiskAoA a predictive tool for discriminating future decisions.", "skillName": "Predictive_Analytics."}
{"id": 154, "category": "Machine_Learning", "skillText": "Statistical model\n statistical model is a class of mathematical model, which embodies a set of assumptions concerning the generation of some sample data, and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.\n\nThe assumptions embodied by a statistical model describe a set of probability distributions, some of which are assumed to adequately approximate the distribution from which a particular data set is sampled. The probability distributions inherent in statistical models are what distinguishes statistical models from other, non-statistical, mathematical models.\n\nA statistical model is usually specified by mathematical equations that relate one or more random variables and possibly other non-random variables. As such, \"a model is a formal representation of a theory\" (Herman Ad�r quoting Kenneth Bollen).[1]\n\nAll statistical hypothesis tests and all statistical estimators are derived from statistical models. More generally, statistical models are part of the foundation of statistical inference.\n\n\n1\tFormal definition\n2\tAn example\n3\tGeneral remarks\n4\tDimension of a model\n5\tNested models\n6\tComparing models\n7\tSee also\n8\tNotes\n9\tReferences\n10\tFurther reading\nFormal definition\nIn mathematical terms, a statistical model is usually thought of as a pair ( {\\displaystyle S,{\\mathcal {P}}} S,{\\mathcal  {P}}), where {\\displaystyle S} S is the set of possible observations, i.e. the sample space, and {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} is a set of probability distributions on {\\displaystyle S} S.[2]\n\nThe intuition behind this definition is as follows. It is assumed that there is a \"true\" probability distribution that generates the observed data. We choose {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} to represent a set (of distributions) which contains a distribution that adequately approximates the true distribution. Note that we do not require that {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} contains the true distribution, and in practice that is rarely the case. Indeed, as Burnham & Anderson state, \"A model is a simplification or approximation of reality and hence will not reflect all of reality\"[3]�whence the saying \"all models are wrong\".\n\nThe set {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} is almost always parameterized: {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}} {\\mathcal  {P}}=\\{P_{{\\theta }}:\\theta \\in \\Theta \\}. The set {\\displaystyle \\Theta } \\Theta  defines the parameters of the model. A parameterization is generally required to have distinct parameter values give rise to distinct distributions, i.e. {\\displaystyle P_{\\theta _{1}}=P_{\\theta _{2}}\\Rightarrow \\theta _{1}=\\theta _{2}} P_{{\\theta _{1}}}=P_{{\\theta _{2}}}\\Rightarrow \\theta _{1}=\\theta _{2} must hold (in other words, it must be injective). A parameterization that meets the condition is said to be identifiable.[2]\n\nAn example\nHeight and age are each probabilistically distributed over humans. They are stochastically related: when we know that a person is of age 10, this influences the chance of the person being 6 feet tall. We could formalize that relationship in a linear regression model with the following form: heighti = b0 + b1agei + ei, where b0 is the intercept, b1 is a parameter that age is multiplied by to get a prediction of height, e is the error term, and i identifies the person. This implies that height is predicted by age, with some error.\n\nAn admissible model must be consistent with all the data points. Thus, the straight line (heighti = b0 + b1agei) is not a model of the data. The line cannot be a model, unless it exactly fits all the data points�i.e. all the data points lie perfectly on a straight line. The error term, ei, must be included in the model, so that the model is consistent with all the data points.\n\nTo do statistical inference, we would first need to assume some probability distributions for the ei. For instance, we might assume that the ei distributions are i.i.d. Gaussian, with zero mean. In this instance, the model would have 3 parameters: b0, b1, and the variance of the Gaussian distribution.\n\nWe can formally specify the model in the form ( {\\displaystyle S,{\\mathcal {P}}} S,{\\mathcal  {P}}) as follows. The sample space, {\\displaystyle S} S, of our model comprises the set of all possible pairs (age, height). Each possible value of {\\displaystyle \\theta } \\theta  = (b0, b1, s2) determines a distribution on {\\displaystyle S} S; denote that distribution by {\\displaystyle P_{\\theta }} P_{{\\theta }}. If {\\displaystyle \\Theta } \\Theta  is the set of all possible values of {\\displaystyle \\theta } \\theta , then {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}} {\\mathcal  {P}}=\\{P_{{\\theta }}:\\theta \\in \\Theta \\}. (The parameterization is identifiable, and this is easy to check.)\n\nIn this example, the model is determined by (1) specifying {\\displaystyle S} S and (2) making some assumptions relevant to {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}}. There are two assumptions: that height can be approximated by a linear function of age; that errors in the approximation are distributed as i.i.d. Gaussian. The assumptions are sufficient to specify {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}}�as they are required to do.\n\nGeneral remarks\nA statistical model is a special class of mathematical model. What distinguishes a statistical model from other mathematical models is that a statistical model is non-deterministic. Thus, in a statistical model specified via mathematical equations, some of the variables do not have specific values, but instead have probability distributions; i.e. some of the variables are stochastic. In the example above, e is a stochastic variable; without that variable, the model would be deterministic.\n\nStatistical models are often used even when the physical process being modeled is deterministic. For instance, coin tossing is, in principle, a deterministic process; yet it is commonly modeled as stochastic (via a Bernoulli process).\n\nThere are three purposes for a statistical model, according to Konishi & Kitagawa.[4]\n\nPredictions\nExtraction of information\nDescription of stochastic structures\nDimension of a model\nSuppose that we have a statistical model ( {\\displaystyle S,{\\mathcal {P}}} S,{\\mathcal  {P}}) with {\\displaystyle {\\mathcal {P}}=\\{P_{\\theta }:\\theta \\in \\Theta \\}} {\\mathcal  {P}}=\\{P_{{\\theta }}:\\theta \\in \\Theta \\}. The model is said to be parametric if {\\displaystyle \\Theta } \\Theta  has a finite dimension. In notation, we write that {\\displaystyle \\Theta \\subseteq \\mathbb {R} ^{d}} \\Theta \\subseteq {\\mathbb  {R}}^{d} where d is a positive integer ( {\\displaystyle \\mathbb {R} } \\mathbb {R} denotes the real numbers; other sets can be used, in principle). Here, d is called the dimension of the model.\n\nAs an example, if we assume that data arise from a univariate Gaussian distribution, then we are assuming that\n\n{\\displaystyle {\\mathcal {P}}=\\{P_{\\mu ,\\sigma }(x)\\equiv {\\frac {1}{{\\sqrt {2\\pi }}\\sigma }}\\exp \\left(-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}\\right):\\mu \\in \\mathbb {R} ,\\sigma >0\\}} {\\mathcal  {P}}=\\{P_{{\\mu ,\\sigma }}(x)\\equiv {\\frac  {1}{{\\sqrt  {2\\pi }}\\sigma }}\\exp \\left(-{\\frac  {(x-\\mu )^{2}}{2\\sigma ^{2}}}\\right):\\mu \\in {\\mathbb  {R}},\\sigma >0\\}.\nIn this example, the dimension, d, equals 2.\n\nAs another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean). Then the dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note that in geometry, a straight line has dimension 1.)\n\nA statistical model is nonparametric if the parameter set {\\displaystyle \\Theta } \\Theta  is infinite dimensional. A statistical model is semiparametric if it has both finite-dimensional and infinite-dimensional parameters. Formally, if d is the dimension of {\\displaystyle \\Theta } \\Theta  and n is the number of samples, both semiparametric and nonparemtric models have {\\displaystyle d\\rightarrow \\infty } d \\rightarrow \\infty as {\\displaystyle n\\rightarrow \\infty } n\\rightarrow \\infty . If {\\displaystyle d/n\\rightarrow 0} d/n\\rightarrow 0 as {\\displaystyle n\\rightarrow \\infty } n\\rightarrow \\infty , then the model is semiparametric; otherwise, the model is nonparametric.\n\nParametric models are by far the most commonly used statistical models. Regarding semiparametric and nonparametric models, Sir David Cox has said, \"These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies\".[5]\n\nNested models\nTwo statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. For example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions.\n\nIn that example, the first model has a higher dimension than the second model (the zero-mean model has dimension 1). Such is usually, but not always, the case. As a different example, the set of positive-mean Gaussian distributions, which has dimension 2, is nested within the set of all Gaussian distributions.\n\nComparing models\nMain article: Model selection\nIt is assumed that there is a \"true\" probability distribution that generates the observed data. The main goal of model selection is to make statements about which elements of {\\displaystyle {\\mathcal {P}}} {\\mathcal {P}} are most likely to adequately approximate the true distribution.\n\nModels can be compared to each other by exploratory data analysis or confirmatory data analysis. In exploratory analysis, a variety of models are formulated and an assessment is performed of how well each one describes the data. In confirmatory analysis, a previously formulated model or models are compared to the data. Common criteria for comparing models include R2, Bayes factor, and the likelihood-ratio test together with its generalization relative likelihood.\n\nKonishi & Kitagawa state: \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models.\"[6] Relatedly, Sir David Cox has said, \"How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis\".[7]\n\nDeterministic system\nEconometric model\nGraphical model\nIdentifiability\nRegression analysis\nScientific modelling\nStatistical inference\nStatistical theory\nStochastic process\nSystem identification", "skillName": "Statistical_method."}
{"id": 155, "category": "Databases", "skillText": "HBase is an open source, non-relational, distributed database modeled after Google's BigTable and is written in Java. It is developed as part of Apache Software Foundation's Apache Hadoop project and runs on top of HDFS (Hadoop Distributed Filesystem), providing BigTable-like capabilities for Hadoop. That is, it provides a fault-tolerant way of storing large quantities of sparse data (small amounts of information caught within a large collection of empty or unimportant data, such as finding the 50 largest items in a group of 2 billion records, or finding the non-zero items representing less than 0.1% of a huge collection).\n\nHBase features compression, in-memory operation, and Bloom filters on a per-column basis as outlined in the original BigTable paper.[1] Tables in HBase can serve as the input and output for MapReduce jobs run in Hadoop, and may be accessed through the Java API \nbut also through REST, Avro or Thrift gateway APIs. HBase is a column-oriented key-value data store and has idolized widely because of its lineage with Hadoop and HDFS. HBase runs on top of HDFS and is well-suited for faster read and write operations on large datasets with high throughput and low input/output latency.[2]\n\nHBase is not a direct replacement for a classic SQL database, however Apache Phoenix project provides an SQL layer for HBase as well as JDBC driver that can be integrated with various analytics and business intelligence applications. The Apache Trafodion project provides a SQL query engine with ODBC and JDBC drivers and distributed ACID transaction protection across multiple statements, tables and rows that uses HBase as a storage engine.\n\nHBase is now serving several data-driven websites,[3] including Facebook's Messaging Platform.[4][5] Unlike relational and traditional databases, HBase does not support SQL scripting; instead the equivalent is written in Java, employing similarity with a MapReduce application.[2]\n\nIn the parlance of Eric Brewer’s CAP Theorem, HBase is a CP type system.\n\nContents\n\n    1 History\n    2 Use cases & production deployments\n        2.1 Enterprises that use HBase\n    3 See also\n    4 References\n    5 Bibliography\n    6 External links\n\nHistory\n\nApache HBase began as a project by the company Powerset out of a need to process massive amounts of data for the purposes of natural language search. It is now a top-level Apache project.\n\nFacebook elected to implement its new messaging platform using HBase in November 2010.[4]\nUse cases & production deployments\nEnterprises that use HBase\n\nThe following is a list of notable enterprises that have used or are using HBase:\n\n    Adobe\n    Amadeus IT Group, as its main long-term storage DB.\n    Daumkakao[6]\n    Facebook uses HBase for its messaging platform.\n    LinkedIn\n    Netflix[7]\n    Sophos, for some of their back-end systems.\n    Spotify uses HBase as base for Hadoop and machine learning jobs.[8]\n    Tuenti uses HBase for its messaging platform.[9][10]\n    Sears", "skillName": "Apache_HBase."}
{"id": 156, "category": "Databases", "skillText": "MongoDB (from humongous) is a free and open-source cross-platform document-oriented database. Classified as a NoSQL database, MongoDB avoids the traditional table-based relational database structure in favor of JSON-like documents with dynamic schemas (MongoDB calls the format BSON), making the integration of data in certain types of applications easier and faster. MongoDB is developed by MongoDB Inc. and is free and open-source, published under a combination of the GNU Affero General Public License and the Apache License. As of July 2015, MongoDB was the fourth most widely mentioned database engine on the web, and the most popular for document stores.[2]\n\nContents\n\n    1 History\n    2 Main features\n    3 Bug reports and criticisms\n    4 Architecture\n        4.1 Programming language accessibility\n        4.2 Management and graphical front-ends\n        4.3 Licensing and support\n    5 Performance\n        5.1 Performance metrics\n        5.2 Metric collection\n            5.2.1 Utilities\n            5.2.2 Commands\n            5.2.3 Production monitoring\n        5.3 Benchmarks\n    6 Production deployments\n    7 MongoDB World\n    8 See also\n    9 References\n    10 Bibliography\n    11 External links\n\nHistory\nSee also: MongoDB Inc. § History\n\nThe software company 10gen began developing MongoDB in 2007 as a component of a planned platform as a service product. In 2009, the company shifted to an open source development model, with the company offering commercial support and other services. In 2013, 10gen changed its name to MongoDB Inc.[3]\nMain features\n\nSome of the features include:[4]\n\nAd hoc queries\n\nMongoDB supports field, range queries, regular expression searches. Queries can return specific fields of documents and also include user-defined JavaScript functions. Queries can also be configured to return a random sample of results of a given size.\n\nIndexing\n\nAny field in a MongoDB document can be indexed – including within arrays and embedded documents (indices in MongoDB are conceptually similar to those in RDBMSes). Primary and secondary indices are available.\n\nReplication\n\nMongoDB provides high availability with replica sets.[5] A replica set consists of two or more copies of the data. Each replica set member may act in the role of primary or secondary replica at any time. All writes and reads are done on the primary replica by default. Secondary replicas maintain a copy of the data of the primary using built-in replication. When a primary replica fails, the replica set automatically conducts an election process to determine which secondary should become the primary. Secondaries can optionally serve read operations, but that data is only eventually consistent by default.\n\nLoad balancing\n\nMongoDB scales horizontally using sharding.[6] The user chooses a shard key, which determines how the data in a collection will be distributed. The data is split into ranges (based on the shard key) and distributed across multiple shards. (A shard is a master with one or more slaves.). Alternatively, the shard key can be hashed to map to a shard – enabling an even data distribution.\n\nMongoDB can run over multiple servers, balancing the load and/or duplicating data to keep the system up and running in case of hardware failure. MongoDB is easy to deploy, and new machines can be added to a running database.\n\nFile storage\n\nMongoDB can be used as a file system, taking advantage of load balancing and data replication features over multiple machines for storing files.\n\nThis function, called Grid File System,[7] is included with MongoDB drivers and available for many development languages (see \"Language Support\" for a list of supported languages). MongoDB exposes functions for file manipulation and content to developers. GridFS is used, for example, in plugins for NGINX[8] and lighttpd.[9] Instead of storing a file in a single document, GridFS divides a file into parts, or chunks, and stores each of those chunks as a separate document.[10]\n\nIn a multi-machine MongoDB system, files can be distributed and copied multiple times between machines transparently, thus effectively creating a load-balanced and fault-tolerant system.\n\nAggregation\n\nMapReduce can be used for batch processing of data and aggregation operations.\n\nThe aggregation framework enables users to obtain the kind of results for which the SQL GROUP BY clause is used. Aggregation operators can be strung together to form a pipeline – analogous to Unix pipes. The aggregation framework includes the $lookup operator which can join documents from multiple documents, as well as statistical operators such as standard deviation.\n\nServer-side JavaScript execution\n\nJavaScript can be used in queries, aggregation functions (such as MapReduce), and sent directly to the database to be executed.\n\nCapped collections\n\nMongoDB supports fixed-size collections called capped collections. This type of collection maintains insertion order and, once the specified size has been reached, behaves like a circular queue.\nBug reports and criticisms\n\nIn some failure scenarios where an application can access two distinct MongoDB processes, but these processes cannot access each other, it is possible for MongoDB to return stale reads. In this scenario it is also possible for MongoDB to roll back writes that have been acknowledged.[11]\n\nBefore version 2.2, concurrency control was implemented on a per-mongod basis. With version 2.2, concurrency control was implemented at the database level.[12] Since version 3.0,[13] pluggable storage engines were introduced, and each storage engine may implement concurrency control differently.[14] With MongoDB 3.0 concurrency control is implemented at the collection level for the MMAPv1 storage engine,[15] and at the document level with the WiredTiger storage engine.[16] With versions prior to 3.0, one approach to increase concurrency is to use sharding.[17] In some situations, reads and writes will yield their locks. If MongoDB predicts a page is unlikely to be in memory, operations will yield their lock while the pages load. The use of lock yielding expanded greatly in 2.2.[18]\n\nAnother criticism is related to the limitations of MongoDB when used on 32-bit systems.[19] In some cases, this was due to inherent memory limitations.[20][self-published source] MongoDB recommends 64-bit systems and that users provide sufficient RAM for their working set.\n\nMongoDB cannot do collation-based sorting and is limited to byte-wise comparison via memcmp,[21] which will not provide correct ordering for many non-English languages[22] when used with a Unicode encoding.\n\nMongoDB queries against an index are not atomic and can miss documents which are being updated while the query is running and which do match the query both before and after an update.[23]\nArchitecture\nProgramming language accessibility\n\nMongoDB has official drivers for a variety of popular programming languages and development environments.[24] There are also a large number of unofficial or community-supported drivers for other programming languages and frameworks.[25]\nManagement and graphical front-ends\nRecord insertion in MongoDB with Robomongo 0.8.5.\n\nMost administration is done from command line tools such as the mongo shell because MongoDB does not include a GUI-style administrative interface. There are products and third-party projects that offer user interfaces for administration and data viewing.[26]\nLicensing and support\n\nMongoDB is available at no cost under the GNU Affero General Public License.[27] The language drivers are available under an Apache License. In addition, MongoDB Inc. offers proprietary licenses for MongoDB.\nPerformance\nPerformance metrics\n\nProper MongoDB performance is critical for infrastructures that rely on the database to operate applications. In response to growing concern over how to measure this performance and how to lessen slowdowns, many monitoring solutions have begun to provide both open-source and hosted platforms for MongoDB performance monitoring. Currently, many monitoring solutions, including Datadog, focus on the below set of metrics to track MongoDB performance:[28]\n\n    Throughput metrics \n        Number of read requests\n        Number of write requests\n        Number of clients with read operations in progress or queued\n        Number of clients with write operations in progress or queued\n    Database performance \n        Size of oplog (MB)\n        Oplog window (seconds)\n        Replication lag\n        Replication headroom\n        Replica set member state\n    Resource utilization \n        Number of clients currently connected to the database server\n        Number of unused connections available for new clients\n    Resource saturation \n        Number of read requests currently queued\n        Number of write requests currently queued\n    Errors (asserts) \n        Number of message assertions raised\n        Number of warning assertions raised\n        Number of regular assertions raised\n        Number of assertions corresponding to errors generated by users.\n\nMetric collection\n\nThese metrics are typically collected in three ways,[29] depending on what is best suited for each infrastructure.\nUtilities\n\nUtilities, offered by MongoDB, can collect real-time statistics for your cluster and are particularly helpful for ad hoc checks.[30]\n\n    mongostat[31] - a powerful utility that reports real-time stats on connections, inserts, queries, updates, deletes, queued reads and writes, flushes memory usage, and page faults.\n    mongotop[32] - a utility that reports on the amount of time an instance spends performing read and write operations.\n\nThese two utilities are helpful for spot checking but are not considered to collect enough MongoDB cluster performance data to be actionable, especially at scale.\nCommands\n\nCommands can be used in adjunction with utilities because they provide different metrics from your MongoDB cluster.\n\n    serverStatus \n    dbStats \n    collStats \n    getReplicationInfo \n    replSetGetStatus \n    sh.status \n    getProfilingStatus \n\nProduction monitoring\n\nThe third way to access MongoDB cluster metrics involves using more comprehensive, subscription SaaS tools like MongoDB Enterprise Advanced \nor Datadog \n.\nBenchmarks\n\nUnited Software Associates published a benchmark using Yahoo's Cloud Serving Benchmark as a basis of all the tests. MongoDB provides greater performance than Couchbase Server or Cassandra in all the tests they ran, in some cases by as much as 25x.[33]\n\nAnother benchmark for top NoSQL databases utilizing Amazon's Elastic Compute Cloud that was done by End Point arrived at opposite results, placing MongoDB last among the tested databases.[34]\nProduction deployments\n\nLarge-scale deployments of MongoDB are tracked by MongoDB Inc. Notable users of MongoDB include:\n\n    Adobe: Adobe Experience Manager is intended to accelerate development of digital experiences that increase customer loyalty, engagement and demand. Adobe uses MongoDB to store petabytes of data in the large-scale content repositories underpinning the Experience Manager.[35]\n    Amadeus IT Group uses MongoDB for its back-end software.[36]\n    The Compact Muon Solenoid at CERN uses MongoDB as the primary back-end for the Data Aggregation System for the Large Hadron Collider.[37]\n    Craigslist: With 80 million classified ads posted every month, Craigslist needs to archive billions of records in multiple formats, and must be able to query and report on these archives at runtime. Craigslist migrated from MySQL to MongoDB to support its active archive, with continuous availability mandated for regulatory compliance across 700 sites in 70 different countries.[38]\n    eBay uses MongoDB in the search suggestion and the internal Cloud Manager State Hub.[39]\n    FIFA (video game series): EA's Spearhead development studio uses MongoDB[40] to store user data and game state. Auto-sharding allows scaling MongoDB across EA's 250+ servers as user demand grows.\n    Foursquare deploys MongoDB on Amazon AWS to store venues and user check-ins into venues.[41]\n    LinkedIn uses MongoDB for its internal learning platform.[42]\n    McAfee: MongoDB powers McAfee Global Threat Intelligence (GTI), a cloud-based intelligence service that correlates data from millions of sensors around the globe. Billions of documents are stored and analyzed in MongoDB to deliver real-time threat intelligence to other McAfee end-client products.[43]\n    MetLife uses MongoDB for “The Wall\", a customer service application providing a \"360-degree view\" of MetLife customers.[44]\n    SAP uses MongoDB in the SAP PaaS.[45]\n    Shutterfly uses MongoDB for its photo platform. As of 2013, the photo platform stores 18 billion photos uploaded by Shutterfly's 7 million users.[46]\n    Tuenti uses MongoDB as its backend DB.[47]\n    Yandex: The largest search engine in Russia uses MongoDB to manage all user and metadata for its file sharing service. MongoDB has scaled[48] to support tens of billions of objects and TBs of data, growing at 10 million new file uploads per day.\n\nMongoDB World\nMongodb world 2015.jpg\n\nMongoDB World [49] is an annual developer conference hosted by MongoDB, Inc. Started in 2014, MongoDB World provides a multi-day opportunity for communities and experts in MongoDB to network, learn from peers, research upcoming trends and interesting use cases, and hear about new releases and developments from MongoDB, Inc.\nSee also\nPortal icon \tFree software portal\n\n    NoSQL\n    Server-side scripting\n    MEAN, a solutions stack using MongoDB as the database\n    Couchbase, an enterprise NoSQL document database with SQL and Master/Master replication\n    HyperDex, a NoSQL database providing the MongoDB API with stronger consistency guarantees", "skillName": "MongoDB."}
{"id": 157, "category": "Databases", "skillText": "In theoretical computer science, the CAP theorem, also named Brewer's theorem after computer scientist Eric Brewer, states that it is impossible for a distributed computer system to simultaneously provide all three of the following guarantees:[1][2][3]\n\n    Consistency (all nodes see the same data at the same time)\n    Availability (every request receives a response about whether it succeeded or failed)\n    Partition tolerance (the system continues to operate despite arbitrary partitioning due to network failures)\n\nIn 2012 Brewer clarified some of his positions, including why the often-used \"two out of three\" concept can be misleading or misapplied, and the different definition of consistency used in CAP relative to the one used in ACID.[4]\n\nContents\n\n    1 History\n        1.1 Brewer’s 2012 article\n    2 See also\n    3 References\n    4 External links\n\nHistory\n\nAccording to University of California, Berkeley computer scientist Eric Brewer, the theorem first appeared in autumn 1998.[4] It was published as the CAP principle in 1999[5] and presented as a conjecture by Brewer at the 2000 Symposium on Principles of Distributed Computing (PODC).[6] In 2002, Seth Gilbert and Nancy Lynch of MIT published a formal proof of Brewer's conjecture, rendering it a theorem.[1] This last claim has been criticized, however, this reference does not offer a peer-reviewed formal proof - just an informal assertion on a blog posting.[7]\nBrewer’s 2012 article\n\nCAP Twelve Years Later: How the \"Rules\" Have Changed \nSee also\n\n    Consistency model\n    Fallacies of Distributed Computing\n    Paxos (computer science)\n    Project management triangle\n    Raft (computer science)\n    Trilemma", "skillName": "DB2."}
{"id": 158, "category": "Databases", "skillText": "Object database\nAn object database (also object-oriented database management system, OODBMS) is a database management system in which information is represented in the form of objects as used in object-oriented programming. Object databases are different from relational databases which are table-oriented. Object-relational databases are a hybrid of both approaches.\n\nObject databases have been considered since the early 1980s.[2]\n\n1\tOverview\n2\tHistory\n3\tTimeline\n4\tAdoption of object databases\n5\tTechnical features\n6\tStandards\n7\tComparison with RDBMSs\n8\tSee also\n9\tReferences\n10\tExternal links\nOverview\nObject-oriented database management systems (OODBMSs) combine database capabilities with object-oriented programming language capabilities. OODBMSs allow object-oriented programmers to develop the product, store them as objects, and replicate or modify existing objects to make new objects within the OODBMS. Because the database is integrated with the programming language, the programmer can maintain consistency within one environment, in that both the OODBMS and the programming language will use the same model of representation. Relational DBMS projects, by way of contrast, maintain a clearer division between the database model and the application.\n\nAs the usage of web-based technology increases with the implementation of Intranets and extranets, companies have a vested interest in OODBMSs to display their complex data. Using a DBMS that has been specifically designed to store data as objects gives an advantage to those companies that are geared towards multimedia presentation or organizations that utilize computer-aided design (CAD).[3]\n\nSome object-oriented databases are designed to work well with object-oriented programming languages such as Delphi, Ruby, Python, Perl, Java, C#, Visual Basic .NET, C++, Objective-C and Smalltalk; others such as JADE have their own programming languages. OODBMSs use exactly the same model as object-oriented programming languages.\n\nHistory\nObject database management systems grew out of research during the early to mid-1970s into having intrinsic database management support for graph-structured objects. The term \"object-oriented database system\" first appeared around 1985.[4] Notable research projects included Encore-Ob/Server (Brown University), EXODUS (University of Wisconsin�Madison), IRIS (Hewlett-Packard), ODE (Bell Labs), ORION (Microelectronics and Computer Technology Corporation or MCC), Vodak (GMD-IPSI), and Zeitgeist (Texas Instruments). The ORION project had more published papers than any of the other efforts. Won Kim of MCC compiled the best of those papers in a book published by The MIT Press.[5]\n\nEarly commercial products included Gemstone (Servio Logic, name changed to GemStone Systems), Gbase (Graphael), and Vbase (Ontologic). The early to mid-1990s saw additional commercial products enter the market. These included ITASCA (Itasca Systems), Jasmine (Fujitsu, marketed by Computer Associates), Matisse (Matisse Software), Objectivity/DB (Objectivity, Inc.), ObjectStore (Progress Software, acquired from eXcelon which was originally Object Design), ONTOS (Ontos, Inc., name changed from Ontologic), O2[6] (O2 Technology, merged with several companies, acquired by Informix, which was in turn acquired by IBM), POET (now FastObjects from Versant which acquired Poet Software), Versant Object Database (Versant Corporation), VOSS (Logic Arts) and JADE (Jade Software Corporation). Some of these products remain on the market and have been joined by new open source and commercial products such as InterSystems Cach�.\n\nObject database management systems added the concept of persistence to object programming languages. The early commercial products were integrated with various languages: GemStone (Smalltalk), Gbase (LISP), Vbase (COP) and VOSS (Virtual Object Storage System for Smalltalk). For much of the 1990s, C++ dominated the commercial object database management market. Vendors added Java in the late 1990s and more recently, C#.\n\nStarting in 2004, object databases have seen a second growth period when open source object databases emerged that were widely affordable and easy to use, because they are entirely written in OOP languages like Smalltalk, Java, or C#, such as Versant's db4o (db4objects), DTS/S1 from Obsidian Dynamics and Perst (McObject), available under dual open source and commercial licensing.\n\nTimeline\n1966\nMUMPS\n1979\nInterSystems M\n1980\nTORNADO � an object database for CAD/CAM[7]\n1982\nGemstone started (as Servio Logic) to build a set theoretic model data base machine.\n1985 � Term Object Database first introduced\n1986\nServio Logic (Gemstone Systems) Ships Gemstone 1.0\n1988\nVersant Corporation started (as Object Sciences Corp)\nObjectivity, Inc. founded\nEarly 1990s\nServio Logic changes name to Gemstone Systems\nGemstone (Smalltalk)-(C++)-(Java)\nGBase (LISP)\nVBase (O2- ONTOS � INFORMIX)\nObjectivity/DB\nMid 1990's\nInterSystems Cach�\nVersant Object Database\nObjectStore\nODABA\nZODB\nPoet\nJADE\nMatisse\nIllustra Informix\nWebcrossing\n2000's\ndb4o project started by Carl Rosenberger\nObjectDB\n2001 IBM acquires Informix\n2003 odbpp public release\n2004 db4o's commercial launch as db4objects, Inc.\n2008 db4o acquired by Versant Corporation\n2010 VMware acquires GemStone[8]\n2012 Wakanda first production versions with open source and commercial licenses\n2013 GemTalk Systems acquires GemStone products from VMware[9]\n2014 Realm\nAdoption of object databases\nObject databases based on persistent programming acquired a niche in application areas such as engineering and spatial databases, telecommunications, and scientific areas such as high energy physics and molecular biology.\n\nAnother group of object databases focuses on embedded use in devices, packaged software, and real-time systems.\n\nTechnical features\nMost object databases also offer some kind of query language, allowing objects to be found using a declarative programming approach. It is in the area of object query languages, and the integration of the query and navigational interfaces, that the biggest differences between products are found. An attempt at standardization was made by the ODMG with the Object Query Language, OQL.\n\nAccess to data can be faster because joins are often not needed (as in a tabular implementation of a relational database). This is because an object can be retrieved directly without a search, by following pointers.\n\nAnother area of variation between products is in the way that the schema of a database is defined. A general characteristic, however, is that the programming language and the database schema use the same type definitions.\n\nMultimedia applications are facilitated because the class methods associated with the data are responsible for its correct interpretation.\n\nMany object databases, for example Gemstone or VOSS, offer support for versioning. An object can be viewed as the set of all its versions. Also, object versions can be treated as objects in their own right. Some object databases also provide systematic support for triggers and constraints which are the basis of active databases.\n\nThe efficiency of such a database is also greatly improved in areas which demand massive amounts of data about one item. For example, a banking institution could get the user's account information and provide them efficiently with extensive information such as transactions, account information entries etc. The Big O Notation for such a database paradigm drops from O(n) to O(1), greatly increasing efficiency in these specific cases.\n\nStandards\nThe Object Data Management Group was a consortium of object database and object-relational mapping vendors, members of the academic community, and interested parties. Its goal was to create a set of specifications that would allow for portable applications that store objects in database management systems. It published several versions of its specification. The last release was ODMG 3.0. By 2001, most of the major object database and object-relational mapping vendors claimed conformance to the ODMG Java Language Binding. Compliance to the other components of the specification was mixed. In 2001, the ODMG Java Language Binding was submitted to the Java Community Process as a basis for the Java Data Objects specification. The ODMG member companies then decided to concentrate their efforts on the Java Data Objects specification. As a result, the ODMG disbanded in 2001.\n\nMany object database ideas were also absorbed into SQL:1999 and have been implemented in varying degrees in object-relational database products.\n\nIn 2005 Cook, Rai, and Rosenberger proposed to drop all standardization efforts to introduce additional object-oriented query APIs but rather use the OO programming language itself, i.e., Java and .NET, to express queries. As a result, Native Queries emerged. Similarly, Microsoft announced Language Integrated Query (LINQ) and DLINQ, an implementation of LINQ, in September 2005, to provide close, language-integrated database query capabilities with its programming languages C# and VB.NET 9.\n\nIn February 2006, the Object Management Group (OMG) announced that they had been granted the right to develop new specifications based on the ODMG 3.0 specification and the formation of the Object Database Technology Working Group (ODBT WG). The ODBT WG planned to create a set of standards that would incorporate advances in object database technology (e.g., replication), data management (e.g., spatial indexing), and data formats (e.g., XML) and to include new features into these standards that support domains where object databases are being adopted (e.g., real-time systems). The work of the ODBT WG was suspended in March 2009 when, subsequent to the economic turmoil in late 2008, the ODB vendors involved in this effort decided to focus their resources elsewhere.\n\nIn January 2007 the World Wide Web Consortium gave final recommendation status to the XQuery language. XQuery uses XML as its data model. Some of the ideas developed originally for object databases found their way into XQuery, but XQuery is not intrinsically object-oriented. Because of the popularity of XML, XQuery engines compete with object databases as a vehicle for storage of data that is too complex or variable to hold conveniently in a relational database. XQuery also allows modules to be written to provide encapsulation features that have been provided by Object-Oriented systems.\n\nXQuery v1 and XPath v2 are so complex (no FOSS software is implementing this standards after 10 years of its publication) when comparing with XPath v1 and XSLT v1 implementations, and XML not fitted all community demands as an open format. Since early 2000s JSON is gaining community and applications, overcoming XML in the 2010's. JSONiq, a query-analog of XQuery for JSON (sharing same XQuery core expressions and operations), demonstred the functional equivalence between JSON and XML formats. In this context, the main strategy of OODBMS maintainers was to retrofitting JSON (by using JSON as internal data type).\n\nIn January 2016, with the PostgreSQL 9.5 release[10] was the first FOSS OODBMS to offer a effcient JSON internal datatype (JSONB) with a complete set of functions and operations, for all basic relational and non-relational manipulations.\n\nComparison with RDBMSs\nAn object database stores complex data and relationships between data directly, without mapping to relational rows and columns, and this makes them suitable for applications dealing with very complex data.[11] Objects have a many to many relationship and are accessed by the use of pointers. Pointers are linked to objects to establish relationships. Another benefit of an OODBMS is that it can be programmed with small procedural differences without affecting the entire system.[12]\n\nSee also\nComparison of object database management systems\nComponent-oriented database\nEDA database\nEnterprise Objects Framework\nNoSQL\nObject Data Management Group\nObject-relational database\nPersistence (computer science)\nRelational model", "skillName": "ODBMS."}
{"id": 159, "category": "Databases", "skillText": "MySQL (officially pronounced as /maɪ ˌɛskjuːˈɛl/ \"My S-Q-L\",[5]) is an open-source relational database management system (RDBMS).[6] In July 2013, it was the world's second most[a] widely used RDBMS, and the most widely used open-source client–server model RDBMS.[9] Its name is a combination of \"My\", the name of co-founder Michael Widenius' daughter,[10] and \"SQL\", the abbreviation for Structured Query Language. The MySQL development project has made its source code available under the terms of the GNU General Public License, as well as under a variety of proprietary agreements. MySQL was owned and sponsored by a single for-profit firm, the Swedish company MySQL AB, now owned by Oracle Corporation.[11] For proprietary use, several paid editions are available, and offer additional functionality.\n\nMySQL is a popular choice of database for use in web applications, and is a central component of the widely used LAMP open-source web application software stack (and other \"AMP\" stacks). LAMP is an acronym for \"Linux, Apache, MySQL, Perl/PHP/Python\". Free-software open-source projects that require a full-featured database management system often use MySQL. Applications that use the MySQL database include: TYPO3, MODx, Joomla, WordPress, phpBB, MyBB, Drupal and other software. MySQL is also used in many high-profile, large-scale websites, including Google[12][13] (though not for searches), Facebook,[14][15][16] Twitter,[17] Flickr,[18] and YouTube.[19]\n\nOn all platforms except Windows, MySQL ships with no GUI tools to administer MySQL databases or manage data contained within the databases. Users may use the included command line tools,[20][21] or install MySQL Workbench \nvia a separate download. Many third party GUI tools are also available.\n\nContents\n\n    1 Overview\n    2 History\n        2.1 Milestones\n        2.2 Versions\n        2.3 Legal disputes and acquisitions\n    3 Features\n        3.1 Limitations\n        3.2 Deployment\n        3.3 Backup software\n        3.4 High availability\n        3.5 Cloud deployment\n    4 User interfaces\n        4.1 Graphical user interfaces\n        4.2 Command-line interfaces\n    5 Application programming interfaces\n    6 Project forks\n    7 See also\n    8 Notes\n    9 References\n    10 External links\n\nOverview\n\nMySQL is written in C and C++. Its SQL parser is written in yacc, but it uses a home-brewed lexical analyzer.[22] MySQL works on many system platforms, including AIX, BSDi, FreeBSD, HP-UX, eComStation, i5/OS, IRIX, Linux, OS X, Microsoft Windows, NetBSD, Novell NetWare, OpenBSD, OpenSolaris, OS/2 Warp, QNX, Oracle Solaris, Symbian, SunOS, SCO OpenServer, SCO UnixWare, Sanos and Tru64. A port of MySQL to OpenVMS also exists.[23]\n\nThe MySQL server software itself and the client libraries use dual-licensing distribution. They are offered under GPL version 2,[24] beginning from 28 June 2000[25] (which in 2009 has been extended with a FLOSS License Exception)[26] or to use a proprietary license.[27]\n\nSupport can be obtained from the official manual.[28] Free support additionally is available in different IRC channels and forums. Oracle offers paid support via its MySQL Enterprise products. They differ in the scope of services and in price. Additionally, a number of third party organisations exist to provide support and services, including MariaDB and Percona.\n\nMySQL has received positive reviews, and reviewers noticed it \"performs extremely well in the average case\". and that the \"developer interfaces are there, and the documentation (not to mention feedback in the real world via Web sites and the like) is very, very good\".[29] It has also been tested to be a \"fast, stable and true multi-user, multi-threaded sql database server\".[30]\nHistory\n\nMySQL was created by a Swedish company, MySQL AB, founded by David Axmark, Allan Larsson and Michael \"Monty\" Widenius. The first version of MySQL appeared on 23 May 1995. It was initially created for personal usage from mSQL based on the low-level language ISAM, which the creators considered too slow and inflexible. They created a new SQL interface, while keeping the same API as mSQL. By keeping the API consistent with the mSQL system, many developers were able to use MySQL instead of the (proprietarily licensed) mSQL antecedent.[citation needed][dubious – discuss]\nMilestones\n\nNotable milestones in MySQL development include:\n\n    Original development of MySQL by Michael Widenius and David Axmark beginning in 1994[31]\n    First internal release on 23 May 1995\n    Version 3.19: End of 1996, from www.tcx.se\n    Version 3.20: January 1997\n    Windows version was released on 8 January 1998 for Windows 95 and NT\n    Version 3.21: production release 1998, from www.mysql.com\n    Version 3.22: alpha, beta from 1998\n    Version 3.23: beta from June 2000, production release 22 January 2001[32]\n    Version 4.0: beta from August 2002, production release March 2003 (unions)\n    Version 4.01: beta from August 2003, Jyoti[clarification needed][citation needed] adopts MySQL for database tracking\n    Version 4.1: beta from June 2004, production release October 2004 (R-trees and B-trees, subqueries, prepared statements)\n    Version 5.0: beta from March 2005, production release October 2005 (cursors, stored procedures, triggers, views, XA transactions)\n\n    The developer of the Federated Storage Engine states that \"The Federated Storage Engine is a proof-of-concept storage engine\",[33] but the main distributions of MySQL version 5.0 included it and turned it on by default. Documentation of some of the short-comings appears in \"MySQL Federated Tables: The Missing Manual\".[34]\n\n    Sun Microsystems acquired MySQL AB in 2008.[35]\n    Version 5.1: production release 27 November 2008 (event scheduler, partitioning, plugin API, row-based replication, server log tables)\n\n    Version 5.1 contained 20 known crashing and wrong result bugs in addition to the 35 present in version 5.0 (almost all fixed as of release 5.1.51).[36]\n    MySQL 5.1 and 6.0-alpha showed poor performance when used for data warehousing – partly due to its inability to utilize multiple CPU cores for processing a single query.[37]\n\n    Oracle acquired Sun Microsystems on 27 January 2010.[38][39][40]\n    The day Oracle announced the purchase of Sun, Michael \"Monty\" Widenius forked MySQL, launching MariaDB, and took a swath of MySQL developers with him.[41]\n    MySQL Server 5.5 was generally available (as of December 2010). Enhancements and features include:\n        The default storage engine is InnoDB, which supports transactions and referential integrity constraints.\n        Improved InnoDB I/O subsystem[42]\n        Improved SMP support[43]\n        Semisynchronous replication.\n        SIGNAL and RESIGNAL statement in compliance with the SQL standard.\n        Support for supplementary Unicode character sets utf16, utf32, and utf8mb4.\n        New options for user-defined partitioning.\n    MySQL Server 6.0.11-alpha was announced[44] on 22 May 2009 as the last release of the 6.0 line. Future MySQL Server development uses a New Release Model. Features developed for 6.0 are being incorporated into future releases.\n    The general availability of MySQL 5.6 was announced in February 2013. New features included performance improvements to the query optimizer, higher transactional throughput in InnoDB, new NoSQL-style memcached APIs, improvements to partitioning for querying and managing very large tables, TIMESTAMP column type that correctly stores milliseconds, improvements to replication, and better performance monitoring by expanding the data available through the PERFORMANCE_SCHEMA.[45] The InnoDB storage engine also included support for full-text search and improved group commit performance.\n    The general availability of MySQL 5.7 was announced in October 2015.[46]\n\nVersions\n\nThe following chart provides an overview of various MySQL versions and their development statuses:[47][48][49][50][51][52][53][54]\nLegal disputes and acquisitions\n\nOn 15 June 2001, NuSphere sued MySQL AB, TcX DataKonsult AB and its original authors Michael (\"Monty\") Widenius and David Axmark in U.S District Court in Boston for \"breach of contract, tortious interference with third party contracts and relationships and unfair competition\".[55][56]\n\nIn 2002, MySQL AB sued Progress NuSphere for copyright and trademark infringement in United States district court. NuSphere had allegedly violated MySQL's copyright by linking MySQL's GPL'ed code with NuSphere Gemini table without being in compliance with the license.[57] After a preliminary hearing before Judge Patti Saris on 27 February 2002, the parties entered settlement talks and eventually settled.[58] After the hearing, FSF commented that \"Judge Saris made clear that she sees the GNU GPL to be an enforceable and binding license.\"[59]\n\nIn October 2005, Oracle Corporation acquired Innobase OY, the Finnish company that developed the third-party InnoDB storage engine that allows MySQL to provide such functionality as transactions and foreign keys. After the acquisition, an Oracle press release mentioned that the contracts that make the company's software available to MySQL AB would be due for renewal (and presumably renegotiation) some time in 2006.[60] During the MySQL Users Conference in April 2006, MySQL issued a press release that confirmed that MySQL and Innobase OY agreed to a \"multi-year\" extension of their licensing agreement.[61]\n\nIn February 2006, Oracle Corporation acquired Sleepycat Software,[62] makers of the Berkeley DB, a database engine providing the basis for another MySQL storage engine. This had little effect, as Berkeley DB was not widely used, and was dropped (due to lack of use) in MySQL 5.1.12, a pre-GA release of MySQL 5.1 released in October 2006.[63]\n\nIn January 2008, Sun Microsystems bought MySQL for $1 billion.[64]\n\nIn April 2009, Oracle Corporation entered into an agreement to purchase Sun Microsystems,[65] then owners of MySQL copyright and trademark. Sun's board of directors unanimously approved the deal. It was also approved by Sun's shareholders, and by the U.S. government on 20 August 2009.[66] On 14 December 2009, Oracle pledged to continue to enhance MySQL[67] as it had done for the previous four years.\n\nA movement against Oracle's acquisition of MySQL, to \"Save MySQL\"[68] from Oracle was started by one of the MySQL founders, Monty Widenius. The petition of 50,000+ developers and users called upon the European Commission to block approval of the acquisition. At the same time, several Free Software opinion leaders (including Eben Moglen, Pamela Jones of Groklaw, Jan Wildeboer and Carlo Piana, who also acted as co-counsel in the merger regulation procedure) advocated for the unconditional approval of the merger.[citation needed] As part of the negotiations with the European Commission, Oracle committed that MySQL server will continue until at least 2015 to use the dual-licensing strategy long used by MySQL AB, with proprietary and GPL versions available. The antitrust of the EU had been \"pressuring it to divest MySQL as a condition for approval of the merger\". But, as revealed by WikiLeaks, the US Department of Justice and Antitrust, at the request of Oracle, pressured the EU to approve the merger unconditionally.[69] The European Commission eventually unconditionally approved Oracle's acquisition of MySQL on 21 January 2010.[70]\n\nIn January 2009, prior to Oracle's acquisition of MySQL, Monty Widenius started a GPL-only fork, MariaDB. MariaDB is based on the same code base as MySQL server 5.5 and aims to maintain compatibility with Oracle-provided versions.[71]\nFeatures\n\nMySQL is offered under two different editions: the open source MySQL Community Server and the proprietary Enterprise Server.[72] MySQL Enterprise Server is differentiated by a series of proprietary extensions which install as server plugins, but otherwise shares the version numbering system and is built from the same code base.\n\nMajor features as available in MySQL 5.6:\n\n    A broad subset of ANSI SQL 99, as well as extensions\n    Cross-platform support\n    Stored procedures, using a procedural language that closely adheres to SQL/PSM[73]\n    Triggers\n    Cursors\n    Updatable views\n    Online DDL when using the InnoDB Storage Engine.\n    Information schema\n    Performance Schema that collects and aggregates statistics about server execution and query performance for monitoring purposes.[74]\n    A set of SQL Mode options to control runtime behavior, including a strict mode to better adhere to SQL standards.\n    X/Open XA distributed transaction processing (DTP) support; two phase commit as part of this, using the default InnoDB storage engine\n    Transactions with savepoints when using the default InnoDB Storage Engine. The NDB Cluster Storage Engine also supports transactions.\n    ACID compliance when using InnoDB and NDB Cluster Storage Engines[75]\n    SSL support\n    Query caching\n    Sub-SELECTs (i.e. nested SELECTs)\n    Built-in Replication support (i.e. Master-Master Replication & Master-Slave Replication) with one master per slave, many slaves per master.[76] Multi-master replication is provided in MySQL Cluster,[77] and multi-master support can be added to unclustered configurations using Galera Cluster.[78]\n    Full-text indexing and searching[b]\n    Embedded database library\n    Unicode support[c]\n    Partitioned tables with pruning of partitions in optimizer\n    Shared-nothing clustering through MySQL Cluster\n    Multiple storage engines, allowing one to choose the one that is most effective for each table in the application.[d]\n    Native storage engines InnoDB, MyISAM, Merge, Memory (heap), Federated, Archive, CSV, Blackhole, NDB Cluster.\n    Commit grouping, gathering multiple transactions from multiple connections together to increase the number of commits per second.\n\nThe developers release minor updates of the MySQL Server approximately every two months. The sources can be obtained from MySQL's website or from MySQL's GitHub repository, both under the GPL license.\nLimitations\n\nWhen using some storage engines other than the default of InnoDB, MySQL does not comply with the full SQL standard for some of the implemented functionality, including foreign key references [79] and check constraints.[80]\n\nUp until MySQL 5.7, triggers are limited to one per action / timing, meaning that at most one trigger can be defined to be executed after an INSERT operation, and one before INSERT on the same table.[81] No triggers can be defined on views.[81]\n\nMySQL database's inbuilt functions like UNIX_TIMESTAMP() will return 0 after 03:14:07 UTC on 19 January 2038.[82]\nDeployment\nLAMP software bundle, displayed here together with Squid.\n\nMySQL can be built and installed manually from source code, but it is more commonly installed from a binary package unless special customizations are required. On most Linux distributions, the package management system can download and install MySQL with minimal effort, though further configuration is often required to adjust security and optimization settings.\n\nThough MySQL began as a low-end alternative to more powerful proprietary databases, it has gradually evolved to support higher-scale needs as well. It is still most commonly used in small to medium scale single-server deployments, either as a component in a LAMP-based web application or as a standalone database server. Much of MySQL's appeal originates in its relative simplicity and ease of use, which is enabled by an ecosystem of open source tools such as phpMyAdmin. In the medium range, MySQL can be scaled by deploying it on more powerful hardware, such as a multi-processor server with gigabytes of memory.\n\nThere are however limits to how far performance can scale on a single server ('scaling up'), so on larger scales, multi-server MySQL ('scaling out') deployments are required to provide improved performance and reliability. A typical high-end configuration can include a powerful master database which handles data write operations and is replicated to multiple slaves that handle all read operations.[83] The master server continually pushes binlog events to connected slaves so in the event of failure a slave can be promoted to become the new master, minimizing downtime. Further improvements in performance can be achieved by caching the results from database queries in memory using memcached, or breaking down a database into smaller chunks called shards which can be spread across a number of distributed server clusters.[84]\nBackup software\n\nBackup software are computer programs used to perform backup; they create supplementary exact copies of files, databases or entire computers. These programs may later use the supplementary copies to restore the original contents in the event of data loss.\n\nFilesystem snapshot or volume manager snapshot backups are performed by using an external tool provided by the operating system (such as Logical Volume Manager in Linux) or storage device, with additional support from MySQL for ensuring consistency of such snapshots.\n\nmysqldump is a logical backup tool included with both community and enterprise editions of MySQL. It supports backing up from all storage engines. MySQL Enterprise Backup is a hot backup utility included as part of the MySQL Enterprise subscription from Oracle, offering native InnoDB hot backup, as well as backup for other storage engines. XtraBackup is an open-source MySQL hot backup software program. Some notable features include hot, non-locking backups for InnoDB storage, incremental backups, streaming, parallel-compressed backups, throttling based on the number of I/O operations per second, etc.[85]\nHigh availability\n\nEnsuring high availability requires a certain amount of redundancy in the system. For database systems, the redundancy traditionally takes the form of having a primary server acting as a master, and using replication to keep secondaries available to take over in case the primary fails. This means that the \"server\" that the application connects to is in reality a collection of servers, not a single server. In a similar manner, if the application is using a sharded database, it is in reality working with a collection of servers, not a single server. In this case, a collection of servers is usually referred to as a farm.[86]\n\nOne of the projects aiming to provide high availability for MySQL is MySQL Fabric, an integrated system for managing a collection of MySQL servers, and a framework on top of which high availability and database sharding is built. MySQL Fabric is open-source and is intended to be extensible, easy to use, and to support procedure execution even in the presence of failure, providing an execution model usually called resilient execution. MySQL client libraries are extended so they are hiding the complexities of handling failover in the event of a server failure, as well as correctly dispatching transactions to the shards. As of September 2013, there is support for Fabric-aware versions of Connector/J, Connector/PHP, Connector/Python, as well as some rudimentary support for Hibernate and Doctrine. As of May 2014, MySQL Fabric is in the general availability stage of development.[87]\nCloud deployment\nMain article: Cloud database\n\nMySQL can also be run on cloud computing platforms such as Amazon EC2. Some common deployment models for MySQL on the cloud are:\n\nVirtual machine image\n    In this implementation, cloud users can upload a machine image of their own with MySQL installed, or use a ready-made machine image with an optimized installation of MySQL on it, such as the one provided by Amazon EC2.[88]\n\nMySQL as a service\n    Some cloud platforms offer MySQL \"as a service\". In this configuration, application owners do not have to install and maintain the MySQL database on their own. Instead, the database service provider takes responsibility for installing and maintaining the database, and application owners pay according to their usage.[89] Notable cloud-based MySQL services are the Amazon Relational Database Service; Rackspace; HP Converged Cloud; Heroku and Jelastic.\n\nManaged MySQL cloud hosting\n    In this implementation, the database is not offered as a service, but the cloud provider hosts the database and manages it on the application owner's behalf. As of 2011, of the major cloud providers, only Terremark and Rackspace offer managed hosting for MySQL databases.[90][91]\n\nUser interfaces\nGraphical user interfaces\n\nA graphical user interface (GUI) is a type of interface that allows users to interact with electronic devices or programs through graphical icons and visual indicators such as secondary notation, as opposed to text-based interfaces, typed command labels or text navigation. GUIs are easier to learn than command-line interfaces (CLIs), which require commands to be typed on the keyboard.\n\nThird-party proprietary and free graphical administration applications (or \"front ends\") are available that integrate with MySQL and enable users to work with database structure and data visually. Some well-known front ends are:\nMySQL Workbench running on OS X\n\nMySQL Workbench\n    MySQL Workbench is the official integrated environment for MySQL. It was developed by MySQL AB, and enables users to graphically administer MySQL databases and visually design database structures. MySQL Workbench replaces the previous package of software, MySQL GUI Tools. Similar to other third-party packages, but still considered the authoritative MySQL front end, MySQL Workbench lets users manage database design & modeling, SQL development (replacing MySQL Query Browser) and Database administration (replacing MySQL Administrator).\n    MySQL Workbench is available in two editions, the regular free and open source Community Edition which may be downloaded from the MySQL website, and the proprietary Standard Edition which extends and improves the feature set of the Community Edition.\n\nAdminer\n    Adminer (formerly known as phpMinAdmin) is a free MySQL front end for managing content in MySQL databases (since version 2, it also works on PostgreSQL, MS SQL, SQLite and Oracle SQL databases). Adminer is distributed under the Apache license (or GPL v2) in the form of a single PHP file (around 300 KiB in size), and is capable of managing multiple databases, with many CSS skins available. Its author is Jakub Vrána who started to develop this tool as a light-weight alternative to phpMyAdmin, in July 2007.\n\nDatabase Workbench\n    Database Workbench is a software application for development and administration of multiple relational databases using SQL, with interoperationality between different database systems, developed by Upscene Productions.\n    Because Databases Workbench supports multiple database systems, it can provide software developers with the same interface and development environment for these otherwise different database systems and also includes cross database tools.\n    Database Workbench supports the following relational databases: Oracle Database, Microsoft SQL Server, SQL Anywhere, Firebird, NexusDB, InterBase, MySQL and MariaDB. Database Workbench 5 runs on 32-bit or 64 bit Windows platforms. Under Linux, FreeBSD or Mac OS X Database Workbench can operate using Wine.\n\nDBEdit\n    DBEdit is a database editor, which can connect to an Oracle, DB2, MySQL and any database that provides a JDBC driver. It runs on Windows, Linux and Solaris. DBEdit is free and open source software and distributed under the GNU General Public License. The source code is hosted on SourceForge.\n\nHeidiSQL\n    HeidiSQL, previously known as MySQL-Front, is a free and open source client, or frontend for MySQL (and for its forks like MariaDB and Percona Server, Microsoft SQL Server and PostgreSQL). HeidiSQL is developed by German programmer Ansgar Becker and a few other contributors in Delphi. To manage databases with HeidiSQL, users must login to a local or remote MySQL server with acceptable credentials, creating a session. Within this session users may manage MySQL Databases within the connected MySQL server, disconnecting from the server when done. Its feature set is sufficient for most common and advanced database, table and data record operations but remains in active development to move towards the full functionality expected in a MySQL Frontend.\n\nLibreOffice Base\n    LibreOffice Base allows the creation and management of databases, preparation of forms and reports that provide end users easy access to data. Like Microsoft Access, it can be used as a front-end for various database systems, including Access databases (JET), ODBC data sources, and MySQL or PostgreSQL[92]\n\nNavicat\n    Navicat is a series of graphical database management and development software produced by PremiumSoft CyberTech Ltd. for MySQL, MariaDB, Oracle, SQLite, PostgreSQL and Microsoft SQL Server. It has an Explorer-like graphical user interface and supports multiple database connections for local and remote databases. Its design is made to meet the needs of a variety of audiences, from database administrators and programmers to various businesses/companies that serve clients and share information with partners.\n    Navicat is a cross-platform tool and works on Microsoft Windows, OS X and Linux platforms. Upon purchase, users are able to select a language for the software from eight available languages: English, French, German, Spanish, Japanese, Polish, Simplified Chinese and Traditional Chinese.\n\nOpenOffice.org\n    OpenOffice.org Base is freely available and can manage MySQL databases if the entire suite is installed.\n\nphpMyAdmin\n    phpMyAdmin is a free and open source tool written in PHP intended to handle the administration of MySQL with the use of a web browser. It can perform various tasks such as creating, modifying or deleting databases, tables, fields or rows; executing SQL statements; or managing users and permissions. The software, which is available in 78 languages,[93] is maintained by The phpMyAdmin Project.[94]\n    It can import data from CSV and SQL, and transform stored data into any format using a set of predefined functions, like displaying BLOB-data as images or download-links.\n\nSQLBuddy\n    SQLBuddy is an open source web based application written in PHP intended to handle the administration of MySQL and SQLite with the use of a Web browser. The project places an emphasis on ease of installation and a simple user interface.\n\nSQLeo\n    SQLeo is a Visual query builder that helps users create or understand SQL queries. The source code is hosted on SourceForge.\n\nSQLyog\n    SQLyog is a GUI tool available in free as well as paid versions. Data manipulations (e.g., insert, update, and delete) may be done from a spreadsheet-like interface. Its editor has syntax highlighting and various automatic formatting options. Both raw table data and a result set from a query can be manipulated. Its data search feature uses Google-like search syntax and translates to SQL transparently for the user. It has a backup tool for performing unattended backups. Backups may be compressed and optionally stored as a file-per-table as well as identified with a timestamp.\n\nToad for MySQL\n    Toad for MySQL is a software application from Dell Software that database developers, database administrators and data analysts use to manage both relational and non-relational databases using SQL. Toad supports many databases and environments. It runs on all 32-bit/64-bit Windows platforms, including Microsoft Windows Server,Windows XP, Windows Vista,Windows 7 and 8 (32-Bit or 64-Bit). Dell Software has also released a Toad Mac Edition. Dell provides Toad in commercial and trial/freeware versions. The Freeware version is available from the ToadWorld.com community.\n\nWebmin\n    Webmin is a web-based system configuration tool for Unix-like systems, although recent versions can also be installed and run on Windows. With it, it is possible to configure operating system internals, such as users, disk quotas, services or configuration files, as well as modify and control open source apps, such as the Apache HTTP Server, PHP or MySQL.\n    Webmin is largely based on Perl, running as its own process and web server. It defaults to TCP port 10000 for communicating, and can be configured to use SSL if OpenSSL is installed with additional required Perl Modules.\n    It is built around modules, which have an interface to the configuration files and the Webmin server. This makes it easy to add new functionality. Due to Webmin's modular design, it is possible for anyone who is interested to write plugins for desktop configuration.\n    Webmin also allows for controlling many machines through a single interface, or seamless login on other webmin hosts on the same subnet or LAN.\n\nCommand-line interfaces\n\nA command-line interface is a means of interacting with a computer program where the user issues commands to the program by typing in successive lines of text (command lines). MySQL ships with many command line tools, from which the main interface is the mysql client.[20][21]\n\nMySQL Utilities is a set of utilities designed to perform common maintenance and administrative tasks. Originally included as part of the MySQL Workbench, the utilities are a stand-alone download available from Oracle.\n\nPercona Toolkit is a cross-platform toolkit for MySQL, developed in Perl.[95] Percona Toolkit can be used to prove replication is working correctly, fix corrupted data, automate repetitive tasks, and speed up servers. Percona Toolkit is included with several Linux distributions such as CentOS and Debian, and packages are available for Fedora and Ubuntu as well. Percona Toolkit was originally developed as Maatkit, but as of late 2011, Maatkit is no longer developed.\nApplication programming interfaces\n\nMany programming languages with language-specific APIs include libraries for accessing MySQL databases. These include MySQL Connector/Net for integration with Microsoft's Visual Studio (languages such as C# and VB are most commonly used) and the JDBC driver for Java. In addition, an ODBC interface called MySQL Connector/ODBC allows additional programming languages that support the ODBC interface to communicate with a MySQL database, such as ASP or ColdFusion. The HTSQL – URL-based query method also ships with a MySQL adapter, allowing direct interaction between a MySQL database and any web client via structured URLs.\nProject forks\n\nIn software engineering, a project fork happens when developers take a copy of source code from one software package and start independent development on it, creating a distinct and separate piece of software – a new (third-party) version. The term often implies not merely creating a development branch, but also a split in the developer community (a form of schism).[96] MySQL forks include the following:\n\nDrizzle\n    Drizzle is a free software/open source relational database management system (DBMS) that was forked from the now-defunct 6.0 development branch of the MySQL DBMS.[97] Like MySQL, Drizzle has a client/server architecture and uses SQL as its primary command language. Drizzle is distributed under version 2 and 3 of the GNU General Public License (GPL) with portions, including the protocol drivers and replication messaging under the BSD license.\n\nMariaDB\n    MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Being a fork of a leading open source software system, it is notable for being led by the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle.[41] Contributors are required to share their copyright with the MariaDB Foundation.[98] MariaDB intends to maintain high compatibility with MySQL, ensuring a \"drop-in\" replacement capability with library binary equivalency and exact matching with MySQL APIs and commands.[99] It includes the XtraDB storage engine for replacing InnoDB,[100] as well as a new storage engine, Aria, that intends to be both a transactional and non-transactional engine perhaps even included in future versions of MySQL.[101]\n\nPercona Server\n    Percona Server, forked by Percona, aims to retain close compatibility to the official MySQL releases, while focusing on performance and increased visibility into server operations.[102] Also included in Percona Server is XtraDB, Percona's fork of the InnoDB Storage Engine. Percona freely includes a number of scalability, availability, security and backup features only available in MySQL's commercial Enterprise edition.[103]\n\nWebScaleSQL\n    WebScaleSQL is a software branch of MySQL 5.6, and was announced on 27 March 2014 by Facebook, Google, LinkedIn and Twitter as a joint effort to provide a centralized development structure for extending MySQL with new features specific to its large-scale deployments, such as building large replicated databases running on server farms. Thus, WebScaleSQL opens a path toward deduplicating the efforts each company had been putting into maintaining its own branch of MySQL, and toward bringing together more developers. By combining the efforts of these companies and incorporating various changes and new features into MySQL, WebScaleSQL aims at supporting the deployment of MySQL in large-scale environments.[104][105] The project's source code is licensed under version 2 of the GNU General Public License, and is hosted on GitHub.[106][107]", "skillName": "MySQL."}
{"id": 160, "category": "Databases", "skillText": "Relational database management system\nA relational database management system (RDBMS) is a database management system (DBMS) that is based on the relational model as invented by E. F. Codd, of IBM's San Jose Research Laboratory. In 2016, many of the databases in widespread use are based on the relational database model.\n\nRDBMSs are a common choice for the storage of information in new databases used for financial records, manufacturing and logistical information, personnel data, and other applications since the 1980s. Relational databases have often replaced legacy hierarchical databases and network databases because they are easier to understand and use. However, relational databases have received unsuccessful challenge attempts by object database management systems in the 1980s and 1990s (which were introduced trying to address the so-called object-relational impedance mismatch between relational databases and object-oriented application programs) and also by XML database management systems in the 1990s.[citation needed] Despite such attempts, RDBMSs keep most of the market share, which has also grown over the years.\n\n1\tMarket share\n2\tHistory\n3\tHistorical usage of the term\n4\tSee also\n5\tReferences\nMarket share\nAccording to research company Gartner, the five leading commercial relational database vendors by revenue in 2011 were Oracle (48.8%), IBM (20.2%), Microsoft (17.0%), SAP including Sybase (4.6%), and Teradata (3.7%).[1]\n\nThe four leading open source implementations are Firebird, MySQL, PostgreSQL, and SQLite. MariaDB is a prominent fork of MySQL prompted by Oracle's acquisition of MySQL AB.\n\nAccording to Gartner, in 2008, the percentage of database sites using any given technology were (a given site may deploy multiple technologies):[2]\n\nOracle Database - 70%\nMicrosoft SQL Server - 68%\nMySQL (Oracle Corporation) - 50%\nIBM DB2 - 39%\nIBM Informix - 18%\nSAP Sybase Adaptive Server Enterprise - 15%\nSAP Sybase IQ - 14%\nTeradata - 11%\nAccording to DB-Engines, the most widely used systems are Oracle, MySQL, Microsoft SQL Server, PostgreSQL and IBM DB2.[3]\n\nHistory\nIn 1974, IBM began developing System R, a research project to develop a prototype RDBMS.[4][5] However, the first commercially available RDBMS was Oracle, released in 1979 by Relational Software, now Oracle Corporation.[6] Other examples of an RDBMS include DB2, SAP Sybase ASE, and Informix. In 1984, the first RDBMS for Macintosh began being developed, code-named Silver Surfer, it was later released in 1987 as 4th Dimension and known today as 4D.[7]\n\nHistorical usage of the term\nThe term \"relational database\" was invented by E. F. Codd at IBM in 1970. Codd introduced the term in his seminal paper \"A Relational Model of Data for Large Shared Data Banks\".[8] In this paper and later papers, he defined what he meant by \"relational\". One well-known definition of what constitutes a relational database system is composed of Codd's 12 rules. However, many of the early implementations of the relational model did not conform to all of Codd's rules, so the term gradually came to describe a broader class of database systems, which at a minimum:\n\nPresent the data to the user as relations (a presentation in tabular form, i.e. as a collection of tables with each table consisting of a set of rows and columns);\nProvide relational operators to manipulate the data in tabular form.\nThe first systems that were relatively faithful implementations of the relational model were from the University of Michigan; Micro DBMS (1969), the Massachusetts Institute of Technology;[9] (1971), and from IBM UK Scientific Centre at Peterlee; IS1 (1970�72) and its followon PRTV (1973�79). The first system sold as an RDBMS was Multics Relational Data Store, first sold in 1978. Others have been Ingres and IBM BS12.\n\nThe most common definition of an RDBMS is a product that presents a view of data as a collection of rows and columns, even if it is not based strictly upon relational theory. By this definition, RDBMS products typically implement some but not all of Codd's 12 rules.\n\nA second school of thought argues that if a database does not implement all of Codd's rules (or the current understanding on the relational model, as expressed by Christopher J Date, Hugh Darwen and others), it is not relational. This view, shared by many theorists and other strict adherents to Codd's principles, would disqualify most DBMSs as not relational. For clarification, they often refer to some RDBMSs as truly-relational database management systems (TRDBMS), naming others pseudo-relational database management systems (PRDBMS). It can also be said as the raw database management system.\n\nAs of 2009, most commercial relational DBMSes employ SQL as their query language.[10]\n\nAlternative query languages have been proposed and implemented, notably the pre-1996 implementation of Ingres QUEL.\n\nSee als\nSQL\nWikibook SQL\nOnline analytical processing (OLAP) and ROLAP (Relational Online Analytical Processing)\nData warehouse\nStar schema\nSnowflake schema", "skillName": "RDBMS."}
{"id": 161, "category": "Databases", "skillText": "A NoSQL (originally referring to \"non SQL\" or \"non relational\")[1] database provides a mechanism for storage and retrieval of data which is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but did not obtain the \"NoSQL\" moniker until a surge of popularity in the early twenty-first century,[2] triggered by the needs of Web 2.0 companies such as Facebook, Google and Amazon.com.[3][4][5] NoSQL databases are increasingly used in big data and real-time web applications.[6] NoSQL systems are also sometimes called \"Not only SQL\" to emphasize that they may support SQL-like query languages.[7][8]\n\nMotivations for this approach include: simplicity of design, simpler \"horizontal\" scaling to clusters of machines (which is a problem for relational databases),[2] and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve. Sometimes the data structures used by NoSQL databases are also viewed as \"more flexible\" than relational database tables.[9]\n\nMany NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance the lack of ability to perform ad-hoc JOINs across tables), lack of standardized interfaces, and huge previous investments in existing relational databases.[10] Most NoSQL stores lack true ACID transactions, although a few databases, such as MarkLogic, Aerospike, FairCom c-treeACE, Google Spanner (though technically a NewSQL database), Symas LMDB and OrientDB have made them central to their designs. (See ACID and JOIN Support.)\n\nInstead, most NoSQL databases offer a concept of \"eventual consistency\" in which database changes are propagated to all nodes \"eventually\" (typically within milliseconds) so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.[11] Additionally, some NoSQL systems may exhibit lost writes and other forms of data loss.[12] Fortunately, some NoSQL systems provide concepts such as write-ahead logging to avoid data loss.[13] For distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Even current relational databases \"do not allow referential integrity constraints to span databases.\"[14] There are few systems that maintain both ACID transactions and X/Open XA standards for distributed transaction processing.\n\nContents\n\n    1 History\n    2 Types and examples of NoSQL databases\n        2.1 Key-value stores\n        2.2 Document store\n        2.3 Graph\n        2.4 Object database\n        2.5 Tabular\n        2.6 Tuple store\n        2.7 Triple/quad store (RDF) database\n        2.8 Hosted\n        2.9 Multivalue databases\n        2.10 Multimodel database\n    3 Performance\n    4 Handling relational data\n        4.1 Multiple queries\n        4.2 Caching/replication/non-normalized data\n        4.3 Nesting data\n    5 ACID and JOIN Support\n    6 See also\n    7 References\n    8 Further reading\n    9 External links\n\nHistory\n\nThe term NoSQL was used by Carlo Strozzi in 1998 to name his lightweight, Strozzi NoSQL open-source relational database that did not expose the standard SQL interface, but was still relational.[15] His NoSQL RDBMS is distinct from the circa-2009 general concept of NoSQL databases. Strozzi suggests that, as the current NoSQL movement \"departs from the relational model altogether; it should therefore have been called more appropriately 'NoREL'\",[16] referring to 'No Relational'.\n\nJohan Oskarsson of Last.fm reintroduced the term NoSQL in early 2009 when he organized an event to discuss \"open source distributed, non relational databases\".[17] The name attempted to label the emergence of an increasing number of non-relational, distributed data stores, including open source clones of Google's BigTable/MapReduce and Amazon's Dynamo. Most of the early NoSQL systems did not attempt to provide atomicity, consistency, isolation and durability guarantees, contrary to the prevailing practice among relational database systems.[18]\n\nBased on 2014 revenue, the NoSQL market leaders are MarkLogic, MongoDB, and Datastax.[19] Based on 2015 popularity rankings, the most popular NoSQL databases are MongoDB, Apache Cassandra, and Redis.[20]\nTypes and examples of NoSQL databases\n\nThere have been various approaches to classify NoSQL databases, each with different categories and subcategories, some of which overlap. What follows is a basic classification by data model, with examples:\n\n    Column: Accumulo, Cassandra, Druid, HBase, Vertica\n    Document: Apache CouchDB, Clusterpoint, Couchbase, DocumentDB, HyperDex, Lotus Notes, MarkLogic, MongoDB, OrientDB, Qizx, RethinkDB\n    Key-value: Aerospike, Couchbase, Dynamo, FairCom c-treeACE, FoundationDB, HyperDex, MemcacheDB, MUMPS, Oracle NoSQL Database, OrientDB, Redis, Riak, Berkeley DB\n    Graph: AllegroGraph, InfiniteGraph,Giraph, MarkLogic, Neo4J, OrientDB, Virtuoso, Stardog\n    Multi-model: Alchemy Database, ArangoDB, CortexDB, FoundationDB, MarkLogic, OrientDB\n\nA more detailed classification is the following, based on one from Stephen Yen:[21]\nType \tExamples of this type\nKey-Value Cache \tCoherence, eXtreme Scale, GigaSpaces, GemFire, Hazelcast, Infinispan, JBoss Cache, Memcached, Repcached, Terracotta, Velocity\nKey-Value Store \tFlare, Keyspace, RAMCloud, SchemaFree, Hyperdex, Aerospike\nKey-Value Store (Eventually-Consistent) \tDovetailDB, Oracle NoSQL Database, Dynamo, Riak, Dynomite, MotionDb, Voldemort, SubRecord\nKey-Value Store (Ordered) \tActord, FoundationDB, Lightcloud, LMDB, Luxio, MemcacheDB, NMDB, Scalaris, TokyoTyrant\nData-Structures Server \tRedis\nTuple Store \tApache River, Coord, GigaSpaces\nObject Database \tDB4O, Objectivity/DB, Perst, Shoal, ZopeDB\nDocument Store \tClusterpoint, Couchbase, CouchDB, DocumentDB, Lotus Notes, MarkLogic, MongoDB, Qizx, RethinkDB, XML-databases\nWide Column Store \tBigTable, Cassandra, Druid, HBase, Hypertable, KAI, KDI, OpenNeptune, Qbase\n\nCorrelation databases are model-independent, and instead of row-based or column-based storage, use value-based storage.\nKey-value stores\nMain article: Key-value database\n\nKey-value (KV) stores use the associative array (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.[22][23]\n\nThe key-value model is one of the simplest non-trivial data models, and richer data models are often implemented as an extension of it. The key-value model can be extended to a discretely ordered model that maintains keys in lexicographic order. This extension is computationally powerful, in that it can efficiently retrieve selective key ranges.[24]\n\nKey-value stores can use consistency models ranging from eventual consistency to serializability. Some databases support ordering of keys. There are various hardware implementations, and some users maintain data in memory (RAM), while others employ solid-state drives or rotating disks.\n\nExamples include Oracle NoSQL Database, Redis, and dbm.\nDocument store\nMain articles: Document-oriented database and XML database\n\nThe central concept of a document store is the notion of a \"document\". While each document-oriented database implementation differs on the details of this definition, in general, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, YAML, and JSON as well as binary forms like BSON. Documents are addressed in the database via a unique key that represents that document. One of the other defining characteristics of a document-oriented database is that in addition to the key lookup performed by a key-value store, the database offers an API or query language that retrieves documents based on their contents\n\nDifferent implementations offer different ways of organizing and/or grouping documents:\n\n    Collections\n    Tags\n    Non-visible metadata\n    Directory hierarchies\n\nCompared to relational databases, for example, collections could be considered analogous to tables and documents analogous to records. But they are different: every record in a table has the same sequence of fields, while documents in a collection may have fields that are completely different.\nGraph\nMain article: Graph database\n\nThis kind of database is designed for data whose relations are well represented as a graph consisting of elements interconnected with a finite number of relations between them. The type of data could be social relations, public transport links, road maps or network topologies.\n\nGraph databases and their query language\n\nName \tLanguage(s) \tNotes\nAllegroGraph \tSPARQL \tRDF triple store\nDEX/Sparksee \tC++, Java, .NET, Python \tGraph database\nFlockDB \tScala \tGraph database\nIBM DB2 \tSPARQL \tRDF triple store added in DB2 10\nInfiniteGraph \tJava \tGraph database\nMarkLogic \tJava, JavaScript, SPARQL, XQuery \tMulti-model document database and RDF triple store\nNeo4j \tCypher \tGraph database\nOWLIM \tJava, SPARQL 1.1 \tRDF triple store\nOracle \tSPARQL 1.1 \tRDF triple store added in 11g\nOrientDB \tJava \tMulti-model document and graph database\nSqrrl Enterprise \tJava \tGraph database\nOpenLink Virtuoso \tC++, C#, Java, SPARQL \tMiddleware and database engine hybrid\nStardog \tJava, SPARQL \tGraph database\nObject database\nMain article: Object database\n\n    db4o\n    GemStone/S\n    InterSystems Caché\n    JADE\n    NeoDatis ODB\n    ObjectDatabase++\n    ObjectDB\n    Objectivity/DB\n    ObjectStore\n    ODABA\n    Perst\n    OpenLink Virtuoso\n    Versant Object Database\n    ZODB\n\nTabular\n\n    Apache Accumulo\n    BigTable\n    Apache Hbase\n    Hypertable\n    Mnesia\n    OpenLink Virtuoso\n\nTuple store\n\n    Apache River\n    GigaSpaces\n    Tarantool\n    TIBCO ActiveSpaces\n    OpenLink Virtuoso\n\nTriple/quad store (RDF) database\nMain articles: Triplestore and Named graph\n\n    AllegroGraph\n    Apache JENA (It is a framework, not a database)\n    MarkLogic\n    Ontotext-OWLIM\n    Oracle NoSQL database\n    SparkleDB\n    Virtuoso Universal Server\n    Stardog\n\nHosted\n\n    Amazon DynamoDB\n    Amazon SimpleDB\n    Datastore on Google Appengine\n    Clusterpoint database\n    Cloudant Data Layer (CouchDB)\n    Freebase\n    Microsoft Azure Tables[25]\n    Microsoft Azure DocumentDB[26]\n    OpenLink Virtuoso\n    Drenel Hosted MongoDB\n\nMultivalue databases\n\n    D3 Pick database\n    Extensible Storage Engine (ESE/NT)\n    InfinityDB\n    InterSystems Caché\n    jBASE Pick database\n    Northgate Information Solutions Reality, the original Pick/MV Database\n    OpenQM\n    Revelation Software's OpenInsight\n    Rocket U2\n\nMultimodel database\n\n    OrientDB\n    FoundationDB\n    ArangoDB\n    MarkLogic\n\nPerformance\n\nBen Scofield rated different categories of NoSQL databases as follows:[27]\nData Model \tPerformance \tScalability \tFlexibility \tComplexity \tFunctionality\nKey–Value Store \thigh \thigh \thigh \tnone \tvariable (none)\nColumn-Oriented Store \thigh \thigh \tmoderate \tlow \tminimal\nDocument-Oriented Store \thigh \tvariable (high) \thigh \tlow \tvariable (low)\nGraph Database \tvariable \tvariable \thigh \thigh \tgraph theory\nRelational Database \tvariable \tvariable \tlow \tmoderate \trelational algebra\n\nPerformance and scalability comparisons are sometimes done with the YCSB benchmark.\nSee also: Comparison of structured storage software\nHandling relational data\n\nSince most NoSQL databases lack ability for joins in queries, the database schema generally needs to be designed differently. There are three main techniques for handling relational data in a NoSQL database. (See table Join and ACID Support for NoSQL databases that support joins.)\nMultiple queries\n\nInstead of retrieving all the data with one query, it's common to do several queries to get the desired data. NoSQL queries are often faster than traditional SQL queries so the cost of having to do additional queries may be acceptable. If an excessive number of queries would be necessary, one of the other two approaches is more appropriate.\nCaching/replication/non-normalized data\n\nInstead of only storing foreign keys, it's common to store actual foreign values along with the model's data. For example, each blog comment might include the username in addition to a user id, thus providing easy access to the username without requiring another lookup. When a username changes however, this will now need to be changed in many places in the database. Thus this approach works better when reads are much more common than writes.[28]\nNesting data\n\nWith document databases like MongoDB it's common to put more data in a smaller number of collections. For example, in a blogging application, one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments. Thus in this approach a single document contains all the data you need for a specific task.\nACID and JOIN Support\n\nIf a database is marked as supporting ACID or joins, then the documentation for the database makes that claim. The degree to which the capability is fully supported in a manner similar to most SQL databases or the degree to which it meets the needs of a specific application is left up to the reader to assess.\nDatabase \tACID \tJoins\nAerospike \tYes \tNo\nArangoDB \tYes \tYes\nCouchDB \tYes \tYes\nc-treeACE \tYes \tYes\nHyperDex \tYes[nb 1] \tYes\nInfinityDB \tYes \tNo\nLMDB \tYes \tNo\nMarkLogic \tYes \tYes[nb 2]\nOrientDB \tYes \tYes\n\nHyperDex currently offers ACID support via its Warp extension, which is a commercial add-on.\n\n    Joins do not necessarily apply to document databases, but MarkLogic can do joins using semantics.[29]\n\nSee also\n\n    CAP theorem\n    Comparison of object database management systems\n    Comparison of structured storage software\n    Correlation database\n    Distributed cache\n    Faceted search\n    MultiValue database\n    Multi-model database\n    Triplestore", "skillName": "DB1."}
{"id": 162, "category": "Databases", "skillText": "The hierarchical database model mandates that each child record has only one parent, whereas each parent record can have one or more child records. In order to retrieve data from a hierarchical database the whole tree needs to be traversed starting from the root node. This model is recognized as the first database model created by IBM in the 1960s[citation needed].\n\nContents\n\n    1 History\n    2 Examples of hierarchical data represented as relational tables\n    3 See also\n    4 References\n    5 External links\n\nHistory\n\nThe hierarchical structure was developed by IBM in the 1960s, and used in early mainframe DBMS. Records' relationships form a treelike model. This structure is simple but inflexible because the relationship is confined to a one-to-many relationship. The IBM Information Management System (IMS) and the RDM Mobile are examples of a hierarchical database system with multiple hierarchies over the same data. RDM Mobile is a newly designed embedded database for a mobile computer system.[citation needed]\n\nThe hierarchical data model lost traction as Codd's relational model became the de facto standard used by virtually all mainstream database management systems. A relational-database implementation of a hierarchical model was first discussed in published form in 1992[1] (see also nested set model). Hierarchical data organization schemes resurfaced with the advent of XML in the late 1990s[2] (see also XML database). The hierarchical structure is used primarily today for storing geographic information and file systems.[citation needed]\n\nCurrently hierarchical databases are still widely used especially in applications that require very high performance and availability such as banking and telecommunications. One of the most widely used commercial hierarchical databases is IMS.[3] Another example of the use of hierarchical databases is Windows Registry in the Microsoft Windows operating systems.[4]\nExamples of hierarchical data represented as relational tables\n\nAn organization could store employee information in a table that contains attributes/columns such as employee number, first name, last name, and department number. The organization provides each employee with computer hardware as needed, but computer equipment may only be used by the employee to which it is assigned. The organization could store the computer hardware information in a separate table that includes each part's serial number, type, and the employee that uses it. The tables might look like this:\nemployee table EmpNo \tFirst Name \tLast Name \tDept. Num\n100 \tsweet sonija \tMohommad \t10-L\n101 \tkhalid ayub \tHashim \t10-L\n102 \tAbd e Wahab \tAW \t20-B\n103 \tsaadoo \tSandakelum \t20-B\n\t\ncomputer table Serial Num \tType \tUser EmpNo\n3009734-4 \tComputer \t100\n3-23-283742 \tMonitor \t100\n2-22-723423 \tMonitor \t100\n232342 \tPrinter \t100\n\nIn this model, the employee data table represents the \"parent\" part of the hierarchy, while the computer table represents the \"child\" part of the hierarchy. In contrast to tree structures usually found in computer software algorithms, in this model the children point to the parents. As shown, each employee may possess several pieces of computer equipment, but each individual piece of computer equipment may have only one employee owner.\n\nConsider the following structure:\nEmpNo \tDesignation \tReportsTo\n10 \tDirector \t\n20 \tSenior Manager \t10\n30 \tTypist \t20\n40 \tProgrammer \t20\n\nIn this, the \"child\" is the same type as the \"parent\". The hierarchy stating EmpNo 10 is boss of 20, and 30 and 40 each report to 20 is represented by the \"ReportsTo\" column. In Relational database terms, the ReportsTo column is a foreign key referencing the EmpNo column. If the \"child\" data type were different, it would be in a different table, but there would still be a foreign key referencing the EmpNo column of the employees table.\n\nThis simple model is commonly known as the adjacency list model, and was introduced by Dr. Edgar F. Codd after initial criticisms surfaced that the relational model could not model hierarchical data.\nSee also\n\n    Tree structure\n    Hierarchical query\n    Hierarchical clustering", "skillName": "DB5."}
{"id": 163, "category": "Databases", "skillText": "In mathematics graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices, nodes, or points which are connected by edges, arcs, or lines. A graph may be undirected, meaning that there is no distinction between the two vertices associated with each edge, or its edges may be directed from one vertex to another; see Graph (discrete mathematics) for more detailed definitions and for other variations in the types of graph that are commonly considered. Graphs are one of the prime objects of study in discrete mathematics.\n\nRefer to the glossary of graph theory for basic definitions in graph theory.\n\nContents\n\n    1 Definitions\n        1.1 Graph\n    2 Applications\n    3 History\n    4 Graph drawing\n    5 Graph-theoretic data structures\n    6 Problems in graph theory\n        6.1 Enumeration\n        6.2 Subgraphs, induced subgraphs, and minors\n        6.3 Graph coloring\n        6.4 Subsumption and unification\n        6.5 Route problems\n        6.6 Network flow\n        6.7 Visibility problems\n        6.8 Covering problems\n        6.9 Decomposition problems\n        6.10 Graph classes\n    7 See also\n        7.1 Related topics\n        7.2 Algorithms\n        7.3 Subareas\n        7.4 Related areas of mathematics\n        7.5 Generalizations\n        7.6 Prominent graph theorists\n    8 Notes\n    9 References\n    10 External links\n        10.1 Online textbooks\n\nDefinitions\n\nDefinitions in graph theory vary. The following are some of the more basic ways of defining graphs and related mathematical structures.\nGraph\n\nIn the most common sense of the term,[1] a graph is an ordered pair G = (V, E) comprising a set V of vertices or nodes or points together with a set E of edges or arcs or lines, which are 2-element subsets of V (i.e. an edge is related with two vertices, and the relation is represented as an unordered pair of the vertices with respect to the particular edge). To avoid ambiguity, this type of graph may be described precisely as undirected and simple.\n\nOther senses of graph stem from different conceptions of the edge set. In one more generalized notion,[2] V is a set together with a relation of incidence that associates with each edge two vertices. In another generalized notion, E is a multiset of unordered pairs of (not necessarily distinct) vertices. Many authors call this type of object a multigraph or pseudograph.\n\nAll of these variants and others are described more fully below.\n\nThe vertices belonging to an edge are called the ends or end vertices of the edge. A vertex may exist in a graph and not belong to an edge.\n\nV and E are usually taken to be finite, and many of the well-known results are not true (or are rather different) for infinite graphs because many of the arguments fail in the infinite case. The order of a graph is |V|, its number of vertices. The size of a graph is |E|, its number of edges. The degree or valency of a vertex is the number of edges that connect to it, where an edge that connects a vertex to itself (a loop) is counted twice.\n\nFor an edge {x, y}, graph theorists usually use the somewhat shorter notation xy.\nApplications\nThe network graph formed by Wikipedia editors (edges) contributing to different Wikipedia language versions (vertices) during one month in summer 2013[3]\n\nGraphs can be used to model many types of relations and processes in physical, biological,[4] social and information systems. Many practical problems can be represented by graphs. Emphasizing their application to real-world systems, the term network is sometimes defined to mean a graph in which attributes (e.g. names) are associated with the nodes and/or edges.\n\nIn computer science, graphs are used to represent networks of communication, data organization, computational devices, the flow of computation, etc. For instance, the link structure of a website can be represented by a directed graph, in which the vertices represent web pages and directed edges represent links from one page to another. A similar approach can be taken to problems in social media,[5] travel, biology, computer chip design, and many other fields. The development of algorithms to handle graphs is therefore of major interest in computer science. The transformation of graphs is often formalized and represented by graph rewrite systems. Complementary to graph transformation systems focusing on rule-based in-memory manipulation of graphs are graph databases geared towards transaction-safe, persistent storing and querying of graph-structured data.\n\nGraph-theoretic methods, in various forms, have proven particularly useful in linguistics, since natural language often lends itself well to discrete structure. Traditionally, syntax and compositional semantics follow tree-based structures, whose expressive power lies in the principle of compositionality, modeled in a hierarchical graph. More contemporary approaches such as head-driven phrase structure grammar model the syntax of natural language using typed feature structures, which are directed acyclic graphs. Within lexical semantics, especially as applied to computers, modeling word meaning is easier when a given word is understood in terms of related words; semantic networks are therefore important in computational linguistics. Still other methods in phonology (e.g. optimality theory, which uses lattice graphs) and morphology (e.g. finite-state morphology, using finite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such as TextGraphs \n, as well as various 'Net' projects, such as WordNet, VerbNet, and others.\n\nGraph theory is also used to study molecules in chemistry and physics. In condensed matter physics, the three-dimensional structure of complicated simulated atomic structures can be studied quantitatively by gathering statistics on graph-theoretic properties related to the topology of the atoms. In chemistry a graph makes a natural model for a molecule, where vertices represent atoms and edges bonds. This approach is especially used in computer processing of molecular structures, ranging from chemical editors to database searching. In statistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such systems. Similarly, in computational neuroscience graphs can be used to represent functional connections between brain areas that interact to give rise to various cognitive processes, where the vertices represent different areas of the brain and the edges represent the connections between those areas. Graphs are also used to represent the micro-scale channels of porous media, in which the vertices represent the pores and the edges represent the smaller channels connecting the pores.\n\nGraph theory is also widely used in sociology as a way, for example, to measure actors' prestige or to explore rumor spreading, notably through the use of social network analysis software. Under the umbrella of social networks are many different types of graphs.[6] Acquaintanceship and friendship graphs describe whether people know each other. Influence graphs model whether certain people can influence the behavior of others. Finally, collaboration graphs model whether two people work together in a particular way, such as acting in a movie together.\n\nLikewise, graph theory is useful in biology and conservation efforts where a vertex can represent regions where certain species exist (or inhabit) and the edges represent migration paths, or movement between the regions. This information is important when looking at breeding patterns or tracking the spread of disease, parasites or how changes to the movement can affect other species.\n\nIn mathematics, graphs are useful in geometry and certain parts of topology such as knot theory. Algebraic graph theory has close links with group theory.\n\nA graph structure can be extended by assigning a weight to each edge of the graph. Graphs with weights, or weighted graphs, are used to represent structures in which pairwise connections have some numerical values. For example, if a graph represents a road network, the weights could represent the length of each road.\nHistory\nThe Königsberg Bridge problem\n\nThe paper written by Leonhard Euler on the Seven Bridges of Königsberg and published in 1736 is regarded as the first paper in the history of graph theory.[7] This paper, as well as the one written by Vandermonde on the knight problem, carried on with the analysis situs initiated by Leibniz. Euler's formula relating the number of edges, vertices, and faces of a convex polyhedron was studied and generalized by Cauchy[8] and L'Huillier,[9] and represents the beginning of the branch of mathematics known as topology.\n\nMore than one century after Euler's paper on the bridges of Königsberg and while Listing was introducing the concept of topology, Cayley was led by an interest in particular analytical forms arising from differential calculus to study a particular class of graphs, the trees.[10] This study had many implications for theoretical chemistry. The techniques he used mainly concern the enumeration of graphs with particular properties. Enumerative graph theory then arose from the results of Cayley and the fundamental results published by Pólya between 1935 and 1937. These were generalized by De Bruijn in 1959. Cayley linked his results on trees with contemporary studies of chemical composition.[11] The fusion of ideas from mathematics with those from chemistry began what has become part of the standard terminology of graph theory.\n\nIn particular, the term \"graph\" was introduced by Sylvester in a paper published in 1878 in Nature, where he draws an analogy between \"quantic invariants\" and \"co-variants\" of algebra and molecular diagrams:[12]\n\n    \"[…] Every invariant and co-variant thus becomes expressible by a graph precisely identical with a Kekuléan diagram or chemicograph. […] I give a rule for the geometrical multiplication of graphs, i.e. for constructing a graph to the product of in- or co-variants whose separate graphs are given. […]\" (italics as in the original).\n\nThe first textbook on graph theory was written by Dénes Kőnig, and published in 1936.[13] Another book by Frank Harary, published in 1969, was \"considered the world over to be the definitive textbook on the subject\",[14] and enabled mathematicians, chemists, electrical engineers and social scientists to talk to each other. Harary donated all of the royalties to fund the Pólya Prize.[15]\n\nOne of the most famous and stimulating problems in graph theory is the four color problem: \"Is it true that any map drawn in the plane may have its regions colored with four colors, in such a way that any two regions having a common border have different colors?\" This problem was first posed by Francis Guthrie in 1852 and its first written record is in a letter of De Morgan addressed to Hamilton the same year. Many incorrect proofs have been proposed, including those by Cayley, Kempe, and others. The study and the generalization of this problem by Tait, Heawood, Ramsey and Hadwiger led to the study of the colorings of the graphs embedded on surfaces with arbitrary genus. Tait's reformulation generated a new class of problems, the factorization problems, particularly studied by Petersen and Kőnig. The works of Ramsey on colorations and more specially the results obtained by Turán in 1941 was at the origin of another branch of graph theory, extremal graph theory.\n\nThe four color problem remained unsolved for more than a century. In 1969 Heinrich Heesch published a method for solving the problem using computers.[16] A computer-aided proof produced in 1976 by Kenneth Appel and Wolfgang Haken makes fundamental use of the notion of \"discharging\" developed by Heesch.[17][18] The proof involved checking the properties of 1,936 configurations by computer, and was not fully accepted at the time due to its complexity. A simpler proof considering only 633 configurations was given twenty years later by Robertson, Seymour, Sanders and Thomas.[19]\n\nThe autonomous development of topology from 1860 and 1930 fertilized graph theory back through the works of Jordan, Kuratowski and Whitney. Another important factor of common development of graph theory and topology came from the use of the techniques of modern algebra. The first example of such a use comes from the work of the physicist Gustav Kirchhoff, who published in 1845 his Kirchhoff's circuit laws for calculating the voltage and current in electric circuits.\n\nThe introduction of probabilistic methods in graph theory, especially in the study of Erdős and Rényi of the asymptotic probability of graph connectivity, gave rise to yet another branch, known as random graph theory, which has been a fruitful source of graph-theoretic results.\nGraph drawing\nMain article: Graph drawing\n\nGraphs are represented visually by drawing a dot or circle for every vertex, and drawing an arc between two vertices if they are connected by an edge. If the graph is directed, the direction is indicated by drawing an arrow.\n\nA graph drawing should not be confused with the graph itself (the abstract, non-visual structure) as there are several ways to structure the graph drawing. All that matters is which vertices are connected to which others by how many edges and not the exact layout. In practice it is often difficult to decide if two drawings represent the same graph. Depending on the problem domain some layouts may be better suited and easier to understand than others.\n\nThe pioneering work of W. T. Tutte was very influential in the subject of graph drawing. Among other achievements, he introduced the use of linear algebraic methods to obtain graph drawings.\n\nGraph drawing also can be said to encompass problems that deal with the crossing number and its various generalizations. The crossing number of a graph is the minimum number of intersections between edges that a drawing of the graph in the plane must contain. For a planar graph, the crossing number is zero by definition.\n\nDrawings on surfaces other than the plane are also studied.\nGraph-theoretic data structures\nMain article: Graph (abstract data type)\n\nThere are different ways to store graphs in a computer system. The data structure used depends on both the graph structure and the algorithm used for manipulating the graph. Theoretically one can distinguish between list and matrix structures but in concrete applications the best structure is often a combination of both. List structures are often preferred for sparse graphs as they have smaller memory requirements. Matrix structures on the other hand provide faster access for some applications but can consume huge amounts of memory.\n\nList structures include the incidence list, an array of pairs of vertices, and the adjacency list, which separately lists the neighbors of each vertex: Much like the incidence list, each vertex has a list of which vertices it is adjacent to.\n\nMatrix structures include the incidence matrix, a matrix of 0's and 1's whose rows represent vertices and whose columns represent edges, and the adjacency matrix, in which both the rows and columns are indexed by vertices. In both cases a 1 indicates two adjacent objects and a 0 indicates two non-adjacent objects. The Laplacian matrix is a modified form of the adjacency matrix that incorporates information about the degrees of the vertices, and is useful in some calculations such as Kirchhoff's theorem on the number of spanning trees of a graph. The distance matrix, like the adjacency matrix, has both its rows and columns indexed by vertices, but rather than containing a 0 or a 1 in each cell it contains the length of a shortest path between two vertices.\nProblems in graph theory\nEnumeration\n\nThere is a large literature on graphical enumeration: the problem of counting graphs meeting specified conditions. Some of this work is found in Harary and Palmer (1973).\nSubgraphs, induced subgraphs, and minors\n\nA common problem, called the subgraph isomorphism problem, is finding a fixed graph as a subgraph in a given graph. One reason to be interested in such a question is that many graph properties are hereditary for subgraphs, which means that a graph has the property if and only if all subgraphs have it too. Unfortunately, finding maximal subgraphs of a certain kind is often an NP-complete problem. For example:\n\n    Finding the largest complete subgraph is called the clique problem (NP-complete).\n\nA similar problem is finding induced subgraphs in a given graph. Again, some important graph properties are hereditary with respect to induced subgraphs, which means that a graph has a property if and only if all induced subgraphs also have it. Finding maximal induced subgraphs of a certain kind is also often NP-complete. For example:\n\n    Finding the largest edgeless induced subgraph or independent set is called the independent set problem (NP-complete).\n\nStill another such problem, the minor containment problem, is to find a fixed graph as a minor of a given graph. A minor or subcontraction of a graph is any graph obtained by taking a subgraph and contracting some (or no) edges. Many graph properties are hereditary for minors, which means that a graph has a property if and only if all minors have it too. For example, Wagner's Theorem states:\n\n    A graph is planar if it contains as a minor neither the complete bipartite graph K3,3 (see the Three-cottage problem) nor the complete graph K5.\n\nA similar problem, the subdivision containment problem, is to find a fixed graph as a subdivision of a given graph. A subdivision or homeomorphism of a graph is any graph obtained by subdividing some (or no) edges. Subdivision containment is related to graph properties such as planarity. For example, Kuratowski's Theorem states:\n\n    A graph is planar if it contains as a subdivision neither the complete bipartite graph K3,3 nor the complete graph K5.\n\nAnother problem in subdivision containment is Kelmans-Seymour conjecture:\n\n    Every 5-vertex-connected graph that is not planar contains a subdivision of the 5-vertex complete graph K5.\n\nAnother class of problems has to do with the extent to which various species and generalizations of graphs are determined by their point-deleted subgraphs. For example:\n\n    The reconstruction conjecture\n\nGraph coloring\n\nMany problems have to do with various ways of coloring graphs, for example:\n\n    Four-color theorem\n    Strong perfect graph theorem\n    Erdős–Faber–Lovász conjecture (unsolved)\n    Total coloring conjecture, also called Behzad's conjecture (unsolved)\n    List coloring conjecture (unsolved)\n    Hadwiger conjecture (graph theory) (unsolved)\n\nSubsumption and unification\n\nConstraint modeling theories concern families of directed graphs related by a partial order. In these applications, graphs are ordered by specificity, meaning that more constrained graphs—which are more specific and thus contain a greater amount of information—are subsumed by those that are more general. Operations between graphs include evaluating the direction of a subsumption relationship between two graphs, if any, and computing graph unification. The unification of two argument graphs is defined as the most general graph (or the computation thereof) that is consistent with (i.e. contains all of the information in) the inputs, if such a graph exists; efficient unification algorithms are known.\n\nFor constraint frameworks which are strictly compositional, graph unification is the sufficient satisfiability and combination function. Well-known applications include automatic theorem proving and modeling the elaboration of linguistic structure.\nRoute problems\n\n    Hamiltonian path problem\n    Minimum spanning tree\n    Route inspection problem (also called the \"Chinese postman problem\")\n    Seven bridges of Königsberg\n    Shortest path problem\n    Steiner tree\n    Three-cottage problem\n    Traveling salesman problem (NP-hard)\n\nNetwork flow\n\nThere are numerous problems arising especially from applications that have to do with various notions of flows in networks, for example:\n\n    Max flow min cut theorem\n\nVisibility problems\n\n    Museum guard problem\n\nCovering problems\n\nCovering problems in graphs are specific instances of subgraph-finding problems, and they tend to be closely related to the clique problem or the independent set problem.\n\n    Set cover problem\n    Vertex cover problem\n\nDecomposition problems\n\nDecomposition, defined as partitioning the edge set of a graph (with as many vertices as necessary accompanying the edges of each part of the partition), has a wide variety of question. Often, it is required to decompose a graph into subgraphs isomorphic to a fixed graph; for instance, decomposing a complete graph into Hamiltonian cycles. Other problems specify a family of graphs into which a given graph should be decomposed, for instance, a family of cycles, or decomposing a complete graph Kn into n − 1 specified trees having, respectively, 1, 2, 3, …, n − 1 edges.\n\nSome specific decomposition problems that have been studied include:\n\n    Arboricity, a decomposition into as few forests as possible\n    Cycle double cover, a decomposition into a collection of cycles covering each edge exactly twice\n    Edge coloring, a decomposition into as few matchings as possible\n    Graph factorization, a decomposition of a regular graph into regular subgraphs of given degrees\n\nGraph classes\n\nMany problems involve characterizing the members of various classes of graphs. Some examples of such questions are below:\n\n    Enumerating the members of a class\n    Characterizing a class in terms of forbidden substructures\n    Ascertaining relationships among classes (e.g. does one property of graphs imply another)\n    Finding efficient algorithms to decide membership in a class\n    Finding representations for members of a class\n\nSee also\n\n    Gallery of named graphs\n    Glossary of graph theory\n    List of graph theory topics\n    List of unsolved problems in graph theory\n    Publications in graph theory\n\nRelated topics\n\n    Algebraic graph theory\n    Citation graph\n    Conceptual graph\n    Data structure\n    Disjoint-set data structure\n    Dual-phase evolution\n    Entitative graph\n    Existential graph\n    Graph algebra\n    Graph automorphism\n    Graph coloring\n    Graph database\n    Graph data structure\n    Graph drawing\n    Graph equation\n    Graph rewriting\n    Graph sandwich problem\n    Graph property\n    Intersection graph\n    Logical graph\n    Loop\n    Network theory\n    Null graph\n    Pebble motion problems\n    Percolation\n    Perfect graph\n    Quantum graph\n    Random regular graphs\n    Semantic networks\n    Spectral graph theory\n    Strongly regular graphs\n    Symmetric graphs\n    Transitive reduction\n    Tree data structure\n\nAlgorithms\n\n    Bellman–Ford algorithm\n    Dijkstra's algorithm\n    Ford–Fulkerson algorithm\n    Kruskal's algorithm\n    Nearest neighbour algorithm\n    Prim's algorithm\n    Depth-first search\n    Breadth-first search\n\nSubareas\n\n    Algebraic graph theory\n    Geometric graph theory\n    Extremal graph theory\n    Probabilistic graph theory\n    Topological graph theory\n\nRelated areas of mathematics\n\n    Combinatorics\n    Group theory\n    Knot theory\n    Ramsey theory\n\nGeneralizations\n\n    Hypergraph\n    Abstract simplicial complex\n\nProminent graph theorists\n\n    Alon, Noga\n    Berge, Claude\n    Bollobás, Béla\n    Bondy, Adrian John\n    Brightwell, Graham\n    Chudnovsky, Maria\n    Chung, Fan\n    Dirac, Gabriel Andrew\n    Erdős, Paul\n    Euler, Leonhard\n    Faudree, Ralph\n    Golumbic, Martin\n    Graham, Ronald\n    Harary, Frank\n    Heawood, Percy John\n    Kotzig, Anton\n    Kőnig, Dénes\n    Lovász, László\n    Murty, U. S. R.\n    Nešetřil, Jaroslav\n    Rényi, Alfréd\n    Ringel, Gerhard\n    Robertson, Neil\n    Seymour, Paul\n    Szemerédi, Endre\n    Thomas, Robin\n    Thomassen, Carsten\n    Turán, Pál\n    Tutte, W. T.\n    Whitney, Hassler", "skillName": "DB6."}
{"id": 164, "category": "Databases", "skillText": "Database, also called electronic database, any collection of data, or information, that is specially organized for rapid search and retrieval by a computer. Databases are structured to facilitate the storage, retrieval, modification, and deletion of data in conjunction with various data-processing operations. A database management system (DBMS) extracts information from the database in response to queries.\n\nA brief treatment of databases follows. For full treatment, see computer science: Information systems and databases; information processing.\n\nA database is stored as a file or a set of files on magnetic disk or tape, optical disk, or some other secondary storage device. The information in these files may be broken down into records, each of which consists of one or more fields. Fields are the basic units of data storage, and each field typically contains information pertaining to one aspect or attribute of the entity described by the database. Records are also organized into tables that include information about relationships between its various fields. Although database is applied loosely to any collection of information in computer files, a database in the strict sense provides cross-referencing capabilities. Using keywords and various sorting commands, users can rapidly search, rearrange, group, and select the fields in many records to retrieve or create reports on particular aggregates of data.\n\nDatabase records and files must be organized to allow retrieval of the information. Queries are the main way users retrieve database information. The power of a DBMS comes from its ability to define new relationships from the basic ones given by the tables and to use them to get responses to queries. Typically, the user provides a string of characters, and the computer searches the database for a corresponding sequence and provides the source materials in which those characters appear; a user can request, for example, all records in which the contents of the field for a person’s last name is the word Smith.\nSimilar Topics\n\n    data compression\n    byte\n    data processing\n    data structure\n    bibliography\n    data mining\n    bit\n    information retrieval\n    library classification\n    information science\n\nThe many users of a large database must be able to manipulate the information within it quickly at any given time. Moreover, large business and other organizations tend to build up many independent files containing related and even overlapping data, and their data-processing activities often require the linking of data from several files. Several different types of DBMS have been developed to support these requirements: flat, hierarchical, network, relational, and object-oriented.\n\nEarly systems were arranged sequentially (i.e., alphabetically, numerically, or chronologically); the development of direct-access storage devices made possible random access to data via indexes. In flat databases, records are organized according to a simple list of entities; many simple databases for personal computers are flat in structure. The records in hierarchical databases are organized in a treelike structure, with each level of records branching off into a set of smaller categories. Unlike hierarchical databases, which provide single links between sets of records at different levels, network databases create multiple linkages between sets by placing links, or pointers, to one set of records in another; the speed and versatility of network databases have led to their wide use within businesses and in e-commerce. Relational databases are used where associations between files or records cannot be expressed by links; a simple flat list becomes one row of a table, or “relation,” and multiple relations can be mathematically associated to yield desired information. Various iterations of SQL (Structured Query Language) are widely employed in DBMS for relational databases. Object-oriented databases store and manipulate more complex data structures, called “objects,” which are organized into hierarchical classes that may inherit properties from classes higher in the chain; this database structure is the most flexible and adaptable.\n\nThe information in many databases consists of natural-language texts of documents; number-oriented databases primarily contain information such as statistics, tables, financial data, and raw scientific and technical data. Small databases can be maintained on personal-computer systems and may be used by individuals at home. These and larger databases have become increasingly important in business life, in part because they are now commonly designed to be integrated with other office software, including spreadsheet programs.\nBritannica Stories\n\n    In The News / Animals\n    Hawaiian Crow Discovered to Use Tools\n    Hawaiian Crow Discovered to Use Tools\n    Demystified / History\n    Who Were the Assassins?\n    Who Were the Assassins?\n    Spotlight / History\n    ¡Viva México!\n    ¡Viva México!\n    In The News / Geography\n    Obama Creates “Underwater Yosemite”\n    Obama Creates “Underwater Yosemite”\n\nSee All Storieskeyboard_arrow_right\n\nTypical commercial database applications include airline reservations, production management functions, medical records in hospitals, and legal records of insurance companies. The largest databases are usually maintained by governmental agencies, business organizations, and universities. These databases may contain texts of such materials as abstracts, reports, legal statutes, wire services, newspapers and journals, encyclopaedias, and catalogs of various kinds. Reference databases contain bibliographies or indexes that serve as guides to the location of information in books, periodicals, and other published literature. Thousands of these publicly accessible databases now exist, covering topics ranging from law, medicine, and engineering to news and current events, games, classified advertisements, and instructional courses.\nTest Your Knowledge\nComputers and Technology\nComputers and Technology\n\nIncreasingly, formerly separate databases are being combined electronically into larger collections known as data warehouses. Businesses and government agencies then employ “data mining” software to analyze multiple aspects of the data for various patterns. For example, a government agency might flag for human investigation a company or individual that purchased a suspicious quantity of certain equipment or materials, even though the purchases were spread around the country or through various subsidiaries.", "skillName": "Database2."}
{"id": 165, "category": "Databases", "skillText": "MonetDB is an open source column-oriented database management system developed at the Centrum Wiskunde & Informatica (CWI) in the Netherlands. It was designed to provide high performance on complex queries against large databases, such as combining tables with hundreds of columns and millions of rows. MonetDB has been applied in high-performance applications for online analytical processing (OLAP), data mining, GIS,[1] RDF,[2] text retrieval and sequence alignment processing.[3]\n\nContents\n\n    1 History\n    2 Architecture\n        2.1 Query Recycling\n        2.2 Database Cracking\n    3 Components\n        3.1 SQL\n        3.2 GIS\n        3.3 SciQL\n        3.4 Data Vaults\n            3.4.1 SAM/BAM\n        3.5 RDF/SPARQL\n        3.6 R integration\n        3.7 Python integration\n        3.8 MonetDBLite\n        3.9 Former extensions\n    4 See also\n    5 References\n    6 Bibliography\n    7 External links\n\nHistory\nThe older MonetDB logo\n\nData mining projects in the 1990s required improved analytical database support. This resulted in a CWI spin-off called Data Distilleries, which used early MonetDB implementations in its analytical suite. Data Distilleries eventually became a subsidiary of SPSS in 2003, which in turn was acquired by IBM in 2009.[4]\n\nMonetDB in its current form was first created in 2002 by doctoral student Peter Alexander Boncz \nand professor Martin L. Kersten as part of the 1990s' MAGNUM research project at University of Amsterdam.[5] It was initially called simply Monet, after the French impressionist painter Claude Monet. The first version under an open-source software license (a modified version of the Mozilla Public License) was released on September 30, 2004. When MonetDB version 4 was released into the open-source domain and many extensions to the code base were added by the MonetDB/CWI team. These included a new SQL frontend, supporting the SQL:2003 standard.[6]\n\nMonetDB introduced innovations in all layers of the DBMS: a storage model based on vertical fragmentation, a modern CPU-tuned query execution architecture that often gave MonetDB a speed advantage over the same algorithm over a typical interpreter-based RDBMS. It was one of the first database systems to tune query optimization for CPU caches. MonetDB includes automatic and self-tuning indexes, run-time query optimization, and a modular software architecture.[7][8]\n\nBy 2008, a follow-on project called X100 (MonetDB/X100) started, which evolved into the VectorWise technology. VectorWise was acquired by Actian Corporation, integrated with the Ingres database and sold as a commercial product.[9][10]\n\nIn 2011 a major effort to renovate the MonetDB codebase was started. As part of it, the code for the MonetDB 4 kernel and its XQuery components were frozen. In MonetDB 5, parts of the SQL layer were pushed into the kernel.[6] The resulting changes created a difference in internal APIs, as it transitioned from MonetDB Instruction Language (MIL) to MonetDB Assembly Language (MAL). Older, no-longer maintained top-level query interfaces were also removed. First was XQuery, which relied on MonetDB 4 and was never ported to version 5.[11] The experimental Jaql interface support was removed with the October 2014 release.[12] With the July2015 release, MonetDB gained support for read-only data sharding and persistent indices. In this release the deprecated streaming data module DataCell was also removed from the main codebase in an effort to streamline the code.[13] In addition, the license has been changed into the Mozilla Public License, version 2.0 \n.\nArchitecture\n\nMonetDB architecture is represented in three layers, each with its own set of optimizers.[14] The front-end is the top layer, providing query interface for SQL, with SciQL and SPARQL interfaces under development. Queries are parsed into domain-specific representations, like relational algebra for SQL, and optimized. The generated logical execution plans are then translated into MonetDB Assembly Language (MAL) instructions, which are passed to the next layer. The middle or back-end layer provides a number of cost-based optimizers for the MAL. The bottom layer is the database kernel, which provides access to the data stored in Binary Association Tables (BATs). Each BAT is a table consisting of an Object-identifier and value columns, representing a single column in the database.[14]\n\nMonetDB internal data representation also relies on the memory addressing ranges of contemporary CPUs using demand paging of memory mapped files, and thus departing from traditional DBMS designs involving complex management of large data stores in limited memory.\nQuery Recycling\n\nQuery recycling is an architecture for reusing the byproducts of the operator-at-a-time paradigm in a column store DBMS. Recycling makes use of the generic idea of storing and reusing the results of expensive computations. Unlike low-level instruction caches, query recycling uses an optimizer to pre-select instructions to cache. The technique is designed to improve query response times and throughput, while working in a self-organizing fashion.[15] The authors from the CWI Database Architectures group, composed of Milena Ivanova, Martin Kersten, Niels Nes and Romulo Goncalves, won the \"Best Paper Runner Up\" at annual ACM SIGMOD conference for their work on Query Recycling.[16][17]\nDatabase Cracking\n\nMonetDB was one of the first databases to introduce Database Cracking. Database Cracking is an incremental partial indexing and/or sorting of the data. It directly exploits the columnar nature of MonetDB. Cracking is a technique that shifts the cost of index maintenance from updates to query processing. The query pipeline optimizers are used to massage the query plans to crack and to propagate this information. The technique allows for improved access times and self-organized behavior.[18] Database Cracking received the ACM SIGMOD 2011 J.Gray best dissertation award.[19]\nComponents\n\nA number of extensions exist for MonetDB that extend the functionality of the database engine. Due to the three-layer architecture, top-level query interfaces can benefit from optimizations done in the backend and kernel layers.\nSQL\n\nMonetDB/SQL is a top-level extension, which provides complete support for transactions in compliance with the SQL:2003 standard.[14]\nGIS\n\nMonetDB/GIS is an extension to MonetDB/SQL with support for the Simple Features Access standard of Open Geospatial Consortium (OGC).[1]\nSciQL\n\nSciQL an SQL-based query language for science applications with arrays as first class citizens. SciQL allows MonetDB to effectively function as an array database. SciQL is used in the European Union PlanetData \nand TELEIOS \nproject, together with the Data Vault technology, providing transparent access to large scientific data repositories.[20] Data Vaults map the data from the distributed repositories to SciQL arrays, allowing for improved handling of spatio-temporal data in MonetDB.[21] SciQL will be further extended for the Human Brain Project.[22]\nData Vaults\n\nData Vault is a database-attached external file repository MonetDB, similar to the SQL/MED standard. The Data Vault technology allows for transparent integration with distributed/remote repositories file repositories. It is designed for scientific data data exploration and mining, specifically for remote sensing data.[21] There is support for the GeoTIFF (Earth observation), FITS (astronomy), MiniSEED (seismology) and NetCDF formats.[21][23] The data is stored in the file repository in the original format, and loaded in the database in a lazy fashion, only when needed. The system can also process the data upon ingestion, if the data format requires it. [24] As a result, even very large file repositories can be efficiently analyzed, as only the required data is processed in the database. The data can be accessed through either the MonetDB SQL or SciQL interfaces. The Data Vault technology was used in the European Union's TELEIOS \nproject, which was aimed at building a virtual observatory for Earth observation data.[23] Data Vaults for FITS files have also been used for processing astronomical survey data for the The INT Photometric H-Alpha Survey (IPHAS) [25][26]\nSAM/BAM\n\nMonetDB has a SAM/BAM module for efficient processing of sequence alignment data. Aimed at the bioinformatics research, the module has a SAM/BAM data loader and a set of SQL UDFs for working with DNA data.[3] The module uses the popular SAMtools library.[27]\nRDF/SPARQL\n\nMonetDB/RDF is a SPARQL-based extension for working with linked data, which adds support for RDF and allowing MonetDB to function as a triplestore. Under development for the Linked Open Data 2 project.[2]\nR integration\n\nMonetDB/R module allows for UDFs written in R to be executed in the SQL layer of the system. This is done using the native R support for running embedded in another application, inside the RDBMS in this case. Previously the MonetDB.R connector allowed the using MonetDB data sources and process them in an R session. The newer R integration feature of MonetDB does not require data to be transferred between the RDBMS and the R session, reducing overhead and improving performance. The feature is intended to give users access to functions of the R statistical software for in-line analysis of data stored in the RDBMS. It complements the existing support for C UDFs and is intended to be used for in-database processing.[28]\nPython integration\n\nSimilarly to the embedded R UDFs in MonetDB, the database now has support for UDFs written in Python/NumPy. The implementation uses Numpy arrays (themselves Python wrappers for C arrays), as a result there is limited overhead - providing a functional Python integration with speed matching native SQL functions. The Embedded Python functions also support mapped operations, allowing user to execute Python functions in parallel within SQL queries. The practical side of the feature gives users access to Python/NumPy/SciPy libraries, which can provide a large selection of statistical/analytical functions.[29]\nMonetDBLite\n\nFollowing the release of remote driver for R (MonetDB.R) and R UDFs in MonetDB (MonetDB/R), the authors created an embedded version of MonetDB in R called MonetDBLite. It is distributed as an R package, removing the need to manage a database server, required for the previous R integrations. The DBMS runs within the R process itself, eliminating socket communication and serialisation overhead - greatly improving efficiency. The idea behind it is to deliver an SQLite-like package for R, with the performance on an in-memory optimized columnar store.[30]\nFormer extensions\n\nA number of former extensions have been deprecated and removed from the stable code base over time. Some notable examples include an XQuery extension removed in MonetDB version 5; a JAQL extension, and a streaming data extension called Data Cell.[14] [31][32]", "skillName": "MonetDB."}
{"id": 166, "category": "Databases", "skillText": "Much like a relational database, one stores information in a triplestore and retrieves it via a query language. Unlike a relational database, a triplestore is optimized for the storage and retrieval of triples. In addition to queries, triples can usually be imported/exported using Resource Description Framework (RDF) and other formats.\n\nContents\n\n    1 Implementations\n    2 Related database types\n    3 See also\n    4 References\n    5 External links\n\nImplementations\nMain article: List of subject-predicate-object databases\n\nSome triplestores have been built as database engines from scratch, while others have been built on top of existing commercial relational database engines (e.g., SQL-based),[2] or NoSQL document-oriented database engines.[3][4] Like the early development of online analytical processing (OLAP) databases, this intermediate approach allowed large and powerful database engines to be constructed for little programming effort in the initial phases of triplestore development. Long-term though it seems likely that native triplestores will have the advantage for performance. A difficulty with implementing triplestores over SQL is that although triples may thus be stored, implementing efficient querying of a graph-based RDF model (e.g., mapping from SPARQL) onto SQL queries is difficult.[5]\nRelated database types\n\nAdding a name to the triple makes a \"quad store\" or named graph.\n\nA graph database has a more generalized structure than triplestore. Uses graph structures with nodes, edges, and properties to represent and store data. Provides index-free adjacency, meaning every element contains a direct pointer to its adjacent elements and no index lookups are necessary. General graph databases that can store any graph are distinct from specialized graph databases such as triplestores and network databases.\nSee also\n\tThis article may require cleanup to meet Wikipedia's quality standards. The specific problem is: prose in \"See also\" section Please help improve this article if you can. (August 2015) (Learn how and when to remove this template message)\n\n    Dataspaces - notes that fact-based, subject-predicate-object triples (data entities) rely on existing matching and mapping generation techniques. The triple data structure allows a pay-as-you-go approach to data integration which effectively postpones the labor-intensive aspects of integration to the very end, just before the integrated data is absolutely needed.\n    Entity–relationship model - covers entities (things) and the relationships that can exist among them.\n    ISO/IEC 19788 - Metadata for learning resources (MLR). In a MLR triple, the subject is always the literal of an identifier of the learning resource, such as a URI or ISBN. The predicate is also a literal, the MLR data element specification identifier. Finally, the object can be a literal or a resource class (a set of accepted values, such as a list of terms identifiers from a controlled vocabulary list).\n    Metaweb's Graphd \n    tuple store (owned by Google) used in Freebase and Knowledge Graph\n    Metadata - syntax section - subject-predicate-object triple a/k/a class-attribute-value triple. The first two elements of the triple (class, attribute) are pieces of some structural metadata having a defined semantic. The third element is a value, preferably from some controlled vocabulary, some reference (master) data. The combination of the metadata and master data elements results in a statement which is a metacontent statement i.e. \"metacontent = metadata + master data\". All these elements can be thought of as vocabulary. Both metadata and master data are vocabularies which can be assembled into metacontent statements. There are many sources of these vocabularies, both meta and master data: UML, EDIFACT, XSD, Dewey/UDC/LoC, SKOS, ISO-25964, Pantone, Linnaean Binomial Nomenclature, etc. Using controlled vocabularies for the components of metacontent statements, whether for indexing or finding, is endorsed by ISO-25964: If both the indexer and the searcher are guided to choose the same term for the same concept, then relevant documents will be retrieved.\n    Outline of databases\n    Semantic data model - covers semantic information, symbols (instance data), meaning from instances, facts as binary relations between data elements. Object-RelationType-Object'\n    RDFLib - a Python library for working with RDF including both in-memory and persistent Graph backends. Supports subject-predicate-object triple pattern matching.\n    Semantic wiki and Semantic MediaWiki - illustrates subject-predicate-object support for Wikis, advanced query support, and implementations by organizations including: Pfizer, Harvard Pilgrim Health Care, Johnson & Johnson Pharmaceutical Research and Development, Pacific Northwest National Laboratory,Metropolitan Museum of Art, and the U.S. Department of Defense.\n    SPARQL W3C specification involving subject-predicate-object triples and List of SPARQL implementations\n\nRelational databases like MySQL, PostgreSQL and SQLite3 represent and store data in tables and rows. They're based on a branch of algebraic set theory known as relational algebra. Meanwhile, non-relational databases like MongoDB represent data in collections of JSON documents. The Mongo import utility can import JSON, CSV and TSV file formats. Mongo query targets of data are technically represented as BSON (binary JASON).\n\nRelational databases use Structured Querying Language (SQL), making them a good choice for applications that involve the management of several transactions. The structure of a relational database allows you to link information from different tables through the use of foreign keys (or indexes), which are used to uniquely identify any atomic piece of data within that table. Other tables may refer to that foreign key, so as to create a link between their data pieces and the piece pointed to by the foreign key. This comes in handy for applications that are heavy into data analysis.\n\nIf you want your application to handle a lot of complicated querying, database transactions and routine analysis of data, you’ll probably want to stick with a relational database. And if your application is going to focus on doing many database transactions, it’s important that those transactions are processed reliably. This is where ACID (the set of properties that guarantee database transactions are processed reliably) really matters, and where referential integrity comes into play.\nReferential integrity (and minimizing ORM Impedance Mismatch)\n\nReferential integrity is the concept in which multiple database tables share a relationship based on the data stored in the tables, and that relationship must remain consistent. This is usually enforced with cascading actions of adding, deleting and updating. To illustrate an example of enforcing referential integrity, let’s consider an app that helps victims of human trafficking locate a safe house and access victim services in real time.\n\nSuppose city or county X has two tables; a Trafficking Victim Shelter table and a Trafficking Shelter Funding table. In the Trafficking Shelter table we have two columns; the Shelter ID (which could be its EIN/FEIN number) and the name of the shelter. In the Trafficking Shelter Funding table, we also have two columns; the Shelter ID and the amount of funding received for that given Shelter ID. Now, suppose a dearth in funding forced Shelter A in city/county X to close its doors. We would need to remove that shelter from locale X since it’s no longer in existence. And since Shelter A also exists in the Shelter Funding table, we need to remove it from there as well. By enforcing referential integrity, we can make this accurate -- and with minimal headaches.\n\nHere’s how:\n\nFirst, define the Shelter ID column in the Shelter table to be our primary key. Then, define the Shelter ID column in the Shelter Funding table to be a foreign key that points to a primary key (that is the Shelter ID column in the Shelter table). Once we define our foreign-to-primary key relationship, we need to add constraints.  One constraint in particular is known as cascading delete. This means that anytime a shelter is deleted from the Shelter table in our database, all entries for that same shelter would be automatically removed from the Shelter Funding table.\n\nRelate_01\n\nNow, take note of what was designated as the primary key, and why. In our little example of anti-trafficking charities, every non-profit NGO with 501(3)c status is issued an EIN, much like an individual’s social security number. So, in tables where other data is linked to any particular trafficking victim’s shelter in the shelter table, it makes sense to have that unique identifier serve as the primary key and to have the foreign keys point to it.\n\nRelate_02\n\nKeep in mind, there are three rules that referential integrity enforces:\n\n\n    We may not add a record to the Shelter Funding table unless the foreign key for that record points to an existing shelter in the Shelter table. You can think of this as a “No Unattended Child” rule or a “No Orphans” rule.\n\n    If a record in the shelter table is deleted, all corresponding records in the Shelter Funding table must also be deleted. The best way to handle this is by using cascade delete.\n\n    If the primary key for a record in the Shelter table changes, all corresponding records in the Shelter Funding (and other possible future tables with data relating to the Shelter table) must also be modified using something called a cascade update.\n\n\nThe burden of instilling and maintaining referential integrity rests on the person who designs the database schema. If designing a database schema seems like a daunting task, consider this: Prior to 1970 (when the relational database was born) all databases were flat; data was stored in a long text file called a tab delimited file where each entry was separated by the pipe character (“|”). Searching for specific information to compare and analyze was a difficult, tedious, time-consuming endeavor. With relational databases you can easily search, sort and analyze (usually for comparison to other data purposes) specific pieces of data without having to search sequentially through an entire file (or database), including all the data pieces you’re not interested in.\n\nIn the previous example of a relational database (Postgresql), we don’t need to search through an entire database worth of information just to find the information on a shelter that either had its funding slashed or that was forced to close for lack of funds. We can use a simple SQL query to find which shelters closed in a particular region or locale without having to traverse all of the data, including shelters not in that specific area, by using a an SQL SELECT * FROM statement.\n\nObject Relational Mapping (ORM) refers to the programmatic process of converting data between incompatible type systems in object-oriented programming languages (like Ruby). In the context of a Ruby program (a Rails app in particular), the concept of ORM libraries was briefly discussed in our tutorial on Getting started with Rails.\nWhen to non-relate\n\nWhile relational databases are great, they do come with trade-offs. One of those is ORM Impedence Mismatching, because relational databases were not initially created with the OOP languages in mind. The best way to avoid this issue is to create your database schema with referential integrity at its core. So, when using a relational database with an OOP (like Ruby), you have to think about how to set up your primary and foreign keys, the use of constraints (including the cascade delete and update), and how you write your migrations.\n\nBut, if you’re dealing with a phenomenally huge amount of data, it can be way too tedious, and the probability of error (in the form of an ORM Impedance Mismatch issue) increases. In that situation you may need to consider going with a non-relational database. A non-relational database just stores data without explicit and structured mechanisms to link data from different tables (or buckets) to one another.\n\nMongo is a popular non-relational database for MongoDB Ember Angular and Node.js (MEAN) stack developers because it’s basically written in JavaScript; JSON is JavaScript Object Notation, which is a lightweight data interchange format. If your data model turns out to be very complex, or if you find yourself having to de-normalize your database schema, non-relational databases like Mongo may be the best way to go. Other reasons for choosing a non-relational database include:\n\n\n    The need to store serialized arrays in JSON objects\n\n    Storing records in the same collection that have different fields or attributes\n\n    Finding yourself de-normalizing your database schema or coding around performance and horizontal scalability issues\n\n    Problems easily pre-defining your schema because of the nature of your data model\n\n\nSuppose we were developing an app, and our example for the trafficking victim safe houses was part of a data model that was too complex and had too many tables, making referential integrity extremely difficult. We might handle the representation of our trafficking victim service-providing NGO’s like this instead, using Mongo:\n\nRelate_03\n\nNote the nice, easily readable output. Mongo is accessible with JavaScript, and from a MEAN stack developer’s point of view, it wouldn’t make sense to go with any database that wasn’t easily accessible. Additionally, the MongoDB site is well documented and provides clear, concise examples for how to set up a Mongo database and make the most of it. As a NoSQL database, MongoDB allows developers to define the application’s flow entirely on the code side. One of the biggest issues MEAN stack developers have with relational databases is facing the unavoidable fact that the objects represented in the database are stored in a format that is unable to be easily used by the frontend and vice-versa.\n\nBut it isn’t only MEAN stack developers who decided that a non-relational database was the best way to go. Steve Klabnik (a well-known member of the Ruby/Ruby on Rails community and the maintainer of the open source project Hackety-Hack) also chose MongoDB. Of course, he had to make trade-offs in taking this route. This included difficulty in getting Hackety-Hack refactored to be set up for user authentication with Facebook, Twitter, Linkedin and Github accounts. But other Rails developers also like Mongo for its superior horizontal scalability.\n\nOne of the biggest advantages in going with a non-relational database is that your database is not at risk for SQL injection attacks, because non-relational databases don’t use SQL and are, for the most part, schema-less. Another major advantage, at least with Mongo, is that you can theoretically shard it forever (although that does bring up replication issues). Sharding distributes the data across partitions to overcome hardware limitations.\nNon-relational database disadvantages\n\nIn non-relational databases like Mongo, there are no joins like there would be in relational databases. This means you need to perform multiple queries and join the data manually within your code -- and that can get very ugly, very fast.\n\nSince Mongo doesn’t automatically treat operations as transactions the way a relational database does, you must manually choose to create a transaction and then manually verify it, manually commit it or roll it back. Even the documentation on the MongoDB site warns you that without taking some potentially time-consuming precautions, and since documents can be fairly complex and nested, the success or failure of a database operation cannot be all or nothing. To put it simply, some operations will succeed while others fail.\n\nOf course, this all brings us back to the beginning; knowing how to ask exactly the right questions in order to effectively whiteboard your data model. It's this key step that will allow you to determine the best route for you regarding your application’s flow. Taking the time to pinpoint the right questions will serve as a solid guide when choosing the programming language to write your application in, and the use of one particular database over another.\nA hierarchical database model is a data model in which the data is organized into a tree-like structure. The data is stored as records which are connected to one another through links. A record is a collection of fields, with each field containing only one value. The entity type of a record defines which fields the record contains.\nExample of a hierarchical model\n\nA record in the hierarchical database model corresponds to a row (or tuple) in the relational database model and an entity type corresponds to a table (or relation).", "skillName": "DB4."}
{"id": 167, "category": "Databases", "skillText": "A data mart is the access layer of the data warehouse environment that is used to get data out to the users. The data mart is a subset of the data warehouse that is usually oriented to a specific business line or team. Data marts are small slices of the data warehouse. Whereas data warehouses have an enterprise-wide depth, the information in data marts pertains to a single department. In some deployments, each department or business unit is considered the owner of its data mart including all the hardware, software and data.[1] This enables each department to use, manipulate and develop their data any way they see fit; without altering information inside other data marts or the data warehouse. In other deployments where conformed dimensions are used, this business unit ownership will not hold true for shared dimensions like customer, product, etc.\n\nThe reasons why organizations are building data warehouses and data marts are because the information in the database is not organized in a way that makes it easy for organizations to find what they need. Also, complicated queries might take a long time to answer what people want to know since the database systems are designed to process millions of transactions per day. While transactional databases are designed to be updated, data warehouses or marts are read only. Data warehouses are designed to access large groups of related records.\n\nData marts improve end-user response time by allowing users to have access to the specific type of data they need to view most often by providing the data in a way that supports the collective view of a group of users.\n\nA data mart is basically a condensed and more focused version of a data warehouse that reflects the regulations and process specifications of each business unit within an organization. Each data mart is dedicated to a specific business function or region. This subset of data may span across many or all of an enterprise’s functional subject areas. It is common for multiple data marts to be used in order to serve the needs of each individual business unit (different data marts can be used to obtain specific information for various enterprise departments, such as accounting, marketing, sales, etc.).\n\nThe related term spreadmart is a derogatory label describing the situation that occurs when one or more business analysts develop a system of linked spreadsheets to perform a business analysis, then grow it to a size and degree of complexity that makes it nearly impossible to maintain.\n\nContents\n\n    1 Data mart vs data warehouse\n    2 Design schemas\n    3 Reasons for creating a data mart\n    4 Dependent data mart\n    5 See also\n    6 References\n    7 Bibliography\n    8 External links\n\nData mart vs data warehouse\n\nData warehouse:\n\n    Holds multiple subject areas\n    Holds very detailed information\n    Works to integrate all data sources\n    Does not necessarily use a dimensional model but feeds dimensional models.\n\nData mart:\n\n    Often holds only one subject area- for example, Finance, or Sales\n    May hold more summarized data (although many hold full detail)\n    Concentrates on integrating information from a given subject area or set of source systems\n    Is built focused on a dimensional model using a star schema.\n\nDesign schemas\n\n    Star schema - fairly popular design choice; enables a relational database to emulate the analytical functionality of a multidimensional database\n    Snowflake schema\n\nReasons for creating a data mart\n\n    Easy access to frequently needed data\n    Creates collective view by a group of users\n    Improves end-user response time\n    Ease of creation\n    Lower cost than implementing a full data warehouse\n    Potential users are more clearly defined than in a full data warehouse\n    Contains only business essential data and is less cluttered.\n\nDependent data mart\n\nAccording to the Inmon school of data warehousing, a dependent data mart is a logical subset (view) or a physical subset (extract) of a larger data warehouse, isolated for one of the following reasons:\n\n    A need refreshment for a special data model or schema: e.g., to restructure for OLAP\n    Performance: to offload the data mart to a separate computer for greater efficiency or to eliminate the need to manage that workload on the centralized data warehouse.\n    Security: to separate an authorized data subset selectively\n    Expediency: to bypass the data governance and authorizations required to incorporate a new application on the Enterprise Data Warehouse\n    Proving Ground: to demonstrate the viability and ROI (return on investment) potential of an application prior to migrating it to the Enterprise Data Warehouse\n    Politics: a coping strategy for IT (Information Technology) in situations where a user group has more influence than funding or is not a good citizen on the centralized data warehouse.\n    Politics: a coping strategy for consumers of data in situations where a data warehouse team is unable to create a usable data warehouse.\n\nAccording to the Inmon school of data warehousing, tradeoffs inherent with data marts include limited scalability, duplication of data, data inconsistency with other silos of information, and inability to leverage enterprise sources of data.\n\nThe alternative school of data warehousing is that of Ralph Kimball. In his view, a data warehouse is nothing more than the union of all the data marts. This view helps to reduce costs and provides fast development, but can create an inconsistent data warehouse, especially in large organizations. Therefore, Kimball's approach is more suitable for small-to-medium corporations.[2]", "skillName": "Data_mart."}
{"id": 168, "category": "Databases", "skillText": "Microsoft SQL Server is a relational database management system developed by Microsoft. As a database server, it is a software product with the primary function of storing and retrieving data as requested by other software applications which may run either on the same computer or on another computer across a network (including the Internet).\n\nMicrosoft markets at least a dozen different editions of Microsoft SQL Server, aimed at different audiences and for workloads ranging from small single-machine applications to large Internet-facing applications with many concurrent users.\n\nContents\n\n    1 History\n        1.1 Genesis\n        1.2 SQL Server 2005\n        1.3 SQL Server 2008\n        1.4 SQL Server 2008 R2\n        1.5 SQL Server 2012\n        1.6 SQL Server 2014\n        1.7 SQL Server 2016\n    2 Editions\n        2.1 Mainstream editions\n        2.2 Specialized editions\n        2.3 Discontinued editions\n    3 Architecture\n    4 Data storage\n        4.1 Buffer management\n        4.2 Concurrency and locking\n    5 Data retrieval and programmability\n        5.1 T-SQL\n        5.2 SQL Native Client (aka SNAC)\n        5.3 SQL CLR\n    6 Services\n        6.1 Service Broker\n        6.2 Replication Services\n        6.3 Analysis Services\n        6.4 Reporting Services\n        6.5 Notification Services\n        6.6 Integration Services\n        6.7 Full Text Search Service\n        6.8 SQLCMD\n        6.9 Visual Studio\n        6.10 SQL Server Management Studio\n        6.11 Business Intelligence Development Studio\n    7 See also\n    8 References\n    9 Further reading\n    10 External links\n\nHistory\nGenesis\nSQL Server release history Version \tYear \tRelease name \tCode name \tInternal version\n1.0 (OS/2) \t1989 \tSQL Server 1.0 (16 bit) \tAshton-Tate / Microsoft SQL Server \t-\n1.1 (OS/2) \t1991 \tSQL Server 1.1 (16 bit) \t- \t-\n4.2A (OS/2) \t1992 \tSQL Server 4.2A (16 bit) \t- \t-\n4.2B (OS/2) \t1993 \tSQL Server 4.2B (16 bit) \t- \t-\n4.21a (WinNT) \t1993 \tSQL Server 4.21a \tSQLNT \t-\n6.0 \t1995 \tSQL Server 6.0 \tSQL95 \t-\n6.5 \t1996 \tSQL Server 6.5 \tHydra \t-\n7.0 \t1998 \tSQL Server 7.0 \tSphinx \t515\n- \t1999 \tSQL Server 7.0 OLAP Tools \tPalato mania \t-\n8.0 \t2000 \tSQL Server 2000 \tShiloh \t539\n8.0 \t2003 \tSQL Server 2000 64-bit Edition \tLiberty \t539\n9.0 \t2005 \tSQL Server 2005 \tYukon \t611/612\n10.0 \t2008 \tSQL Server 2008 \tKatmai \t661\n10.25 \t2010 \tAzure SQL Database \tCloud Database or CloudDB \t-\n10.50 \t2010 \tSQL Server 2008 R2 \tKilimanjaro (aka KJ) \t665\n11.0 \t2012 \tSQL Server 2012 \tDenali \t706\n12.0 \t2014 \tSQL Server 2014 \tSQL14 \t782\n13.0 \t2016 \tSQL Server 2016 \t- \t852\nOld version\nOlder version, still supported\nLatest version\nLatest preview version\n\nIn 1988 Microsoft joined Ashton-Tate and Sybase to create a variant of Sybase SQL Server for IBM OS/2 (then developed jointly with Microsoft), which was released the following year.[4] This was the first version of Microsoft SQL Server, and served as Microsoft's entry to the enterprise-level database market, competing against Oracle, IBM, and later, Sybase. SQL Server 4.2 was shipped in 1992, bundled with OS/2 version 1.3, followed by version 4.21 for Windows NT, released alongside Windows NT 3.1. SQL Server 6.0 was the first version designed for NT, and did not include any direction from Sybase.\n\nAbout the time Windows NT was released in July 1993, Sybase and Microsoft parted ways and each pursued its own design and marketing schemes. Microsoft negotiated exclusive rights to all versions of SQL Server written for Microsoft operating systems. (In 1996 Sybase changed the name of its product to Adaptive Server Enterprise to avoid confusion with Microsoft SQL Server.) Until 1994, Microsoft's SQL Server carried three Sybase copyright notices as an indication of its origin.\n\nSQL Server 7.0 and SQL Server 2000 included modifications and extensions to the Sybase code base, adding support for the IA-64 architecture. By SQL Server 2005 the legacy Sybase code had been completely rewritten.[5]\n\nSince the release of SQL Server 2000, advances have been made in performance, the client IDE tools, and several complementary systems that are packaged with SQL Server 2005. These include:\n\n    an extract-transform-load (ETL) tool (SQL Server Integration Services or SSIS)\n    a Reporting Server\n    an OLAP and data mining server (Analysis Services)\n    several messaging technologies, specifically Service Broker and Notification Services\n\nSQL Server 2005\n\nSQL Server 2005 (formerly codenamed \"Yukon\") released in November 2005. It included native support for managing XML data, in addition to relational data. For this purpose, it defined an xml data type that could be used either as a data type in database columns or as literals in queries. XML columns can be associated with XSD schemas; XML data being stored is verified against the schema. XML is converted to an internal binary data type before being stored in the database. Specialized indexing methods were made available for XML data. XML data is queried using XQuery; SQL Server 2005 added some extensions to the T-SQL language to allow embedding XQuery queries in T-SQL. In addition, it also defines a new extension to XQuery, called XML DML, that allows query-based modifications to XML data. SQL Server 2005 also allows a database server to be exposed over web services using Tabular Data Stream (TDS) packets encapsulated within SOAP (protocol) requests. When the data is accessed over web services, results are returned as XML.[6]\n\nCommon Language Runtime (CLR) integration was introduced with this version, enabling one to write SQL code as Managed Code by the CLR. For relational data, T-SQL has been augmented with error handling features (try/catch) and support for recursive queries with CTEs (Common Table Expressions). SQL Server 2005 has also been enhanced with new indexing algorithms, syntax and better error recovery systems. Data pages are checksummed for better error resiliency, and optimistic concurrency support has been added for better performance. Permissions and access control have been made more granular and the query processor handles concurrent execution of queries in a more efficient way. Partitions on tables and indexes are supported natively, so scaling out a database onto a cluster is easier. SQL CLR was introduced with SQL Server 2005 to let it integrate with the .NET Framework.[7]\n\nSQL Server 2005 introduced Multi-Version Concurrency Control (MVCC). User facing features include new transaction isolation level called SNAPSHOT and a variation of the READ COMMITTED isolation level based on statement-level data snapshots.\n\nSQL Server 2005 introduced \"MARS\" (Multiple Active Results Sets), a method of allowing usage of database connections for multiple purposes.[8]\n\nSQL Server 2005 introduced DMVs (Dynamic Management Views), which are specialized views and functions that return server state information that can be used to monitor the health of a server instance, diagnose problems, and tune performance.[9]\n\nService Pack 1 (SP1) of SQL Server 2005 introduced Database Mirroring, a high availability option that provides redundancy and failover capabilities at the database level.[10] Failover can be performed manually or can be configured for automatic failover. Automatic failover requires a witness partner and an operating mode of synchronous (also known as high-safety or full safety).[11] Database Mirroring was included in the first release of SQL Server 2005 for evaluation purposes only. Prior to SP1, it was not enabled by default, and was not supported by Microsoft.[citation needed]\n\nExtended support for SQL Server 2005 ended on April 12, 2016.\nSQL Server 2008\n\nSQL Server 2008 (formerly codenamed \"Katmai\")[12][13] was released on August 6, 2008, announced to the SQL Server Special Interest Group at the ESRI 2008 User's Conference on August 6, 2008 by Ed Katibah (Spatial Program Manager at Microsoft), and aims to make data management self-tuning, self organizing, and self maintaining with the development of SQL Server Always On technologies, to provide near-zero downtime. SQL Server 2008 also includes support for structured and semi-structured data, including digital media formats for pictures, audio, video and other multimedia data. In current versions, such multimedia data can be stored as BLOBs (binary large objects), but they are generic bitstreams. Intrinsic awareness of multimedia data will allow specialized functions to be performed on them. According to Paul Flessner, senior Vice President of Server Applications at Microsoft, SQL Server 2008 can be a data storage backend for different varieties of data: XML, email, time/calendar, file, document, spatial, etc. as well as perform search, query, analysis, sharing, and synchronization across all data types.[13]\n\nOther new data types include specialized date and time types and a Spatial data type for location-dependent data.[14] Better support for unstructured and semi-structured data is provided using the new FILESTREAM[15] data type, which can be used to reference any file stored on the file system.[16] Structured data and metadata about the file is stored in SQL Server database, whereas the unstructured component is stored in the file system. Such files can be accessed both via Win32 file handling APIs as well as via SQL Server using T-SQL; doing the latter accesses the file data as a BLOB. Backing up and restoring the database backs up or restores the referenced files as well.[17] SQL Server 2008 also natively supports hierarchical data, and includes T-SQL constructs to directly deal with them, without using recursive queries.[17]\n\nThe full-text search functionality has been integrated with the database engine. According to a Microsoft technical article, this simplifies management and improves performance.[18]\n\nSpatial data will be stored in two types. A \"Flat Earth\" (GEOMETRY or planar) data type represents geospatial data which has been projected from its native, spherical, coordinate system into a plane. A \"Round Earth\" data type (GEOGRAPHY) uses an ellipsoidal model in which the Earth is defined as a single continuous entity which does not suffer from the singularities such as the international dateline, poles, or map projection zone \"edges\". Approximately 70 methods are available to represent spatial operations for the Open Geospatial Consortium Simple Features for SQL, Version 1.1.[19]\n\nSQL Server includes better compression features, which also helps in improving scalability.[20] It enhanced the indexing algorithms and introduced the notion of filtered indexes. It also includes Resource Governor that allows reserving resources for certain users or workflows. It also includes capabilities for transparent encryption of data (TDE) as well as compression of backups.[15] SQL Server 2008 supports the ADO.NET Entity Framework and the reporting tools, replication, and data definition will be built around the Entity Data Model.[21] SQL Server Reporting Services will gain charting capabilities from the integration of the data visualization products from Dundas Data Visualization, Inc., which was acquired by Microsoft.[22] On the management side, SQL Server 2008 includes the Declarative Management Framework which allows configuring policies and constraints, on the entire database or certain tables, declaratively.[14] The version of SQL Server Management Studio included with SQL Server 2008 supports IntelliSense for SQL queries against a SQL Server 2008 Database Engine.[23] SQL Server 2008 also makes the databases available via Windows PowerShell providers and management functionality available as Cmdlets, so that the server and all the running instances can be managed from Windows PowerShell.[24]\n\nThe final SQL Server 2008 service pack (10.00.6000, Service Pack 4) was released on September 30, 2014.[25]\nSQL Server 2008 R2\n\nSQL Server 2008 R2 (10.50.1600.1, formerly codenamed \"Kilimanjaro\") was announced at TechEd 2009, and was released to manufacturing on April 21, 2010.[26] SQL Server 2008 R2 adds certain features to SQL Server 2008 including a master data management system branded as Master Data Services, a central management of master data entities and hierarchies. Also Multi Server Management, a centralized console to manage multiple SQL Server 2008 instances and services including relational databases, Reporting Services, Analysis Services & Integration Services.[27]\n\nSQL Server 2008 R2 includes a number of new services,[28] including PowerPivot for Excel and SharePoint, Master Data Services, StreamInsight \n, Report Builder 3.0, Reporting Services \nAdd-in for SharePoint, a Data-tier function in Visual Studio that enables packaging of tiered databases as part of an application, and a SQL Server Utility named UC (Utility Control Point), part of AMSM (Application and Multi-Server Management) that is used to manage multiple SQL Servers.[29]\n\nThe first SQL Server 2008 R2 service pack (10.50.2500, Service Pack 1) was released on July 11, 2011.[30]\n\nThe second SQL Server 2008 R2 service pack (10.50.4000, Service Pack 2) was released on July 26, 2012.[31]\n\nThe final SQL Server 2008 R2 service pack (10.50.6000, Service Pack 3) was released on September 26, 2014.[32]\nSQL Server 2012\n\nAt the 2011 Professional Association for SQL Server (PASS) summit on October 11, Microsoft announced that the next major version of SQL Server (codenamed \"Denali\"), would be SQL Server 2012. It was released to manufacturing on March 6, 2012.[33] SQL Server 2012 Service Pack 1 was released to manufacturing on November 9, 2012, and Service Pack 2 was released to manufacturing on June 10, 2014.\n\nIt was announced to be the last version to natively support OLE DB and instead to prefer ODBC for native connectivity.[34]\n\nSQL Server 2012's new features and enhancements include Always On SQL Server Failover Cluster Instances and Availability Groups which provides a set of options to improve database availability,[35] Contained Databases which simplify the moving of databases between instances, new and modified Dynamic Management Views and Functions,[36] programmability enhancements including new spatial features,[37] metadata discovery, sequence objects and the THROW statement,[38] performance enhancements such as ColumnStore Indexes as well as improvements to OnLine and partition level operations and security enhancements including provisioning during setup, new permissions, improved role management, and default schema assignment for groups.[39][40]\nSQL Server 2014\n\nSQL Server 2014 was released to manufacturing on March 18, 2014, and released to the general public on April 1, 2014 and the build number was 12.0.2000.8 at release.[41] Until November 2013 there were two CTP revisions, CTP1 and CTP2.[42] SQL Server 2014 provides a new in-memory capability for tables that can fit entirely in memory (also known as Hekaton). Whilst small tables may be entirely resident in memory in all versions of SQL Server, they also may reside on disk, so work is involved in reserving RAM, writing evicted pages to disk, loading new pages from disk, locking the pages in RAM while they are being operated on, and many other tasks. By treating a table as guaranteed to be entirely resident in memory much of the 'plumbing' of disk-based databases can be avoided.[43]\n\nFor disk-based SQL Server applications, it also provides the SSD Buffer Pool Extension, which can improve performance by cache between DRAM and spinning media.\n\nSQL Server 2014 also enhances the Always On (HADR) solution by increasing the readable secondaries count and sustaining read operations upon secondary-primary disconnections, and it provides new hybrid disaster recovery and backup solutions with Microsoft Azure, enabling customers to use existing skills with the on-premises version of SQL Server to take advantage of Microsoft's global datacenters. In addition, it takes advantage of new Windows Server 2012 and Windows Server 2012 R2 capabilities for database application scalability in a physical or virtual environment.\n\nMicrosoft provides three versions of SQL Server 2014 for downloading: the one that runs on Microsoft Azure, the SQL Server 2014 CAB, and SQL Server 2014 ISO.[44]\n\nSQL Server 2014 SP1, consisting primarily of bugfixes, was released on May 15, 2015.[45]\nSQL Server 2016\n\nMicrosoft SQL Server 2016 is the most recent version available. The official General Availability (GA) release date for SQL Server 2016 was June 1, 2016. The RTM version is 13.0.1601.5.\nEditions\n\nMicrosoft makes SQL Server available in multiple editions, with different feature sets and targeting different users. These editions are:[46][47]\nMainstream editions\n\nDatacenter\n    SQL Server 2008 R2 Datacenter is a full-featured edition of SQL Server and is designed for datacenters that need high levels of application support and scalability. It supports 256 logical processors and virtually unlimited memory and comes with StreamInsight Premium edition.[48] The Datacenter edition has been retired in SQL Server 2012; all of its features are available in SQL Server 2012 Enterprise Edition.[49]\nEnterprise\n    SQL Server Enterprise Edition includes both the core database engine and add-on services, with a range of tools for creating and managing a SQL Server cluster. It can manage databases as large as 524 petabytes and address 2 terabytes of memory and supports 8 physical processors. SQL Server 2012 Enterprise Edition supports 160 physical processors.[50]\nStandard\n    SQL Server Standard edition includes the core database engine, along with the stand-alone services. It differs from Enterprise edition in that it supports fewer active instances (number of nodes in a cluster) and does not include some high-availability functions such as hot-add memory (allowing memory to be added while the server is still running), and parallel indexes.\nWeb\n    SQL Server Web Edition is a low-TCO option for Web hosting.\nBusiness Intelligence\n    Introduced in SQL Server 2012 and focusing on Self Service and Corporate Business Intelligence. It includes the Standard Edition capabilities and Business Intelligence tools: PowerPivot, Power View, the BI Semantic Model, Master Data Services, Data Quality Services and xVelocity in-memory analytics.[51]\nWorkgroup\n    SQL Server Workgroup Edition includes the core database functionality but does not include the additional services. Note that this edition has been retired in SQL Server 2012.[49]\nExpress\n    SQL Server Express Edition is a scaled down, free edition of SQL Server, which includes the core database engine. While there are no limitations on the number of databases or users supported, it is limited to using one processor, 1 GB memory and 10 GB database files (4 GB database files prior to SQL Server Express 2008 R2).[52] It is intended as a replacement for MSDE. Two additional editions provide a superset of features not in the original Express Edition. The first is SQL Server Express with Tools, which includes SQL Server Management Studio Basic. SQL Server Express with Advanced Services adds full-text search capability and reporting services.[53]\n\nSpecialized editions\n\nAzure\n    Azure SQL Database is the cloud-based version of Microsoft SQL Server, presented as a platform as a service offering on Microsoft Azure.\nCompact (SQL CE)\n    The compact edition is an embedded database engine. Unlike the other editions of SQL Server, the SQL CE engine is based on SQL Mobile (initially designed for use with hand-held devices) and does not share the same binaries. Due to its small size (1 MB DLL footprint), it has a markedly reduced feature set compared to the other editions. For example, it supports a subset of the standard data types, does not support stored procedures or Views or multiple-statement batches (among other limitations). It is limited to 4 GB maximum database size and cannot be run as a Windows service, Compact Edition must be hosted by the application using it. The 3.5 version includes support for ADO.NET Synchronization Services. SQL CE does not support ODBC connectivity, unlike SQL Server proper.\nDeveloper\n    SQL Server Developer Edition includes the same features as SQL Server 2012-2016 Enterprise Edition, but is limited by the license to be only used as a development and test system, and not as production server. This edition is available to download by students free of charge as a part of Microsoft's DreamSpark program.[54]\nEmbedded (SSEE)\n    SQL Server 2005 Embedded Edition is a specially configured named instance of the SQL Server Express database engine which can be accessed only by certain Windows Services.\nEvaluation\n    SQL Server Evaluation Edition, also known as the Trial Edition, has all the features of the Enterprise Edition, but is limited to 180 days, after which the tools will continue to run, but the server services will stop.[55]\nFast Track\n    SQL Server Fast Track is specifically for enterprise-scale data warehousing storage and business intelligence processing, and runs on reference-architecture hardware that is optimized for Fast Track.[56]\nLocalDB\n    Introduced in SQL Server Express 2012, LocalDB is a minimal, on-demand, version of SQL Server that is designed for application developers.[57] It can also be used as an embedded database.[58]\nAnalytics Platform System (APS)\n    Formerly Parallel Data Warehouse (PDW) A massively parallel processing (MPP) SQL Server appliance optimized for large-scale data warehousing such as hundreds of terabytes.[59]\nDatawarehouse Appliance Edition\n    Pre-installed and configured as part of an appliance in partnership with Dell & HP base on the Fast Track architecture. This edition does not include SQL Server Integration Services, Analysis Services, or Reporting Services.\n\nDiscontinued editions\n\nMSDE\n    Microsoft SQL Server Data Engine / Desktop Engine / Desktop Edition. SQL Server 7 and SQL Server 2000. Intended for use as an application component, it did not include GUI management tools. Later, Microsoft also made available a web admin tool. Included with some versions of Microsoft Access, Microsoft development tools, and other editions of SQL Server.[60]\nPersonal Edition\n    SQL Server 2000. Had workload or connection limits like MSDE, but no database size limit. Includes standard management tools. Intended for use as a mobile / disconnected proxy, licensed for use with SQL Server 2000 Standard edition.[60]\n\nArchitecture\n\nThe protocol layer implements the external interface to SQL Server. All operations that can be invoked on SQL Server are communicated to it via a Microsoft-defined format, called Tabular Data Stream (TDS). TDS is an application layer protocol, used to transfer data between a database server and a client. Initially designed and developed by Sybase Inc. for their Sybase SQL Server relational database engine in 1984, and later by Microsoft in Microsoft SQL Server, TDS packets can be encased in other physical transport dependent protocols, including TCP/IP, named pipes, and shared memory. Consequently, access to SQL Server is available over these protocols. In addition, the SQL Server API is also exposed over web services.[47]\nData storage\n\nData storage is a database, which is a collection of tables with typed columns. SQL Server supports different data types, including primary types such as Integer, Float, Decimal, Char (including character strings), Varchar (variable length character strings), binary (for unstructured blobs of data), Text (for textual data) among others. The rounding of floats to integers uses either Symmetric Arithmetic Rounding or Symmetric Round Down (Fix) depending on arguments: SELECT Round(2.5, 0) gives 3.\n\nMicrosoft SQL Server also allows user-defined composite types (UDTs) to be defined and used. It also makes server statistics available as virtual tables and views (called Dynamic Management Views or DMVs). In addition to tables, a database can also contain other objects including views, stored procedures, indexes and constraints, along with a transaction log. A SQL Server database can contain a maximum of 231 objects, and can span multiple OS-level files with a maximum file size of 260 bytes (1 exabyte).[47] The data in the database are stored in primary data files with an extension .mdf. Secondary data files, identified with a .ndf extension, are used to allow the data of a single database to be spread across more than one file, and optionally across more than one file system. Log files are identified with the .ldf extension.[47]\n\nStorage space allocated to a database is divided into sequentially numbered pages, each 8 KB in size. A page is the basic unit of I/O for SQL Server operations. A page is marked with a 96-byte header which stores metadata about the page including the page number, page type, free space on the page and the ID of the object that owns it. Page type defines the data contained in the page: data stored in the database, index, allocation map which holds information about how pages are allocated to tables and indexes, change map which holds information about the changes made to other pages since last backup or logging, or contain large data types such as image or text. While page is the basic unit of an I/O operation, space is actually managed in terms of an extent which consists of 8 pages. A database object can either span all 8 pages in an extent (\"uniform extent\") or share an extent with up to 7 more objects (\"mixed extent\"). A row in a database table cannot span more than one page, so is limited to 8 KB in size. However, if the data exceeds 8 KB and the row contains Varchar or Varbinary data, the data in those columns are moved to a new page (or possibly a sequence of pages, called an Allocation unit) and replaced with a pointer to the data.[61]\n\nFor physical storage of a table, its rows are divided into a series of partitions (numbered 1 to n). The partition size is user defined; by default all rows are in a single partition. A table is split into multiple partitions in order to spread a database over a computer cluster. Rows in each partition are stored in either B-tree or heap structure. If the table has an associated, clustered index to allow fast retrieval of rows, the rows are stored in-order according to their index values, with a B-tree providing the index. The data is in the leaf node of the leaves, and other nodes storing the index values for the leaf data reachable from the respective nodes. If the index is non-clustered, the rows are not sorted according to the index keys. An indexed view has the same storage structure as an indexed table. A table without a clustered index is stored in an unordered heap structure. However, the table may have non-clustered indices to allow fast retrieval of rows. In some situations the heap structure has performance advantages over the clustered structure. Both heaps and B-trees can span multiple allocation units.[62]\nBuffer management\n\nSQL Server buffers pages in RAM to minimize disc I/O. Any 8 KB page can be buffered in-memory, and the set of all pages currently buffered is called the buffer cache. The amount of memory available to SQL Server decides how many pages will be cached in memory. The buffer cache is managed by the Buffer Manager. Either reading from or writing to any page copies it to the buffer cache. Subsequent reads or writes are redirected to the in-memory copy, rather than the on-disc version. The page is updated on the disc by the Buffer Manager only if the in-memory cache has not been referenced for some time. While writing pages back to disc, asynchronous I/O is used whereby the I/O operation is done in a background thread so that other operations do not have to wait for the I/O operation to complete. Each page is written along with its checksum when it is written. When reading the page back, its checksum is computed again and matched with the stored version to ensure the page has not been damaged or tampered with in the meantime.[63]\nConcurrency and locking\n\nSQL Server allows multiple clients to use the same database concurrently. As such, it needs to control concurrent access to shared data, to ensure data integrity—when multiple clients update the same data, or clients attempt to read data that is in the process of being changed by another client. SQL Server provides two modes of concurrency control: pessimistic concurrency and optimistic concurrency. When pessimistic concurrency control is being used, SQL Server controls concurrent access by using locks. Locks can be either shared or exclusive. Exclusive lock grants the user exclusive access to the data—no other user can access the data as long as the lock is held. Shared locks are used when some data is being read—multiple users can read from data locked with a shared lock, but not acquire an exclusive lock. The latter would have to wait for all shared locks to be released. Locks can be applied on different levels of granularity—on entire tables, pages, or even on a per-row basis on tables. For indexes, it can either be on the entire index or on index leaves. The level of granularity to be used is defined on a per-database basis by the database administrator. While a fine grained locking system allows more users to use the table or index simultaneously, it requires more resources. So it does not automatically turn into higher performing solution. SQL Server also includes two more lightweight mutual exclusion solutions—latches and spinlocks—which are less robust than locks but are less resource intensive. SQL Server uses them for DMVs and other resources that are usually not busy. SQL Server also monitors all worker threads that acquire locks to ensure that they do not end up in deadlocks—in case they do, SQL Server takes remedial measures, which in many cases is to kill one of the threads entangled in a deadlock and rollback the transaction it started.[47] To implement locking, SQL Server contains the Lock Manager. The Lock Manager maintains an in-memory table that manages the database objects and locks, if any, on them along with other metadata about the lock. Access to any shared object is mediated by the lock manager, which either grants access to the resource or blocks it.\n\nSQL Server also provides the optimistic concurrency control mechanism, which is similar to the multiversion concurrency control used in other databases. The mechanism allows a new version of a row to be created whenever the row is updated, as opposed to overwriting the row, i.e., a row is additionally identified by the ID of the transaction that created the version of the row. Both the old as well as the new versions of the row are stored and maintained, though the old versions are moved out of the database into a system database identified as Tempdb. When a row is in the process of being updated, any other requests are not blocked (unlike locking) but are executed on the older version of the row. If the other request is an update statement, it will result in two different versions of the rows—both of them will be stored by the database, identified by their respective transaction IDs.[47]\nData retrieval and programmability\n\nThe main mode of retrieving data from a SQL Server database is querying for it. The query is expressed using a variant of SQL called T-SQL, a dialect Microsoft SQL Server shares with Sybase SQL Server due to its legacy. The query declaratively specifies what is to be retrieved. It is processed by the query processor, which figures out the sequence of steps that will be necessary to retrieve the requested data. The sequence of actions necessary to execute a query is called a query plan. There might be multiple ways to process the same query. For example, for a query that contains a join statement and a select statement, executing join on both the tables and then executing select on the results would give the same result as selecting from each table and then executing the join, but result in different execution plans. In such case, SQL Server chooses the plan that is expected to yield the results in the shortest possible time. This is called query optimization and is performed by the query processor itself.[47]\n\nSQL Server includes a cost-based query optimizer which tries to optimize on the cost, in terms of the resources it will take to execute the query. Given a query, then the query optimizer looks at the database schema, the database statistics and the system load at that time. It then decides which sequence to access the tables referred in the query, which sequence to execute the operations and what access method to be used to access the tables. For example, if the table has an associated index, whether the index should be used or not: if the index is on a column which is not unique for most of the columns (low \"selectivity\"), it might not be worthwhile to use the index to access the data. Finally, it decides whether to execute the query concurrently or not. While a concurrent execution is more costly in terms of total processor time, because the execution is actually split to different processors might mean it will execute faster. Once a query plan is generated for a query, it is temporarily cached. For further invocations of the same query, the cached plan is used. Unused plans are discarded after some time.[47][64]\n\nSQL Server also allows stored procedures to be defined. Stored procedures are parameterized T-SQL queries, that are stored in the server itself (and not issued by the client application as is the case with general queries). Stored procedures can accept values sent by the client as input parameters, and send back results as output parameters. They can call defined functions, and other stored procedures, including the same stored procedure (up to a set number of times). They can be selectively provided access to. Unlike other queries, stored procedures have an associated name, which is used at runtime to resolve into the actual queries. Also because the code need not be sent from the client every time (as it can be accessed by name), it reduces network traffic and somewhat improves performance.[65] Execution plans for stored procedures are also cached as necessary.\nT-SQL\nMain article: T-SQL\n\nT-SQL (Transact-SQL) is the secondary means of programming and managing SQL Server. It exposes keywords for the operations that can be performed on SQL Server, including creating and altering database schemas, entering and editing data in the database as well as monitoring and managing the server itself. Client applications that consume data or manage the server will leverage SQL Server functionality by sending T-SQL queries and statements which are then processed by the server and results (or errors) returned to the client application. SQL Server allows it to be managed using T-SQL. For this it exposes read-only tables from which server statistics can be read. Management functionality is exposed via system-defined stored procedures which can be invoked from T-SQL queries to perform the management operation. It is also possible to create linked Servers using T-SQL. Linked servers allow a single query to process operations performed on multiple servers.[66]\nSQL Native Client (aka SNAC)\n\nSQL Native Client is the native client side data access library for Microsoft SQL Server, version 2005 onwards. It natively implements support for the SQL Server features including the Tabular Data Stream implementation, support for mirrored SQL Server databases, full support for all data types supported by SQL Server, asynchronous operations, query notifications, encryption support, as well as receiving multiple result sets in a single database session. SQL Native Client is used under the hood by SQL Server plug-ins for other data access technologies, including ADO or OLE DB. The SQL Native Client can also be directly used, bypassing the generic data access layers.[67]\n\nOn November 28, 2011, a preview release of the SQL Server ODBC driver for Linux was released.[68]\nSQL CLR\nMain article: SQL CLR\n\nMicrosoft SQL Server 2005 includes a component named SQL CLR (\"Common Language Runtime\") via which it integrates with .NET Framework. Unlike most other applications that use .NET Framework, SQL Server itself hosts the .NET Framework runtime, i.e., memory, threading and resource management requirements of .NET Framework are satisfied by SQLOS itself, rather than the underlying Windows operating system. SQLOS provides deadlock detection and resolution services for .NET code as well. With SQL CLR, stored procedures and triggers can be written in any managed .NET language, including C# and VB.NET. Managed code can also be used to define UDT's (user defined types), which can persist in the database. Managed code is compiled to CLI assemblies and after being verified for type safety, registered at the database. After that, they can be invoked like any other procedure.[69] However, only a subset of the Base Class Library is available, when running code under SQL CLR. Most APIs relating to user interface functionality are not available.[69]\n\nWhen writing code for SQL CLR, data stored in SQL Server databases can be accessed using the ADO.NET APIs like any other managed application that accesses SQL Server data. However, doing that creates a new database session, different from the one in which the code is executing. To avoid this, SQL Server provides some enhancements to the ADO.NET provider that allows the connection to be redirected to the same session which already hosts the running code. Such connections are called context connections and are set by setting context connection parameter to true in the connection string. SQL Server also provides several other enhancements to the ADO.NET API, including classes to work with tabular data or a single row of data as well as classes to work with internal metadata about the data stored in the database. It also provides access to the XML features in SQL Server, including XQuery support. These enhancements are also available in T-SQL Procedures in consequence of the introduction of the new XML Datatype (query,value,nodes functions).[70]\nServices\n\nSQL Server also includes an assortment of add-on services. While these are not essential for the operation of the database system, they provide value added services on top of the core database management system. These services either run as a part of some SQL Server component or out-of-process as Windows Service and presents their own API to control and interact with them.\nService Broker\n\nUsed inside an instance, programming environment. For cross instance applications, Service Broker communicates over TCP/IP and allows the different components to be synchronized together, via exchange of messages. The Service Broker, which runs as a part of the database engine, provides a reliable messaging and message queuing platform for SQL Server applications.[71]\nReplication Services\n\nSQL Server Replication Services are used by SQL Server to replicate and synchronize database objects, either in entirety or a subset of the objects present, across replication agents, which might be other database servers across the network, or database caches on the client side. Lulla follows a publisher/subscriber model, i.e., the changes are sent out by one database server (\"publisher\") and are received by others (\"subscribers\"). SQL Server supports three different types of replication:[72]\n\nTransaction replication\n    Each transaction made to the publisher database (master database) is synced out to subscribers, who update their databases with the transaction. Transactional replication synchronizes databases in near real time.[73]\nMerge replication\n    Changes made at both the publisher and subscriber databases are tracked, and periodically the changes are synchronized bi-directionally between the publisher and the subscribers. If the same data has been modified differently in both the publisher and the subscriber databases, synchronization will result in a conflict which has to be resolved, either manually or by using pre-defined policies. rowguid needs to be configured on a column if merge replication is configured.[74]\nSnapshot replication\n    Snapshot replication publishes a copy of the entire database (the then-snapshot of the data) and replicates out to the subscribers. Further changes to the snapshot are not tracked.[75]\n\nAnalysis Services\nMain article: SQL Server Analysis Services\n\nSQL Server Analysis Services adds OLAP and data mining capabilities for SQL Server databases. The OLAP engine supports MOLAP, ROLAP and HOLAP storage modes for data. Analysis Services supports the XML for Analysis standard as the underlying communication protocol. The cube data can be accessed using MDX and LINQ[76] queries.[77] Data mining specific functionality is exposed via the DMX query language. Analysis Services includes various algorithms—Decision trees, clustering algorithm, Naive Bayes algorithm, time series analysis, sequence clustering algorithm, linear and logistic regression analysis, and neural networks—for use in data mining.[78]\nReporting Services\nMain article: SQL Server Reporting Services\n\nSQL Server Reporting Services is a report generation environment for data gathered from SQL Server databases. It is administered via a web interface. Reporting services features a web services interface to support the development of custom reporting applications. Reports are created as RDL files.[79]\n\nReports can be designed using recent versions of Microsoft Visual Studio (Visual Studio.NET 2003, 2005, and 2008)[80] with Business Intelligence Development Studio, installed or with the included Report Builder. Once created, RDL files can be rendered in a variety of formats,[81][82] including Excel, PDF, CSV, XML, BMP, EMF, GIF, JPEG, PNG, and TIFF,[83] and HTML Web Archive.\nNotification Services\nMain article: SQL Server Notification Services\n\nOriginally introduced as a post-release add-on for SQL Server 2000,[84] Notification Services was bundled as part of the Microsoft SQL Server platform for the first and only time with SQL Server 2005.[85][86] SQL Server Notification Services is a mechanism for generating data-driven notifications, which are sent to Notification Services subscribers. A subscriber registers for a specific event or transaction (which is registered on the database server as a trigger); when the event occurs, Notification Services can use one of three methods to send a message to the subscriber informing about the occurrence of the event. These methods include SMTP, SOAP, or by writing to a file in the filesystem.[87] Notification Services was discontinued by Microsoft with the release of SQL Server 2008 in August 2008, and is no longer an officially supported component of the SQL Server database platform.\nIntegration Services\nMain article: SQL Server Integration Services\n\nSQL Server Integration Services (SSIS) provides ETL capabilities for SQL Server for data import, data integration and data warehousing needs. Integration Services includes GUI tools to build workflows such as extracting data from various sources, querying data, transforming data—including aggregation, de-duplication, de-/normalization and merging of data—and then exporting the transformed data into destination databases or files.[88]\nFull Text Search Service\nThe SQL Server Full Text Search service architecture\n\nSQL Server Full Text Search service is a specialized indexing and querying service for unstructured text stored in SQL Server databases. The full text search index can be created on any column with character based text data. It allows for words to be searched for in the text columns. While it can be performed with the SQL LIKE operator, using SQL Server Full Text Search service can be more efficient. Full allows for inexact matching of the source string, indicated by a Rank value which can range from 0 to 1000 - a higher rank means a more accurate match. It also allows linguistic matching (\"inflectional search\"), i.e., linguistic variants of a word (such as a verb in a different tense) will also be a match for a given word (but with a lower rank than an exact match). Proximity searches are also supported, i.e., if the words searched for do not occur in the sequence they are specified in the query but are near each other, they are also considered a match. T-SQL exposes special operators that can be used to access the FTS capabilities.[89][90]\n\nThe Full Text Search engine is divided into two processes: the Filter Daemon process (msftefd.exe) and the Search process (msftesql.exe). These processes interact with the SQL Server. The Search process includes the indexer (that creates the full text indexes) and the full text query processor. The indexer scans through text columns in the database. It can also index through binary columns, and use iFilters to extract meaningful text from the binary blob (for example, when a Microsoft Word document is stored as an unstructured binary file in a database). The iFilters are hosted by the Filter Daemon process. Once the text is extracted, the Filter Daemon process breaks it up into a sequence of words and hands it over to the indexer. The indexer filters out noise words, i.e., words like A, And etc., which occur frequently and are not useful for search. With the remaining words, an inverted index is created, associating each word with the columns they were found in. SQL Server itself includes a Gatherer component that monitors changes to tables and invokes the indexer in case of updates.[91]\n\nWhen a full text query is received by the SQL Server query processor, it is handed over to the FTS query processor in the Search process. The FTS query processor breaks up the query into the constituent words, filters out the noise words, and uses an inbuilt thesaurus to find out the linguistic variants for each word. The words are then queried against the inverted index and a rank of their accurateness is computed. The results are returned to the client via the SQL Server process.[91]\nSQLCMD\n\nSQLCMD is a command line application that comes with Microsoft SQL Server, and exposes the management features of SQL Server. It allows SQL queries to be written and executed from the command prompt. It can also act as a scripting language to create and run a set of SQL statements as a script. Such scripts are stored as a .sql file, and are used either for management of databases or to create the database schema during the deployment of a database.\n\nSQLCMD was introduced with SQL Server 2005 and this continues with SQL Server 2012 and 2014. Its predecessor for earlier versions was OSQL and ISQL, which is functionally equivalent as it pertains to TSQL execution, and many of the command line parameters are identical, although SQLCMD adds extra versatility.\nVisual Studio\nMain article: Microsoft Visual Studio\n\nMicrosoft Visual Studio includes native support for data programming with Microsoft SQL Server. It can be used to write and debug code to be executed by SQL CLR. It also includes a data designer that can be used to graphically create, view or edit database schemas. Queries can be created either visually or using code. SSMS 2008 onwards, provides intellisense for SQL queries as well.\nSQL Server Management Studio\nMain article: SQL Server Management Studio\n\nSQL Server Management Studio is a GUI tool included with SQL Server 2005 and later for configuring, managing, and administering all components within Microsoft SQL Server. The tool includes both script editors and graphical tools that work with objects and features of the server.[92] SQL Server Management Studio replaces Enterprise Manager as the primary management interface for Microsoft SQL Server since SQL Server 2005. A version of SQL Server Management Studio is also available for SQL Server Express Edition, for which it is known as SQL Server Management Studio Express (SSMSE).[93]\n\nA central feature of SQL Server Management Studio is the Object Explorer, which allows the user to browse, select, and act upon any of the objects within the server.[94] It can be used to visually observe and analyze query plans and optimize the database performance, among others.[95] SQL Server Management Studio can also be used to create a new database, alter any existing database schema by adding or modifying tables and indexes, or analyze performance. It includes the query windows which provide a GUI based interface to write and execute queries.[47]\nBusiness Intelligence Development Studio\nMain article: Business Intelligence Development Studio\n\nBusiness Intelligence Development Studio (BIDS) is the IDE from Microsoft used for developing data analysis and Business Intelligence solutions utilizing the Microsoft SQL Server Analysis Services, Reporting Services and Integration Services. It is based on the Microsoft Visual Studio development environment but is customized with the SQL Server services-specific extensions and project types, including tools, controls and projects for reports (using Reporting Services), Cubes and data mining structures (using Analysis Services).[96] For SQL Server 2012 and later, this IDE has been renamed SQL Server Data Tools (SSDT).", "skillName": "Microsoft_SQL_Server."}
{"id": 169, "category": "Databases", "skillText": "IBM DB2 is a family of database server products developed by IBM. These products all support the relational model, but in recent years some products have been extended to support object-relational features and non-relational structures like JSON and XML.\n\nHistorically and unlike other database vendors, IBM produced a platform-specific DB2 product for each of its major operating systems. However, in the 1990s IBM changed track and produced a DB2 \"common server\" product, designed with a common code base to run on different platforms.\n\nContents\n\n    1 Current editions\n    2 History\n    3 Editions\n        3.1 IBM DB2 Everyplace (DB2e)\n    4 Competition\n    5 Technical information\n    6 Error processing\n    7 Versions\n        7.1 Linux, UNIX, and Windows (LUW)\n        7.2 z/OS\n        7.3 iSeries\n    8 See also\n    9 References\n    10 External links\n\nCurrent editions\n\nToday, there are three main products in the DB2 family: DB2 for Linux, UNIX and Windows (informally known as DB2 LUW), DB2 for z/OS (mainframe), and DB2 for i (formerly OS/400).\n\nA fourth product, DB2 for VM / VSE is also available.\nHistory\n\nDB2 traces its roots back to the beginning of the 1970s when Edgar F. Codd, a researcher working for IBM, described the theory of relational databases and in June 1970 published the model for data manipulation.[1]\n\nIn 1974 the IBM San Jose Research center developed a relational DBMS, System R, to implement Codd's concepts.[2] A key development of the System R project was SQL. To apply the relational model Codd needed a relational database language he named DSL/Alpha.[3] At the time IBM didn't believe in the potential of Codd's ideas, leaving the implementation to a group of programmers not under Codd's supervision, who violated several fundamentals of Codd's relational model; the result was Structured English QUEry Language or SEQUEL. When IBM released its first relational database product, they wanted to have a commercial-quality sublanguage as well, so it overhauled SEQUEL and renamed the basically new language Structured Query Language (SQL) to differentiate it from SEQUEL.[citation needed] The acronym SEQUEL was changed to SQL because \"SEQUEL\" was a trademark of the UK-based Hawker Siddeley aircraft company.[3]\n\nIBM bought Metaphor Computer Systems to utilize their GUI interface and encapsulating SQL platform that had already been in use since the mid 80's. In parallel with the development of SQL IBM also developed Query by Example (QBE), the first graphical query language.\n\nIBM's first commercial relational database product, SQL/DS, was released for the DOS/VSE and VM/CMS operating systems in 1981. In 1976 IBM released Query by Example for the VM platform where the table-oriented front-end produced a linear-syntax language that drove transactions to its relational database.[4] Later the QMF feature of DB2 produced real SQL and brought the same \"QBE\" look and feel to DB2.\n\nThe name DB2, or IBM Database 2, was first given to the Database Management System or DBMS in 1983 when IBM released DB2 on its MVS mainframe platform.[5]\n\nWhen Informix Corporation acquired Illustra and made their database engine an object-SQL DBMS by introducing their Universal Server, both Oracle and IBM followed suit by changing their database engines to be capable of object-relational extensions. In 2001, IBM bought Informix Software and in the following years incorporated Informix technology into the DB2 product suite. Today, DB2 can technically be considered to be an object-SQL DBMS.\n\nFor some years DB2, as a full-function DBMS, was exclusively available on IBM mainframes. Later IBM brought DB2 to other platforms, including OS/2, UNIX and MS Windows servers, then Linux (including Linux on z Systems) and PDAs. This process occurred through the 1990s. The inspiration for the mainframe version of DB2's architecture came in part from IBM IMS, a hierarchical database, and its dedicated database manipulation language, IBM DL/I. DB2 is also embedded in the i5/OS operating system for IBM System i (iSeries, formerly the AS/400), and versions are available for z/VSE and z/VM.An earlier version of the code that would become DB2 LUW (Linux, Unix, Windows) was part of an Extended Edition component of OS/2 called Database Manager.\n\nIBM extended the functionality of Database Manager a number of times, including the addition of distributed database functionality by means of Distributed Relational Database Architecture (DRDA) that allowed shared access to a database in a remote location on a LAN. (Note that DRDA is based on objects and protocols defined by Distributed Data Management Architecture (DDM).)\n\nEventually IBM declared that insurmountable complexity existed in the Database Manager code, and took the difficult decision to completely rewrite the software in their Toronto Lab. The new version of Database Manager, called DB2 like its mainframe parent, ran on the OS/2 and RS/6000 platforms, was called DB2/2 and DB2/6000 respectively. Other versions of DB2, with different code bases, followed the same '/' naming convention and became DB2/400 (for the AS/400), DB2/VSE (for the DOS/VSE environment) and DB2/VM (for the VM operating system). IBM lawyers stopped this handy naming convention from being used and decided that all products needed to be called \"product FOR platform\" (for example, DB2 for OS/390). The next iteration of the mainframe and the server-based products were named DB2 Universal Database (or DB2 UDB), a name that had already been used for the Linux-Unix-Windows version, with the introduction of widespread confusion over which version (mainframe or server) of the DBMS was being referred to. At this point, the mainframe version of DB2 and the server version of DB2 were coded in entirely different languages (PL/S for the mainframe and C++ for the server), but shared similar functionality and used a common architecture for SQL optimization: the Starburst Optimizer.\n\nOver the years DB2 has both exploited and driven numerous hardware enhancements, particularly on IBM System z with such features as Parallel Sysplex data sharing. In fact, DB2 UDB Version 8 for z/OS now requires a 64-bit system and cannot run on earlier processors, and DB2 for z/OS maintains certain unique software differences in order to serve its sophisticated customers. Although the ultimate expression of software-hardware co-evolution is the IBM mainframe, to some extent that phenomenon occurs on other platforms as well, as IBM's software engineers collaborate with their hardware counterparts.\n\nIn the mid-1990s, IBM released a clustered DB2 implementation called DB2 Parallel Edition, which initially ran on AIX. This edition allowed scalability by providing a shared nothing architecture, in which a single large database is partitioned across multiple DB2 servers that communicate over a high-speed interconnect. This DB2 edition was eventually ported to all Linux, UNIX, and Windows (LUW) platforms and was renamed to DB2 Extended Enterprise Edition (EEE). IBM now refers to this product as the Database Partitioning Feature (DPF) and sells it as an add-on to their flagship DB2 Enterprise product.\n\nIn mid 2006, IBM announced \"Viper,\" which is the codename for DB2 9 on both distributed platforms and z/OS. DB2 9 for z/OS was announced in early 2007. IBM claimed that the new DB2 was the first relational database to store XML \"natively\". Other enhancements include OLTP-related improvements for distributed platforms, business intelligence/data warehousing-related improvements for z/OS, more self-tuning and self-managing features, additional 64-bit exploitation (especially for virtual storage on z/OS), stored procedure performance enhancements for z/OS, and continued convergence of the SQL vocabularies between z/OS and distributed platforms.\n\nIn October 2007, IBM announced \"Viper 2,\" which is the codename for DB2 9.5 on the distributed platforms. There were three key themes for the release \n, Simplified Management, Business Critical Reliability and Agile XML development.\n\nIn June 2009, IBM announced \"Cobra\" (the codename for DB2 9.7 for LUW \n). DB2 9.7 adds data compression for database indexes, temporary tables, and large objects. DB2 9.7 also supports native XML data in hash partitioning (database partitioning), range partitioning (table partitioning), and multi-dimensional clustering. These native XML features allows users to directly work with XML in data warehouse environments. DB2 9.7 also adds several features that make it easier for Oracle Database users to work with DB2. These include support for the most commonly used SQL syntax, PL/SQL syntax, scripting syntax, and data types from Oracle Database. DB2 9.7 also enhanced its concurrency model to exhibit behavior that is familiar to users of Oracle Database and Microsoft SQL Server.\n\nIn October 2009, IBM introduced its second major release of the year when it announced DB2 pureScale \n. DB2 pureScale is a database cluster solution for non-mainframe platforms, suitable for Online Transaction Processing (OLTP) workloads. IBM based the design of DB2 pureScale on the Parallel Sysplex implementation of DB2 data sharing on the mainframe. DB2 pureScale provides a fault-tolerant architecture and shared-disk storage. A DB2 pureScale system can grow to 128 database servers, and provides continuous availability and automatic load balancing.\n\nIn 2009, it was announced that DB2 can be an engine in MySQL. This allows users on the System i platform to natively access the DB2 under the IBM i operating system (formerly called OS/400), and for users on other platforms to access these files through the MySQL interface. On the System i and its predecessors the AS/400 and the System/38, DB2 is tightly integrated into the operating system, and comes as part of the operating system. It provides journaling, triggers and other features.\n\nIn early 2012, IBM announced the next version of DB2, DB2 10.1 (code name Galileo) for Linux, UNIX, and Windows.DB2 10.1 contained a number of new data management capabilities including row and column access control which enables ‘fine-grained’ control of the database and multi-temperature data management that moves data to cost effective storage based on how\"hot\" or \"cold\" (how frequently the data is accessed) the data is. IBM also introduced ‘adaptive compression’ capability in DB2 10.1, a new approach to compressing data tables.\n\nIn June 2013, IBM released DB2 10.5 (code name “Kepler”), the latest version of DB2 on Linux, UNIX and Windows.With this latest release, IBM has combined the functionality and tools offered in the prior generation of DB2 and InfoSphere Warehouse on Linux, UNIX and Windows to create a single multi-workload database software. DB2 10.5 has a number of new capabilities including IBM BLU Acceleration, a collection of innovations from the IBM Research and Development Labs for accelerating reporting and analytics. IBM BLU Acceleration integrates Dynamic In-memory (in-memory columnar processing) technology with other innovations such as Parallel Vector Processing, Actionable Compression, and Data Skipping. DB2 pureScale \nclustered database technology is now fully integrated with DB2 high-availability disaster recovery functionality. In addition, DB2 10.5 supports online fix pack updates, which allow users to perform, fix pack maintenance operations on individual members running in a pureScale cluster with minimal impact to users. IBM has also added a number of mobile capabilities to DB2 10.5. DB2 now allows users to store and manage JSON objects.\n\nOn April 12 2016, IBM announced DB2 LUW 11.1 with a planned release date of June 15 for download.\nEditions\n\nIBM has changed the packaging structure in the latest release of DB2 for Linux, Unix and Windows and now offers seven editions: Advanced Enterprise Server Edition \n, Advanced Workgroup Server Edition \n, Enterprise Server Edition \n, Workgroup Server Edition \n, Express Edition \n, Developer Edition \nand Express-C \n. Each of these editions have been packaged for different deployment scenarios and workloads Applications built for lower editions of DB2 are guaranteed to work on higher editions but at a higher level of performance.\n\nThe no-charge edition of DB2 is called DB2 Express-C. DB2 Express-C is in some ways similar to the open source databases such as MySQL and PostgreSQL as it is offered unsupported, free of charge for unrestricted use including use in production environments. Users needing enterprise level support and fixpacks must buy any standard DB2 Edition. DB2 Express-C, however, is based on the same code as other DB2 for Linux, Unix and Windows editions and is not open source. DB2 Express-C is also similar to the free editions of Oracle database and Microsoft SQL Server, except that DB2 Express-C has no limit on number of users or on database size. DB2 Express-C runs on 32 and 64bit Windows, Linux on x86, x64 and POWER processors, Solaris on x64 CPU and Intel machines running Mac OS X. It can be installed on machines of any size, but the database engine will use only two CPU cores and 16GB of RAM. Additionally, IBM provides an optional yearly subscription for users who require technical support or additional functionality.\n\nDB2 for z/OS \n(the mainframe) is available in its traditional product packaging, or in the Value Unit Edition \n, which allows customers to instead pay a one-time charge.\n\nDB2 also powers IBM InfoSphere Warehouse, which offers data warehouse capabilities. InfoSphere Warehouse is available for z/OS. It includes several BI features such as ETL, data mining, OLAP acceleration, and in-line analytics.\n\nDB2 10.5 for Linux, UNIX and Windows, contains all of the functionality and tools offered in the prior generationof DB2 and InfoSphere Warehouse on Linux, UNIX and Windows.\nIBM DB2 Everyplace (DB2e)\n\nIBM has withdrawn from marketing the IBM DB2 Everyplace products. It announced April 30, 2013 as the end of support date.[6]\nCompetition\n\nIDC's Worldwide Database Management Systems 2009–2013 Forecast and 2008 Vendor Shares[7] ranked Oracle database as the leader in DBMS marketing share, followed by IBM DB2 and then by Microsoft SQL Server. Other competitors included open-source products such as Firebird, PostgreSQL, MySQL and Ingres, and niche players such as Sybase and MaxDB.\n\nThe DB-Engines Ranking (2013) listed DB2 at rank 5, significantly behind Oracle, Microsoft SQL Server and MySQL.[8]\n\nIn 2009 Gartner declared that \"IBM DB2 9.7 Shakes Up the DBMS Market With Oracle Compatibility\".[9] This headline refers to the addition to DB2 of several features that are familiar to users of Oracle Database, making it easier for people with Oracle Database skills to work with DB2. These new features include DB2 support for the most commonly used SQL, PL/SQL, and scripting syntax from Oracle Database. They also include DB2 support for additional data types and concurrency models.\n\nIn the clustered DBMS arena, where databases can grow to many terabytes, IBM offers two approaches that compete with Oracle Real Application Clusters (RAC): DB2 pureScale and DB2 Database Partitioning Feature (DPF). DB2 pureScale is a shared-disk database cluster solution that is ideal for high-capacity Online Transaction Processing (OLTP) workloads.[citation needed] DB2 DPF lets users partition a database across multiple servers or within a large SMP server, which is ideal for Online Analytical Processing (OLAP) workloads. (Note that DB2 DPF is sold as part of IBM InfoSphere Warehouse, which is the name for DB2 when it is sold in data warehouse environments.)\n\nDB2 for z/OS arguably has fewer direct competitors. Oracle is attracting customers to its Linux on System z products, although apparently not at the expense of DB2.[citation needed] Oracle has a 31-bit RDBMS available for z/OS (Oracle Database 10g Release 2), but Oracle found it difficult to compete with DB2's feature set on z/OS. Oracle has announced it will support 10g on z/OS as long as customers wish, but the company will not introduce future versions of its database product on z/OS. CA-Datacom and Software AG's ADABAS are competing databases for z/OS, and there are certain niche products as well (Model 204, SUPRA SQL,[10] NOMAD, etc.) Non-relational databases that \"compete\" include IMS, and CA-IDMS, among others. At least some open source databases are ostensibly[original research?] compatible with z/OS UNIX System Services.\n\nIBM and DB2 are frequently at or near the top of the TPC-C[11] and TPC-H[12] industry benchmarks published on the Transaction Processing Performance Council's website.\n\nIn 2006 IBM stepped up its competition in the emerging data warehouse appliance market by releasing a product line of pre-configured hardware/software systems combining DB2 Data Warehouse Edition with either IBM system p (AIX) or IBM system x (Linux) servers. This family of \"warehouse appliance-like\" systems was given the name \"IBM Balanced Configuration Unit\", or BCU, and is aimed at the warehouse appliance market typified by Netezza and DATAllegro, but it differentiates itself in that it uses the full-featured version of DB2 instead of a single-purpose warehouse-oriented RDBMS.\nTechnical information\n\nDB2 can be administered from either the command-line or a GUI. The command-line interface requires more knowledge of the product but can be more easily scripted and automated. The GUI is a multi-platform Java client that contains a variety of wizards suitable for novice users. DB2 supports both SQL and XQuery. DB2 has native implementation of XML data storage, where XML data is stored as XML (not as relational data or CLOB data) for faster access using XQuery.\n\nDB2 has APIs for REXX, PL/I, COBOL, RPG, FORTRAN, C++, C, Delphi, .NET CLI, Java, Python, Perl, PHP, Ruby, and many other programming languages. DB2 also supports integration into the Eclipse and Visual Studio integrated development environments.\n\npureQuery is IBM's data access platform focused on applications that access data. pureQuery supports both Java and .NET. pureQuery provides access to data in databases and in-memory Java objects via its tools, APIs, and runtime environment as delivered in IBM Data Studio Developer and IBM Data Studio pureQuery Runtime.[13]\nError processing\n\nAn important feature of DB2 computer programs is error handling. The SQL communications area (SQLCA) structure was once used exclusively within a DB2 program to return error information to the application program after every SQL statement was executed. The primary, but not singularly useful, error diagnostic is held in the field SQLCODE within the SQLCA block.\n\nThe SQL return code values are:\n\n    0 means successful execution.\n    A positive number means successful execution with one or more warnings. An example is +100, which means no rows found.\n    A negative number means unsuccessful with an error. An example is -911, which means a lock timeout (or deadlock) has occurred, triggering a rollback.\n\nLater versions of DB2 added functionality and complexity to the execution of SQL. Multiple errors or warnings could be returned by the execution of an SQL statement; it may, for example, have initiated a Database Trigger and other SQL statements. Instead of the original SQLCA, error information should now be retrieved by successive executions of a GET DIAGNOSTICS statement.\n\nSee SQL return codes for a more comprehensive list of common SQLCODEs.\nVersions\nLinux, UNIX, and Windows (LUW)\n\n    v3.4 - Code name Cobweb\n    v8.1 - v8.2 - Code name Stinger\n    v9.1 - Code name Viper\n    v9.5 - Code name Viper2\n    v9.7 - Code name Cobra\n    v9.8 - Only pureScale\n    v10.1 - Code name Galileo\n    v10.5 - Code name Kepler (Blu Acceleration)\n        v10.5.4 - Cancun release\n    v11.1[14]\n\nz/OS\n\nV8 V9 V10 V11\niSeries\nV5R3 V5R4 V6R1 V7R1", "skillName": "IBM_DB2."}
{"id": 170, "category": "Databases", "skillText": "Apache CouchDB, commonly referred to as CouchDB, is open source database software that focuses on ease of use and having an architecture that \"completely embraces the Web\".[1] It has a document-oriented NoSQL database architecture and is implemented in the concurrency-oriented language Erlang; it uses JSON to store data, JavaScript as its query language using MapReduce, and HTTP for an API.[1]\n\nCouchDB was first released in 2005 and later became an Apache project in 2008.\n\nUnlike a relational database, a CouchDB database does not store data and relationships in tables. Instead, each database is a collection of independent documents. Each document maintains its own data and self-contained schema. An application may access multiple databases, such as one stored on a user's mobile phone and another on a server. Document metadata contains revision information, making it possible to merge any differences that may have occurred while the databases were disconnected.\n\nCouchDB implements a form of Multi-Version Concurrency Control (MVCC) so it doesn't have to lock the database file during writes. Conflicts are left to the application to resolve. Resolving a conflict generally involves first merging data into one of the documents, then deleting the stale one.[2]\n\nOther features include document-level ACID semantics with eventual consistency, (incremental) MapReduce, and (incremental) replication. One of CouchDB's distinguishing features is multi-master replication, which allows it to scale across machines to build high performance systems. A built-in Web application called Futon helps with administration.\n\nContents\n\n    1 History\n    2 Main features\n    3 Use cases and production deployments\n        3.1 Enterprises that use CouchDB\n    4 Data manipulation: documents and views\n        4.1 Accessing data via HTTP\n    5 Open source components\n    6 See also\n    7 References\n    8 Bibliography\n    9 External links\n\nHistory\n\nCouchDB (Couch is an acronym for cluster of unreliable commodity hardware)[3] is a project created in April 2005 by Damien Katz, former Lotus Notes developer at IBM. Damien Katz defined it as a \"storage system for a large scale object database\". His objectives for the architecture were for it to be database architecture of the Internet and that it would be designed from the ground up to serve Web applications. He self-funded the project for almost two years and released it as an open source project under the GNU General Public License.\n\nIn February 2008, it became an Apache Incubator project and was offered under the Apache License instead.[4] A few months after, it graduated to a top-level project.[5] This led to the first stable version being released in July 2010.[6]\n\nIn early 2012, Damien Katz left the project to focus on Couchbase Server.[7]\n\nSince Katz's departure, the Apache CouchDB project has continued, releasing 1.2 in April 2012 and 1.3 in April 2013. In July 2013, the CouchDB community merged the codebase for BigCouch, Cloudant's clustered version of CouchDB, into the Apache project. The BigCouch clustering framework is prepared to be included in an upcoming release of Apache CouchDB.[8]\nMain features\n\nACID Semantics\n    CouchDB provides ACID semantics.[9] It does this by implementing a form of Multi-Version Concurrency Control, meaning that CouchDB can handle a high volume of concurrent readers and writers without conflict.\nBuilt for Offline\n    CouchDB can replicate to devices (like smartphones) that can go offline and handle data sync for you when the device is back online.\nDistributed Architecture with Replication\n    CouchDB was designed with bi-direction replication (or synchronization) and off-line operation in mind. That means multiple replicas can have their own copies of the same data, modify it, and then sync those changes at a later time.\nDocument Storage\n    CouchDB stores data as \"documents\", as one or more field/value pairs expressed as JSON. Field values can be simple things like strings, numbers, or dates; but ordered lists and associative arrays can also be used. Every document in a CouchDB database has a unique id and there is no required document schema.\nEventual Consistency\n    CouchDB guarantees eventual consistency to be able to provide both availability and partition tolerance.\nMap/Reduce Views and Indexes\n    The stored data is structured using views. In CouchDB, each view is constructed by a JavaScript function that acts as the Map half of a map/reduce operation. The function takes a document and transforms it into a single value that it returns. CouchDB can index views and keep those indexes updated as documents are added, removed, or updated.\nHTTP API\n    All items have a unique URI that gets exposed via HTTP. It uses the HTTP methods POST, GET, PUT and DELETE for the four basic CRUD (Create, Read, Update, Delete) operations on all resources.\n\nCouchDB also offers a built-in administration interface accessible via Web called Futon.[10]\nUse cases and production deployments\n\nReplication and synchronization capabilities of CouchDB make it ideal for using it in mobile devices, where network connection is not guaranteed, but the application must keep on working offline.\n\nCouchDB is well suited for applications with accumulating, occasionally changing data, on which pre-defined queries are to be run and where versioning is important (CRM, CMS systems, by example). Master-master replication is an especially interesting feature, allowing easy multi-site deployments.[11]\nEnterprises that use CouchDB\n\nThe following is a list of notable enterprises that have used or are using CouchDB:\n\n    Amadeus IT Group, for some of their back-end systems.[citation needed]\n    Credit Suisse, for internal use at commodities department for their marketplace framework.[12][better source needed]\n    Meebo, for their social platform (Web and applications).[citation needed] Meebo was acquired by Google and most products were shut down on July 12, 2012.[13]\n    NPM, for their package registry.[14]\n    Sophos, for some of their back-end systems.[citation needed]\n    The BBC, for its dynamic content platforms.[15]\n    Canonical began using it in 2009 for its synchronization service \"Ubuntu One\",[16] but stopped using it in November 2011.[17]\n    CANAL+ for international on demand platform at CANAL+ Overseas, serving hundreds of contents worldwide.\n\nData manipulation: documents and views\n\nCouchDB manages a collection of JSON documents. The documents are organised via views. Views are defined with aggregate functions and filters are computed in parallel, much like MapReduce.\n\nViews are generally stored in the database and their indexes updated continuously. CouchDB supports a view system using external socket servers and a JSON-based protocol.[18] As a consequence, view servers have been developed in a variety of languages (JavaScript is the default, but there are also PHP, Ruby, Python and Erlang).\nAccessing data via HTTP\n\nApplications interact with CouchDB via HTTP. The following demonstrates a few examples using cURL, a command-line utility. These examples assume that CouchDB is running on localhost (127.0.0.1) on port 5984.\nAction \tRequest \tResponse\nAccessing server information \t\n\ncurl http://127.0.0.1:5984/\n\n\t\n\n{\n  \"couchdb\": \"Welcome\",\n  \"version\":\"1.1.0\"\n}\n\nCreating a database named wiki \t\n\ncurl -X PUT http://127.0.0.1:5984/wiki\n\n\t\n\n{\"ok\": true}\n\nAttempting to create a second database named wiki \t\n\ncurl -X PUT http://127.0.0.1:5984/wiki\n\n\t\n\n{\n  \"error\":\"file_exists\",\n  \"reason\":\"The database could not be created, the file already exists.\"\n}\n\nRetrieve information about the wiki database \t\n\ncurl http://127.0.0.1:5984/wiki\n\n\t\n\n{\n  \"db_name\": \"wiki\",\n  \"doc_count\": 0,\n  \"doc_del_count\": 0,\n  \"update_seq\": 0,\n  \"purge_seq\": 0,\n  \"compact_running\": false,\n  \"disk_size\": 79,\n  \"instance_start_time\": \"1272453873691070\",\n  \"disk_format_version\": 5\n}\n\nDelete the database wiki \t\n\ncurl -X DELETE http://127.0.0.1:5984/wiki\n\n\t\n\n{\"ok\": true}\n\nCreate a document, asking CouchDB to supply a document id \t\n\ncurl -X POST -H \"Content-Type: application/json\" --data \\\n'{ \"text\" : \"Wikipedia on CouchDB\", \"rating\": 5 }' \\\nhttp://127.0.0.1:5984/wiki\n\n\t\n\n{\n  \"ok\": true,\n  \"id\": \"123BAC\",\n  \"rev\": \"946B7D1C\"\n}\n\nOpen source components\n\nCouchDB includes a number of other open source projects as part of its default package.\nComponent \tDescription \tLicense\nErlang \tErlang is a general-purpose concurrent programming language and runtime system. The sequential subset of Erlang is a functional language, with strict evaluation, single assignment, and dynamic typing. \tModified MPL 1.0, Apache License 2.0 (Release 18.0 and later)\nICU \tInternational Components for Unicode (ICU) is an open source project of mature C/C++ and Java libraries for Unicode support, software internationalization and software globalization. ICU is widely portable to many operating systems and environments. \tMIT License\njQuery \tjQuery is a lightweight cross-browser JavaScript library that emphasizes interaction between JavaScript and HTML. \tDual license: GPL and MIT\nOpenSSL \tOpenSSL is an open source implementation of the SSL and TLS protocols. The core library (written in the C programming language) implements the basic cryptographic functions and provides various utility functions. \tApache-like unique\nSpiderMonkey \tSpiderMonkey is a code name for the first ever JavaScript engine, written by Brendan Eich at Netscape Communications, later released as open source and now maintained by the Mozilla Foundation. \tMPL", "skillName": "CouchDB."}
{"id": 171, "category": "Databases", "skillText": "Oracle Database (commonly referred to as Oracle RDBMS or simply as Oracle) is an object-relational database management system[3] produced and marketed by Oracle Corporation.\n\nLarry Ellison and his two friends and former co-workers, Bob Miner and Ed Oates, started a consultancy called Software Development Laboratories (SDL) in 1977. SDL developed the original version of the Oracle software. The name Oracle comes from the code-name of a CIA-funded project Ellison had worked on while previously employed by Ampex.[4]\n\nContents\n\n    1 Physical and logical structures\n        1.1 Storage\n            1.1.1 Partitioning\n            1.1.2 Monitoring\n            1.1.3 Disk files\n        1.2 Database schema\n            1.2.1 System Global Area\n            1.2.2 Library cache\n            1.2.3 Data dictionary cache\n            1.2.4 Program Global Area\n            1.2.5 Dynamic performance views\n        1.3 Process architectures\n            1.3.1 Oracle processes\n            1.3.2 User processes, connections and sessions\n        1.4 Concurrency and locking\n        1.5 Configuration\n    2 Administration\n    3 Network access\n    4 Internationalization\n    5 History\n        5.1 Corporate/technical timeline\n        5.2 Patch Updates and Security Alerts\n        5.3 Version numbering\n    6 Oracle Database Product Family\n        6.1 Database Editions\n        6.2 Database Options\n    7 Supported platforms\n    8 Related software\n        8.1 Oracle products\n        8.2 Suites\n        8.3 Database features\n        8.4 Utilities\n        8.5 Tools\n        8.6 Other databases marketed by Oracle Corporation\n        8.7 External routines\n    9 Use\n        9.1 Official support\n        9.2 Database-related guidelines\n        9.3 Oracle Certification Program\n        9.4 User groups\n    10 Market position\n        10.1 Competition\n        10.2 Pricing\n    11 See also\n    12 References\n    13 Bibliography\n    14 External links\n\nPhysical and logical structures\n\nAn Oracle database system—identified by an alphanumeric system identifier or SID[5]—comprises at least one instance of the application, along with data storage. An instance—identified persistently by an instantiation number (or activation id: SYS.V_$DATABASE.ACTIVATION#)—comprises a set of operating-system processes and memory-structures that interact with the storage. (Typical processes include PMON (the process monitor) and SMON (the system monitor)). Oracle documentation can refer to an active database instance as a \"shared memory realm\".[6]\n\nUsers of Oracle databases refer to the server-side memory-structure as the SGA (System Global Area). The SGA typically holds cache information such as data-buffers, SQL commands, and user information. In addition to storage, the database consists of online redo logs (or logs), which hold transactional history. Processes can in turn archive the online redo logs into archive logs (offline redo logs), which provide the basis (if necessary) for data recovery and for the physical-standby forms of data replication using Oracle Data Guard.\n\nIf the Oracle database administrator has implemented Oracle RAC (Real Application Clusters), then multiple instances, usually on different servers, attach to a central storage array. This scenario offers advantages such as better performance, scalability and redundancy. However, support becomes more complex, and many sites do not use RAC. In version 10g, grid computing introduced shared resources where an instance can use (for example) CPU resources from another node (computer) in the grid. The advantage of Oracle RAC is that the resources on both nodes are used by the database, and each node uses its own memory and CPU. Information is shared between nodes through the interconnect—the virtual private network.[7]\n\nThe Oracle DBMS can store and execute stored procedures and functions within itself. PL/SQL (Oracle Corporation's proprietary procedural extension to SQL), or the object-oriented language Java can invoke such code objects and/or provide the programming structures for writing them.\nStorage\n\nThe Oracle RDBMS stores data logically in the form of tablespaces and physically in the form of data files (\"datafiles\").[8] Tablespaces can contain various types of memory segments, such as Data Segments, Index Segments, etc. Segments in turn comprise one or more extents. Extents comprise groups of contiguous data blocks. Data blocks form the basic units of data storage.\n\nA DBA can impose maximum quotas on storage per user within each tablespace.[9]\nPartitioning\n\nThe partitioning feature was introduced in Oracle 8.[10] This allows the partitioning of tables based on different set of keys. Specific partitions can then be easily added or dropped to help manage large data sets.\nMonitoring\n\nOracle database management tracks its computer data storage with the help of information stored in the SYSTEM tablespace. The SYSTEM tablespace contains the data dictionary—and often (by default) indexes and clusters. A data dictionary consists of a special collection of tables that contains information about all user-objects in the database. Since version 8i, the Oracle RDBMS also supports \"locally managed\" tablespaces that store space management information in bitmaps in their own headers rather than in the SYSTEM tablespace (as happens with the default \"dictionary-managed\" tablespaces). Version 10g and later introduced the SYSAUX tablespace, which contains some of the tables formerly stored in the SYSTEM tablespace, along with objects for other tools such as OEM, which previously required its own tablespace.[11]\nDisk files\n[icon] \tThis section requires expansion. (September 2009)\n\nDisk files primarily represent one of the following structures:\n\n    Data and index files: These files provide the physical storage of data, which can consist of the data-dictionary data (associated to the tablespace SYSTEM), user data, or index data. These files can be managed manually or managed by Oracle itself (\"Oracle-managed files\"). Note that a datafile has to belong to exactly one tablespace, whereas a tablespace can consist of multiple datafiles.\n    Redo log files, consisting of all changes to the database, used to recover from an instance failure. Note that often a database will store these files multiple times, for extra security in case of disk failure. The identical redo log files are said to belong to the same group.\n    Undo files: These special datafiles, which can only contain undo information, aid in recovery, rollbacks, and read-consistency.\n    Archive log files: These files, copies of the redo log files, are usually stored at different locations. They are necessary (for example) when applying changes to a standby database, or when performing recovery after a media failure. It is possible to archive to multiple locations.\n    Tempfiles: These special datafiles serve exclusively for temporary storage data (used for example for large sorts or for global temporary tables)\n    Control file, necessary for database startup. \"A binary file that records the physical structure of a database and contains the names and locations of redo log files, the time stamp of the database creation, the current log sequence number, checkpoint information, and so on.\"[12]\n\nAt the physical level, data files comprise one or more data blocks, where the block size can vary between data files.\n\nData files can occupy pre-allocated space in the file system of a computer server, utilize raw disk directly, or exist within ASM logical volumes.[13]\nDatabase schema\n\nMost Oracle database installations traditionally came with a default schema called SCOTT. After the installation process sets up sample tables, the user can log into the database with the username scott and the password tiger. The name of the SCOTT schema originated with Bruce Scott, one of the first employees at Oracle (then Software Development Laboratories), who had a cat named Tiger.[14]\n\nOracle Corporation now de-emphasizes the SCOTT schema, as it uses few features of more recent Oracle releases. Most recent examples supplied by Oracle Corporation reference the default HR or OE schemas.\n\nOther default schemas[15][16] include:\n\n    SYS (essential core database structures and utilities)\n    SYSTEM (additional core database structures and utilities, and privileged account)\n    OUTLN (utilized to store metadata for stored outlines for stable query-optimizer execution plans.[17])\n    BI, IX, HR, OE, PM, and SH (expanded sample schemas[18] containing more data and structures than the older SCOTT schema).\n\nSystem Global Area\nMain article: System Global Area\n\nEach Oracle instance uses a System Global Area or SGA—a shared-memory area—to store its data and control-information.[19]\n\nEach Oracle instance allocates itself an SGA when it starts and de-allocates it at shut-down time. The information in the SGA consists of the following elements, each of which has a fixed size, established at instance startup:\n\n    Datafiles\n\nEvery Oracle database has one or more physical datafiles, which contain all the database data. The data of logical database structures, such as tables and indexes, is physically stored in the datafiles allocated for a database.\n\nDatafiles have the following characteristics:\n\n    One or more datafiles form a logical unit of database storage called a tablespace.\n    A datafile can be associated with only one tablespace.\n    Datafiles can be defined to extend automatically when they are full.\n\nData in a datafile is read, as needed, during normal database operation and stored in the memory cache of Oracle Database. For example, if a user wants to access some data in a table of a database, and if the requested information is not already in the memory cache for the database, then it is read from the appropriate datafiles and stored in memory.\n\nModified or new data is not necessarily written to a datafile immediately. To reduce the amount of disk access and to increase performance, data is pooled in memory and written to the appropriate datafiles all at once.\n\n    the redo log buffer: this stores redo entries—a log of changes made to the database. The instance writes redo log buffers to the redo log as quickly and efficiently as possible. The redo log aids in instance recovery in the event of a system failure.\n    the shared pool: this area of the SGA stores shared-memory structures such as shared SQL areas in the library cache and internal information in the data dictionary. An insufficient amount of memory allocated to the shared pool can cause performance degradation.\n    the Large pool Optional area that provides large memory allocations for certain large processes, such as Oracle backup and recovery operations, and I/O server processes\n    Database buffer cache: Caches blocks of data retrieved from the database\n    KEEP buffer pool: A specialized type of database buffer cache that is tuned to retain blocks of data in memory for long periods of time\n    RECYCLE buffer pool: A specialized type of database buffer cache that is tuned to recycle or remove block from memory quickly\n    nK buffer cache: One of several specialized database buffer caches designed to hold block sizes different from the default database block size\n    Java pool:Used for all session-specific Java code and data in the Java Virtual Machine (JVM)\n    Streams pool: Used by Oracle Streams to store information required by capture and apply\n\nWhen you start the instance by using Enterprise Manager or SQL*Plus, the amount of memory allocated for the SGA is displayed.[20]\nLibrary cache\n\nThe library cache[21] stores shared SQL, caching the parse tree and the execution plan for every unique SQL statement. If multiple applications issue the same SQL statement, each application can access the shared SQL area. This reduces the amount of memory needed and reduces the processing-time used for parsing and execution planning.\nData dictionary cache\n\nThe data dictionary comprises a set of tables and views that map the structure of the database.\n\nOracle databases store information here about the logical and physical structure of the database. The data dictionary contains information such as:\n\n    user information, such as user privileges\n    integrity constraints defined for tables in the database\n    names and datatypes of all columns in database tables\n    information on space allocated and used for schema objects\n\nThe Oracle instance frequently accesses the data dictionary to parse SQL statements. Oracle operation depends on ready access to the data dictionary—performance bottlenecks in the data dictionary affect all Oracle users. Because of this, database administrators must make sure that the data dictionary cache[22] has sufficient capacity to cache this data. Without enough memory for the data-dictionary cache, users see a severe performance degradation. Allocating sufficient memory to the shared pool where the data dictionary cache resides precludes this particular performance problem.\nProgram Global Area\n\nThe Program Global Area[23][24] or PGA memory-area of an Oracle instance contains data and control-information for Oracle's server-processes.\n\nThe size and content of the PGA depends on the Oracle-server options installed. This area consists of the following components:\n\n    stack-space: the memory that holds the session's variables, arrays, and so on\n    session-information: unless using the multithreaded server, the instance stores its session-information in the PGA. In a multithreaded server, the session-information goes in the SGA.)\n    private SQL-area: an area that holds information such as bind-variables and runtime-buffers\n    sorting area: an area in the PGA that holds information on sorts, hash-joins, etc.\n\nDBAs can monitor PGA usage via the system view.\nDynamic performance views\n\nThe dynamic performance views (also known as \"fixed views\") within an Oracle database present information from virtual tables (X$ tables)[25] built on the basis of database memory.[26] Database users can access the V$ views (named after the prefix of their synonyms) to obtain information on database structures and performance.\nProcess architectures\nOracle processes\n\nThe Oracle RDBMS typically relies on a group of processes running simultaneously in the background and interacting to monitor and expedite database operations. Typical operating environments might include - temporarily or permanently - some of the following individual processes (shown along with their abbreviated nomenclature):[27]\n\n    advanced queueing processes (Qnnn)[28]\n    archiver processes (ARCn)\n    checkpoint process (CKPT) *REQUIRED*\n    coordinator-of-job-queues process (CJQn): dynamically spawns slave processes for job-queues\n    database writer processes (DBWn) *REQUIRED*\n    Data Pump worker processes (DWnn)[29]\n\n    dispatcher processes (Dnnn): multiplex server-processes on behalf of users\n    main Data Guard Broker monitor process (DMON)[30]\n    job-queue slave processes (Jnnn)[31]\n    log-writer process (LGWR) *REQUIRED*\n    log-write network-server (LNSn):[32] transmits redo logs in Data Guard environments\n    logical standby coordinator process (LSP0): controls Data Guard log-application\n    media-recovery process (MRP): detached recovery-server process\n    memory-manager process (MMAN): used for internal database tasks such as Automatic Shared Memory Management (ASMM)\n    memory-monitor process (MMON): process for automatic problem-detection, self-tuning and statistics-gathering[33]\n    memory-monitor light process (MMNL): gathers and stores Automatic Workload Repository (AWR) data\n    mmon slaves (Mnnnn—M0000, M0001, etc.): background slaves of the MMON process[34]\n    netslave processes (NSVn): Data Guard Broker inter-database communication processes[35]\n    process-monitor process (PMON) *REQUIRED*\n    process-spawner process (PSP0): spawns Oracle background processes after initial instance startup[36]\n    queue-monitor coordinator process (QMNC): dynamically spawns queue monitor slaves[37]\n    queue-monitor processes (QMNn)\n    recoverer process (RECO)\n    remote file-server process (RFS): in Oracle Data Guard, a standby recipient of primary redo-logs[38]\n    monitor for Data Guard management (RSM0): Data Guard Broker Worker process[39]\n    shared server processes (Snnn): serve client-requests\n    space-management coordinator process (SMCO): coordinates space management (from release 11g)[40]\n    system monitor process (SMON) *REQUIRED*\n\nUser processes, connections and sessions\n\nOracle Database terminology distinguishes different computer-science terms in describing how end-users interact with the database:\n\n    user processes involve the invocation of application software[41]\n    a connection refers to the pathway linking a user process to an Oracle instance[42]\n    sessions consist of specific established groups of interactions, with each group involving a client process and an Oracle instance.[43] Each session within an instance has a session identifier - a session ID or \"SID\"[44][45] (distinct from the Oracle system-identifier SID).\n\nConcurrency and locking\n\nOracle databases control simultaneous access to data resources with locks (alternatively documented as \"enqueues\").[46] The databases also utilize \"latches\" - low-level serialization mechanisms to protect shared data structures in the System Global Area.[47]\n\nOracle locks fall into three categories:[48]\n\n    DML locks (or data locks) protect data\n    DDL locks (or data dictionary locks) protect the structure of schema objects\n    System locks (including latches, mutexes and internal locks) protect internal database structures like data files.\n\nConfiguration\n\nDatabase administrators control many of the tunable variations in an Oracle instance by means of values in a parameter file.[49] This file in its ASCII default form (\"pfile\") normally has a name of the format init<SID-name>.ora. The default binary equivalent server parameter file (\"spfile\") (dynamically reconfigurable to some extent)[50] defaults to the format spfile<SID-name>.ora. Within an SQL-based environment, the views V$PARAMETER[51] and V$SPPARAMETER[52] give access to reading parameter values.\nAdministration\n\nThe \"Scheduler\" (DBMS_SCHEDULER package, available from Oracle 10g onwards) and the Job subsystem (DBMS_JOB package) permit the automation of predictable processing.[53]\n\nOracle Resource Manager aims to allocate CPU resources between users and groups of users when such resources become scarce.[54]\n\nOracle Corporation stated in product announcements that manageability for DBAs had improved from Oracle9i to 10g. Lungu and Vătuiu (2008) assessed the relative manageability by performing common DBA tasks and measuring timings. [55] They performed their tests on a single Pentium CPU (1.7 GHz) with 512 MB RAM,running Windows Server 2000. From Oracle9i to 10g, installation improved 36%, day-to-day administration 63%, backup and recovery 63%, and performance diagnostics and tuning 74%, for a weighted total improvement of 56%. The researchers concluded that \"Oracle10g represents a giant step forward from Oracle9i in making the database easier to use and manage\".\nNetwork access\n\nOracle Net Services allow client or remote applications to access Oracle databases via network sessions using various protocols.\nInternationalization\n\nOracle Database software comes in 63 language-versions (including regional variations such as British English and American English). Variations between versions cover the names of days and months, abbreviations, time-symbols (such as A.M. and A.D.), and sorting.[56]\n\nOracle Corporation has translated Oracle Database error-messages into Arabic, Catalan, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Greek, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Spanish, Swedish, Thai and Turkish.[57]\n\nOracle Corporation provides database developers with tools and mechanisms for producing internationalized database applications: referred to internally as \"Globalization\".[58][59]\nHistory\nCorporate/technical timeline\n\n    1977: Larry Ellison and friends founded Software Development Laboratories (SDL).\n    1978: Oracle Version 1, written in assembly language, runs on PDP-11 under RSX, in 128K of memory. Implementation separates Oracle code and user code. Oracle V1 is never officially released.[60]\n    1979: SDL changed its company-name to \"Relational Software, Inc.\" (RSI) and introduced its product Oracle V2 as an early relational database system - often cited[61][62] as the first commercially sold RDBMS.[63] The version did not support transactions, but implemented the basic SQL functionality of queries and joins. (RSI never released a version 1 - instead calling the first version version 2 as a marketing gimmick.)[64]\n    1982: RSI in its turn changed its name, becoming known as \"Oracle Corporation\",[65] to align itself more closely with its flagship product.\n    1983: The company released Oracle version 3, which it had re-written using the C programming language, and which supported COMMIT and ROLLBACK functionality for transactions. Version 3 extended platform support from the existing Digital VAX/VMS systems to include Unix environments.[65]\n    1984: Oracle Corporation released Oracle version 4, which supported read-consistency.[66] In October it also released the first Oracle for the IBM PC.[67]\n    1985: Oracle Corporation released Oracle version 5, which supported the client–server model—a sign of networks becoming more widely available in the mid-1980s.\n    1986: Oracle version 5.1 started supporting distributed queries.\n    1988: Oracle RDBMS version 6 came out with support for PL/SQL embedded within Oracle Forms v3 (version 6 could not store PL/SQL in the database proper), row-level locking and hot backups.[68]\n    1989: Oracle Corporation entered the application-products market and developed its ERP product, (later to become part of the Oracle E-Business Suite), based on the Oracle relational database.\n    1990: the release of Oracle Applications release 8[65]\n    1992: Oracle version 7 appeared with support for referential integrity, stored procedures and triggers.\n    1997: Oracle Corporation released version 8, which supported object-oriented development and multimedia applications.\n    1999: The release of Oracle8i aimed to provide a database inter-operating better with the Internet (the i in the name stands for \"Internet\"). The Oracle8i database incorporated a native Java virtual machine (Oracle JVM, also known as \"Aurora\").[69]\n    2000: Oracle E-Business Suite 11i pioneers integrated enterprise application software[65]\n    2001: Oracle9i went into release with 400 new features, including the ability to read and write XML documents. 9i also provided an option for Oracle RAC, or \"Real Application Clusters\", a computer-cluster database, as a replacement for the Oracle Parallel Server (OPS) option.\n    2002: the release of Oracle 9i Database Release 2 (9.2.0)[70]\n    2003: Oracle Corporation released Oracle Database 10g, which supported regular expressions. (The g stands for \"grid\"; emphasizing a marketing thrust of presenting 10g as \"grid computing ready\".)\n    2005: Oracle Database 10.2.0.1—also known as Oracle Database 10g Release 2 (10gR2)—appeared.\n    2006: Oracle Corporation announces Unbreakable Linux[65] and acquires i-flex\n    2007: Oracle Database 10g release 2 sets a new world record TPC-H 3000 GB benchmark result[71]\n    2007: Oracle Corporation released Oracle Database 11g for Linux and for Microsoft Windows.\n    2008: Oracle Corporation acquires BEA Systems.\n    2010: Oracle Corporation acquires Sun Microsystems.\n    2011: Oracle Corporation acquires web content management system FatWire Software.\n    2011: On 18 October, Oracle Corporation acquires Endeca Technologies Inc. faceted search engine software vendor.\n    2013: Oracle Corporation released Oracle Database 12c[72] for Linux, Solaris and Windows. (The c stands for \"cloud\".)\n\nPatch Updates and Security Alerts\n\nOracle Corporation releases Critical Patch Updates (CPUs) or Security Patch Updates (SPUs)[73] and Security Alerts to close security holes that could be used for data theft.Critical Patch Updates (CPUs) and Security Alerts \ncome out quarterly on the Tuesday closest to 17th day of the month.\n\n    Customers may receive release notification by email \n    .\n    White Paper: Critical Patch Update Implementation Best Practices \n\nVersion numbering\n\nOracle products follow a custom release numbering and naming convention. With the Oracle RDBMS 10g release, Oracle Corporation began using the \"10g\" label in all versions of its major products, although some sources refer to Oracle Applications Release 11i as Oracle 11i.[clarification needed] The suffixes \"i\", \"g\" and \"c\" do not actually represent a low-order part of the version number, as letters typically represent in software industry version numbering; that is, there is no predecessor version of Oracle 10g called Oracle 10f. Instead, the letters stand for \"internet\", \"grid\" and \"cloud\", respectively.[74] Consequently, many simply drop the \"g\" or \"i\" suffix when referring to specific versions of an Oracle product.\n\nMajor database-related products and some of their versions include:\n\n    Oracle Application Server 10g (also known as \"Oracle AS 10g\"): a middleware product;\n    Oracle Applications Release 11i (aka Oracle e-Business Suite, Oracle Financials or Oracle 11i): a suite of business applications;\n    Oracle Developer Suite 10g (9.0.4);\n    Oracle JDeveloper 10g: a Java integrated development environment;\n\nSince version 2, Oracle's RDBMS release numbering has used the following codes:\n\n    Oracle v2 : 2.3\n    Oracle v3 : 3.1.3\n    Oracle v4 : 4.1.4.0-4.1.4.4\n    Oracle v5 : 5.0.22, 5.1.17, 5.1.22\n    Oracle v6 : 6.0.17-6.0.36 (no OPS code), 6.0.37 (with OPS)\n    Oracle7: 7.0.12–7.3.4\n    Oracle8 Database: 8.0.3–8.0.6\n    Oracle8i Database Release 1: 8.1.5.0–8.1.5.1\n    Oracle8i Database Release 2: 8.1.6.0–8.1.6.3\n    Oracle8i Database Release 3: 8.1.7.0–8.1.7.4\n    Oracle9i Database Release 1: 9.0.1.0–9.0.1.5 (Patchset as of December 2003)\n    Oracle9i Database Release 2: 9.2.0.1–9.2.0.8 (Patchset as of April 2007)\n    Oracle Database 10g Release 1: 10.1.0.2–10.1.0.5 (Patchset as of February 2006)\n    Oracle Database 10g Release 2: 10.2.0.1–10.2.0.5 (Patchset as of April 2010)[75]\n    Oracle Database 11g Release 1: 11.1.0.6–11.1.0.7 (Patchset as of September 2008)\n    Oracle Database 11g Release 2: 11.2.0.1–11.2.0.4 (Patchset as of August 2013)\n    Oracle Database 12c Release 1: 12.1.0.1 (Patchset as of June 2013)\n    Oracle Database 12c Release 1: 12.1.0.2 (Patchset as of July 2014)\n\nThe version-numbering syntax within each release follows the pattern: major.maintenance.application-server.component-specific.platform-specific.\n\nFor example, \"10.2.0.1 for 64-bit Solaris\" means: 10th major version of Oracle, maintenance level 2, Oracle Application Server (OracleAS) 0, level 1 for Solaris 64-bit.\n\nThe Oracle Database Administrator's Guide \noffers further information on Oracle release numbers.\nOracle Database Product Family\n\nBased on licensing and pricing, Oracle Corporation groups its Oracle Database-related product portfolio into the \"Oracle Database Product Family\", which consists of the following:[76]\n\n    Oracle Database Editions, variations of the software designed for different scenarios.\n    Database Options, extra cost offers providing additional database functionality.\n    Oracle Data Models, database schemas, offering pre-built data models with database analytics and business intelligence capabilities for specific industries.\n    Management packs, integrated set of Oracle Enterprise Manager tools for maintaining various aspects of Oracle Database.\n    Some of Oracle engineered systems, either specifically built for Oracle Database deployment or supporting such capability.\n    Other related products intended for use with Oracle Database.\n\nDatabase Editions\n\nAs of 2016 the latest Oracle Database version (12.1.0.2) comes in two editions:[76]\n\n    Oracle Database 12c Enterprise Edition (EE): Oracle Corporation's flagship database product. A fully featured edition of Oracle Database, it also allows purchase of add-on features in the form of Database Options and Management packs and imposes no limitation on server resources available to the database.[77]\n    Oracle Database 12c Standard Edition 2 (SE2): intended for small- to medium-sized implementations, this edition comes with Real Application Clusters option included, a reduced set of database features, and the licensing restriction to run on servers or clusters with a maximum of 2 sockets total and capped to use a maximum of 16 concurrent user threads.[78] Oracle positions SE2 as a starter edition, stressing complete upward compatibility and ease of upgrade to the more costly Enterprise Edition.[76][79]\n\nOracle Corporation also makes the following editions available:[80]\n\n    Oracle Database Express Edition 11gR2 (Oracle Database XE), a free-to-use entry-level version of Oracle Database 11gR2 available for Windows and Linux platforms limited to using only one CPU, up to 1 GB of RAM and storing up to 11 GB of user data. Oracle Database XE is a separate product from the rest of Oracle Database product family. It provides a subset of Standard Edition functionality (lacking features such as Java Virtual Machine, managed backup and recovery and high availability), is community-supported and comes with its own license terms.[81] Express Edition was first introduced in 2005 with Oracle 10g release with a limitation to a maximum of 4 GB of user data.[82] Oracle 11g Express Edition, released on 24 September 2011,[83] increased user data cap to 11 GB.[84]\n    Oracle Database Personal Edition, a single-user, single-machine development and deployment license, allows use of all database features and extra-cost database options (with the exception of the Oracle RAC option). It is available for purchase for Windows and Linux platforms only and does not include management packs.[80]\n\nUp to and including Oracle Database 12.1.0.1, Oracle also offered the following:[85]\n\n    Standard Edition (SE) ran on single or clustered servers with a maximum capacity of 4 CPU sockets. It was largely the same as the current SE2 offer, including Real Application Clusters option at no additional cost, however allowing twice as much CPU sockets in a server or a cluster.\n    Standard Edition One (SE1), introduced with Oracle 10g, offered the same features as SE and was licensed to run on single servers with a maximum of two CPU sockets.\n\nOracle Corporation discontinued SE and SE1 with the 12.1.0.2 release and stopped offering new licenses for these editions on December 1, 2015.[86] Industry journalists and some[quantify] ISVs perceived Oracle's desupport of affordable SE1 and restrictive updates to SE in the form of SE2 (specifically, the introduction of thread throttling and halving the number of licensable CPU sockets without changing price-per-socket) as an attempt to repress customers' efforts to scale SE/SE1 installations up to \"enterprise\" class by means of virtualization, while at the same time pushing them towards the more expensive Enterprise Edition or to Oracle Cloud Database as a service.[87][88]\nDatabase Options\n\nOracle Corporation refers to a number of add-on database features as \"Database Options\".[76] These aim to enhance and complement existing database functionality to meet customer-specific requirements.[89] All Database Options are only available for Enterprise Edition and offered for an extra cost.[76][90] The one exception to these two rules is Oracle Real Application Clusters option which comes included with Oracle Database 12c Standard Edition 2 at no additional cost.[80]\n\n    Oracle Active Data Guard \n    extends Oracle Data Guard functionality with advanced features, allowing read-only access to data in a physical standby database to offload primary of such tasks as reporting, ad-hoc queries, data extraction and backup, offloading redo transport and minimizing standby impact on commit response times (using Far Sync feature), providing option for rolling upgrades for non-RAC customers, managing clients workload across replicated database and improving automated service failover (using Global Data Services), etc.\n    Oracle Advanced Analytics \n    allows access to in-database data mining algorithms and use of Oracle R Enterprise functionality, an integration with open-source R statistical programming language and environment.\n    Oracle Advanced Compression \n    complements Enterprise Edition basic table compression feature with comprehensive data compression and Information Lifecycle Management capabilities, including those specifically tailored to Oracle's engineered systems, like Oracle Exadata.\n    Oracle Advanced Security \n    provides Transparent Data Encryption and Data Redaction security features, the former allowing encryption of data stored in a database (all or a subset of it), exported using Data Pump, or backed up using Oracle Recovery Manager, and the latter allowing redaction of sensitive database data (e.g., credit card or social security numbers) returned to database applications.\n    Oracle Database In-Memory \n    , an in-memory, column-oriented data store, has been seamlessly integrated[citation needed] into the Oracle Database. This technology aims to improve the performance of analytic workloads without impacting the performance of transactions that continue to use Oracle's traditional row format in memory. Note: data is persisted on disk only in a row format, so no additional storage is required. The product's performance comes through the in-memory columnar format and throughthe use of SIMD vector processing (Single Instruction processing Multiple Data values). Database In-Memory features include:\n        An In-Memory column store, a new[when?] component of the SGA called the In-Memory Area. One can allocate a little or a lot of memory to the In-Memory Area. The larger the In-Memory Area, the greater the number of objects that can be brought into memory in the In-Memory columnar format. Unlike in a pure in-memory database not all of the data in the Oracle Database requires populating into memory in the columnar format.\n        Only objects with the INMEMORY attribute get populated into the In-Memory column store. The INMEMORY attribute can be specified on a tablespace, table, (sub)partition, or materialized view. If it is enabled at the tablespace level, then all tables and materialized views in the tablespace are enabled for In-Memory by default.\n        Data is populated into a new In-Memory column store by a set of background processes referred to as worker processes (ora_w001_orcl). Each worker process receives a subset of database blocks from the object to populate into the In-Memory column store. Population is a streaming mechanism, simultaneously columnizing and compressing the data.\n        Oracle takes advantage of SIMD vector processing to scan the data in the columnar format. Instead of evaluating each entry in the column one at a time, SIMD vector processing allows a set of column values to be evaluated together in a single CPU instruction. The column format used in the IM column store has been specifically designed[by whom?] to maximize the number of column entries that can be loaded into the vector registers on the CPU and evaluated in a single CPU instruction.\n        Fault tolerance for In-Memory Column Store runs on Oracle Engineered Systems (Oracle Exadata, Oracle Database Appliance and Oracle Supercluster), mirroring the data in memory across RAC nodes. If one RAC node fails, the database simply reads from the other side of the mirror.\n        In-Memory Aggregation improves performance of typical analytic queries using efficient in-memory arrays for joins and aggregation.[91][need quotation to verify]\n    Oracle Database Vault \n    enforces segregation of duties, principle of least privilege and other data access controls, allowing protection of application data from access by privileged database users.\n    Oracle Label Security \n    is a sophisticated and flexible framework for a fine-grained label based access control (LBAC) implementation.\n    Oracle Multitenant \n    is the capability that allows database consolidation and provides additional abstraction layer. In a Multitenant configuration, one Oracle database instance known as \"container database\" (CDB) acts as a federated database system for a collection of up to 252 distinct portable collections of database objects, referred to as \"pluggable databases\" (PDB), each appearing to an outside client as a regular non-CDB Oracle database.\n    Oracle On-Line Analytical Processing (OLAP) \n    is Oracle implementation of online analytical processing.\n    Oracle Partitioning \n    allows partitioning of tables and indices, where large objects are stored in database as a collection of individual smaller pieces at the same time appearing on application level as a uniform data object.\n    Oracle RAC One Node \n    is a one-node version of Oracle Real Application Clusters, providing capabilities for database failover and high availability in the form of rolling upgrades, online instance migration, application continuity and automated quality of service management.\n    Oracle Real Application Clusters \n    (Oracle RAC) is the cluster solution for Oracle Database.\n    Oracle Real Application Testing \n    is a suite of features that enable comprehensive testing of system changes in a simulation of production-level workload and use. The option includes Database Replay, SQL Performance Analyzer, Database Consolidation Workbench and SQL Tuning Sets.\n    Oracle Spatial and Graph[92] complements the Oracle Locator feature (available in all editions of Oracle Database[93]) with advanced spatial capabilities enabling the development of complex geographic information systems and includes network data model and RDF/OWL Semantic graphs.\n    Oracle TimesTen Application-Tier Database Cache \n    allows caching subsets of a database in the application tier for improved response time. It is built using Oracle TimesTen In-Memory Database.\n\nSupported platforms\n\nOracle Database 12c is supported on the following OS and architecture combinations:\n\n    Linux on x86-64 (only Red Hat Enterprise Linux, Oracle Linux and SUSE distributions are supported[94])\n    Microsoft Windows on x86-64\n    Oracle Solaris on SPARC and x86-64\n    IBM AIX on POWER Systems\n    IBM Linux on z Systems\n    HP-UX on Itanium\n\nIn 2011, Oracle Corporation announced the availability of Oracle Database Appliance, a pre-built, pre-tuned, highly available clustered database server built using two SunFire X86 servers and direct attached storage.\n\nSome Oracle Enterprise edition databases running on certain Oracle-supplied hardware can utilize Hybrid Columnar Compression for more efficient storage.[95]\nRelated software\nOracle products\n\n    Oracle Database Firewall[96] analyzes database traffic on a network to prevent threats such as SQL injection.[97]\n\nSuites\n\nIn addition to its RDBMS, Oracle Corporation has released several related suites of tools and applications relating to implementations of Oracle databases. For example:\n\n    Oracle Application Server, a J2EE-based application server, aids in developing and deploying applications that use Internet technologies and a browser.\n    Oracle Collaboration Suite contains messaging, groupware and collaboration applications.\n    Oracle Developer Suite contains software development tools, including JDeveloper.\n    Oracle E-Business Suite collects together applications for enterprise resource planning (including Oracle Financials), customer relationship management and human resources management (Oracle HR).\n    Oracle Enterprise Manager (OEM) used by database administrators (DBAs) to manage the DBMS, and recently in version 10g, a web-based rewrite of OEM called \"Oracle Enterprise Manager Database Control\". Oracle Corporation has dubbed the super-Enterprise-Manager used to manage a grid of multiple DBMS and Application Servers \"Oracle Enterprise Manager Grid Control\".\n    Oracle Programmer/2000, a bundling of interfaces for 3GL programming languages, marketed with Oracle7 and Oracle8.[98][99]\n    Oracle WebCenter Suite[100]\n\nDatabase features\n\nApart from the clearly defined database options, Oracle databases may include many semi-autonomous software sub-systems, which Oracle Corporation sometimes refers to as \"features\" in a sense subtly different from the normal usage of the word. For example, Oracle Data Guard counts officially as a feature, but the command-stack within SQL*Plus, though a usability feature, does not appear in the list of \"features\" in Oracle's list \n.[original research?] Such \"features\" may include (for example):\n\n    Active Session History (ASH), the collection of data for immediate monitoring of very recent database activity.[101]\n    Automatic Workload Repository (AWR)],[102][103] providing monitoring and statistical services to Oracle database installations from Oracle version 10. Prior to the release of Oracle version 10, the Statspack facility[104] provided similar functionality.\n    Clusterware\n    Data Aggregation and Consolidation \n    Data Guard \n    for high availability\n    Generic Connectivity \n    for connecting to non-Oracle systems.\n    Data Pump utilities, which aid in importing and exporting data and metadata between databases[105]\n    SQL*Loader, utility that facilitates high performance data loading.\n    Database Resource Manager (DRM), which controls the use of computing resources.[106]\n    Fast-start parallel rollback[107]\n    Fine-grained auditing (FGA) (in Oracle Enterprise Edition)[108] supplements standard security-auditing features[109]\n    Flashback for selective data recovery and reconstruction[110]\n    iSQL*Plus \n    , a web-browser-based graphical user interface (GUI) for Oracle database data-manipulation (compare SQL*Plus)\n    Oracle Data Access Components (ODAC), tools that consist of:[111]\n        Oracle Data Provider for .NET (ODP.NET)[112]\n        Oracle Developer Tools (ODT) for Visual Studio\n        Oracle Providers for ASP.NET\n        Oracle Database Extensions for .NET\n        Oracle Provider for OLE DB\n        Oracle Objects for OLE\n        Oracle Services for Microsoft Transaction Server\n    Oracle-managed files \n    (OMF) - a feature allowing automated naming, creation and deletion of datafiles at the operating-system level.\n    Oracle Multimedia (known as \"Oracle interMedia\" before Oracle 11g) for storing and integrating multimedia data within a database[113]\n    Oracle Spatial and Graph\n    Recovery Manager \n    (rman) for database backup, restoration and recovery\n    SQL*Plus, a program that allows users to interact with Oracle database(s) via SQL and PL/SQL commands on a command-line. Compare iSQL*Plus.\n    SQLcl, a command-line interface for queries, developed on the basis of Oracle SQL Developer[114]\n\n    Universal Connection Pool (UCP), a connection pool based on Java and supporting JDBC, LDAP, and JCA[115]\n\n    Virtual Private Database[116] (VPD), an implementation of fine-grained access control.[117]\n    Oracle Application Express, a no-cost environment for database-oriented software-development[118]\n    Oracle XML DB,[119] or XDB,[120] a no-cost component in each edition of the database, provides high-performance technology for storing and retrieving native XML.\n\n    Oracle GoldenGate 11g \n    (distributed real-time data acquisition)\n    Oracle Text uses standard SQL to index, search, and analyze text and documents stored in the Oracle database.\n\nThis list is incomplete; you can help by expanding it.\nUtilities\n\nOracle Corporation classifies as \"utilities\" bundled software supporting data transfer, data maintenance and database administration.[121]\n\nUtilities included in Oracle database distributions include:\n\n    oradebug - interfaces with Oracle session tracing[122]\n\nTools\n\nUsers can develop their own applications in Java and PL/SQL using tools such as:\n\n    Oracle Forms\n    Oracle JDeveloper\n    Oracle Reports\n\nAs of 2007 Oracle Corporation had started a drive toward \"wizard\"-driven environments with a view to enabling non-programmers to produce simple data-driven applications.[123]\n\nThe Database Upgrade Assistant (DBUA)[124] provides a GUI for the upgrading of an Oracle database.[125]\n\nJAccelerator (NCOMP) - a native-compilation Java \"accelerator\", integrates hardware-optimized Java code into an Oracle 10g database.[126]\n\nOracle SQL Developer, a free graphical tool for database development, allows developers to browse database objects, run SQL statements and SQL scripts, and edit and debug PL/SQL statements. It incorporates standard and customized reporting.\n\nOracle's OPatch provides patch management for Oracle databases.[127]\n\nThe SQLTXPLAIN tool (or SQLT) provides tuning assistance for Oracle SQL queries.[128]\nOther databases marketed by Oracle Corporation\n\nHaving acquired other technology in the database field, Oracle Corporation can also offer:\n\n    TimesTen, a memory-resident database that can cache transactions and synchronize data with a centralized Oracle database server. It functions as a real-time infrastructure software product intended for the management of low-latency, high-volume data, of events and of transactions.\n    BerkeleyDB, a simple, high-performance, embedded database\n    Oracle Rdb, a legacy relational database for the OpenVMS operating-system\n    MySQL, a relational database purchased as part of Oracle Corporation's takeover of MySQL's immediate previous owner, Sun Microsystems\n    Oracle NoSQL Database, a scalable, distributed key-value NoSQL database[129]\n\nExternal routines\n\nPL/SQL routines within Oracle databases can access external routines registered in operating-system shared libraries.[130][131]\nUse\n\nThe Oracle RDBMS has had a reputation among novice users as difficult to install on Linux systems.[citation needed] Oracle Corporation has packaged recent versions for several popular Linux distributions in an attempt to minimize installation challenges beyond the level of technical expertise required to install a database server.[citation needed]\nOfficial support\n\nUsers who have Oracle support contracts can use Oracle's \"My Oracle Support\" or \"MOS\"[132] web site - known as \"MetaLink\" until a re-branding exercise completed in October 2010. The support site provides users of Oracle Corporation products with a repository of reported problems, diagnostic scripts and solutions. It also integrates with the provision of support tools, patches and upgrades.\n\nThe Remote Diagnostic Agent or RDA[133] can operate as a command-line diagnostic tool executing a script. The data captured provides an overview of the Oracle Database environment intended for diagnostic and trouble-shooting. Within RDA, the HCVE (Health Check Validation Engine)[134] can verify and isolate host system environmental issues that may affect the performance of Oracle software.\nDatabase-related guidelines\n\nOracle Corporation also endorses certain practices and conventions as enhancing the use of its database products. These include:\n\n    Oracle Maximum Availability Architecture (MAA)[135] guidelines on developing high-availability systems\n    Optimal Flexible Architecture (OFA), blueprints for mapping Oracle-database objects to file-systems\n\nOracle Certification Program\nMain article: Oracle Certification Program\n\nThe Oracle Certification Program, a professional certification program, includes the administration of Oracle Databases as one of its main certification paths. It contains three levels:\n\n    Oracle Certified Associate (OCA)\n    Oracle Certified Professional (OCP)\n    Oracle Certified Master (OCM)\n\nUser groups\n\nA variety of official (Oracle-sponsored) and unofficial Oracle User Groups has grown up of users and developers of Oracle databases. They include:\n\n    Geographical/regional user groups[136]\n    Independent Oracle Users Group\n    Industry-centric user groups\n    Oracle Technology Network\n    Oracle Health Sciences User Group \n    Product-centric user groups\n    The OakTable Network\n    Usenet newsgroups\n\nMarket position\n\nAs of 2013 Oracle holds #1 DBMS market share worldwide based on the revenue share ahead of its four closest competitors - IBM , Microsoft, SAP and Teradata.[137]\nCompetition\n\nIn the market for relational databases, Oracle Database competes against commercial products such as IBM's DB2 UDB and Microsoft SQL Server. Oracle and IBM tend to battle for the mid-range database market on UNIX and Linux platforms, while Microsoft dominates the mid-range database market on Microsoft Windows platforms. However, since they share many of the same customers, Oracle and IBM tend to support each other's products in many middleware and application categories (for example: WebSphere, PeopleSoft, and Siebel Systems CRM), and IBM's hardware divisions work closely[citation needed] with Oracle on performance-optimizing server-technologies (for example, Linux on z Systems). The two companies have a relationship perhaps[original research?] best described as \"coopetition\". Niche commercial competitors include Teradata (in data warehousing and business intelligence), Software AG's ADABAS, Sybase, and IBM's Informix, among many others.\n\nIncreasingly, the Oracle database products compete against such open-source software relational database systems as PostgreSQL, Firebird, and MySQL. Oracle acquired Innobase, supplier of the InnoDB codebase to MySQL, in part to compete better against open source alternatives, and acquired Sun Microsystems, owner of MySQL, in 2010. Database products licensed as open source are, by the legal terms of the Open Source Definition, free to distribute and free of royalty or other licensing fees.\nPricing\n\nOracle Corporation offers term licensing for all Oracle products. It bases the list price for a term-license on a specific percentage of the perpetual license price. Prospective purchasers can obtain licenses based either on the number of processors in their target machines or on the number of potential seats (\"named users\").[138]\n\nEnterprise Edition (DB EE)\n    As of July 2010, the database that costs the most per machine-processor among Oracle database editions, at $47,500 per processor. The term \"per processor\" for Enterprise Edition is defined with respect to physical cores and a processor core multiplier (common processors = 0.5*cores). e.g. An 8-processor, 32-core server using Intel Xeon 56XX CPUs would require 16 processor licenses.[139][140]\nStandard Edition (DB SE)\n    Cheaper: it can run on up to four processors but has fewer features than Enterprise Edition—it lacks proper parallelization,[141] etc.; but remains quite suitable for running medium-sized applications. There are not additional cost for Oracle RAC on the latest Oracle 11g R2 standard edition release.\nStandard ONE (DB SE1 or DB SEO)\n    Sells even more cheaply, but remains limited to two CPUs. Standard Edition ONE sells on a per-seat basis with a five-user minimum. Oracle Corporation usually sells the licenses with an extra 22% cost for support and upgrades (access to My Oracle Support—Oracle Corporation's support site), which customers must renew annually.\nOracle Express Edition (DB XE)[142] (Oracle XE)\n    An addition to the Oracle database product family (beta version released in 2005, production version released in February 2006), offers a free version of the Oracle RDBMS, but one limited to 11 GB of user data and to 1 GB of memory used by the database (SGA+PGA).[143] XE will use no more than one CPU and lacks an internal JVM. XE runs on 32-bit and 64-bit Windows and 64-bit Linux, but not on AIX, Solaris, HP-UX and the other operating systems available for other editions. Support is via a free Oracle Discussion Forum \n    only.", "skillName": "Oracle_Database."}
{"id": 172, "category": "Databases", "skillText": "Databases\nSQL and relational databases\nNoSQL Databases\nNoSQL, Mongo, Redis\nNoSQL, Teradata\nExcel\nMongoDB\nsql\nnosql\nrelatinal database\nnonrelational database\nOracle\nMySQL \nMicrosoft SQL Server\nMongoDB \nPostgreSQL\nDB2\nCassandra \nMicrosoft Access\nSQLite\nRedis \nElasticsearch \nTeradata\nSAP Adaptive Server\nSolr\nHBase\nFileMaker\nHive\nSplunk\nSAP HANA \nMariaDB\nNeo4j \nInformix\nMemcached\nCouchbase \nAmazon DynamoDB \nCouchDB\nMicrosoft Azure SQL Database\nNetezza\nVertica\nFirebird\nRiak KV \nIngres\nMarkLogic\ndBASE\nGreenplum\nAmazon Redshift \nImpala\nDB2\nMySQL\nOracle\nPostgreSQL\nSQLite\nSQL Server\nSybase\nRethinkDB\nBerkeley DB\nmemcached\nredis\ncouchDB\nmongoDB", "skillName": "Databases."}
{"id": 173, "category": "Databases", "skillText": "SPARQL (pronounced \"sparkle\", a recursive acronym[2] for SPARQL Protocol and RDF Query Language) is an RDF query language, that is, a semantic query language for databases, able to retrieve and manipulate data stored in Resource Description Framework (RDF) format.[3][4] It was made a standard by the RDF Data Access Working Group (DAWG) of the World Wide Web Consortium, and is recognized as one of the key technologies of the semantic web. On 15 January 2008, SPARQL 1.0 became an official W3C Recommendation,[5][6] and SPARQL 1.1 in March, 2013.[7]\n\nSPARQL allows for a query to consist of triple patterns, conjunctions, disjunctions, and optional patterns.[8]\n\nImplementations for multiple programming languages exist.[9] There exist tools that allow one to connect and semi-automatically construct a SPARQL query for a SPARQL endpoint, for example ViziQuer.[10] In addition, there exist tools that translate SPARQL queries to other query languages, for example to SQL[11] and to XQuery.[12]\n\nContents\n\n    1 Advantages\n    2 Query forms\n    3 Example\n    4 Extensions\n    5 Implementations\n    6 References\n    7 External links\n\nAdvantages\n\nSPARQL allows users to write queries against what can loosely be called \"key-value\" data or, more specifically, data that follows the RDF specification of the W3C. The entire database is thus a set of \"subject-predicate-object\" triples. This is analogous to some NoSQL databases' usage of the term \"document-key-value\", such as MongoDB.\n\nRDF data can also be considered in SQL relational database terms as a table with three columns – the subject column, the predicate column and the object column. Unlike relational databases, the object column is heterogeneous: the per-cell data type is usually implied (or specified in the ontology) by the predicate value. Alternately, again comparing to SQL relations, all of the triples for a given subject could be represented as a row, with the subject being the primary key and each possible predicate being a column and the object is the value in the cell. However, SPARQL/RDF becomes easier and more powerful for columns that could contain multiple values (like \"children\") for the same key, and where the column itself could be a joinable variable in the query, rather than directly specified.\n\nSPARQL thus provides a full set of analytic query operations such as JOIN, SORT, AGGREGATE for data whose schema is intrinsically part of the data rather than requiring a separate schema definition. Schema information (the ontology) is often provided externally, though, to allow different datasets to be joined in an unambiguous manner. In addition, SPARQL provides specific graph traversal syntax for data that can be thought of as a graph and fig.\n\nThe example below demonstrates a simple query that leverages the ontology definition \"foaf\", often called the \"friend-of-a-friend\" ontology.\n\nSpecifically, the following query returns names and emails of every person in the dataset:\n\nPREFIX foaf: <http://xmlns.com/foaf/0.1/>\nSELECT ?name ?email\nWHERE {\n  ?person a foaf:Person.\n  ?person foaf:name ?name.\n  ?person foaf:mbox ?email.\n}\n\nThis query joins together all of the triples with a matching subject, where the type predicate, \"a\", is a person (foaf:Person) and the person has one or more names (foaf:name) and mailboxes (foaf:mbox).\n\nThe author of this query chose to reference the subject using the variable name \"?person\" for readable clarity. Since the first element of the triple is always the subject, the author could have just as easily used any variable name, such as \"?subj\" or \"?x\". Whatever name is chosen, it must be the same on each line of the query to signify that the query engine is to join triples with the same subject.\n\nThe result of the join is a set of rows – ?person, ?name, ?email. This query returns the ?name and ?email because ?person is often a complex URI rather than a human-friendly string. Note that some of the ?people may have multiple mailboxes, so in the returned set, a ?name row may appear multiple times, once for each mailbox.\n\nThis query can be distributed to multiple SPARQL endpoints (services that accept SPARQL queries and return results), computed, and results gathered, a procedure known as federated query.\n\nWhether in a federated manner or locally, additional triple definitions in the query could allow joins to different subject types, such as automobiles, to allow simple queries, for example, to return a list of names and emails for people who drive automobiles with a high fuel efficiency.\nQuery forms\n\nIn the case of queries that read data from the database, the SPARQL language specifies four different query variations for different purposes.\n\nSELECT query\n    Used to extract raw values from a SPARQL endpoint, the results are returned in a table format.\nCONSTRUCT query\n    Used to extract information from the SPARQL endpoint and transform the results into valid RDF.\nASK query\n    Used to provide a simple True/False result for a query on a SPARQL endpoint.\nDESCRIBE query\n    Used to extract an RDF graph from the SPARQL endpoint, the content of which is left to the endpoint to decide based on what the maintainer deems as useful information.\n\nEach of these query forms takes a WHERE block to restrict the query although in the case of the DESCRIBE query the WHERE is optional.\n\nSPARQL 1.1 specifies a language for updating the database with several new query forms.\nExample\n\nAnother SPARQL query example that models the question \"What are all the country capitals in Africa?\":\n\nPREFIX ex: <http://example.com/exampleOntology#>\nSELECT ?capital ?country\nWHERE {\n  ?x ex:cityname ?capital ;\n     ex:isCapitalOf ?y .\n  ?y ex:countryname ?country ;\n     ex:isInContinent ex:Africa .\n}\n\nVariables are indicated by a \"?\" or \"$\" prefix. Bindings for ?capital and the ?country will be returned.\n\nThe SPARQL query processor will search for sets of triples that match these four triple patterns, binding the variables in the query to the corresponding parts of each triple. Important to note here is the \"property orientation\" (class matches can be conducted solely through class-attributes or properties – see Duck typing)\n\nTo make queries concise, SPARQL allows the definition of prefixes and base URIs in a fashion similar to Turtle. In this query, the prefix \"ex\" stands for “http://example.com/exampleOntology#”.\nExtensions\n\nGeoSPARQL defines filter functions for geographic information system (GIS) queries using well-understood OGC standards (GML, WKT, etc.).\n\nSPARUL is another extension to SPARQL. It enables the RDF store to be updated with this declarative query language, by adding INSERT and DELETE methods.\nImplementations\nMain article: List of SPARQL implementations\n\nOpen source, reference SPARQL implementations\n\n    Jena (framework) from Apache Software Foundation\n    Virtuoso\n\nSee List of SPARQL implementations for more comprehensive coverage, including triplestore, APIs, and other storages that have implemented the SPARQL standard.", "skillName": "SPARQL."}
{"id": 174, "category": "Databases", "skillText": "NoSQL\nA NoSQL (originally referring to \"non SQL\" or \"non relational\")[1] database provides a mechanism for storage and retrieval of data which is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but did not obtain the \"NoSQL\" moniker until a surge of popularity in the early twenty-first century,[2] triggered by the needs of Web 2.0 companies such as Facebook, Google and Amazon.com.[3][4][5] NoSQL databases are increasingly used in big data and real-time web applications.[6] NoSQL systems are also sometimes called \"Not only SQL\" to emphasize that they may support SQL-like query languages.[7][8]\n\nMotivations for this approach include: simplicity of design, simpler \"horizontal\" scaling to clusters of machines (which is a problem for relational databases),[2] and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve. Sometimes the data structures used by NoSQL databases are also viewed as \"more flexible\" than relational database tables.[9]\n\nMany NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance the lack of ability to perform ad-hoc JOINs across tables), lack of standardized interfaces, and huge previous investments in existing relational databases.[10] Most NoSQL stores lack true ACID transactions, although a few databases, such as MarkLogic, Aerospike, FairCom c-treeACE, Google Spanner (though technically a NewSQL database), Symas LMDB and OrientDB have made them central to their designs. (See ACID and JOIN Support.)\n\nInstead, most NoSQL databases offer a concept of \"eventual consistency\" in which database changes are propagated to all nodes \"eventually\" (typically within milliseconds) so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.[11] Additionally, some NoSQL systems may exhibit lost writes and other forms of data loss.[12] Fortunately, some NoSQL systems provide concepts such as write-ahead logging to avoid data loss.[13] For distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Even current relational databases \"do not allow referential integrity constraints to span databases.\"[14] There are few systems that maintain both ACID transactions and X/Open XA standards for distributed transaction processing.\n\nContents  [hide]\n1\tHistory\n2\tTypes and examples of NoSQL databases\n2.1\tKey-value stores\n2.2\tDocument store\n2.3\tGraph\n2.4\tObject database\n2.5\tTabular\n2.6\tTuple store\n2.7\tTriple/quad store (RDF) database\n2.8\tHosted\n2.9\tMultivalue databases\n2.10\tMultimodel database\n3\tPerformance\n4\tHandling relational data\n4.1\tMultiple queries\n4.2\tCaching/replication/non-normalized data\n4.3\tNesting data\n5\tACID and JOIN Support\n6\tSee also\n7\tReferences\n8\tFurther reading\n9\tExternal links\nHistory\nThe term NoSQL was used by Carlo Strozzi in 1998 to name his lightweight, Strozzi NoSQL open-source relational database that did not expose the standard SQL interface, but was still relational.[15] His NoSQL RDBMS is distinct from the circa-2009 general concept of NoSQL databases. Strozzi suggests that, as the current NoSQL movement \"departs from the relational model altogether; it should therefore have been called more appropriately 'NoREL'\",[16] referring to 'No Relational'.\n\nJohan Oskarsson of Last.fm reintroduced the term NoSQL in early 2009 when he organized an event to discuss \"open source distributed, non relational databases\".[17] The name attempted to label the emergence of an increasing number of non-relational, distributed data stores, including open source clones of Google's BigTable/MapReduce and Amazon's Dynamo. Most of the early NoSQL systems did not attempt to provide atomicity, consistency, isolation and durability guarantees, contrary to the prevailing practice among relational database systems.[18]\n\nBased on 2014 revenue, the NoSQL market leaders are MarkLogic, MongoDB, and Datastax.[19] Based on 2015 popularity rankings, the most popular NoSQL databases are MongoDB, Apache Cassandra, and Redis.[20]\n\nTypes and examples of NoSQL databases\nThere have been various approaches to classify NoSQL databases, each with different categories and subcategories, some of which overlap. What follows is a basic classification by data model, with examples:\n\nColumn: Accumulo, Cassandra, Druid, HBase, Vertica\nDocument: Apache CouchDB, Clusterpoint, Couchbase, DocumentDB, HyperDex, Lotus Notes, MarkLogic, MongoDB, OrientDB, Qizx, RethinkDB\nKey-value: Aerospike, Couchbase, Dynamo, FairCom c-treeACE, FoundationDB, HyperDex, MemcacheDB, MUMPS, Oracle NoSQL Database, OrientDB, Redis, Riak, Berkeley DB\nGraph: AllegroGraph, InfiniteGraph,Giraph, MarkLogic, Neo4J, OrientDB, Virtuoso, Stardog\nMulti-model: Alchemy Database, ArangoDB, CortexDB, FoundationDB, MarkLogic, OrientDB\nA more detailed classification is the following, based on one from Stephen Yen:[21]\n\nType\tExamples of this type\nKey-Value Cache\tCoherence, eXtreme Scale, GigaSpaces, GemFire, Hazelcast, Infinispan, JBoss Cache, Memcached, Repcached, Terracotta, Velocity\nKey-Value Store\tFlare, Keyspace, RAMCloud, SchemaFree, Hyperdex, Aerospike\nKey-Value Store (Eventually-Consistent)\tDovetailDB, Oracle NoSQL Database, Dynamo, Riak, Dynomite, MotionDb, Voldemort, SubRecord\nKey-Value Store (Ordered)\tActord, FoundationDB, Lightcloud, LMDB, Luxio, MemcacheDB, NMDB, Scalaris, TokyoTyrant\nData-Structures Server\tRedis\nTuple Store\tApache River, Coord, GigaSpaces\nObject Database\tDB4O, Objectivity/DB, Perst, Shoal, ZopeDB\nDocument Store\tClusterpoint, Couchbase, CouchDB, DocumentDB, Lotus Notes, MarkLogic, MongoDB, Qizx, RethinkDB, XML-databases\nWide Column Store\tBigTable, Cassandra, Druid, HBase, Hypertable, KAI, KDI, OpenNeptune, Qbase\nCorrelation databases are model-independent, and instead of row-based or column-based storage, use value-based storage.\n\nKey-value stores\nMain article: Key-value database\nKey-value (KV) stores use the associative array (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.[22][23]\n\nThe key-value model is one of the simplest non-trivial data models, and richer data models are often implemented as an extension of it. The key-value model can be extended to a discretely ordered model that maintains keys in lexicographic order. This extension is computationally powerful, in that it can efficiently retrieve selective key ranges.[24]\n\nKey-value stores can use consistency models ranging from eventual consistency to serializability. Some databases support ordering of keys. There are various hardware implementations, and some users maintain data in memory (RAM), while others employ solid-state drives or rotating disks.\n\nExamples include Oracle NoSQL Database, Redis, and dbm.\n\nDocument store\nMain articles: Document-oriented database and XML database\nThe central concept of a document store is the notion of a \"document\". While each document-oriented database implementation differs on the details of this definition, in general, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, YAML, and JSON as well as binary forms like BSON. Documents are addressed in the database via a unique key that represents that document. One of the other defining characteristics of a document-oriented database is that in addition to the key lookup performed by a key-value store, the database offers an API or query language that retrieves documents based on their contents\n\nDifferent implementations offer different ways of organizing and/or grouping documents:\n\nCollections\nTags\nNon-visible metadata\nDirectory hierarchies\nCompared to relational databases, for example, collections could be considered analogous to tables and documents analogous to records. But they are different: every record in a table has the same sequence of fields, while documents in a collection may have fields that are completely different.\n\nGraph\nMain article: Graph database\nThis kind of database is designed for data whose relations are well represented as a graph consisting of elements interconnected with a finite number of relations between them. The type of data could be social relations, public transport links, road maps or network topologies.\n\nGraph databases and their query language\nName\tLanguage(s)\tNotes\nAllegroGraph\tSPARQL\tRDF triple store\nDEX/Sparksee\tC++, Java, .NET, Python\tGraph database\nFlockDB\tScala\tGraph database\nIBM DB2\tSPARQL\tRDF triple store added in DB2 10\nInfiniteGraph\tJava\tGraph database\nMarkLogic\tJava, JavaScript, SPARQL, XQuery\tMulti-model document database and RDF triple store\nNeo4j\tCypher\tGraph database\nOWLIM\tJava, SPARQL 1.1\tRDF triple store\nOracle\tSPARQL 1.1\tRDF triple store added in 11g\nOrientDB\tJava\tMulti-model document and graph database\nSqrrl Enterprise\tJava\tGraph database\nOpenLink Virtuoso\tC++, C#, Java, SPARQL\tMiddleware and database engine hybrid\nStardog\tJava, SPARQL\tGraph database\nObject database\nMain article: Object database\ndb4o\nGemStone/S\nInterSystems Cach�\nJADE\nNeoDatis ODB\nObjectDatabase++\nObjectDB\nObjectivity/DB\nObjectStore\nODABA\nPerst\nOpenLink Virtuoso\nVersant Object Database\nZODB\nTabular\nApache Accumulo\nBigTable\nApache Hbase\nHypertable\nMnesia\nOpenLink Virtuoso\nTuple store\nApache River\nGigaSpaces\nTarantool\nTIBCO ActiveSpaces\nOpenLink Virtuoso\nTriple/quad store (RDF) database\nMain articles: Triplestore and Named graph\nAllegroGraph\nApache JENA (It is a framework, not a database)\nMarkLogic\nOntotext-OWLIM\nOracle NoSQL database\nSparkleDB\nVirtuoso Universal Server\nStardog\nHosted\nAmazon DynamoDB\nAmazon SimpleDB\nDatastore on Google Appengine\nClusterpoint database\nCloudant Data Layer (CouchDB)\nFreebase\nMicrosoft Azure Tables[25]\nMicrosoft Azure DocumentDB[26]\nOpenLink Virtuoso\nDrenel Hosted MongoDB\nMultivalue databases\nD3 Pick database\nExtensible Storage Engine (ESE/NT)\nInfinityDB\nInterSystems Cach�\njBASE Pick database\nNorthgate Information Solutions Reality, the original Pick/MV Database\nOpenQM\nRevelation Software's OpenInsight\nRocket U2\nMultimodel database\nOrientDB\nFoundationDB\nArangoDB\nMarkLogic\nPerformance\nBen Scofield rated different categories of NoSQL databases as follows:[27]\n\nData Model\tPerformance\tScalability\tFlexibility\tComplexity\tFunctionality\nKey�Value Store\thigh\thigh\thigh\tnone\tvariable (none)\nColumn-Oriented Store\thigh\thigh\tmoderate\tlow\tminimal\nDocument-Oriented Store\thigh\tvariable (high)\thigh\tlow\tvariable (low)\nGraph Database\tvariable\tvariable\thigh\thigh\tgraph theory\nRelational Database\tvariable\tvariable\tlow\tmoderate\trelational algebra\nPerformance and scalability comparisons are sometimes done with the YCSB benchmark.\n\nSee also: Comparison of structured storage software\nHandling relational data\nSince most NoSQL databases lack ability for joins in queries, the database schema generally needs to be designed differently. There are three main techniques for handling relational data in a NoSQL database. (See table Join and ACID Support for NoSQL databases that support joins.)\n\nMultiple queries\nInstead of retrieving all the data with one query, it's common to do several queries to get the desired data. NoSQL queries are often faster than traditional SQL queries so the cost of having to do additional queries may be acceptable. If an excessive number of queries would be necessary, one of the other two approaches is more appropriate.\n\nCaching/replication/non-normalized data\nInstead of only storing foreign keys, it's common to store actual foreign values along with the model's data. For example, each blog comment might include the username in addition to a user id, thus providing easy access to the username without requiring another lookup. When a username changes however, this will now need to be changed in many places in the database. Thus this approach works better when reads are much more common than writes.[28]\n\nNesting data\nWith document databases like MongoDB it's common to put more data in a smaller number of collections. For example, in a blogging application, one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments. Thus in this approach a single document contains all the data you need for a specific task.\n\nACID and JOIN Support\nIf a database is marked as supporting ACID or joins, then the documentation for the database makes that claim. The degree to which the capability is fully supported in a manner similar to most SQL databases or the degree to which it meets the needs of a specific application is left up to the reader to assess.\n\nDatabase\tACID\tJoins\nAerospike\tYes\tNo\nArangoDB\tYes\tYes\nCouchDB\tYes\tYes\nc-treeACE\tYes\tYes\nHyperDex\tYes[nb 1]\tYes\nInfinityDB\tYes\tNo\nLMDB\tYes\tNo\nMarkLogic\tYes\tYes[nb 2]\nOrientDB\tYes\tYes\nJump up ^ HyperDex currently offers ACID support via its Warp extension, which is a commercial add-on.\nJump up ^ Joins do not necessarily apply to document databases, but MarkLogic can do joins using semantics.[29]\nSee also\nCAP theorem\nComparison of object database management systems\nComparison of structured storage software\nCorrelation database\nDistributed cache\nFaceted search\nMultiValue database\nMulti-model database\nTriplestore", "skillName": "NoSQL."}
{"id": 175, "category": "Databases", "skillText": "Databases\nSQL and relational databases\nNoSQL Databases\nNoSQL, Mongo, Redis\nNoSQL, Teradata\nExcel\nMongoDB\nsql\nnosql\nrelatinal database\nnonrelational database\nOracle\nMySQL \nMicrosoft SQL Server\nMongoDB \nPostgreSQL\nDB2\nCassandra \nMicrosoft Access\nSQLite\nRedis \nElasticsearch \nTeradata\nSAP Adaptive Server\nSolr\nHBase\nFileMaker\nHive\nSplunk\nSAP HANA \nMariaDB\nNeo4j \nInformix\nMemcached\nCouchbase \nAmazon DynamoDB \nCouchDB\nMicrosoft Azure SQL Database\nNetezza\nVertica\nFirebird\nRiak KV \nIngres\nMarkLogic\ndBASE\nGreenplum\nAmazon Redshift \nImpala\nDB2\nMySQL\nOracle\nPostgreSQL\nSQLite\nSQL Server\nSybase\nRethinkDB\nBerkeley DB\nmemcached\nredis\ncouchDB\nmongoDB\n\nWith non-relational databases you can store any type of content. Incorporate any kind of data in a single database. Build any feature. Faster. With less money.\n\n\nRelational (SQL)\tNon-Relational (NoSQL)\nStuck. Data now includes rich data types – tweets, videos, podcasts, animated gifs – which are hard, if not impossible, to store in a relational database. Development slows to a crawl, and ops is caught playing whack-a-mole.\tDo the Impossible. NoSQL can incorporate literally any type of data, while providing all the features needed to build content-rich apps.\nCan’t Scale. Your audience is global, in many countries, speaking many languages, accessing content on many devices. Scaling a relational database is not trivial. And it isn’t cheap.\tScale Big. Scaling is built into the database. It is automatic and transparent. You can scale as your audience grows, both within a data center and across regions.\n$$$$. Large teams tied up for long periods of time make these applications expensive to build and maintain. Proprietary software and hardware, plus separate databases and file systems needed to manage your content, add to the cost.\t$. More productive teams, plus commodity hardware, make your projects cost 10% what they would with a relational database.\n\n\nDownload the white paper to understand in depth\n\n    Why organizations of all sizes are seeking alternatives to legacy relational databases like MySQL, SQL and PostgreSQL\n    The differences and similarities between NoSQL databases and relational databases\n    How to evaluate commercial support and community strength when selecting a NoSQL database\n\n1) Relational databases, which can also be called relational database management systems (RDBMS) or SQL databases.  The most popular of these are Microsoft SQL Server, Oracle Database, MySQL, and IBM DB2.  These RDBMS’s are mostly used in large enterprise scenarios, with the exception of MySQL, which is mostly used to store data for web applications, typically as part of the popular LAMP stack (Linux, Apache, MySQL, PHP/ Python/ Perl).\n\n2) Non-relational databases, also called NoSQL databases, the most popular being MongoDB, DocumentDB, Cassandra, Coachbase, HBase, Redis, and Neo4j.  These databases are usually grouped into four categories: Key-value stores, Graph stores, Column stores, and Document stores (see Types of NoSQL databases).\n\nAll relational databases can be used to manage transaction-oriented applications (OLTP), and most non-relational databases that are in the categories Document stores and Column stores can also be used for OLTP, adding to the confusion.  OLTP databases can be thought of as “Operational” databases, characterized by frequent, short transactions that include updates and that touch a small amount of data and where concurrency of thousands of transactions is very important (examples including banking applications and online reservations).  Integrity of data is very important so they support ACID transactions (Atomicity, Consistency, Isolation, Durability).  This is opposed to data warehouses, which are considered “Analytical” databases characterized by long, complex queries that touch a large amount of data and require a lot of resources.  Updates are infrequent.  An example is analysis of sales over the past year.\n\nRelational databases usually work with structured data, while non-relational databases usually work with semi-structured data (i.e. XML, JSON).\n\nLet’s look at each group in more detail:\nRelational Databases\n\nA relational database is organized based on the relational model of data, as proposed by E.F. Codd in 1970.  This model organizes data into one or more tables (or “relations”) of rows and columns, with a unique key for each row.  Generally, each entity type that is described in a database has its own table with the rows representing instances of that type of entity and the columns representing values attributed to that instance.  Since each row in a table has its own unique key, rows in a table can be linked to rows in other tables by storing the unique key of the row to which it should be linked (where such unique key is known as a “foreign key”).  Codd showed that data relationships of arbitrary complexity can be represented using this simple set of concepts.\n\nVirtually all relational database systems use SQL (Structured Query Language) as the language for querying and maintaining the database.\n\nThe reasons for the dominance of relational databases are: simplicity, robustness, flexibility, performance, scalability and compatibility in managing generic data.\n\nBut to offer all of this, relational databases have to be incredibly complex internally.  For example, a relatively simple SELECT statement could have dozens of potential query execution paths, which a query optimizer would evaluate at run time.  All of this is hidden to users, but under the hood, the RDBMS determines the best “execution plan” to answer requests by using things like cost-based algorithms.\n\nFor large databases, especially ones used for web applications, the main concern is scalability.  As more and more applications are created in environments that have massive workloads (i.e. Amazon), their scalability requirements can change very quickly and grow very large.  Relational databases scale well, but usually only when that scaling happens on a single server (“scale-up”).  When the capacity of that single server is reached, you need to “scale-out” and distribute that load across multiple servers, moving into so-called distributed computing.  This is when the complexity of relational databases starts to cause problems with their potential to scale.  If you try to scale to hundreds or thousands of servers the complexities become overwhelming.  The characteristics that make relational databases so appealing are the very same that also drastically reduce their viability as platforms for large distributed systems.\nNon-relational databases\n\nA NoSQL database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases.\n\nMotivations for this approach include:\n\n    Simplicity of design.  Not having to deal with the “impedance mismatch” between the object-oriented approach to write applications and the schema-based tables and rows of a relational database.  For example, storing all the customer order info in one document as opposed to having to join many tables together, resulting in less code to write, debug, and maintain\n    Better “horizontal” scaling to clusters of machines, which solves the problem when the number of concurrent users skyrockets for applications that are accessible via the web and mobile devices.  Using documents makes it much easier to scale-out as all the info for that customer order is contained in one place as opposed to being spread out on multiple tables.  NoSQL databases automatically spread data across servers without requiring application changes (auto-sharding), meaning that they natively and automatically spread data across an arbitrary number of servers, without requiring the application to even be aware of the composition of the server pool.  Data and query load are automatically balanced across servers, and when a server goes down, it can be quickly and transparently replaced with no application disruption\n    Finer control over availability.  Servers can be added or removed without application downtime.  Most NoSQL databases support data replication, storing multiple copies of data across the cluster or even across data centers, to ensure high availability and disaster recovery\n    To easily capture all kinds of data “Big Data” which include unstructured and semi-structured data.  Allowing for a flexible database that can easily and quickly accommodate any new type of data and is not disrupted by content structure changes.  This is because document database are schemaless, allowing you to freely add fields to JSON documents without having to first define changes (schema-on-read instead of schema-on-write).  You can have documents with a different number of fields than other documents.  For example, a patient record that may or may not contain fields that list allergies\n    Speed.  The data structures used by NoSQL databases (i.e. JSON documents) differ from those used by default in relational databases, making many operations faster in NoSQL than relational databases due to not having to join tables (at the cost of increased storage space due to duplication of data – but storage space is so cheap nowadays this is usually not an issue).  In fact, most NoSQL databases do not even support joins\n    Cost.  NoSQL databases usually use clusters of cheap commodity servers, while RDBMS tend to rely on expensive proprietary servers and storage systems.  Also, the licenses for RDBMS systems can be quite expensive while many NoSQL databases are open source and therefore free\n\nThe particular suitability of a given NoSQL database depends on the problem it must solve.\n\nNoSQL databases are increasingly used in big data and real-time web applications.  They became popular with the introduction of the web, when databases went from a max of a few hundred users on an internal company application to thousands or millions of users on a web application.  NoSQL systems are also called “Not only SQL” to emphasize that they may also support SQL-like query languages.\n\nMany NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability and partition tolerance.  Some reasons that block adoption of NoSQL stores include the use of low-level query languages, the lack of standardized interfaces, and huge investments in existing SQL.  Also, most NoSQL stores lack true ACID transactions or only support transactions in certain circumstances and at certain levels (e.g., document level).  Finally, RDBMS’s are usually much simpler to use as they have GUI’s where many NoSQL solution use a command-line interface.\nComparing the two\n\nOne of the most severe limitations of relational databases is that each item can only contain one attribute.  If we use a bank example, each aspect of a customer’s relationship with a bank is stored as separate row items in separate tables.  So the customer’s master details are in one table, the account details are in another table, the loan details in yet another, investments in a different table, and so on.  All these tables are linked to each other through the use of relations such as primary keys and foreign keys.\n\nNon-relational databases, specifically a database’s key-value stores or key-value pairs, are radically different from this model.  Key-value pairs allow you to store several related items in one “row” of data in the same table.  We place the word “row” in quotes because a row here is not really the same thing as the row of a relational table.  For instance, in a non-relational table for the same bank, each row would contain the customer’s details as well as their account, loan and investment details.  All data relating to one customer would be conveniently stored together as one record.\n\nThis seems an obviously superior method of storing data, but it has a major drawback: key-value stores, unlike relational databases, cannot enforce relationships between data items.  For instance, in our key-value database, the customer details (name, social security, address, account number, loan processing number, etc.) would all be stored as one data record (instead of being stored in several tables, as in the relational model).  The customer’s transactions (account withdrawals, account deposits, loan repayments, bank charges, etc.) would also be stored as another single data record.\n\nIn the relational model, there is an built-in and foolproof method of ensuring and enforcing business logic and rules at the database layer, for instance that a withdrawal is charged to the correct bank account, through primary keys and foreign keys.  In key-value stores, this responsibility falls squarely on the application logic and many people are very uncomfortable leaving this crucial responsibility just to the application.  This is one reason why relational databases will continued to be used.\n\nHowever, when it comes to web-based applications that use databases, the aspect of rigorously enforcing business logic is often not a top priorities.  The highest priority is the ability to service large numbers of user requests, which are typically read-only queries.  For example, on a site like eBay, the majority of users simply browse and look through posted items (read-only operations).  Only a fraction of these users actually place bids or reserve the items (read-write operations).  And remember, we are talking about millions, sometimes billions, of page views per day.  The eBay site administrators are more interested in quick response time to ensure faster page loading for the site’s users, rather than the traditional priorities of enforcing business rules or ensuring a balance between reads and writes.\n\nRelational-model databases can be tweaked and set up to run large-scale read-only operations through data warehousing, and thus potentially serve a large amount of users who are querying a large amount of data, especially when using relational MPP architectures like Analytics Platform System, Teradata, Oracle Exadata, or IBM Netezza, which all support scaling.  As mentioned before, data warehouses are distinct from typical databases in that they are used for more complex analysis of data.  This differs from the transactional (OLTP) database, whose main use is to support operational systems and offer day-to-day, small scale reporting.\n\nHowever, the real challenge is the relational model’s lack of scalability when dealing with OLTP applications, or any solution with a lot of individual writes, which is the domain of relational SMP architectures.  This is where non-relational models can really shine.  They can easily distribute their data loads across dozens, hundreds and in extreme cases (think Google search) even thousands of servers.  With each server handling only a small percentage of the total requests from users, response time is very good for each individual user.  Although this distributed computing model can be built for relational databases, it is a real pain to implement, especially when there are a lot of writes (i.e OLTP), requiring techniques like sharding which usually requires significant coding outside of the application’s business logic.  This is because the relational model insists on data integrity at all levels, which must be maintained, even as the data is accessed and modified by several different servers.  This is the reason for the non-relational model as the architecture of choice for web applications such as cloud-computing and social networking.\n\nSo in summary, RDBMS’s suffer from no horizontal scaling for high transaction loads (millions of read-writes), while NoSQL databases solve high transaction loads but at the cost of data integrity and joins.\n\nKeep in mind many solutions will use a combination of relational and non-relational databases (see What is Polyglot Persistence?).\n\nAlso keep in mind that you may not need the performance of a non-relational database and instead just going with storing files in HDFS and using Apache Hive will be enough (Apache Hive is a data warehouse infrastructure built on top of Hadoop for providing data summarization, query, and analysis that it provides via an SQL-like language called HiveQL).\n\nAnd to end on a note that adds to the confusion, we have a another category forming called NewSQL: NewSQL is a class of modern RDBMS’s that seek to provide the same scalable performance of NoSQL systems for OLTP read-write workloads while still maintaining the ACID guarantees of a traditional relational database system.  The disadvantages is they are not for OLAP-style queries, and they are inappropriate for databases over a few terabytes.  Examples include VoltDB, NuoDB, MemSQL, SAP HANA, Splice Machine, Clustrix, and Altibase.", "skillName": "DSENG04_Relational_non-relational_databases."}
{"id": 176, "category": "Databases", "skillText": "A database is a collection of information that is organized so that it can easily be accessed, managed, and updated. In one view, databases can be classified according to types of content: bibliographic, full-text, numeric, and images.\n\nIn computing, databases are sometimes classified according to their organizational approach. The most prevalent approach is the relational database, a tabular database in which data is defined so that it can be reorganized and accessed in a number of different ways. A distributed database is one that can be dispersed or replicated among different points in a network. An object-oriented programming database is one that is congruent with the data defined in object classes and subclasses.\n\nComputer databases typically contain aggregations of data records or files, such as sales transactions, product catalogs and inventories, and customer profiles. Typically, a database manager provides users the capabilities of controlling read/write access, specifying report generation, and analyzing usage. Databases and database managers are prevalent in large mainframe systems, but are also present in smaller distributed workstation and mid-range systems such as the AS/400 and on personal computers. SQL (Structured Query Language) is a standard language for making interactive queries from and updating a database such as IBM's DB2, Microsoft's SQL Server, and database products from Oracle, Sybase, and Computer Associates.", "skillName": "database."}
{"id": 177, "category": "Databases", "skillText": "A correlation database is a database management system (DBMS) that is data-model-independent and designed to efficiently handle unplanned, ad hoc queries in an analytical system environment.\n\nUnlike row-oriented relational database management systems, which use a records-based storage approach, or column-oriented databases which use a column-based storage method, a correlation database uses a value-based storage (VBS) architecture in which each unique data value is stored only once and an auto-generated indexing system maintains the context for all values.[1]\n\nContents\n\n    1 Structure\n    2 Comparison of DBMS storage structures\n        2.1 Storage in RDBMS\n        2.2 Storage in column-oriented databases\n        2.3 Storage in CDBMS\n    3 Advantages and disadvantages\n    4 References\n\nStructure\n\nBecause a correlation DBMS stores each unique data value only once, the physical database size is significantly smaller than relational or column-oriented databases, without the use of data compression techniques. Above approximately 30GB, a correlation DBMS may become smaller than the raw data set.[citation needed]\n\nThe VBS model used by a CDBMS consists of three primary physical sets of objects that are stored and managed:\n\n    a data dictionary (metadata);\n    an indexing and linking data set (additional metadata); and\n    the actual data values that comprise the stored information.\n\nIn the VBS model, each unique value in the raw data is stored only once; therefore, the data is always normalized at the level of unique values.[2] This eliminates the need to normalize data sets in the logical schema.\n\nData values are stored together in ordered sets based on data types: all integers in one set, characters in another, etc. This optimizes the data handling processes that access the values.\n\nIn addition to typical data values, the data value store contains a special type of data for storing relationships between tables. This functions similarly to foreign keys in RDBMS structures, but with a CDBMS, the relationship is known by the dictionary and stored as a data value, making navigation between tables completely automatic.\n\nThe data dictionary contains typical metadata plus additional statistical data about the tables, columns and occurrences of values in the logical schema. It also maintains information about the relationships between the logical tables. The index and linking storage includes all of the data used to locate the contents of a record from the ordered values in the data store.\n\nWhile not a RAM-based storage system, a CDBMS is designed to use as much RAM as the operating system can provide. For large databases, additional RAM improves performance. Generally, 4GB of RAM will provide optimized access times up to about 100 million records. 8GB of RAM is adequate for databases up to 10 times that size.[3] Because the incremental RAM consumed decreases as the database grows, 16GB of RAM will generally support databases containing up to approximately 20 billion records.\nComparison of DBMS storage structures\n\nThe sample records shown below illustrate the physical differences in the storage structures used in relational, column-oriented and correlation databases.\nCust ID \tName \tCity \tState\n12222 \tABC Corp \tMinneapolis \tMN\n19434 \tA1 Mfg \tDuluth \tMN\n20523 \tJ&J Inc \tSt. Paul \tMN\nStorage in RDBMS\n\nThe record-based structure used in an RDBMS stores elements in the same row adjacent to each other. Variations like clustered indexing may change the sequence of the rows, but all rows, columns and values will be stored as in the table. The above table might be stored as:\n\n      12222,ABC Corp,Minneapolis,MN;19434,A1 Mfg,Duluth,MN;20523,J&J Inc,St. Paul,MN\n\nStorage in column-oriented databases\n\nIn the column-based structure, elements of the same column are stored adjacent to each other. Consecutive duplicates within a single column may be automatically removed or compressed efficiently.\n\n      12222,19434,20523;ABC Corp,A1 Mfg,J&J Inc;Minneapolis,Duluth,St.Paul;MN,MN,MN\n\nStorage in CDBMS\n\nIn the VBS structure used in a CDBMS, each unique value is stored once and given an abstract (numeric) identifier, regardless of the number of occurrences or locations in the original data set. The original dataset is then constructed by referencing those logical identifiers. The correlation index may resemble the storage below. Note that the value \"MN\" which occurs multiple times in the data above is only included once. As the amount of repeat data grows, this benefit multiplies.\n\n      1:12222,2:19434,3:20523,4:ABC Corp,5:A1 Mfg,6:J&J Inc,7:Minneapolis,8:Duluth,9:St.Paul,10:MN\n\nThe records in our example table above can then be expressed as:\n\n      11:[1,4,7,10],12:[2,5,8,10],13:[3,6,9,10]\n\nThis correlation process is a form of database normalization. Just as one can achieve some benefits of column-oriented storage within an RDBMS, so too can one achieve some benefits of the correlation database through database normalization. However, in a traditional RDBMS this normalization process requires work in the form of table configuration, stored procedures, and SQL statements. We say that a database is a correlation database when it naturally expresses a fully normalized schema without this extra configuration. As a result, a correlation database may have more focused optimizations for this fully normalized structure.\n\nThis correlation process is similar to what occurs in a text-search oriented Inverted index.\nAdvantages and disadvantages\n\nFor analytical data warehouse applications, a CDBMS has several advantages over alternative database structures. First, because the database engine itself indexes all data and auto-generates its own schema on the fly while loading, it can be implemented quickly and is easy to update. There is no need for physical pre-design and no need to ever restructure the database. Second, a CDBMS enables creation and execution of complex queries such as associative queries (\"show everything that is related to x\") that are difficult if not impossible to model in SQL. The primary advantage of the CDBMS is that it is optimized for executing ad hoc queries - queries not anticipated during the data warehouse design phase.[4]\n\nA CDBMS has two drawbacks in comparison to database alternatives. Unlike relational databases, which can be used in a wide variety of applications, a correlation database is designed specifically for analytical applications and does not provide transaction management features; it cannot be used for transactional processing. Second, because it indexes all data during the load process, the physical load speed of a CDBMS is slower than relational or column-oriented structures. However, because it eliminates the need for logical or physical pre-design, the overall \"time to use\" of a CDBMS is generally similar to or somewhat faster than alternative structures.\nA triplestore or RDF store is a purpose-built database for the storage and retrieval of triples[1] through semantic queries. A triple is a data entity composed of subject-predicate-object, like \"Bob is 35\" or \"Bob knows Fred\".", "skillName": "DB3."}
{"id": 178, "category": "Databases", "skillText": "A database is an organized collection of data.[1] It is the collection of schemas, tables, queries, reports, views, and other objects. The data are typically organized to model aspects of reality in a way that supports processes requiring information, such as modelling the availability of rooms in hotels in a way that supports finding a hotel with vacancies.\n\nA database management system (DBMS) is a computer software application that interacts with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases. Well-known DBMSs include MySQL, PostgreSQL, MongoDB, Microsoft SQL Server, Oracle, Sybase, SAP HANA, and IBM DB2. A database is not generally portable across different DBMSs, but different DBMS can interoperate by using standards such as SQL and ODBC or JDBC to allow a single application to work with more than one DBMS. Database management systems are often classified according to the database model that they support; the most popular database systems since the 1980s have all supported the relational model as represented by the SQL language.[disputed – discuss] Sometimes a DBMS is loosely referred to as a 'database'.\nContents\n\n    1 Terminology and overview\n    2 Applications\n    3 General-purpose and special-purpose DBMSs\n    4 History\n        4.1 1960s, navigational DBMS\n        4.2 1970s, relational DBMS\n        4.3 Integrated approach\n        4.4 Late 1970s, SQL DBMS\n        4.5 1980s, on the desktop\n        4.6 1990s, object-oriented\n        4.7 2000s, NoSQL and NewSQL\n    5 Research\n    6 Examples\n    7 Design and modeling\n        7.1 Models\n        7.2 External, conceptual, and internal views\n    8 Languages\n    9 Performance, security, and availability\n        9.1 Storage\n        9.2 Security\n        9.3 Transactions and concurrency\n        9.4 Migration\n        9.5 Building, maintaining, and tuning\n        9.6 Backup and restore\n        9.7 Static analysis\n        9.8 Other\n    10 See also\n    11 Notes\n    12 References\n    13 Sources\n    14 Further reading\n    15 External links\n\nTerminology and overview\n\nFormally, a \"database\" refers to a set of related data and the way it is organized. Access to these data is usually provided by a \"database management system\" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.\n\nBecause of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it.\n\nOutside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index). This article is concerned only with databases where the size and usage requirements necessitate use of a database management system.[2]\n\nExisting DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:\n\n    Data definition – Creation, modification and removal of definitions that define the organization of the data.\n    Update – Insertion, modification, and deletion of the actual data.[3]\n    Retrieval – Providing information in a form directly usable or for further processing by other applications. The retrieved data may be made available in a form basically the same as it is stored in the database or in a new form obtained by altering or combining existing data from the database.[4]\n    Administration – Registering and monitoring users, enforcing data security, monitoring performance, maintaining data integrity, dealing with concurrency control, and recovering information that has been corrupted by some event such as an unexpected system failure.[5]\n\nBoth a database and its DBMS conform to the principles of a particular database model.[6] \"Database system\" refers collectively to the database model, database management system, and database.[7]\n\nPhysically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. RAID is used for recovery of data if any of the disks fail. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions from databases before the inception of Structured Query Language (SQL). The data recovered was disparate, redundant and disorderly, since there was no proper method to fetch it and arrange it in a concrete structure.[citation needed]\n\nSince DBMSs comprise a significant economical market, computer and storage vendors often take into account DBMS requirements in their own development plans.[8]\n\nDatabases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.\nApplications\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (March 2013) (Learn how and when to remove this template message)\n\nDatabases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).\n\nDatabases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples of database applications include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.\nGeneral-purpose and special-purpose DBMSs\n\nA DBMS has evolved into a complex software system and its development typically requires thousands of human years of development effort.[a] Some general-purpose DBMSs such as Adabas, Oracle and DB2 have been undergoing upgrades since the 1970s. General-purpose DBMSs aim to meet the needs of as many applications as possible, which adds to the complexity. However, the fact that their development cost can be spread over a large number of users means that they are often the most cost-effective approach. However, a general-purpose DBMS is not always the optimal solution: in some cases a general-purpose DBMS may introduce unnecessary overhead. Therefore, there are many examples of systems that use special-purpose databases. A common example is an email system that performs many of the functions of a general-purpose DBMS such as the insertion and deletion of messages composed of various items of data or associating messages with a particular email address; but these functions are limited to what is required to handle email and don't provide the user with all of the functionality that would be available using a general-purpose DBMS.\n\nMany other databases have application software that accesses the database on behalf of end-users, without exposing the DBMS interface directly. Application programmers may use a wire protocol directly, or more likely through an application programming interface. Database designers and database administrators interact with the DBMS through dedicated interfaces to build and maintain the applications' databases, and thus need some more knowledge and understanding about how DBMSs operate and the DBMSs' external interfaces and tuning parameters.\nHistory\n\nFollowing the technology progress in the areas of processors, computer memory, computer storage, and computer networks, the sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. The development of database technology can be divided into three eras based on data model or structure: navigational,[9] SQL/relational, and post-relational.\n\nThe two main early navigational data models were the hierarchical model, epitomized by IBM's IMS system, and the CODASYL model (network model), implemented in a number of products such as IDMS.\n\nThe relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2015 they remain dominant : IBM DB2, Oracle, MySQL, and Microsoft SQL Server are the top DBMS.[10] The dominant database language, standardised SQL for the relational model, has influenced database languages for other data models.[citation needed]\n\nObject databases were developed in the 1980s to overcome the inconvenience of object-relational impedance mismatch, which led to the coining of the term \"post-relational\" and also the development of hybrid object-relational databases.\n\nThe next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key-value stores and document-oriented databases. A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.\n1960s, navigational DBMS\nFurther information: Navigational database\nBasic structure of navigational CODASYL database model\n\nThe introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites[11] a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense.\n\nAs computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the \"Database Task Group\" within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the \"CODASYL approach\", and soon a number of commercial products based on this approach entered the market.\n\nThe CODASYL approach relied on the \"manual\" navigation of a linked data set which was formed into a large network. Applications could find records by one of three methods:\n\n    Use of a primary key (known as a CALC key, typically implemented by hashing)\n    Navigating relationships (called sets) from one record to another\n    Scanning all the records in a sequential order\n\nLater systems added B-trees to provide alternate access paths. Many CODASYL databases also added a very straightforward query language. However, in the final tally, CODASYL was very complex and required significant training and effort to produce useful applications.\n\nIBM also had their own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed, and Bachman's 1973 Turing Award presentation was The Programmer as Navigator. IMS is classified[by whom?] as a hierarchical database. IDMS and Cincom Systems' TOTAL database are classified as network databases. IMS remains in use as of 2014.[12]\n1970s, relational DBMS\n\nEdgar Codd worked at IBM in San Jose, California, in one of their offshoot offices that was primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[13]\n\nIn this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to use a \"table\" of fixed-length records, with each table used for a different type of entity. A linked-list system would be very inefficient when storing \"sparse\" databases where some of the data for any one record could be left empty. The relational model solved this by splitting the data into a series of normalized tables (or relations), with optional elements being moved out of the main table to where they would take up room only if needed. Data may be freely inserted, deleted and edited in these tables, with the DBMS doing whatever maintenance needed to present a table view to the application/user.\nIn the relational model, records are \"linked\" using virtual keys not stored in the database but defined as needed between the data contained in the records.\n\nThe relational model also allowed the content of the database to evolve without constant rewriting of links and pointers. The relational part comes from entities referencing other entities in what is known as one-to-many relationship, like a traditional hierarchical model, and many-to-many relationship, like a navigational (network) model. Thus, a relational model can express both hierarchical and navigational models, as well as its native tabular model, allowing for pure or combined modeling in terms of these three models, as the application requires.\n\nFor instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single record, and unused items would simply not be placed in the database. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.\n\nLinking the information back together is the key to this system. In the relational model, some bit of information was used as a \"key\", uniquely defining a particular record. When information was being collected about a user, information stored in the optional tables would be found by searching for this key. For instance, if the login name of a user is unique, addresses and phone numbers for that user would be recorded with the login name as its key. This simple \"re-linking\" of related data back into a single collection is something that traditional computer languages are not designed for.\n\nJust as the navigational approach would require programs to loop in order to collect records, the relational approach would require loops to collect information about any one record. Codd's solution to the necessary looping was a set-oriented language, a suggestion that would later spawn the ubiquitous SQL. Using a branch of mathematics known as tuple calculus, he demonstrated that such a system could support all the operations of normal databases (inserting, updating etc.) as well as providing a simple system for finding and returning sets of data in a single operation.\n\nCodd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.\n\nIBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs.\n\nIn 1970, the University of Michigan began development of the MICRO Information Management System[14] based on D.L. Childs' Set-Theoretic Data model. [15][16][17] MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System.[18] The system remained in production until 1998.\nIntegrated approach\nMain article: Database machine\n\nIn the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.\n\nAnother approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).\nLate 1970s, SQL DBMS\n\nIBM started working on a prototype system loosely based on Codd's concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL[citation needed] – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (DB2).\n\nLarry Ellison's Oracle started from a different chain, based on IBM's papers on System R, and beat IBM to market when the first version was released in 1978.[citation needed]\n\nStonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).\n\nIn Sweden, Codd's paper was also read and Mimer SQL was developed from the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise. In the early 1980s, Mimer introduced transaction handling for high robustness in applications, an idea that was subsequently implemented on most other DBMSs.\n\nAnother data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two have become irrelevant.[citation needed]\n1980s, on the desktop\n\nThe 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff the creator of dBASE stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\"[19] dBASE was one of the top selling software titles in the 1980s and early 1990s.\n1990s, object-oriented\n\nThe 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be relations to objects and their attributes and not to individual fields.[20] The term \"object-relational impedance mismatch\" described the inconvenience of translating between programmed objects and database tables. Object databases and object-relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object-relational mappings (ORMs) attempt to solve the same problem.\n2000s, NoSQL and NewSQL\nMain articles: NoSQL and NewSQL\n\nXML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in enterprise database management, where XML is being used as the machine-to-machine data interoperability standard. XML database management systems include commercial software MarkLogic and Oracle Berkeley DB XML, and a free use software Clusterpoint Distributed XML/JSON Database. All are enterprise software database platforms and support industry standard ACID-compliant transaction processing with strong database consistency characteristics and high level of database security.[21][22][23]\n\nNoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally. The most popular NoSQL systems include MongoDB, Couchbase, Riak, Memcached, Redis, CouchDB, Hazelcast, Apache Cassandra, and HBase,[24] which are all open-source software products.\n\nIn recent years, there was a high demand for massively distributed databases with high partition tolerance but according to the CAP theorem it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.\n\nNewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system. Such databases include ScaleBase, Clustrix, EnterpriseDB, MemSQL, NuoDB,[25] and VoltDB.\nResearch\n\nDatabase technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, and related concurrency control techniques, query languages and query optimization methods, RAID, and more.\n\nThe database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).\nExamples\n\nOne way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.\n\n    An in-memory database is a database that primarily resides in main memory, but is typically backed-up by non-volatile computer data storage. Main memory databases are faster than disk databases, and so are often used where response time is critical, such as in telecommunications network equipment.[26] SAP HANA platform is a very hot topic for in-memory database. By May 2012, HANA was able to run on servers with 100TB main memory powered by IBM. The co founder of the company claimed that the system was big enough to run the 8 largest SAP customers.\n    An active database includes an event-driven architecture which can respond to conditions both inside and outside the database. Possible uses include security monitoring, alerting, statistics gathering and authorization. Many databases provide active database features in the form of database triggers.\n    A cloud database relies on cloud technology. Both the database and most of its DBMS reside remotely, \"in the cloud\", while its applications are both developed by programmers and later maintained and utilized by (application's) end-users through a web browser and Open APIs.\n    Data warehouses archive data from operational databases and often from external sources such as market research firms. The warehouse becomes the central source of data for use by managers and other end-users who may not have access to operational data. For example, sales data might be aggregated to weekly totals and converted from internal product codes to use UPCs so that they can be compared with ACNielsen data. Some basic and essential components of data warehousing include extracting, analyzing, and mining data, transforming, loading, and managing data so as to make them available for further use.\n    A deductive database combines logic programming with a relational database, for example by using the Datalog language.\n    A distributed database is one in which both the data and the DBMS span multiple computers.\n    A document-oriented database is designed for storing, retrieving, and managing document-oriented, or semi structured data, information. Document-oriented databases are one of the main categories of NoSQL databases.\n    An embedded database system is a DBMS which is tightly integrated with an application software that requires access to stored data in such a way that the DBMS is hidden from the application’s end-users and requires little or no ongoing maintenance.[27]\n    End-user databases consist of data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. Some of them are much simpler than full-fledged DBMSs, with more elementary DBMS functionality.\n    A federated database system comprises several distinct databases, each with its own DBMS. It is handled as a single database by a federated database management system (FDBMS), which transparently integrates multiple autonomous DBMSs, possibly of different types (in which case it would also be a heterogeneous database system), and provides them with an integrated conceptual view.\n    Sometimes the term multi-database is used as a synonym to federated database, though it may refer to a less integrated (e.g., without an FDBMS and a managed integrated schema) group of databases that cooperate in a single application. In this case, typically middleware is used for distribution, which typically includes an atomic commit protocol (ACP), e.g., the two-phase commit protocol, to allow distributed (global) transactions across the participating databases.\n    A graph database is a kind of NoSQL database that uses graph structures with nodes, edges, and properties to represent and store information. General graph databases that can store any graph are distinct from specialized graph databases such as triplestores and network databases.\n    An array DBMS is a kind of NoSQL DBMS that allows to model, store, and retrieve (usually large) multi-dimensional arrays such as satellite images and climate simulation output.\n    In a hypertext or hypermedia database, any word or a piece of text representing an object, e.g., another piece of text, an article, a picture, or a film, can be hyperlinked to that object. Hypertext databases are particularly useful for organizing large amounts of disparate information. For example, they are useful for organizing online encyclopedias, where users can conveniently jump around the text. The World Wide Web is thus a large distributed hypertext database.\n    A knowledge base (abbreviated KB, kb or Δ[28][29]) is a special kind of database for knowledge management, providing the means for the computerized collection, organization, and retrieval of knowledge. Also a collection of data representing problems with their solutions and related experiences.\n\n    A mobile database can be carried on or synchronized from a mobile computing device.\n    Operational databases store detailed data about the operations of an organization. They typically process relatively high volumes of updates using transactions. Examples include customer databases that record contact, credit, and demographic information about a business' customers, personnel databases that hold information such as salary, benefits, skills data about employees, enterprise resource planning systems that record details about product components, parts inventory, and financial databases that keep track of the organization's money, accounting and financial dealings.\n    A parallel database seeks to improve performance through parallelization for tasks such as loading data, building indexes and evaluating queries.\n\n        The major parallel DBMS architectures which are induced by the underlying hardware architecture are:\n\n            Shared memory architecture, where multiple processors share the main memory space, as well as other data storage.\n            Shared disk architecture, where each processing unit (typically consisting of multiple processors) has its own main memory, but all units share the other storage.\n            Shared nothing architecture, where each processing unit has its own main memory and other storage.\n\n    Probabilistic databases employ fuzzy logic to draw inferences from imprecise data.\n    Real-time databases process transactions fast enough for the result to come back and be acted on right away.\n    A spatial database can store the data with multidimensional features. The queries on such data include location-based queries, like \"Where is the closest hotel in my area?\".\n    A temporal database has built-in time aspects, for example a temporal data model and a temporal version of SQL. More specifically the temporal aspects usually include valid-time and transaction-time.\n    A terminology-oriented database builds upon an object-oriented database, often customized for a specific field.\n    An unstructured data database is intended to store in a manageable and protected way diverse objects that do not fit naturally and conveniently in common databases. It may include email messages, documents, journals, multimedia objects, etc. The name may be misleading since some objects can be highly structured. However, the entire possible object collection does not fit into a predefined structured framework. Most established DBMSs now support unstructured data in various ways, and new dedicated DBMSs are emerging.\n\nDesign and modeling\nMain article: Database design\n\nThe first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity-relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organisation, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.\n\nProducing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.\n\nHaving produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modelling notation used to express that design.)\n\nThe most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.\n\nThe final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like. This is often called physical database design. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.\n\nAnother aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.\nModels\nMain article: Database model\nCollage of five types of database models\n\nA database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.\n\nCommon logical data models for databases include:\n\n    Navigational databases\n        Hierarchical database model\n        Network model\n        Graph database\n    Relational model\n    Entity–relationship model\n        Enhanced entity–relationship model\n    Object model\n    Document model\n    Entity–attribute–value model\n    Star schema\n\nAn object-relational database combines the two related structures.\n\nPhysical data models include:\n\n    Inverted index\n    Flat file\n\nOther models include:\n\n    Associative model\n    Multidimensional model\n    Array model\n    Multivalue model\n\nSpecialized models are optimized for particular types of data:\n\n    XML database\n    Semantic model\n    Content store\n    Event store\n    Time series model\n\nExternal, conceptual, and internal views\nTraditional view of data[30]\n\nA database management system provides three views of the database data:\n\n    The external level defines how each group of end-users sees the organization of data in the database. A single database can have any number of views at the external level.\n    The conceptual level unifies the various external views into a compatible global view.[31] It provides the synthesis of all the external views. It is out of the scope of the various database end-users, and is rather of interest to database application developers and database administrators.\n    The internal level (or physical level) is the internal organization of data inside a DBMS. It is concerned with cost, performance, scalability and other operational matters. It deals with storage layout of the data, using storage structures such as indexes to enhance performance. Occasionally it stores data of individual views (materialized views), computed from generic data, if performance justification exists for such redundancy. It balances all the external views' performance requirements, possibly conflicting, in an attempt to optimize overall performance across all activities.\n\nWhile there is typically only one conceptual (or logical) and physical (or internal) view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are the interest of the human resources department. Thus different departments need different views of the company's database.\n\nThe three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model. The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.\n\nThe conceptual view provides a level of indirection between internal and external. On one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.\n\nSeparating the external, conceptual and internal levels was a major feature of the relational database model implementations that dominate 21st century databases.[31]\nLanguages\n\nDatabase languages are special-purpose languages, which do one or more of the following:\n\n    Data definition language – defines data types and the relationships among them\n    Data manipulation language – performs tasks such as inserting, updating, or deleting data occurrences\n    Query language – allows searching for information and computing derived information\n\nDatabase languages are specific to a particular data model. Notable examples include:\n\n    SQL combines the roles of data definition, data manipulation, and query in a single language. It was one of the first commercial languages for the relational model, although it departs in some respects from the relational model as described by Codd (for example, the rows and columns of a table can be ordered). SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. The standards have been regularly enhanced since and is supported (with varying degrees of conformance) by all mainstream commercial relational DBMSs.[32][33]\n    OQL is an object model language standard (from the Object Data Management Group). It has influenced the design of some of the newer query languages like JDOQL and EJB QL.\n    XQuery is a standard XML query language implemented by XML database systems such as MarkLogic and eXist, by relational databases with XML capability such as Oracle and DB2, and also by in-memory XML processors such as Saxon.\n    SQL/XML combines XQuery with SQL.[34]\n\nA database language may also incorporate features like:\n\n    DBMS-specific Configuration and storage engine management\n    Computations to modify query results, like counting, summing, averaging, sorting, grouping, and cross-referencing\n    Constraint enforcement (e.g. in an automotive database, only allowing one engine type per car)\n    Application programming interface version of the query language, for programmer convenience\n\nPerformance, security, and availability\n\nBecause of the critical importance of database technology to the smooth running of an enterprise, database systems include complex mechanisms to deliver the required performance, security, and availability, and allow database administrators to control the use of these features.\nStorage\nMain articles: Computer data storage and Database engine\n\nDatabase storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, \"data about the data\", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often utilizing the operating systems' file systems as intermediates for storage layout), storage properties and configuration setting are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).\n\nSome DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.\n\nVarious low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.\nMaterialized views\nMain article: Materialized view\n\nOften storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing of them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.\nReplication\nMain article: Database replication\n\nOccasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to a same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.\nSecurity\nAccuracy dispute\n\tThis article appears to contradict the article Database security. Please see discussion on the linked talk page. (March 2013) (Learn how and when to remove this template message)\nMain article: Database security\n\nDatabase security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).\n\nDatabase access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or utilizing specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.\n\nThis may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.\n\nData security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).\n\nChange and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this to the database. Monitoring can be set up to attempt to detect security breaches.\nTransactions and concurrency\nFurther information: Concurrency control\n\nDatabase transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).\n\nThe acronym ACID describes some ideal properties of a database transaction: Atomicity, Consistency, Isolation, and Durability.\nMigration\n\n    See also section Database migration in article Data migration\n\nA database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to move, migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help importing databases from other popular DBMSs.\nBuilding, maintaining, and tuning\nMain article: Database tuning\n\nAfter designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be utilized for this purpose. A DBMS provides the needed user interfaces to be utilized by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).\n\nWhen the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.\n\nAfter the database is created, initialised and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.\nBackup and restore\nMain article: Backup\n\nSometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When this state is needed, i.e., when it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are utilized to restore that state.\nStatic analysis\n\nStatic analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[35] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.\nOther\n\nOther DBMS features might include:\n\n    Database logs\n    Graphics component for producing graphs and charts, especially in a data warehouse system\n    Query optimizer – Performs query optimization on every query to choose for it the most efficient query plan (a partial order (tree) of operations) to be executed to compute the query result. May be specific to a particular storage engine.\n    Tools or hooks for database design, application programming, application program maintenance, database performance analysis and monitoring, database configuration monitoring, DBMS hardware configuration (a DBMS and related database may span computers, networks, and storage units) and related database mapping (especially for a distributed DBMS), storage allocation and database layout monitoring, storage migration, etc.\n\nSee also\nMain article: Outline of databases\nBook icon \t\n\n    Book: Databases\n\n    Comparison of database tools\n    Comparison of object database management systems\n    Comparison of object-relational database management systems\n    Comparison of relational database management systems\n    Data hierarchy\n    Data bank\n    Data store\n    Database theory\n    Database testing\n    Database-centric architecture\n    Question-focused dataset", "skillName": "Database."}
{"id": 179, "category": "Databases", "skillText": "Elasticsearch is a search engine based on Lucene. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. Elasticsearch is developed in Java and is released as open source under the terms of the Apache License. Elasticsearch is the most popular enterprise search engine followed by Apache Solr, also based on Lucene.[1]\n\nContents\n\n    1 History\n    2 Overview\n    3 Users\n    4 See also\n    5 References\n    6 External links\n\nHistory\n\nShay Banon created Compass in 2004.[2] While thinking about the third version of Compass he realized that it would be necessary to rewrite big parts of Compass to \"create a scalable search solution\".[2] So he created \"a solution built from the ground up to be distributed\" and used a common interface, JSON over HTTP, suitable for programming languages other than Java as well.[2] Shay Banon released the first version of Elasticsearch in February 2010.[3]\n\nElasticsearch BV was founded in 2012 to provide commercial services and products around Elasticsearch and related software.[4] In June 2014, the company announced raising $70 million in a Series C funding round, just 18 months after forming the company. The round was led by New Enterprise Associates (NEA). Additional funders include Benchmark Capital and Index Ventures. This round brings total funding to $104M.[5]\nVersion \tOriginal release date \tLatest version \tRelease date \tMaintenance Status[6]\n0.4 \t2010-02-08 \t0.4.0 \t2010-02-08 \tNo longer supported\n0.5 \t2010-03-05[7] \t0.5.1 \t2010-03-09 \tNo longer supported\n0.6 \t2010-04-09[8] \t0.6.0 \t2010-04-09 \tNo longer supported\n0.7 \t2010-05-14[9] \t0.7.1 \t2010-05-17[10] \tNo longer supported\n0.8 \t2010-05-27[11] \t0.8.0 \t2010-05-27 \tNo longer supported\n0.9 \t2010-07-26[12] \t0.9.0 \t2010-07-26 \tNo longer supported\n0.10 \t2010-08-27[13] \t0.10.0 \t2010-08-27 \tNo longer supported\n0.11 \t2010-09-29[14] \t0.11.0 \t2010-09-29 \tNo longer supported\n0.12 \t2010-10-18[15] \t0.12.1 \t2010-10-27 \tNo longer supported\n0.13 \t2010-11-18[16] \t0.13.1 \t2010-12-03 \tNo longer supported\n0.14 \t2010-12-27[17] \t0.14.4 \t2011-01-31 \tNo longer supported\n0.15 \t2011-02-18[18] \t0.15.2 \t2011-03-07 \tNo longer supported\n0.16 \t2011-04-23[19] \t0.16.5 \t2011-07-26 \tNo longer supported\n0.17 \t2011-07-19[20] \t0.17.10 \t2011-11-16 \tNo longer supported\n0.18 \t2011-10-26[21] \t0.18.7 \t2012-01-10[22] \tNo longer supported\n0.19 \t2012-03-01[23] \t0.19.12 \t2012-12-04[24] \tNo longer supported\n0.20 \t2012-12-07[25] \t0.20.6 \t2013-03-25[26] \tNo longer supported\n0.90 \t2013-04-29[27] \t0.90.13 \t2014-03-25[28] \tNo longer supported\n1.0 \t2014-02-12[29] \t1.0.3 \t2014-04-16[30] \tNo longer supported\n1.1 \t2014-03-25[28] \t1.1.2 \t2014-05-22[31] \tNo longer supported\n1.2 \t2014-05-22[31] \t1.2.4 \t2014-08-13[32] \tNo longer supported\n1.3 \t2014-07-23[33] \t1.3.9 \t2015-02-19[34] \tNo longer supported\n1.4 \t2014-11-05[35] \t1.4.5 \t2015-04-27[36] \tStill supported\n1.5 \t2015-03-23[37] \t1.5.2 \t2015-04-27[36] \tStill supported\n1.6 \t2015-06-09[38] \t1.6.2 \t2015-07-29[39] \tStill supported\n1.7 \t2015-07-16[40] \t1.7.5 \t2016-02-02[41] \tStill supported\n2.0 \t2015-10-28[42] \t2.0.2 \t2015-12-17[43] \tStill supported\n2.1 \t2015-11-24[44] \t2.1.2 \t2016-02-02[41] \tStill supported\n2.2 \t2016-02-02[41] \t2.2.2 \t2016-03-30[45] \tStill supported\n2.3 \t2016-03-30[45] \t2.3.3 \t2016-05-18[46] \tLatest\n5.0 \t2016-04-05[47] \t5.0.0 Alpha 4 \t2016-06-30[48] \tLatest preview version\nLegend:\nOld version\nOlder version, still supported\nLatest version\nLatest preview version\nOverview\n\nElasticsearch can be used to search all kinds of documents. It provides scalable search, has near real-time search, and supports multitenancy.[49] \"Elasticsearch is distributed, which means that indices can be divided into shards and each shard can have zero or more replicas. Each node hosts one or more shards, and acts as a coordinator to delegate operations to the correct shard(s). Rebalancing and routing are done automatically [...]\".[49]\n\nElasticsearch uses Lucene and tries to make all its features available through the JSON and Java API. It supports facetting and percolating,[50] which can be useful for notifying if new documents match for registered queries.\n\nAnother feature is called \"gateway\" and handles the long-term persistence of the index;[51] for example, an index can be recovered from the gateway in the event of a server crash. Elasticsearch supports real-time GET requests, which makes it suitable as a NoSQL datastore,[52] but it lacks distributed transactions.[53]\nUsers\n\nNotable users of Elasticsearch[54] include Wikimedia,[55] Facebook,[56] StumbleUpon,[57] Mozilla,[58][59] Amadeus IT Group, Quora,[60] Foursquare,[61] Etsy,[62] SoundCloud,[63] GitHub,[64] FDA,[65] CERN,[66] Stack Exchange,[67] Center for Open Science,[68] Reverb,[69] Netflix,[70] Pixabay,[71] Motili, Sophos and the Slurm Workload Manager.[citation needed]", "skillName": "Elasticsearch."}
{"id": 180, "category": "Databases", "skillText": "PostgreSQL, often simply Postgres, is an object-relational database management system (ORDBMS) with an emphasis on extensibility and standards-compliance. As a database server, its primary function is to store data securely, supporting best practices, and to allow for retrieval at the request of other software applications. It can handle workloads ranging from small single-machine applications to large Internet-facing applications with many concurrent users.\n\nPostgreSQL implements the majority of the core SQL:2011 standard,[10][11] is ACID-compliant and transactional (including most DDL statements) avoiding locking issues using multiversion concurrency control (MVCC), provides immunity to dirty reads and full serializability; handles complex SQL queries using many indexing methods that are not available in other databases; has updateable views and materialized views, triggers, foreign keys; supports functions and stored procedures, and other expandability,[12] and has a large number of extensions written by third parties. In addition to the possibility of working with the major proprietary and open source databases, PostgreSQL supports migration from them, by its extensive standard SQL support and available migration tools. Proprietary extensions in databases such as Oracle can be emulated by built-in and third-party open source compatibility extensions. Recent versions also provide replication of the database for availability and scalability.\n\nPostgreSQL is cross-platform and runs on many operating systems including Linux, FreeBSD, OS X, Solaris, and Microsoft Windows. On OS X, PostgreSQL has been the default database starting with Mac OS X 10.7 Lion Server,[13][14][15] and PostgreSQL client tools are bundled within the desktop edition. The vast majority of Linux distributions have it available in supplied packages.\n\nPostgreSQL is developed by the PostgreSQL Global Development Group, a diverse group of many companies and individual contributors.[16] It is free and open-source software, released under the terms of the PostgreSQL License, a permissive free-software license.\n\nContents\n\n    1 Name\n    2 History\n    3 Multiversion concurrency control (MVCC)\n    4 Storage and replication\n        4.1 Replication\n        4.2 Indexes\n        4.3 Schemas\n        4.4 Data types\n        4.5 User-defined objects\n        4.6 Inheritance\n        4.7 Other storage features\n    5 Control and connectivity\n        5.1 Foreign data wrappers\n        5.2 Interfaces\n        5.3 Procedural languages\n        5.4 Triggers\n        5.5 Asynchronous notifications\n        5.6 Rules\n        5.7 Other querying features\n    6 Security\n    7 Upcoming features\n    8 Add-ons\n    9 Benchmarks and performance\n    10 Platforms\n    11 Database administration\n    12 Prominent users\n    13 Release history\n    14 See also\n    15 References\n    16 Further reading\n    17 External links\n\nName\n\nPostgreSQL's developers pronounce it /ˈpoʊstɡrɛs ˌkjuː ˈɛl/.[17] It is abbreviated as Postgres, its original name. Because of ubiquitous support for the SQL Standard among most relational databases, the community considered changing the name back to Postgres. However, the PostgreSQL Core Team announced in 2007 that the product would continue to use the name PostgreSQL.[18] The name refers to the project's origins as a \"post-Ingres\" database, being a development from University Ingres DBMS (Ingres being an acronym for INteractive Graphics REtrieval System).[19][20]\nHistory\n\nPostgreSQL evolved from the Ingres project at the University of California, Berkeley. In 1982 the leader of the Ingres team, Michael Stonebraker, left Berkeley to make a proprietary version of Ingres.[19] He returned to Berkeley in 1985, and started a post-Ingres project to address the problems with contemporary database systems that had become increasingly clear during the early 1980s. The new project, POSTGRES, aimed to add the fewest features needed to completely support types.[21] These features included the ability to define types and to fully describe relationships – something used widely before but maintained entirely by the user. In POSTGRES, the database \"understood\" relationships, and could retrieve information in related tables in a natural way using rules. POSTGRES used many of the ideas of Ingres, but not its code.[22]\n\nStarting in 1986, the POSTGRES team published a number of papers describing the basis of the system, and by 1987 had a prototype version shown at the 1988 ACM SIGMOD Conference. The team released version 1 to a small number of users in June 1989, then version 2 with a re-written rules system in June 1990. Version 3, released in 1991, again re-wrote the rules system, and added support for multiple storage managers and an improved query engine. By 1993, the great number of users began to overwhelm the project with requests for support and features. After releasing version 4.2[23] on June 30, 1994—primarily a cleanup—the project ended. Berkeley had released POSTGRES under an MIT-style license, which enabled other developers to use the code for any use. At the time, POSTGRES used an Ingres-influenced POSTQUEL query language interpreter, which could be interactively used with a console application named monitor.\n\nIn 1994, Berkeley graduate students Andrew Yu and Jolly Chen replaced the POSTQUEL query language interpreter with one for the SQL query language, creating Postgres95. The front-end program monitor was also replaced by psql. Yu and Chen released the code on the web.\n\nOn July 8, 1996, Marc Fournier at Hub.org Networking Services provided the first non-university development server for the open-source development effort.[1] With the participation of Bruce Momjian and Vadim B. Mikheev, work began to stabilize the code inherited from Berkeley. The first open-source version was released on August 1, 1996.[citation needed]\n\nIn 1996, the project was renamed to PostgreSQL to reflect its support for SQL. The online presence at the website PostgreSQL.org began on October 22, 1996.[24] The first PostgreSQL release formed version 6.0 on January 29, 1997. Since then a group of developers and volunteers around the world have maintained the software as The PostgreSQL Global Development Group.\n\nThe PostgreSQL project continues to make major releases (approximately annually) and minor \"bugfix\" releases, all available under its free and open-source software PostgreSQL License. Code comes from contributions from proprietary vendors, support companies, and open-source programmers at large.\nMultiversion concurrency control (MVCC)\n\nPostgreSQL manages concurrency through a system known as multiversion concurrency control (MVCC), which gives each transaction a \"snapshot\" of the database, allowing changes to be made without being visible to other transactions until the changes are committed. This largely eliminates the need for read locks, and ensures the database maintains the ACID (atomicity, consistency, isolation, durability) principles in an efficient manner. PostgreSQL offers three levels of transaction isolation: Read Committed, Repeatable Read and Serializable. Because PostgreSQL is immune to dirty reads, requesting a Read Uncommitted transaction isolation level provides read committed instead. Prior to PostgreSQL 9.1, requesting Serializable provided the same isolation level as Repeatable Read. PostgreSQL 9.1 and later support full serializability via the serializable snapshot isolation (SSI) technique.[25]\nStorage and replication\nReplication\n\nPostgreSQL, beginning with version 9.0, includes built-in binary replication, based on shipping the changes (write-ahead logs) to replica nodes asynchronously.\n\nVersion 9.0 also introduced the ability to run read-only queries against these replicated nodes, where earlier versions would only allow that after promoting them to be a new master. This allows splitting read traffic among multiple nodes efficiently. Earlier replication software that allowed similar read scaling normally relied on adding replication triggers to the master, introducing additional load onto it.\n\nBeginning from version 9.1, PostgreSQL also includes built-in synchronous replication[26] that ensures that, for each write transaction, the master waits until at least one replica node has written the data to its transaction log. Unlike other database systems, the durability of a transaction (whether it is asynchronous or synchronous) can be specified per-database, per-user, per-session or even per-transaction. This can be useful for work loads that do not require such guarantees, and may not be wanted for all data as it will have some negative effect on performance due to the requirement of the confirmation of the transaction reaching the synchronous standby.\n\nThere can be a mixture of synchronous and asynchronous standby servers. A list of synchronous standby servers can be specified in the configuration which determines which servers are candidates for synchronous replication. The first in the list which is currently connected and actively streaming is the one that will be used as the current synchronous server. When this fails, it falls to the next in line.\n\nSynchronous multi-master replication is currently not included in the PostgreSQL core. Postgres-XC which is based on PostgreSQL provides scalable synchronous multi-master replication,[27] available in version 1.2.1 (April 2015 version) is licensed under the same license as PostgreSQL. A similar project is called Postgres-XL and is available under the Mozilla Public License.[28] Postgres-R is yet another older fork.[29] Bi-Directional Replication (BDR) is an asynchronous multi-master replication system for PostgreSQL.[30]\n\nThe community has also written some tools to make managing replication clusters easier, such as repmgr.\n\nThere are also several asynchronous trigger-based replication packages for PostgreSQL. These remain useful even after introduction of the expanded core capabilities, for situations where binary replication of an entire database cluster is not the appropriate approach:\n\n    Slony-I\n    Londiste, part of SkyTools (developed by Skype)\n    Bucardo multi-master replication (developed by Backcountry.com)[31]\n    SymmetricDS multi-master, multi-tier replication\n\nIndexes\n\nPostgreSQL includes built-in support for regular B-tree and hash indexes, and four index access methods: generalized search trees (GiST), generalized inverted indexes (GIN), Space-Partitioned GiST (SP-GiST) [32] and Block Range Indexes (BRIN). Hash indexes are implemented, but discouraged because they cannot be recovered after a crash or power loss. In addition, user-defined index methods can be created, although this is quite an involved process. Indexes in PostgreSQL also support the following features:\n\n    Expression indexes can be created with an index of the result of an expression or function, instead of simply the value of a column.\n    Partial indexes, which only index part of a table, can be created by adding a WHERE clause to the end of the CREATE INDEX statement. This allows a smaller index to be created.\n    The planner is capable of using multiple indexes together to satisfy complex queries, using temporary in-memory bitmap index operations.\n    As of PostgreSQL 9.1, k-nearest neighbors (k-NN) indexing (also referred to KNN-GiST[33]) provides efficient searching of \"closest values\" to that specified, useful to finding similar words, or close objects or locations with geospatial data. This is achieved without exhaustive matching of values.\n    In PostgreSQL 9.2 and above, index-only scans often allow the system to fetch data from indexes without ever having to access the main table.\n    PostgreSQL 9.5 introduced Block Range Indexes (BRIN).\n\nSchemas\n\nIn PostgreSQL, a schema holds all objects (with the exception of roles and tablespaces). Schemas effectively act like namespaces, allowing objects of the same name to co-exist in the same database.\n\nBy default, newly created databases have a schema called \"public\", but any additional schemas can be added, and the public schema isn't mandatory. A \"search_path\" determines the order in which the system checks schemas for unqualified objects (those without a prefixed schema); one can configure search paths on a database or role level. The search path, by default, contains the special schema name of \"$user\", which first looks for a schema named after the connected database user (e.g. if the user \"dave\" were connected, it would first look for a schema also named \"dave\" when referring to any objects). If such a schema is not found, it then proceeds to the next listed schema. New objects are created in whichever valid schema (one that presently exists) appears first in the search path.\nData types\n\nA wide variety of native data types are supported, including:\n\n    Boolean\n    Arbitrary precision numerics\n    Character (text, varchar, char)\n    Binary\n    Date/time (timestamp/time with/without timezone, date, interval)\n    Money\n    Enum\n    Bit strings\n    Text search type\n    Composite\n    HStore (an extension enabled key-value store within PostgreSQL)\n    Arrays (variable length and can be of any data type, including text and composite types) up to 1 GB in total storage size\n    Geometric primitives\n    IPv4 and IPv6 addresses\n    CIDR blocks and MAC addresses\n    XML supporting XPath queries\n    UUID\n    JSON (since version 9.2), and a faster binary JSONB (since version 9.4; not the same as BSON[34])\n\nIn addition, users can create their own data types which can usually be made fully indexable via PostgreSQL's indexing infrastructures - GiST, GIN, SP-GiST. Examples of these include the geographic information system (GIS) data types from the PostGIS project for PostgreSQL.\n\nThere is also a data type called a \"domain\", which is the same as any other data type but with optional constraints defined by the creator of that domain. This means any data entered into a column using the domain will have to conform to whichever constraints were defined as part of the domain.\n\nStarting with PostgreSQL 9.2, a data type that represents a range of data can be used which are called range types. These can be discrete ranges (e.g. all integer values 1 to 10) or continuous ranges (e.g. any point in time between 10:00 am and 11:00 am). The built-in range types available include ranges of integers, big integers, decimal numbers, time stamps (with and without time zone) and dates.\n\nCustom range types can be created to make new types of ranges available, such as IP address ranges using the inet type as a base, or float ranges using the float data type as a base. Range types support inclusive and exclusive range boundaries using the [] and () characters respectively. (e.g. '[4,9)' represents all integers starting from and including 4 up to but not including 9.) Range types are also compatible with existing operators used to check for overlap, containment, right of etc.\nUser-defined objects\n\nNew types of almost all objects inside the database can be created, including:\n\n    Casts\n    Conversions\n    Data types\n    Domains\n    Functions, including aggregate functions and window functions\n    Indexes including custom indexes for custom types\n    Operators (existing ones can be overloaded)\n    Procedural languages\n\nInheritance\n\nTables can be set to inherit their characteristics from a \"parent\" table. Data in child tables will appear to exist in the parent tables, unless data is selected from the parent table using the ONLY keyword, i.e. SELECT * FROM ONLY parent_table;. Adding a column in the parent table will cause that column to appear in the child table.\n\nInheritance can be used to implement table partitioning, using either triggers or rules to direct inserts to the parent table into the proper child tables.\n\nAs of 2010, this feature is not fully supported yet—in particular, table constraints are not currently inheritable. All check constraints and not-null constraints on a parent table are automatically inherited by its children. Other types of constraints (unique, primary key, and foreign key constraints) are not inherited.\n\nInheritance provides a way to map the features of generalization hierarchies depicted in Entity Relationship Diagrams (ERD) directly into the PostgreSQL database.\nOther storage features\n\n    Referential integrity constraints including foreign key constraints, column constraints, and row checks\n    Binary and textual large-object storage\n    Tablespaces\n    Per-column collation (from 9.1)\n    Online backup\n    Point-in-time recovery, implemented using write-ahead logging\n    In-place upgrades with pg_upgrade for less downtime (supports upgrades from 8.3.x and later)\n\nControl and connectivity\nForeign data wrappers\n\nAs of version 9.1, PostgreSQL can link to other systems to retrieve data via foreign data wrappers (FDWs). These can take the form of any data source, such as a file system, another RDBMS, or a web service. This means regular database queries can use these data sources like regular tables, and even join multiple data sources together.\nInterfaces\n\nPostgreSQL has several interfaces available and is also widely supported among programming language libraries. Built-in interfaces include libpq (PostgreSQL's official C application interface) and ECPG (an embedded C system). External interfaces include:\n\n    libpqxx: C++ interface\n    PostgresDAC: PostgresDAC (for Embarcadero RadStudio/Delphi/CBuilder XE-XE3)\n    DBD::Pg: Perl DBI driver\n    JDBC: JDBC interface\n    Lua: Lua interface\n    Npgsql: .NET data provider\n    ST-Links SpatialKit: Link Tool to ArcGIS\n    PostgreSQL.jl: Julia interface\n    node-postgres: Node.js interface\n    pgoledb: OLEDB interface\n    psqlODBC: ODBC interface\n    psycopg2:[35] Python interface (also used by HTSQL)\n    pgtclng: Tcl interface\n    pyODBC: Python library\n    php5-pgsql: PHP driver based on libpq\n    postmodern: A Common Lisp interface\n    pq \n    : A pure Go PostgreSQL driver for the Go database/sql package. The driver passes the compatibility test suite.[36]\n    dpq \n    : D interface to libpq\n    epgsql: Erlang interface\n\nProcedural languages\n\nProcedural languages allow developers to extend the database with custom subroutines (functions), often called stored procedures. These functions can be used to build triggers (functions invoked upon modification of certain data) and custom aggregate functions. Procedural languages can also be invoked without defining a function, using the \"DO\" command at SQL level.\n\nLanguages are divided into two groups: \"Safe\" languages are sandboxed and can be safely used by any user. Procedures written in \"unsafe\" languages can only be created by superusers, because they allow bypassing the database's security restrictions, but can also access sources external to the database. Some languages like Perl provide both safe and unsafe versions.\n\nPostgreSQL has built-in support for three procedural languages:\n\n    Plain SQL (safe). Simpler SQL functions can get expanded inline into the calling (SQL) query, which saves function call overhead and allows the query optimizer to \"see inside\" the function.\n    PL/pgSQL (safe), which resembles Oracle's PL/SQL procedural language and SQL/PSM.\n    C (unsafe), which allows loading custom shared libraries into the database. Functions written in C offer the best performance, but bugs in code can crash and potentially corrupt the database. Most built-in functions are written in C.\n\nIn addition, PostgreSQL allows procedural languages to be loaded into the database through extensions. Three language extensions are included with PostgreSQL to support Perl, Python and Tcl. There are external projects to add support for many other languages,[37] including Java, JavaScript (PL/V8), R, Ruby, and others.\nTriggers\n\nTriggers are events triggered by the action of SQL DML statements. For example, an INSERT statement might activate a trigger that checks if the values of the statement are valid. Most triggers are only activated by either INSERT or UPDATE statements.\n\nTriggers are fully supported and can be attached to tables. In PostgreSQL 9.0 and above, triggers can be per-column and conditional, in that UPDATE triggers can target specific columns of a table, and triggers can be told to execute under a set of conditions as specified in the trigger's WHERE clause. As of PostgreSQL 9.1, triggers can be attached to views by utilising the INSTEAD OF condition. Views in versions prior to 9.1 can have rules, though. Multiple triggers are fired in alphabetical order. In addition to calling functions written in the native PL/pgSQL, triggers can also invoke functions written in other languages like PL/Python or PL/Perl.\nAsynchronous notifications\n\nPostgreSQL provides an asynchronous messaging system that is accessed through the NOTIFY, LISTEN and UNLISTEN commands. A session can issue a NOTIFY command, along with the user-specified channel and an optional payload, to mark a particular event occurring. Other sessions are able to detect these events by issuing a LISTEN command, which can listen to a particular channel. This functionality can be used for a wide variety of purposes, such as letting other sessions know when a table has updated or for separate applications to detect when a particular action has been performed. Such a system prevents the need for continuous polling by applications to see if anything has yet changed, and reducing unnecessary overhead. Notifications are fully transactional, in that messages are not sent until the transaction they were sent from is committed. This eliminates the problem of messages being sent for an action being performed which is then rolled back.\n\nMany of the connectors for PostgreSQL provide support for this notification system (including libpq, JDBC, Npgsql, psycopg and node.js) so it can be used by external applications.\nRules\n\nRules allow the \"query tree\" of an incoming query to be rewritten. Rules, or more properly, \"Query Re-Write Rules\", are attached to a table/class and \"Re-Write\" the incoming DML (select, insert, update, and/or delete) into one or more queries that either replace the original DML statement or execute in addition to it. Query Re-Write occurs after DML statement parsing, but before query planning.\nOther querying features\n\n    Transactions\n    Full text search\n    Views\n        Materialized views[38]\n        Updateable views[39]\n        Recursive views[40]\n    Inner, outer (full, left and right), and cross joins\n    Sub-selects\n        Correlated sub-queries[41]\n    Regular expressions[42]\n    Common table expressions and writable common table expressions\n    Encrypted connections via TLS (current versions do not use vulnerable SSL, even with that configuration option)[43]\n    Domains\n    Savepoints\n    Two-phase commit\n    TOAST (The Oversized-Attribute Storage Technique) is used to transparently store large table attributes (such as big MIME attachments or XML messages) in a separate area, with automatic compression.\n    Embedded SQL is implemented using preprocessor. SQL code is first written embedded into C code. Then code is run through ECPG preprocessor, which replaces SQL with calls to code library. Then code can be compiled using a C compiler. Embedding works also with C++ but it does not recognize all C++ constructs.\n\nSecurity\n\nPostgreSQL manages its internal security on a per-role basis. A role is generally regarded to be a user (a role that can log in), or a group (a role of which other roles are members). Permissions can be granted or revoked on any object down to the column level, and can also allow/prevent the creation of new objects at the database, schema or table levels.\n\nThe sepgsql extension (provided with PostgreSQL as of version 9.1) provides an additional layer of security by integrating with SELinux. This utilises PostgreSQL's SECURITY LABEL feature.\n\nPostgreSQL natively supports a broad number of external authentication mechanisms, including:\n\n    password (either MD5 or plain-text)\n    GSSAPI\n    SSPI\n    Kerberos\n    ident (maps O/S user-name as provided by an ident server to database user-name)\n    peer (maps local user name to database user name)\n    LDAP\n        Active Directory\n    RADIUS\n    certificate\n    PAM\n\nThe GSSAPI, SSPI, Kerberos, peer, ident and certificate methods can also use a specified \"map\" file that lists which users matched by that authentication system are allowed to connect as a specific database user.\n\nThese methods are specified in the cluster's host-based authentication configuration file (pg_hba.conf), which determines what connections are allowed. This allows control over which user can connect to which database, where they can connect from (IP address/IP address range/domain socket), which authentication system will be enforced, and whether the connection must use TLS.\nUpcoming features\n\nUpcoming features in version 9.6 include:\n\n    Extension cascade support to install dependencies[44]\n    Parallel sequential scans,[45] parallel joins[46] and parallel aggregates[47]\n    Cube extension kNN support[48]\n    Sort pushdown,[49] join pushdown[50] and DML (UPDATE/DELETE) pushdown[51] for postgres_fdw\n    pg_visibility extension for examining visibility maps[52]\n    Command progress reporting[53]\n    Detailed wait information in pg_stat_activity[54]\n    Frozen page data in visibility map for skipping vacuum on already-frozen data[55]\n    New remote_apply replication mode which waits for confirmation that a standby has applied changes[56]\n    Index-only scans for partial indexes[57]\n    Support for multiple synchronous standbys[58]\n    Phrase full text search[59]\n    User-defined expiration of snapshots to control table bloat[60]\n\nAdd-ons\n\n    Apache MADlib: an open source analytics library for PostgreSQL providing mathematical, statistical and machine-learning methods for structured and unstructured data[61]\n    Ora2Pg: an OpenSource, GPL licensed Oracle and MySQL migration tool written in Perl Ora2Pg homepage \n    MySQL migration wizard: included with EnterpriseDB's PostgreSQL installer (source code also available)[62]\n    Performance Wizard: included with EnterpriseDB's PostgreSQL installer (source code also available)[62]\n    pgRouting: extended PostGIS to provide geospatial routing functionality[63] (GNU GPL)\n    PostGIS: a popular add-on which provides support for geographic objects (GNU GPL)\n    Postgres Enterprise Manager: a non-free tool consisting of a service, multiple agents, and a GUI which provides remote monitoring, management, reporting, capacity planning and tuning[64]\n    ST-Links SpatialKit: Extension for directly connecting to spatial databases[65]\n\nBenchmarks and performance\n\nMany informal performance studies of PostgreSQL have been done.[66] Performance improvements aimed at improving scalability started heavily with version 8.1. Simple benchmarks between version 8.0 and version 8.4 showed that the latter was more than 10 times faster on read-only workloads and at least 7.5 times faster on both read and write workloads.[67]\n\nThe first industry-standard and peer-validated benchmark was completed in June 2007 using the Sun Java System Application Server (proprietary version of GlassFish) 9.0 Platform Edition, UltraSPARC T1-based Sun Fire server and PostgreSQL 8.2.[68] This result of 778.14 SPECjAppServer2004 JOPS@Standard compares favourably with the 874 JOPS@Standard with Oracle 10 on an Itanium-based HP-UX system.[66]\n\nIn August 2007, Sun submitted an improved benchmark score of 813.73 SPECjAppServer2004 JOPS@Standard. With the system under test at a reduced price, the price/performance improved from $US 84.98/JOPS to $US 70.57/JOPS.[69]\n\nThe default configuration of PostgreSQL uses only a small amount of dedicated memory for performance-critical purposes such as caching database blocks and sorting. This limitation is primarily because older operating systems required kernel changes to allow allocating large blocks of shared memory.[70] PostgreSQL.org provides advice on basic recommended performance practice in a wiki.[71]\n\nIn April 2012, Robert Haas of EnterpriseDB demonstrated PostgreSQL 9.2's linear CPU scalability using a server with 64 cores.[72]\n\nMatloob Khushi performed benchmarking between Postgresql 9.0 and MySQL 5.6.15 for their ability to process genomic data. In his performance analysis he found that PostgreSQL extracts overlapping genomic regions eight times faster than MySQL using two datasets of 80,000 each forming random human DNA regions. Insertion and data uploads in PostgreSQL were also better, although general searching capability of both databases was almost equivalent.[73]\nPlatforms\n\nPostgreSQL is available for the following operating systems: Linux (all recent distributions), Windows (Windows 2000 SP4 and later) (compilable by e.g. Visual Studio, now with up to most recent 2015 version), DragonFly BSD, FreeBSD, OpenBSD, NetBSD, Mac OS X,[15] AIX, BSD/OS, HP-UX, IRIX, OpenIndiana,[74] OpenSolaris, SCO OpenServer, SCO UnixWare, Solaris and Tru64 Unix. In 2012, support for the following obsolete systems was removed (still supported in 9.1):[75] DG/UX, NeXTSTEP, SunOS 4, SVR4, Ultrix 4, and Univel. Most other Unix-like systems should also work.\n\nPostgreSQL works on any of the following instruction set architectures: x86 and x86-64 on Windows and other operating systems; these are supported on other than Windows: IA-64 Itanium, PowerPC, PowerPC 64, S/390, S/390x, SPARC, SPARC 64, Alpha, ARMv8-A (64-bit)[76] and older ARM (32-bit, including older such as ARMv6 in Raspberry Pi[77]), MIPS, MIPSel, M68k, and PA-RISC. It is also known to work on M32R, NS32k, and VAX. In addition to these, it is possible to build PostgreSQL for an unsupported CPU by disabling spinlocks.[78]\nDatabase administration\nSee also: Comparison of database tools\n\nOpen source front-ends and tools for administering PostgreSQL include:\n\npsql\n    The primary front-end for PostgreSQL is the psql command-line program, which can be used to enter SQL queries directly, or execute them from a file. In addition, psql provides a number of meta-commands and various shell-like features to facilitate writing scripts and automating a wide variety of tasks; for example tab completion of object names and SQL syntax.\npgAdmin \n    The pgAdmin package is a free and open source graphical user interface administration tool for PostgreSQL, which is supported on many computer platforms.[79] The program is available in more than a dozen languages. The first prototype, named pgManager, was written for PostgreSQL 6.3.2 from 1998, and rewritten and released as pgAdmin under the GNU General Public License (GPL) in later months. The second incarnation (named pgAdmin II) was a complete rewrite, first released on January 16, 2002. The third version, pgAdmin III, was originally released under the Artistic License and then released under the same license as PostgreSQL. Unlike prior versions that were written in Visual Basic, pgAdmin III is written in C++, using the wxWidgets framework allowing it to run on most common operating systems. The query tool includes a scripting language called pgScript \n    for supporting admin and development tasks. In December 2014, Dave Page, the pgAdmin project founder and primary developer,[80] announced that with the shift towards web-based models work has started on pgAdmin 4 with the aim of facilitating Cloud deployments.[81] Although still at the concept stage,[82] the plan is to build a single Python-based pgAdmin that users can either deploy on a web server or run from their desktop.\nphpPgAdmin\n    phpPgAdmin is a web-based administration tool for PostgreSQL written in PHP and based on the popular phpMyAdmin interface originally written for MySQL administration.[83]\nPostgreSQL Studio\n    PostgreSQL Studio allows users to perform essential PostgreSQL database development tasks from a web-based console. PostgreSQL Studio allows users to work with cloud databases without the need to open firewalls.[84]\nTeamPostgreSQL\n    AJAX/JavaScript-driven web interface for PostgreSQL. Allows browsing, maintaining and creating data and database objects via a web browser. The interface offers tabbed SQL editor with auto-completion, row-editing widgets, click-through foreign key navigation between rows and tables, 'favorites' management for commonly used scripts, among other features. Supports SSH for both the web interface and the database connections. Installers are available for Windows, Mac and Linux, as well as a simple cross-platform archive that runs from a script.[85]\nSQLeo\n    SQLeo is a Visual query builder, that help users to create or understand SQL queries. It has a specific feature to avoid \"ERROR: current transaction is aborted, commands ignored until end of transaction block\". The source code is hosted on SourceForge.\nLibreOffice/OpenOffice.org Base\n    LibreOffice/OpenOffice.org Base can be used as a front-end for PostgreSQL.[86][87]\npgFouine\n    The pgFouine PostgreSQL log analyzer generates detailed reports from a PostgreSQL log file and provides VACUUM analysis.[88]\n\nA number of companies offer proprietary tools for PostgreSQL. They often consist of a universal core that is adapted for various specific database products. These tools mostly share the administration features with the open source tools but offer improvements in data modeling, importing, exporting or reporting.\nProminent users\n\nProminent organizations and products that use PostgreSQL as the primary database include:\n\n    Yahoo! for web user behavioral analysis, storing two petabytes and claimed to be the largest data warehouse using a heavily modified version of PostgreSQL with an entirely different column-based storage engine and different query processing layer. While for performance, storage, and query purposes the database bears little resemblance to PostgreSQL, the front-end maintains compatibility so that Yahoo can use many off-the-shelf tools already written to interact with PostgreSQL.[89][90]\n    In 2009, social networking website MySpace used Aster Data Systems's nCluster database for data warehousing, which was built on unmodified PostgreSQL.[91][92]\n    Geni.com uses PostgreSQL for their main genealogy database.[93]\n    OpenStreetMap, a collaborative project to create a free editable map of the world.[94]\n    Afilias, domain registries for .org, .info and others.[95]\n    Sony Online multiplayer online games.[96]\n    BASF, shopping platform for their agribusiness portal.[97]\n    Reddit social news website.[98]\n    Skype VoIP application, central business databases.[99]\n    Sun xVM, Sun's virtualization and datacenter automation suite.[100]\n    MusicBrainz, open online music encyclopedia.[101]\n    The International Space Station for collecting telemetry data in orbit and replicating it to the ground.[102]\n    MyYearbook social networking site.[103]\n    Instagram, a popular mobile photo sharing service[104]\n    Disqus, an online discussion and commenting service[105]\n    TripAdvisor, travel information website of mostly user-generated content[106]\n\nPostgreSQL is offered by some major vendors as software as a service:\n\n    Heroku, a platform as a service provider, has supported PostgreSQL since the start in 2007.[107] They offer value-add features like full database \"roll-back\" (ability to restore a database from any point in time),[108] which is based on WAL-E, open source software developed by Heroku.[109]\n    In January 2012, EnterpriseDB released a cloud version of both PostgreSQL and their own proprietary Postgres Plus Advanced Server with automated provisioning for failover, replication, load-balancing, and scaling. It runs on Amazon Web Services.[110]\n    VMware offers vFabric Postgres for private clouds on vSphere since May 2012.[111]\n    In November 2013, Amazon.com announced that they are adding PostgreSQL to their Relational Database Service offering.[112][113]\n\nRelease history\nRelease \tFirst release \tLatest minor version \tLatest release \tEnd of Life \tMilestones\n0.01 \t1995-05-01 \t0.03 \t1995-07-21 \t ? \tInitial release as Postgres95\n1.0 \t1995-09-05 \t1.09 \t1996-11-04 \t ? \tChanged copyright to a more liberal license\n6.0 \t1997-01-29 \t− \t\t ? \tName change from Postgres95 to PostgreSQL (in 1996[1]), unique indexes, pg_dumpall utility, ident authentication.\n6.1 \t1997-06-08 \t6.1.1 \t1997-07-22 \t ? \tMulti-column indexes, sequences, money data type, GEQO (GEnetic Query Optimizer).\n6.2 \t1997-10-02 \t6.2.1 \t1997-10-17 \t ? \tJDBC interface, triggers, server programming interface, constraints.\n6.3 \t1998-03-01 \t6.3.2 \t1998-04-07 \t2003-04 \tSQL92 subselect capability, PL/pgTCL\n6.4 \t1998-10-30 \t6.4.2 \t1998-12-20 \t2003-10 \tVIEWs and RULEs, PL/pgSQL\n6.5 \t1999-06-09 \t6.5.3 \t1999-10-13 \t2004-06 \tMVCC, temporary tables, more SQL statement support (CASE, INTERSECT, and EXCEPT)\n7.0 \t2000-05-08 \t7.0.3 \t2000-11-11 \t2004-05 \tForeign keys, SQL92 syntax for joins\n7.1 \t2001-04-13 \t7.1.3 \t2001-08-15 \t2006-04 \tWrite-ahead log, Outer joins\n7.2 \t2002-02-04 \t7.2.8 \t2005-05-09 \t2007-02 \tPL/Python, OIDs no longer required, internationalization of messages\n7.3 \t2002-11-27 \t7.3.21 \t2008-01-07 \t2007-11 \tSchema, Internationalization\n7.4 \t2003-11-17 \t7.4.30 \t2010-10-04 \t2010-10 \tOptimization all-round\n8.0 \t2005-01-19 \t8.0.26 \t2010-10-04 \t2010-10 \tNative server on Microsoft Windows, savepoints, tablespaces, exception handling in functions, point-in-time recovery\n8.1 \t2005-11-08 \t8.1.23 \t2010-12-16 \t2010-11 \tPerformance optimization, two-phase commit, table partitioning, index bitmap scan, shared row locking, roles\n8.2 \t2006-12-05 \t8.2.23 \t2011-09-26 \t2011-12 \tPerformance optimization, online index builds, advisory locks, warm standby\n8.3 \t2008-02-04 \t8.3.23 \t2013-02-07 \t2013-12 \tHeap-only tuples, full text search, SQL/XML, ENUM types, UUID types\n8.4 \t2009-07-01 \t8.4.22 \t2014-07-24 \t2014-07 \tWindowing functions, default and variadic parameters for functions, column-level permissions, parallel database restore, per-database collation, common table expressions and recursive queries\n9.0 \t2010-09-20 \t9.0.23 \t2015-10-08 \t2015-09 \tBuilt-in binary streaming replication, Hot standby, 64-bit Windows, per-column triggers and conditional trigger execution, exclusion constraints, anonymous code blocks, named parameters, password rules\n9.1 \t2011-09-12 \t9.1.22 \t2016-05-13 \t2016-09 \tSynchronous replication, per-column collations, unlogged tables, k-nearest neighbors (k-NN) indexing, serializable snapshot isolation, writeable common table expressions, SE-Linux integration, extensions, SQL/MED attached tables (Foreign Data Wrappers), triggers on views\n9.2 \t2012-09-10 \t9.2.17 \t2016-05-13 \t2017-09 \tCascading streaming replication, index-only scans, native JSON support, improved lock management, range types, pg_receivexlog tool, space-partitioned GiST indexes\n9.3 \t2013-09-09 \t9.3.13 \t2016-05-13 \t2018-09 \tCustom background workers, data checksums, dedicated JSON operators, LATERAL JOIN, faster pg_dump, new pg_isready server monitoring tool, trigger features, view features, writeable foreign tables, materialized views, replication improvements\n9.4 \t2014-12-18 \t9.4.8 \t2016-05-13 \t2019-12 \tJSONB data type, ALTER SYSTEM statement for changing config values, refresh materialized views without blocking reads, dynamic registration/start/stop of background worker processes, Logical Decoding API, GiN index improvements, Linux huge page support, database cache reloading via pg_prewarm\n9.5 \t2016-01-07 \t9.5.3 \t2016-05-13 \t2021-01 \tUPSERT, row level security, JSONB-modifying operators/functions, IMPORT FOREIGN SCHEMA, TABLESAMPLE, CUBE/ROLLUP, grouping sets, foreign table inheritance, pg_rewind tool, index-only scans for GiST indexes and new BRIN (Block Range Indexes) index to speed up queries on very large tables[114]\nCommunity-supported\nCommunity support ended[115]", "skillName": "PostgreSQL."}
{"id": 181, "category": "Databases", "skillText": "Often abbreviated DB, a database is basically a collection of information organized in such a way that a computer program can quickly select desired pieces of data. You can think of a database as an electronic filing system.\n\nTraditional databases are organized by fields, records, and files. A field is a single piece of information; a record is one complete set of fields; and a file is a collection of records. For example, a telephone book is analogous to a file. It contains a list of records, each of which consists of three fields: name, address, and telephone number.\n\nAn alternative concept in database design is known as Hypertext. In a Hypertext database, any object, whether it be a piece of text, a picture, or a film, can be linkedto any other object. Hypertext databases are particularly useful for organizing large amounts of disparate information, but they are not designed for numerical analysis.\n\nTo access information from a database, you need a database management system (DBMS). This is a collection of programs that enables you to enter, organize, and select data in a database.\n\n(2) Increasingly, the term database is used as shorthand for database management system. There are many different types of DBMSs, ranging from small systems that run on personal computers to huge systems that run on mainframes.", "skillName": "db."}
{"id": 182, "category": "Databases", "skillText": "n computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis, and is considered as a core component of business intelligence[1] environment. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.\n\nThe data stored in the warehouse is uploaded from the operational systems (such as marketing, sales, etc., shown in the figure to the right). The data may pass through an operational data store for additional operations before it is used in the DW for reporting.\n\nContents\n\n    1 Types of systems\n    2 Software tools\n    3 Benefits\n    4 Generic data warehouse environment\n    5 History\n    6 Information storage\n        6.1 Facts\n        6.2 Dimensional vs. normalized approach for storage of data\n    7 Design methods\n        7.1 Bottom-up design\n        7.2 Top-down design\n        7.3 Hybrid design\n    8 Data warehouses versus operational systems\n    9 Evolution in organization use\n    10 See also\n    11 References\n    12 Further reading\n    13 External links\n\nTypes of systems\n\nData mart\n    A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area) hence, they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data.[2] Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.\n\nThe difference between data warehouse and data mart\nData warehouse \tdata mart\nenterprise-wide data \tdepartment-wide data\nmultiple subject areas \tsingle subject area\ndifficult to build \teasy to build\ntakes more time to build \tless time to build\nlarger memory \tlimited memory\n\nTypes of data marts\n\n    Dependent data mart\n    Independent data mart\n    Hybrid data mart\n\nOnline analytical processing (OLAP)\n    OLAP is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by Data Mining techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day.The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing & Dicing.[3]\n\nOnline transaction processing (OLTP)\n    OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining data integrity in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually 3NF).[4] Normalization is the norm for data modeling techniques in this system.\n\nPredictive analysis\n    Predictive analysis is about finding and quantifying hidden patterns in the data using complex mathematical models that can be used to predict future outcomes. Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for CRM (customer relationship management).\n\nSoftware tools\n\nThe typical extract-transform-load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.[5]\n\nThis definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, cataloged and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support.[6] However, the means to retrieve and analyze data, to extract, transform and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform and load data into the repository, and tools to manage and retrieve metadata.\nBenefits\n\nA data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to :\n\n    Congregate data from multiple sources into a single database so a single query engine can be used to present data.\n    Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long running, analysis queries in transaction processing databases.\n    Maintain data history, even if the source transaction systems do not.\n    Integrate data from multiple source systems, enabling a central view across the enterprise. This benefit is always valuable, but particularly so when the organization has grown by merger.\n    Improve data quality, by providing consistent codes and descriptions, flagging or even fixing bad data.\n    Present the organization's information consistently.\n    Provide a single common data model for all data of interest regardless of the data's source.\n    Restructure the data so that it makes sense to the business users.\n    Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the operational systems.\n    Add value to operational business applications, notably customer relationship management (CRM) systems.\n    Make decision–support queries easier to write.\n\nGeneric data warehouse environment\n\nThe environment for data warehouses and marts includes the following:\n\n    Source systems that provide data to the warehouse or mart;\n    Data integration technology and processes that are needed to prepare the data for use;\n    Different architectures for storing data in an organization's data warehouse or data marts;\n    Different tools and applications for the variety of users;\n    Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.\n\nIn regards to source systems listed above, Rainer[clarification needed] states, “A common source for the data in data warehouses is the company’s operational databases, which can be relational databases”.[7]\n\nRegarding data integration, Rainer states, “It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse”.[7]\n\nRainer discusses storing data in an organization’s data warehouse or data marts.[7]\n\nMetadata are data about data. “IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures“.[7]\n\nToday, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers.[7] A “data warehouse” is a repository of historical data that are organized by subject to support decision makers in the organization.[7] Once data are stored in a data mart or warehouse, they can be accessed.\nHistory\n\nThe concept of data warehousing dates back to the late 1980s[8] when IBM researchers Barry Devlin and Paul Murphy developed the \"business data warehouse\". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from \"data marts\" that were tailored for ready access by users.\n\nKey developments in early years of data warehousing were:\n\n    1960s — General Mills and Dartmouth College, in a joint research project, develop the terms dimensions and facts.[9]\n    1970s — ACNielsen and IRI provide dimensional data marts for retail sales.[9]\n    1970s — Bill Inmon begins to define and discuss the term: Data Warehouse.[citation needed]\n    1975 — Sperry Univac introduces MAPPER (MAintain, Prepare, and Produce Executive Reports) is a database management and reporting system that includes the world's first 4GL. First platform designed for building Information Centers (a forerunner of contemporary Enterprise Data Warehousing platforms)\n    1983 — Teradata introduces a database management system specifically designed for decision support.\n    1984 — Metaphor Computer Systems, founded by David Liddle and Don Massaro, releases Data Interpretation System (DIS). DIS was a hardware/software package and GUI for business users to create a database management and analytic system.\n    1988 — Barry Devlin and Paul Murphy publish the article An architecture for a business and information system where they introduce the term \"business data warehouse\".[10]\n    1990 — Red Brick Systems, founded by Ralph Kimball, introduces Red Brick Warehouse, a database management system specifically for data warehousing.\n    1991 — Prism Solutions, founded by Bill Inmon, introduces Prism Warehouse Manager, software for developing a data warehouse.\n    1992 — Bill Inmon publishes the book Building the Data Warehouse.[11]\n    1995 — The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.\n    1996 — Ralph Kimball publishes the book The Data Warehouse Toolkit.[12]\n    2012 — Bill Inmon developed and made public technology known as \"textual disambiguation\". Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. Textual disambiguation is accomplished through the execution of textual ETL. Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.\n\nInformation storage\nFacts\n\nA fact is a value or measurement, which represents a fact about the managed entity or system.\n\nFacts as reported by the reporting entity are said to be at raw level. E.g. if a BTS (business transformation service) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 facts or measurements to a management system:\n\n    tch_req_total = 1000\n    tch_req_success = 820\n    tch_req_fail = 180\n\nFacts at raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information out of it. These are called aggregates or summaries or aggregated facts.\n\nE.g. if there are 3 BTSs in a city, then facts above can be aggregated from BTS to city level in network dimension. E.g.\n\n    t c h _ r e q _ s u c c e s s _ c i t y = t c h _ r e q _ s u c c e s s _ b t s 1 + t c h _ r e q _ s u c c e s s _ b t s 2 + t c h _ r e q _ s u c c e s s _ b t s 3 {\\displaystyle tch\\_req\\_success\\_city=tch\\_req\\_success\\_bts1+tch\\_req\\_success\\_bts2+tch\\_req\\_success\\_bts3} tch\\_req\\_success\\_city = tch\\_req\\_success\\_bts1 + tch\\_req\\_success\\_bts2 + tch\\_req\\_success\\_bts3\n    a v g _ t c h _ r e q _ s u c c e s s _ c i t y = ( t c h _ r e q _ s u c c e s s _ b t s 1 + t c h _ r e q _ s u c c e s s _ b t s 2 + t c h _ r e q _ s u c c e s s _ b t s 3 ) / 3 {\\displaystyle avg\\_tch\\_req\\_success\\_city=(tch\\_req\\_success\\_bts1+tch\\_req\\_success\\_bts2+tch\\_req\\_success\\_bts3)/3} avg\\_tch\\_req\\_success\\_city = (tch\\_req\\_success\\_bts1 + tch\\_req\\_success\\_bts2 + tch\\_req\\_success\\_bts3) / 3\n\nDimensional vs. normalized approach for storage of data\n\nThere are three or more leading approaches to storing data in a data warehouse — the most important approaches are the dimensional approach and the normalized approach.\n\nThe dimensional approach refers to Ralph Kimball’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. The normalized approach, also called the 3NF model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.\n\nIn a dimensional approach, transaction data are partitioned into \"facts\", which are generally numeric transaction data, and \"dimensions\", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.\n\nA key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly.[12] Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus,this type of modeling technique is very useful for end-user queries in data warehouse.[3]\n\nThe main disadvantages of the dimensional approach are the following:\n\n    In order to maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.\n    It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.\n\nIn the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. Tables are grouped together by subject areas that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008)[citation needed]. The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse.\n\nBoth normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as Normal Forms). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).\n\nIn Information-Driven Business,[13] Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.[14]\nDesign methods\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2015) (Learn how and when to remove this template message)\nBottom-up design\n\nIn the bottom-up approach, data marts are first created to provide reporting and analytical capabilities for specific business processes. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of \"the bus\", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.[15]\nTop-down design\n\nThe top-down approach is designed using a normalized enterprise data model. \"Atomic\" data, that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.[16]\nHybrid design\n\nData warehouses (DW) often resemble the hub and spokes architecture. Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an operational data store, the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.\n\nThe DW database in a hybrid solution is kept on third normal form to eliminate data redundancy. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a master data management solution where operational, not static information could reside.\n\nThe Data Vault Modeling components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and star schema. The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.\nData warehouses versus operational systems\n\nOperational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. Operational system designers generally follow the Codd rules of database normalization in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. Relational databases are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.\n\nData warehouses are optimized for analytic access patterns. Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases. Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.\nEvolution in organization use\n\nThese terms refer to the level of sophistication of a data warehouse:\n\nOffline operational data warehouse\n    Data warehouses in this stage of evolution are updated on a regular time cycle (usually daily, weekly or monthly) from the operational systems and the data is stored in an integrated reporting-oriented data\nOffline data warehouse\n    Data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting.\nOn time data warehouse\n    Online Integrated Data Warehousing represent the real time Data warehouses stage data in the warehouse is updated for every transaction performed on the source data\nIntegrated data warehouse\n    These data warehouses assemble data from different areas of business, so users can look up the information they need across other systems.[17]", "skillName": "Data_warehouse."}
{"id": 183, "category": "ProductDevelopment", "skillText": "In commerce, time to market (TTM) is the length of time it takes from a product being conceived until its being available for sale. TTM is important in industries where products are outmoded quickly. A common assumption is that TTM matters most for first-of-a-kind products, but actually the leader often has the luxury of time, while the clock is clearly running for the followers.[citation needed]\n\nContents\n\n    1 Measuring TTM\n    2 TTM and quality\n    3 Types of TTM\n    4 History\n    5 See also\n    6 References\n\nMeasuring TTM\n\nThere are no standards for measuring TTM, and measured values can vary greatly. First, there is great variation in how different organizations define the start of the period. For example, in the automotive industry the development period starts when the product concept is approved.[citation needed] Other organizations realize that little will happen until the project is staffed, which can take a long time after approval if developers are tied up on existing projects. Therefore, they consider the start point when the project is fully staffed. The initial part of a project—before approval has been given or full staffing is allocated—has been called the fuzzy front end, and this stage can consume a great deal of time. Even though the fuzzy front end is difficult to measure, it must be included in TTM measurements for effective TTM management.\n\nNext, definitions of the end of the TTM period vary. Those who look at product development as engineering say the project is finished when engineering department transfers it to manufacturing. Others define the conclusion as when they ship the first copy of the new product or when a customer buys it. High-volume industries will often define the end point in terms of reaching a certain production volume, such as a million units per month.[citation needed]\n\nFinally, TTM measurements vary greatly depending on complexity –- complexity of the product itself, the technologies it incorporates, its manufacturing processes, or the organizational complexity of the project (for example, outsourced components). New-to-the-world products are much slower than derivatives of existing products. Some companies have been successful in putting their products into categories of newness, but establishing levels of complexity remains elusive.\n\nAlthough TTM can vary widely, all that matters is an organization's TTM capability relative to its direct competitors. Organizations in other industries may be much faster, but do not pose a competitive threat, although one may be able to learn from them and adapt their techniques.\nTTM and quality\n\nA tacit assumption of many is that TTM and product quality are opposing attributes of a development process. TTM may be improved (shortened) by skipping steps of the development process, thus compromising product quality.[citation needed] For those who use highly structured development processes such as Phase–gate model or Six Sigma, product development is often viewed as a clearly defined sequence of steps to be followed. Skipping a step—due to perceived time pressure, for example—may not only undercut quality but can ultimately lengthen TTM if the organization must complete or repeat the step later. Following this view, TTM is usually improved by following all of the prescribed steps.[citation needed]\n\nOther organizations operate more aggressively, recognizing that not all steps need to be completed for every project. Furthermore, they actively apply tools and techniques that will shorten or overlap steps, cut decision-making time, and automate activities. Many such tools and techniques are available (see References below).\nTypes of TTM\n\nOrganizations pursue TTM improvement for a variety of reasons. Some variations of TTM are\n\n    Pure speed, that is, bring the product to market as quickly as possible. This is valuable in fast-moving industries, but it is not always the best objective.\n    More predictable schedules. Rather than reaching the market as soon as possible, delivering on schedule, for example to have the new product available for a trade show, can be more valuable. In addition to processes such as Stage-Gate or Six Sigma, project risk management (see References below) is an effective tool here.\n    Minimizing resources, especially labor. Many managers figure that the shorter the project the less it will cost, so they attempt to use TTM as a means of cutting expenses. Ironically, a primary means of reducing TTM is to staff the project more heavily,[citation needed] so a faster project may actually be more expensive.\n    Flexibility to make changes. Product innovation is intimately tied to change, and often the need for change appears midstream in a project. Consequently, the ability to make changes during development without being too disruptive can be valuable. For example, one’s goal could be to satisfy customers, which could be achieved by adjusting product requirements during development in response to customer feedback. Then TTM could be measured from the last change in requirements until the product is delivered.\n\nThese types of TTM illustrate that an organization’s TTM goals should be aligned with its business strategy rather than pursuing speed blindly.\nHistory\n\nThe first recorded conference on Time-to-Market was organised by Bart Hall of AiC and held on 25 and 26 October 1995 at the St James Hotel in London. It was chaired by Mike Woodman, then of Logica and now with Coplexis Consulting, and Allen Porter of AIIT.\nSee also\n\n    New product development\n    Flexible product development\n    Six Sigma\n    Follow-the-sun", "skillName": "Time_toMarket."}
{"id": 184, "category": "ProductDevelopment", "skillText": "In business and engineering, new product development (NPD) is the complete process of bringing a new product to market. New product development is described in the literature as the transformation of a market opportunity into a product available for sale[1] and it can be tangible (that is, something physical you can touch) or intangible (like a service, experience, or belief). A good understanding of customer needs and wants, the competitive environment and the nature of the market represent the top required factors for the success of a new product.[2] Cost, time and quality are the main variables that drive the customer needs. Aimed at these three variables, companies develop continuous practices and strategies to better satisfy the customer requirements and increase their market share by a regular development of new products. There are many uncertainties and challenges throughout the process which companies must face. The use of best practices and the elimination of barriers to communication are the main concerns for the management of NPD process.\n\nContents\n\n    1 Process structure\n    2 New Product Development Models\n    3 Marketing considerations\n        3.1 The eight stages\n        3.2 Fuzzy Front End\n        3.3 Other approaches\n    4 NPD organizations\n    5 NPD strategies\n    6 Managing New Product Development\n    7 Related fields\n    8 See also\n    9 References\n\nProcess structure\n\nThe product development process typically consists of several activities that firms employ in the complex process of delivering new products to the market. Every new product will pass through a series of stages from ideation through design, manufacturing and market introduction. The development process basically has three main phases:\n\n    Fuzzy front-end (FFE) is the set of activities employed before the formal and well defined requirements specification is completed. Requirements are a high-level view of what the product should do to meet the perceived market or business need.\n    Product design is the development of both the high-level and detailed-level design of the product: which turns the what of the requirements into a specific how this particular product will meet those requirements. On the marketing and planning side, this phase ends at pre-commercialization analysis[clarification needed] stage.\n    Product implementation is the phase of detailed engineering design of mechanical or electrical hardware, or the software engineering of software or embedded software, or design of soft goods or other product forms, as well as of any test process that may be used to validate that the prototype objects actually meet the design specification and the requirements specification that was previously agreed to.\n    Fuzzy back-end or commercialization phase represent the action steps where the production and market launch occur.\n\nThe front-end marketing phases have been very well researched, with valuable models proposed. Peter Koen et al. provides a five-step front-end activity called front-end innovation: opportunity identification, opportunity analysis, idea genesis, idea selection, and idea and technology development. He also includes an engine in the middle of the five front-end stages and the possible outside barriers that can influence the process outcome. The engine represents the management driving the activities described. The front end of the innovation is the greatest area of weakness in the NPD process. This is mainly because the FFE is often chaotic, unpredictable and unstructured.[3] Engineering design is the process whereby a technical solution is developed iteratively to solve a given problem[4][5][6] The design stage is very important because at this stage most of the product life cycle costs are engaged. Previous research shows that 70% - 80% of the final product quality and 70% of the product entire life-cycle cost are determined in the product design phase, therefore the design-manufacturing interface represent the greatest opportunity for cost reduction.[7] Design projects last from a few weeks to three years with an average of one year.[8] Design and Commercialization phases usually start a very early collaboration. When the concept design is finished it will be sent to manufacturing plant for prototyping, developing a Concurrent Engineering approach by implementing practices such as QFD, DFM/DFA and more. The output of the design (engineering) is a set of product and process specifications – mostly in the form of drawings, and the output of manufacturing is the product ready for sale.[9] Basically, the design team will develop drawings with technical specifications representing the future product, and will send it to the manufacturing plant to be executed. Solving product/process fit problems is of high priority in information communication design because 90% of the development effort must be scrapped if any changes are made after the release to manufacturing.[9]\nNew Product Development Models\n\nConceptual models have been designed in order to facilitate a smooth process. The concept adopted by IDEO, a successful design and consulting firm, is one of the most researched processes in regard to new product development and is a five-step procedure.[10] These steps are listed in chronological order:\n\n    Understand and observe the market, the client, the technology, and the limitations of the problem;\n    Synthesize the information collected at the first step;\n    Visualise new customers using the product;\n    Prototype, evaluate and improve the concept;\n    Implementation of design changes which are associated with more technologically advanced procedures and therefore this step will require more time.\n\nOne of the first developed models that today companies still use in the NPD process is the Booz, Allen and Hamilton (BAH) Model, published in 1982.[11] This is the best known model because it underlies the NPD systems that have been put forward later.[12] This model represent the foundation of all the other models that have been developed afterwards. Significant work has been conducted in order to propose better models, but in fact these models can be easily linked to BAH model. The seven steps of BAH model are: new product strategy, idea generation, screening and evaluation, business analysis, development, testing, and commercialization.\nA pioneer of NPD research is Robert G. Cooper. Over the last two decades he conducted significant work in the area of NPD. The Stage-Gate model developed in the 1980s was proposed as a new tool for managing new products development processes.[13] The 2010 APQC benchmarking study reveals that 88% of U.S. businesses employ a stage-gate system to manage new products, from idea to launch. In return, the companies that adopt this system are reported to receive benefits such as improved teamwork, shorter cycle time, improved success rates, earlier detection of failure, a better launch, and even shorter cycle times – reduced by about 30%.[14] These findings highlight the importance of the stage-gate model, making it the single most important discovery in the area of new product development.\nMarketing considerations\n\nThere have been a number of approaches proposed for analyzing and responding to the marketing challenges of new product development. Two of these are the eight stages process of Koen[clarification needed] and a process known as the fuzzy front end.\nThe eight stages\n\n    Idea Generation is often called the \"NPD\" of the NPD process.[15]\n        Ideas for new products can be obtained from basic research using a SWOT analysis (Strengths, Weaknesses, Opportunities & Threats). Market and consumer trends, company's R&D department, competitors, focus groups, employees, salespeople, corporate spies, trade shows, or ethnographic discovery methods (searching for user patterns and habits) may also be used to get an insight into new product lines or product features.\n        Lots of ideas are generated about the new product. Out of these ideas many are implemented. The ideas are generated in many forms. Many reasons are responsible for generation of an idea.\n        Idea for new product can come from many sources, such as customer, scientists, competitors, employees, channel member, and top management.\n        customer need and wants are the logical place to start the search.\n        Idea Generation or Brainstorming of new product, service, or store concepts - idea generation techniques can begin when you have done your OPPORTUNITY ANALYSIS to support your ideas in the Idea Screening Phase (shown in the next development step).\n    Idea Screening[citation needed]\n        The object is to eliminate unsound concepts prior to devoting resources to them.\n        The screener should ask several questions:\n            Will the customer in the target market benefit from the product?\n            What is the size and growth forecasts of the market segment / target market?\n            What is the current or expected competitive pressure for the product idea?\n            What are the industry sales and market trends the product idea is based on?\n            Is it technically feasible to manufacture the product?\n            Will the product be profitable when manufactured and delivered to the customer at the target price?\n    Idea Development and Testing[citation needed]\n        Develop the marketing and engineering details\n        Product Idea - It is an idea for a possible product that the company can see itself offering to the market.\n        Product Concept - when idea is developed in every aspect so as to make it presentable, it is called a concept.[2] \n        Product Identity - It is the way business perceive an actual or potential product.\n            Investigate intellectual property issues and search patent databases\n            Who is the target market and who is the decision maker in the purchasing process?\n            What product features must the product incorporate?\n            What benefits will the product provide?\n            How will consumers react to the product?\n            How will the product be produced most cost effectively?\n            Prove feasibility through virtual computer aided rendering and rapid prototyping\n            What will it cost to produce it?\n        Testing the Concept - Random people belonging to the target group are chosen to test the concept. Information is provided and questions are asked. Their answers are reactions are noted for further improvement of concept.\n    Business Analysis[citation needed]\n        Estimate likely selling price based upon competition and customer feedback\n        Estimate sales volume based upon size of market and such tools as the Fourt-Woodlock equation\n        Estimate profitability and break-even point\n    Beta Testing and Market Testing[citation needed]\n        Produce a physical prototype or mock-up\n        Test the product (and its packaging) in typical usage situations\n        Conduct focus group customer interviews or introduce at trade show\n        Make adjustments where necessary\n        Produce an initial run of the product and sell it in a test market area to determine customer acceptance\n    Technical Implementation[citation needed]\n        New program initiation\n        Finalize Quality management system\n        Resource estimation\n        Requirement publication\n        Publish technical communications such as data sheets\n        Engineering operations planning\n        Department scheduling\n        Supplier collaboration\n        Logistics plan\n        Resource plan publication\n        Program review and monitoring\n        Contingencies - what-if planning\n    Commercialization (often considered post-NPD)[citation needed]\n        Launch the product\n        Produce and place advertisements and other promotions\n        Fill the distribution pipeline with product\n        Critical path analysis is most useful at this stage\n    New Product Pricing[citation needed]\n        Impact of new product on the entire product portfolio\n        Value Analysis (internal & external)\n        Competition and alternative competitive technologies\n        Differing value segments (price, value and need)\n        Product Costs (fixed & variable)\n        Forecast of unit volumes, revenue, and profit\n\nThese steps may be iterated as needed. Some steps may be eliminated. To reduce the time that the NPD process takes, many companies are completing several steps at the same time (referred to as concurrent engineering or time to market). Most industry leaders see new product development as a proactive process where resources are allocated to identify market changes and seize upon new product opportunities before they occur (in contrast to a reactive strategy in which nothing is done until problems occur or the competitor introduces an innovation). Many industry leaders see new product development as an ongoing process (referred to as continuous development) in which the entire organization is always looking for opportunities.[citation needed]\n\nFor the more innovative products indicated on the diagram above,[clarification needed] great amounts of uncertainty and change may exist which makes it difficult or impossible to plan the complete project before starting it. In this case, a more flexible approach may be advisable.[citation needed]\n\nBecause the NPD process typically requires both engineering and marketing expertise, cross-functional teams are a common way of organizing projects.[16] The team is responsible for all aspects of the project, from initial idea generation to final commercialization, and they usually report to senior management (often to a vice president or Program Manager). In those industries where products are technically complex, development research is typically expensive and product life cycles are relatively short, strategic alliances among several organizations helps to spread the costs, provide access to a wider skill set and speeds up the overall process.[citation needed]\n\nBecause both engineering and marketing expertise are usually critical to the process, choosing an appropriate blend of the two is important. Observe (for example, by looking at the See also or References sections below) that this article is slanted more toward the marketing side. For more of an engineering slant, see the Ulrich and Eppinger, Ullman references below.[17][18]\n\nA new product pricing process is important to reduce risk and increase confidence in the pricing and marketing decisions to be made. Processes have been proposed to break down the complex task of new product pricing into more manageable elements.[19]\n\nThe Path to Developing Successful New Products[20] points out three key processes that can play critical role in product development: Talk to the customer; Nurture a project culture; Keep it focused.\nFuzzy Front End\n\nThe Fuzzy Front End (FFE) is the messy \"getting started\" period of new product engineering development processes. It is in the front end where the organization formulates a concept of the product to be developed and decides whether or not to invest resources in the further development of an idea. It is the phase between first consideration of an opportunity and when it is judged ready to enter the structured development process (Kim and Wilemon, 2007;[21] Koen et al., 2001).[15] It includes all activities from the search for new opportunities through the formation of a germ of an idea to the development of a precise concept. The Fuzzy Front End phase ends when an organization approves and begins formal development of the concept.\n\nAlthough the Fuzzy Front End may not be an expensive part of product development, it can consume 50% of development time (see Chapter 3 of the Smith and Reinertsen reference below),[22] and it is where major commitments are typically made involving time, money, and the product's nature, thus setting the course for the entire project and final end product. Consequently, this phase should be considered as an essential part of development rather than something that happens \"before development,\" and its cycle time should be included in the total development cycle time.\n\nKoen et al. distinguish five different front-end elements (not necessarily in a particular order):[15]\n\n    Opportunity Identification\n    Opportunity Analysis\n    Idea Genesis\n    Idea Selection\n    Idea and Technology Development\n\n    The first element is the opportunity identification. In this element, large or incremental business and technological chances are identified in a more or less structured way. Using the guidelines established here, resources will eventually be allocated to new projects.... which then lead to a structured NPPD (New Product & Process Development) strategy.\n    The second element is the opportunity analysis. It is done to translate the identified opportunities into implications for the business and technology specific context of the company. Here extensive efforts may be made to align ideas to target customer groups and do market studies and/or technical trials and research.\n    The third element is the idea genesis, which is described as evolutionary and iterative process progressing from birth to maturation of the opportunity into a tangible idea. The process of the idea genesis can be made internally or come from outside inputs, e.g. a supplier offering a new material/technology or from a customer with an unusual request.\n    The fourth element is the idea selection. Its purpose is to choose whether to pursue an idea by analyzing its potential business value.\n    The fifth element is the idea and technology development. During this part of the front-end, the business case is developed based on estimates of the total available market, customer needs, investment requirements, competition analysis and project uncertainty. Some organizations consider this to be the first stage of the NPPD process (i.e., Stage 0).\n\nThe Fuzzy Front End is also described in literature[by whom?] as \"Front End of Innovation\", \"Phase 0\", \"Stage 0\" or \"Pre-Project-Activities\".[citation needed]\n\nA universally acceptable definition for Fuzzy Front End or a dominant framework has not been developed so far.[23] In a glossary of PDMA,[24] it is mentioned that the Fuzzy Front End generally consists of three tasks: strategic planning, idea generation, and, especially, pre-technical evaluation. These activities are often chaotic, unpredictable, and unstructured. In comparison, the subsequent new product development process is typically structured, predictable, and formal. The term Fuzzy Front End was first popularized by Smith and Reinertsen (1991).[25] R.G. Cooper (1988)[26] describes the early stages of NPPD as a four-step process in which ideas are generated (I), subjected to a preliminary technical and market assessment (II) and merged to coherent product concepts (III) which are finally judged for their fit with existing product strategies and portfolios (IV).\nOther approaches\n\nOther authors have divided predevelopment product development activities differently:[27]\n\n    Preliminary\n    Technical assessment\n    Source-of-supply assessment: suppliers and partners or alliances\n    Market research: market size and segmentation analysis, VoC (voice of the customer) research\n    Product idea testing\n    Customer value assessment\n    Product definition\n    Business and financial analysis\n\nThese activities yield essential information to make a Go/No-Go to Development decision.\n\nOne of the earliest[citation needed] studies using the case study method defined the front-end to include the interrelated activities of:[28]\n\n    product strategy formulation and communication\n    opportunity identification and assessment\n    idea generation\n    product definition\n    project planning\n    executive reviews\n\nEconomical analysis, benchmarking of competitive products and modeling and prototyping are also important activities during the front-end activities. The outcomes of FFE are the:[citation needed]\n\n    mission statement\n    customer needs\n    details of the selected idea\n    product definition and specifications\n    economic analysis of the product\n    the development schedule\n    project staffing and the budget\n    a business plan aligned with corporate strategy\n\nA conceptual model of Front-End Process was proposed which includes early phases of the innovation process. This model is structured in three phases and three gates:[29]\n\n    Phase 1: Environmental screening or opportunity identification stage in which external changes will be analysed and translated into potential business opportunities.\n    Phase 2: Preliminary definition of an idea or concept.\n    Phase 3: Detailed product, project or service definition, and Business planning.\n\nThe gates are:\n\n    Opportunity screening\n    Idea evaluation\n    Go/No-Go for development\n\nThe final gate leads to a dedicated new product development project. Many professionals and academics consider that the general features of Fuzzy Front End (fuzziness, ambiguity, and uncertainty) make it difficult to see the FFE as a structured process, but rather as a set of interdependent activities ( e.g. Kim and Wilemon, 2002).[30] However, Husig et al., 2005 [10] argue that front-end not need to be fuzzy, but can be handled in a structured manner. In fact Carbone [31][32] showed that when using the front end success factors in an integrated process, product success is increased. Peter Koen[33] argues that in the FFE for incremental, platform and radical projects, three separate strategies and processes are typically involved.[33] The traditional Stage Gate (TM) process was designed for incremental product development, namely for a single product. The FFE for developing a new platform must start out with a strategic vision of where the company wants to develop products and this will lead to a family of products. Projects for breakthrough products start out with a similar strategic vision, but are associated with technologies which require new discoveries.\n\nIncremental, platform and breakthrough products include:[33]\n\n    Incremental products are considered to be cost reductions, improvements to existing product lines, additions to existing platforms and repositioning of existing products introduced in markets.\n    Breakthrough products are new to the company or new to the world and offer a 5-10 times or greater improvement in performance combined with a 30-50% or greater reduction in costs.\n    Platform products establish a basic architecture for a next generation product or process and are substantially larger in scope and resources than incremental projects.\n\nNPD organizations\n\n    Product Development and Management Association (PDMA)\n    Association of International Product Marketing & Management\n    ISPIM (The International Society for Professional Innovation Management)\n    Society of Concurrent Product Development (SCPD) \n\nNPD strategies\n\n    Lean product development\n    Design for six sigma\n    Flexible product development\n    Quality function deployment\n    Phase–gate model\n    User-centered design\n\nManaging New Product Development\nCrystal Clear app kedit.svg\n\tThis section may need to be rewritten entirely to comply with Wikipedia's quality standards, as it consists of lists, where coherent text should be. Phrasing is sometimes confused / confusing at best. You can help. The discussion page may contain suggestions. (September 2014)\n\n[34] Companies must take a holistic approach to managing this process and must continue to innovate and develop new products if they want to grow and prosper.\n\n    CUSTOMER CENTERED New Product Development. Focuses on:\n        Finding new ways to solve customer problems.\n        Create more customer-satisfying experience\n\n        Companies often rely on technology, but the real success comes from understanding customer needs and values.\n        The most successful companies were the ones that:\n\n        Differentiated from others\n        Solved major customer problems\n        Offered a compelling customer value proposition\n        Engaged customer directly\n    TEAM BASED New Product Development\n        An approach:\n        To deserving new products in which various company's departments work closely together overlapping the steps in the product development process in order to:\n            Save time\n            Increase effectiveness\n        Company departments work closely together in cross functional teams overlapping the steps in the product development process (to save time and increase effectiveness).\n        Those departments are: legal, marketing, finances, design and manufacturing, suppliers and customer companies.\n        If there is a problem, all the company can work.\n    SYSTEMATIC New Product Development\n        Development process should be holistic (alternative) and systematic not to good ideas die.\n        This process is installed on Innovation Management System that collect, review, evaluate new product ideas and manage\n            the company appoints to a senior person to be the Innovation Manager who encourage all the company\n            employees, suppliers, distributors and dealers to become involved in finding and developing new products.\n        Then, there is a Cross-Functional Innovation Management Committee which:\n            Evaluate new products ideas\n            Help bringing good ideas\n        To sum up, New-Product success requires:\n        New ways to create valued customer experience, from generating and screening new product ideas to create and roll out want-satisfying products.\n    New Product Development IN TURBULENT TIMES\n        When we are in a tough economic situation usually management reduces spending on: new-product development. Usually it is done from a short-sighted.\n        Though times might call for even:\n            Greater new-product development, offering changing customer needs and tastes.\n            Innovation helps\n            Making the company more competitive\n            Positioning it better for future.\n    Virtual Product Development\n        Uses collaboration technology to remove need for co-located teams\n        Reduces G&A overhead costs of consulting firms\n        Advent of 24-hour development cycle\n\nRelated fields\n\n    Brand management\n    Engineering\n    Industrial design\n    Marketing\n    Product management\n\nSee also\n\n    Choice Modelling\n    Conceptual economy\n    Product\n    Product lifecycle\n    Pro-innovation bias\n    Requirements management\n    Social design\n    Time to market (TTM)\n    Market penetration\n    Virtual Product Development", "skillName": "New_product_development."}
{"id": 185, "category": "ProductDevelopment", "skillText": "The creation of products with new or different characteristics that offer new or additional benefits to the customer.\n\nProduct development may involve modification of an existing product or its presentation, or formulation of an entirely new product that satisfies a newly defined customer want or market niche.", "skillName": "productDevelopment."}
{"id": 186, "category": "ProductDevelopment", "skillText": "Virtual product development (VPD) is the practice of developing and prototyping products in a completely digital 2D/3D environment. VPD has four main components:\n\n    virtual product design (3D shape, 2D graphics/copy)\n    virtual product simulation (drop test, crush test, etc.)\n    virtual product staging (retail space planning, consumer research and behavior analysis)\n    digital manufacturing (process planning, assembly/filling virtualization, plant design).\n\nVPD typically takes place in a collaborative, web-based environment that brings together designers, customers/consumers, and value chain partners around a single source of real-time product “truth.” VPD enables practitioners to arrive at the right idea more quickly, and to accurately predict its performance in both manufacturing and retail settings, ultimately minimizing time to value, market failure potential, and product development costs.\n\nVirtual Process Planning is a relatively new concept for manufacturing companies, although the concept has been in use for the construction industry for several years. BIM (Building Information Modeling) is the system used by many construction, architectural and contracting firms. The detail and scheduling aspects are some of the more valuable aspects of the system. By utilizing Virtual Process Planning, the entire production process can be designed to both maximize efficiency and avoid the trial and error method employed by most manufacturers.\n\nVarious software exists with differing levels of information. The placement of work stations, inventory, personnel and equipment can be valuable for space planning. The interaction of the previously mentioned can also be investigated, allowing the user to identify potential issues from safety, quality and ergonomic standpoints.", "skillName": "VirtualProductDevelopment."}
{"id": 187, "category": "ProductDevelopment", "skillText": "Product life-cycle management (PLM) is the succession of strategies used by business management as a product goes through its life-cycle. The conditions in which a product is sold (advertising, saturation) changes over time and must be managed as it moves through its succession of stages.\n\nContents\n\n    1 Goals\n    2 Product life cycle\n        2.1 Characteristics of PLC stages\n        2.2 Identifying PLC stages\n    3 Limitations\n    4 See also\n    5 References\n    6 External links\n\nGoals\n\nThe goals of Product Life Cycle management (PLM) are to reduce time to market, improve product quality, reduce prototyping costs, identify potential sales opportunities and revenue contributions, and reduce environmental impacts at end-of-life. To create successful new products the company must understand its customers, markets and competitors. Product Lifecycle Management (PLM) integrates people, data, processes and business systems. It provides product information for companies and their extended supply chain enterprise. PLM solutions help organizations overcome the increased complexity and engineering challenges of developing new products for the global competitive markets.[citation needed]\nProduct life cycle\n\nThe concept of product life cycle (PLC) concerns the life of a product in the market with respect to business/commercial costs and sales measures. The product life cycle proceeds through multiple phases, involves many professional disciplines, and requires many skills, tools and processes. PLC management makes the following three assumptions:[citation needed]\n\n    Products have a limited life and thus every product has a life cycle.\n    Product sales pass through distinct stages, each posing different challenges, opportunities, and problems to the seller.\n    Products require different marketing, financing, manufacturing, purchasing, and human resource strategies in each life cycle stage.\n\nCharacteristics of PLC stages\n\nThe four main stages of a product's life cycle and the accompanying characteristics are:[citation needed]\nStage \tCharacteristics\n1. Market introduction stage \t\n\n    costs are very high\n    slow sales volumes to start\n    little or no competition\n    demand has to be created\n    customers have to be prompted to try the product\n    makes little money at this stage\n\n2. Growth stage \t\n\n    costs reduced due to economies of scale\n    sales volume increases significantly\n    profitability begins to rise\n    public awareness increases\n    competition begins to increase with a few new players in establishing market\n    increased competition leads to price decreases\n\n3. Maturity stage \t\n\n    costs are decreased as a result of production volumes increasing and experience curve effects\n    sales volume peaks and market saturation is reached\n    increase in competitors entering the market\n    prices tend to drop due to the proliferation of competing products\n    brand differentiation and feature diversification is emphasized to maintain or increase market share\n    industrial profits go down\n\n4. Saturation and decline stage \t\n\n    costs become counter-optimal\n    sales volume decline\n    prices, profitability diminish\n    profit becomes more a challenge of production/distribution efficiency than increased sales\n\nNote: Product termination is usually not the end of the business cycle, only the end of a single entrant within the larger scope of an ongoing business program.\nIdentifying PLC stages\n\nIdentifying the stage of a product is an art more than a science, but it's possible to find patterns in some of the general product features at each stage. Identifying product stages when the product is in transition is very difficult.[citation needed]\n\n    Identifying\n    features \tStages\n    Introduction \tGrowth \tMaturity \tDecline\n    Sales \tLow \tHigh \tHigh \tLow\n    Investment cost \tVery high \tHigh (lower than intro stage) \tLow \tLow\n    Competition \tLow or no competition \tHigh \tVery high \tVery High\n    Profit \tLow \tHigh \tHigh \tLow\n\nproduct life cycle at different stages\nLimitations\n\nIt is important for marketing managers to understand the limitations of the PLC model. It is difficult for marketing management to gauge accurately where a product is on its life cycle. A rise in sales per se is not necessarily evidence of growth, a fall in sales per se does not typify decline and some products, e.g. Coca-Cola and Pepsi, may not experience a decline.\n\nDiffering products possess different PLC \"shapes\". A fad product develops as a steep sloped growth stage, a short maturity stage, and a steep sloped decline stage. Products such as Coca-Cola and Pepsi experience growth, but also a constant level of sales over a number of decades. A given product (or products collectively within an industry) may hold a unique PLC shape such that use of typical PLC models are only useful as a rough guide for marketing management.\n\nFor specific products, the duration of each PLC stage is unpredictable and it's difficult to detect when maturity or decline has begun.\n\nBecause of these limitations, strict adherence to PLC can lead a company to misleading objectives and strategy prescriptions.\nSee also\n\n    Application lifecycle management\n    Diminishing manufacturing sources and material shortages (DMSMS)\n    Material selection\n    New product development\n    Obsolescence\n    Planned obsolescence\n    Product lifecycle management\n    Product management\n    Product teardown\n    Software product management\n    Technology life cycle\n    Toolkits for user innovation", "skillName": "Product_life-CycleManagement."}
{"id": 188, "category": "ProductDevelopment", "skillText": "Computer-aided manufacturing (CAM) is the use of software to control machine tools and related ones in the manufacturing of workpieces.[1][2][3][4][5] This is not the only definition for CAM, but it is the most common;[1] CAM may also refer to the use of a computer to assist in all operations of a manufacturing plant, including planning, management, transportation and storage.[6][7] Its primary purpose is to create a faster production process and components and tooling with more precise dimensions and material consistency, which in some cases, uses only the required amount of raw material (thus minimizing waste), while simultaneously reducing energy consumption.[citation needed] CAM is now a system used in schools and lower educational purposes. CAM is a subsequent computer-aided process after computer-aided design (CAD) and sometimes computer-aided engineering (CAE), as the model generated in CAD and verified in CAE can be input into CAM software, which then controls the machine tool. CAM is used in many schools alongside computer-aided design (CAD) to create objects.\n\nContents\n\n    1 Overview\n    2 History\n        2.1 Overcoming historical shortcomings\n    3 Machining process\n    4 Software: large vendors\n    5 See also\n    6 References\n    7 External links\n\nOverview\nSee also: Printed circuit board § PCB CAM\nChrome-cobalt disc with crowns for dental implants, manufactured using WorkNC CAM\n\nTraditionally, CAM has been considered as a numerical control (NC) programming tool, where in two-dimensional (2-D) or three-dimensional (3-D) models of components generated in CADAs with other “Computer-Aided” technologies, CAM does not eliminate the need for skilled professionals such as manufacturing engineers, NC programmers, or machinists. CAM, in fact, leverages both the value of the most skilled manufacturing professionals through advanced productivity tools, while building the skills of new professionals through visualization, simulation and optimization tools.\nHistory\n\nEarly commercial applications of CAM was in large companies in the automotive and aerospace industries, for example Pierre Béziers work developing the CAD/CAM application UNISURF in the 1960s for car body design and tooling at Renault.[8]\n\nHistorically, CAM software was seen to have several shortcomings that necessitated an overly high level of involvement by skilled CNC machinists. Fallows created the first CAD software but this had severe shortcomings and was promptly taken back into the developing stage.[citation needed] CAM software would output code for the least capable machine, as each machine tool control added on to the standard G-code set for increased flexibility. In some cases, such as improperly set up CAM software or specific tools, the CNC machine required manual editing before the program will run properly. None of these issues were so insurmountable that a thoughtful engineer or skilled machine operator could not overcome for prototyping or small production runs; G-Code is a simple language. In high production or high precision shops, a different set of problems were encountered where an experienced CNC machinist must both hand-code programs and run CAM software.\n\nIntegration of CAD with other components of CAD/CAM/CAE Product lifecycle management (PLM) environment requires an effective CAD data exchange. Usually it had been necessary to force the CAD operator to export the data in one of the common data formats, such as IGES or STL or Parasolid formats that are supported by a wide variety of software. The output from the CAM software is usually a simple text file of G-code/M-codes, sometimes many thousands of commands long, that is then transferred to a machine tool using a direct numerical control (DNC) program or in modern Controllers using a common USB Storage Device.\n\nCAM packages could not, and still cannot, reason as a machinist can. They could not optimize toolpaths to the extent required of mass production. Users would select the type of tool, machining process and paths to be used. While an engineer may have a working knowledge of G-code programming, small optimization and wear issues compound over time. Mass-produced items that require machining are often initially created through casting or some other non-machine method. This enables hand-written, short, and highly optimized G-code that could not be produced in a CAM package.\n\nAt least in the United States, there is a shortage of young, skilled machinists entering the workforce able to perform at the extremes of manufacturing; high precision and mass production.[9][citation needed] As CAM software and machines become more complicated, the skills required of a machinist or machine operator advance to approach that of a computer programmer and engineer rather than eliminating the CNC machinist from the workforce.\n\nTypical areas of concern:\n\n    High Speed Machining, including streamlining of tool paths\n    Multi-function Machining\n    5 Axis Machining\n    Feature recognition and machining\n    Automation of Machining processes\n    Ease of Use\n\nOvercoming historical shortcomings\n\nOver time, the historical shortcomings of CAM are being attenuated, both by providers of niche solutions and by providers of high-end solutions. This is occurring primarily in three arenas:\n\n    Ease of usage\n    Manufacturing complexity\n    Integration with PLM and the extended enterprise\n\nEase in use\n\n    For the user who is just getting started as a CAM user, out-of-the-box capabilities providing Process Wizards, templates, libraries, machine tool kits, automated feature based machining and job function specific tailorable user interfaces build user confidence and speed the learning curve.\n    User confidence is further built on 3D visualization through a closer integration with the 3D CAD environment, including error-avoiding simulations and optimizations.\n\nManufacturing complexity\n    The manufacturing environment is increasingly complex. The need for CAM and PLM tools by buMs are NC programmer or machinist is similar to the need for computer assistance by the pilot of modern aircraft systems. The modern machinery cannot be properly used without this assistance.\n    Today's CAM systems support the full range of machine tools including: turning, 5 axis machining and wire EDM. Today’s CAM user can easily generate streamlined tool paths, optimized tool axis tilt for higher feed rates, better tool life and surface finish and optimized Z axis depth cuts as well as driving non-cutting operations such as the specification of probing motions.\n\nIntegration with PLM and the extended enterpriseLM to integrate manufacturing with enterprise operations from concept through field support of the finished product.\n    To ensure ease of use appropriate to user objectives, modern CAM solutions are scalable from a stand-alone CAM system to a fully integrated multi-CAD 3D solution-set. These solutions are created to meet the full needs of manufacturing personnel including part planning, shop documentation, resource management and data management and exchange. To prevent these solutions from detailed tool specific information a dedicated tool management\n\nMachining process\n\nMost machining progresses through many stages,[10] each of which is implemented by a variety of basic and sophisticated strategies, depending on the material and the software available.\n\nRoughing\n    This process begins with raw stock, known as billet, and cuts it very roughly to shape of the final model. In milling, the result often gives the appearance of terraces, because the strategy has taken advantage of the ability to cut the model horizontally. Common strategies are zig-zag clearing, offset clearing, plunge roughing, rest-roughing.\n\nSemi-f\n\n    This process begins with a roughed part that unevenly approximates the model and cuts to within a fixed offset distance from the model. The semi-finishing pass must leave a small amount of material so the tool can cut accurately while finishing, but not so little that the tool and material deflect instead of sending. Common strategies are raster passes, waterline passes, constant step-over passes, pencil milling.\n\nFinishing\n    Finishing involves a slow pass across the material in very fine steps to produce the finished part. In finishing, the step between one pass and another is minimal. Feed rates are low and spindle speeds are raised to produce an accurate surface.\n\nContour milling\n    In milling applications on hardware with five or more axes, a separate finishing process called contouring can be performed. Instead of stepping down in fine-grained increments to approximate a surface, the work piece is rotated to make the cutting surfaces of the tool tangent to the ideal part features. This produces an excellent surface finish with high dimensional accuracy.\n\nSoftware: large vendors\nSee also: List of CAM companies and Category:Computer-aided manufacturing software\nFor 3D CAM software for personal 3D printers, see 3D_printing § Printing.\n\nThe top 20 largest CAM software companies, by direct revenues in year 2011, are sorted by revenues:\n\n    EnRoute Software \n    Mastercam \n    GibbsCAM\n    Dassault Systèmes\n    Siemens PLM Software\n    Geometric Technologies - CAMWorks \n    Delcam\n    Vero Software\n    PTC\n    Tebis\n    OPEN MIND Technologies\n    Cimatron\n    C&G Systems \n    Autodesk - HSM\n    MecSoft Corporation \n    Missler Software TopSolid\n    CNC Software\n    CG Tech\n    DP Technology\n    SolidCAM\n    SesCoi\n    NTT Data Engineering Systems\n    Nihon Unisys\n    BobCAD-CAM\n    SharpCam\n    Surfware\n    Dolphin CAD/CAM USA\n    Global flight\n    RoutCad&RoutBot\n    3Shape A/S\n    Esprit\n\nSee also\n\n    Computer-integrated manufacturing (CIM)\n    Digital modeling and fabrication\n    Direct numerical control (DNC)\n    Flexible manufacturing system (FMS)\n    Integrated Computer-Aided Manufacturing (ICAM)\n    Manufacturing process management (MPM)\n    STEP-NC\n    Rapid prototyping and rapid manufacturing – solid freeform fabrication direct from CAD models\n    CNC pocket milling", "skillName": "CAD."}
{"id": 189, "category": "ProductDevelopment", "skillText": "Market penetration refers to the successful selling of a product or service in a specific market, and it is measured by the amount of sales volume of an existing good or service compared to the total target market for that product or service.[1] Market penetration is the key performance metric for a business growth strategy stemming from the Ansoff Matrix (Richardson, M., & Evans, C. (2007). H. Igor Ansoff first devised and published The Ansoff Matrix in the Harvard Business Review in 1957, within an article titled \"Strategies for Diversification.\" The grid/ matrix is utilized across businesses to help evaluate and determine the next stages the company must take in order to grow, and the risks associated with the chosen stratety. With numerous options available, this matrix helps narrow down the best fit for your organization.\n\nThis strategy involves selling current products or services into the existing market in order to obtain a higher market share. This could involve persuading current customers to buy more and new customers to start buying or even converting customers from their competitors. This could be implemented using methods such as competitive pricing, increase in marketing communications or utilizing reward systems such as loyalty points/discounts. New Strategies involve utilizing pathways and finding new ways to improve profits, increase sales and productivity, in order to stay relevant and competitive in the long run.[citation needed]\n\nContents\n\n    1 Definition\n    2 Purpose\n    3 Strategies\n        3.1 Price Adjustments\n        3.2 Increased Promotion\n        3.3 More Distribution Channels\n        3.4 Product Improvements\n        3.5 Market Development\n        3.6 Penetration Pricing\n    4 Construction\n    5 Methodologies\n    6 References\n    7 Further reading\n\nDefinition\n\nMarket penetration although it can be preformed throughout the business’s life, it can be especially helpful in the primary stages of set up. It helps establish the businesses current station and which direction it needs to expand in to achieve market growth. Successful outcomes stem from careful monitoring by key staff and leaders. Timing is key to a successful market growth; this can be dependent on the overall market welfare, the business’s competitors and current events. Questions, brainstorming and discussions can help distinguish whether it is the best time for market growth. These can include questions surrounding market share increases or decreases. Sales can be declining but shows opportunity for the business, it could be the perfect time to make alterations so as to grow market share. Market penetration can also be helpful when sales are proving to slow down, customers often need to be re-introduced to a company or reminded why they need your company’s goods/services. With the consumers attention span becoming less and less, organizations need to constantly keep on top of competitors to stay relevant.\n\nSome factors of market penetration are holding costs, advanced inventory management practices and technology (e.g. ongoing replenishment and vendor managed inventory), supply chain problems and economies of scale (e.g., Chang and Lee 1995, Chen et al. 2005, Gaur and Kesavan 2005, Gaur et al. 2005, Hendricks and Singhal 2005, Huson and Nanda 1995, Lieberman et al. 1996).\n\nMarket penetration, market development, and product development together establish market growth for a company. Overall the major growth opportunities they implement, attempts to peak sales through stressing current products in present markets and present products in new markets. This includes developing new products for existing markets, subsequently. It is about finding new ways to boost sales and keep customers loyal and increase market share. When implementing change companies must be careful not to compromise their existing revenue or customers. If you drastically alter packaging or visual aspects of a company, existing customers may not recognise your brand and opt for a competitor’s product or service. Too much alteration can make consumers wary so change must be implemented in a subtle manner so as to only increase market share and build on your profits. Managers and leaders should monitor this throughout the entire process to ensure smooth changes. Clear and precise planning will also help minimise this risk and will lead to a successful improvement and boost in market share.\n\nA few different options for market penetration are as followed\n\n· Developing a new marketing strategy to entice more customers to purchase or continue purchasing.\n\n· Become price competitive as a swaying factor for customers to choose your product or service over another company.\n\n· Use special promotions or offers to grab attention.\n\n· Utilise the Boston Matrix to decipher which product or service benefits further investment and time and which can be disregarded.\n\n· Purchase a competitors company (in mature markets) to expand market share.\n\nFor a business to come up with a decision using the grid, key personal must consider numerous factors such as market penetration, product development, market development and diversification Richardson, M., & Evans, C. (2007\n\nIt measures the brand popularity. It is defined as the number of people who buy a specific brand or a category of goods at least once in a given period, divided by the size of the relevant market population.[2] Market penetration is one of the four growth strategies of the Product-Market Growth Matrix as defined by Ansoff. Market penetration occurs when a company penetrates a market in which current or similar products already exist. A way[citation needed] to achieve this is by gaining competitors' customers (part of their market share). Other ways include attracting non-users of your product or convincing current clients to use more of your product/service (by advertising, etc.).[3] Ansoff developed the Product-Market Growth Matrix to help firms recognize if there was any advantage to entering a market. The other three growth strategies in the Product-Market Growth Matrix are:\n\n    Product development (existing markets, new products): McDonalds is always within the fast-food industry, but frequently markets new burgers.\n    Market development (new markets, existing products): Apple introduced the iPhone, in a developed cell phone market.\n    Diversification (new markets, new products):\n\nMarket penetration refers to the successful selling of a product or service in a specific market, and it is a measure of the amount of sales volume of an existing good or service compared to the total target market for that product or service.[1] Market penetration involves targeting on selling existing goods or services in the targeted markets to increase a better market share/value (Arkolakis, 2010).[4] It can be achieved in four different way including growing the market share of current goods or services; obtaining dominance of existing markets; reforming a mature market by monopolising the market and driving out competitors; or increasing consumptions by existing customers (Free Management Ebooks, 2013).[5]\nFormula of Caluclating Market Penetration.png\n\n(Farris, Bendle, Pfeifer & Reibstein, 2006)[6]\n\nAnother alternative to calculating market penetration is if the dividend growth rate is more than the ratio of the percentage population of wealth distribution ratio then market penetration is possible.\n\nMarket penetration is a very good way to determine successfulness of the business model and marketing strategy for your product. To check the successfulness, you must have a way to gauge the amount of the market you will probably reach and how much potential localized or otherwise customers there are that would be susceptible to your product. To this end Charles Hill came up with a five step system to understanding you advertising's influence on the market (Nordmeyer, 2016).[7] 1. Identify the demographic most suited to your product. Even though other demographics may use your product it is about identifying the largest demographic so that the majority of your advertising is tailored to them. E.g. for candy for children, salad for adult woman who may be dieting. 2. Decide upon the area in which they live. Location is important and wholly depends on the reach of your brand. If your company operates at a national level, then the entirety of the country will have to be averaged to reach the largest amount of people. The smaller the area the more specific you can be about the people of each demographic within it. 3. Knowing the size of your market. Once you have identified who your demographic is and where they are knowing specifics about how many there are is integral to understanding your market penetration. 4. Understanding competitors market penetration. What benchmark should you go for? When you have the penetration that other products have reached calculate the number that you should reach in your demographic by multiplying the total number of your demographic by whatever the percentage that other products are reaching (Jensen, 2001). 5. Calculate the amount of customers that your business needs to sell to too earn a profit and then compare that to the amount other competitors are reaching, if your business does not make a profit with average market penetration it’s time to rethink your business strategy. Market penetration is a very important tool for understanding potential earnings of a business and is integral to calculating a resourceful business model. There’s always more you can do for a larger market penetration so a good advertising strategy is very important. Market Penetration in an emerging market A model was theorized for market penetration by Yan Dong, Martin Dresner and Chaodong Han.\n\n\nThis is meant for emerging markets but the connection extends across to more established markets as well (Han, Dong, & Dresner, 2013).[8] Essentially the model illustrates that market penetration and understanding of the amount of people you will reach with your product is indicative to how much stock you will order and both that and market penetration are of the upmost importance for financial performance. However, emerging markets are difficult to predict as they are categorized by large amounts of growth. This means demand is very hard to forecast and therefore inventory supply as well. The connection between emerging market penetration and inventory supply are bridged by several factors such as advanced inventory management practices, technologies and holding costs. So while the market penetration may dictate how much supply is needed other factors must dictate how much inventory it is prudent to keep. Understanding market penetration for an emerging market is more difficult due to the lack of established competitors and similar products which you could base your research off. Emerging markets are very susceptible to large companies and are sought after by globalized businesses due to the increase in disposable income the average person will have and weak local competitors. The weakness of local competitors is due to their poor customer service and limit in resources as they don’t have the capital and reach that large corporations have. The four big emerging markets are Brazil, Russia, India and China as they were the fastest to recover after the 2008/2009 economic crisis (Johansson, 2011).[9] These markets are untapped potential for revenue and a good spot for globalization for dominant company’s. Large market penetration is the key to fully tapping these markets.\nPurpose\n\nAs a strategy, market penetration is used when the business seeks to increase sales growth of its existing products or services to its existing markets in order to gain a higher market share.[5][10] This strategy is often used during the early stages of the business or before it enters the market, in order to prove the market existence and show market size for its products or services, also to gain an understanding to the number of competitors and how well they are doing. Hence, the business can decide on either it is a good to enter their target market or not, and how it can make its products or services more attractive to consumers than its competitors. During the operation of the business, if the sales are decreasing or flatlining comparing to previous years, then it is also appropriate to apply market penetration strategy to seek for opportunities to increase sales. Therefore, it is unnecessary to this strategy if the sales are increasing. However, it is exceptional if the sales growth trend shows the gross increase but is much less significant comparing to its competitors, because this could indicate the business's market share is actually shrinking, then this strategy can be a good approach to try regain its market share.[11]\n\nTo achieve the goal of higher market share, the primary idea is that the business has to either increase sales volume to their existing customers by encouraging for more frequent or greater usages, or expanding the population size of customers in the current market by attracting potential new customers to buy its goods or services. Since the market penetration strategy is conducted based on established capabilities and characteristics of the business and the market, therefore it contains the lowest risk out of the four strategies in Ansoff's product-market growth matrix.[12] Hence, a business should give special consideration to conducts it, since this strategy is very important for the evaluation work on the intended market and the existing businesses within this market. Especially when the business or product or service is about to enter the market or during its initial stage, and when it is not comfortable with risk-taking, or the owners of the business do not intend or not in a position to invest heavily into it.[5] The amount of risk involved with each of the four types of Ansoff's strategies increases from market penetration to market development, to production development, to diversification. Because the both market and product development involve with one aspect of new developments, changes, and innovation. The diversification strategy is with most risk because the business is growing into both a new market and product, and thus contains with most uncertainties.\n\nMarket penetration is not only a strategy but also a measurement (in percentage) for popularity of a brand or a product in the category, in other words, the amount of customers in the market that buys from a brand or product.[6]\nStrategies\nPrice Adjustments\n\nOne of the common market penetration strategies is to lower the products’ prices. Businesses aim to generate more sales volume by increasing the number of products purchased by putting on lower prices (price competition) for consumers comparing to the alternative goods. Companies may alternatively pursue strategies of higher prices depending on the demand elasticity of the product, in hopes that it will generate an increased sales volume and result higher market penetration (Joseph, n.d.).[13]\nIncreased Promotion\n\nBusinesses can also increase their market penetration by offering promotions to customers. A promotion is a strategy often linked with pricing, used to raise awareness of the brand and generate profit to maximise their market share (McGrath, 2001).[14]\nMore Distribution Channels\n\nA distribution channel is the connection between businesses and intermediaries before a good or service is purchased by the consumers. Distribution can also contribute to sales volumes for businesses. It can increase consumer awareness, change the strategies of competitors and alter the consumer's perception of the product and the brand, and another method to increase market penetration (Joseph, n.d.).[13]\nProduct Improvements\n\nProduct management is crucial to a high market penetration in the targeted market and by improving the quality of products, businesses are able to attract and out-quality the competitors’ products to match customers’ requirements and eventually lead to more sales made. Product improvements can be utilised to create new interests in a declining product, for example by changing the design on the packaging or material/ingredients.\nMarket Development\n\nMarket development aims at non-buying shoppers in targeted markets and new customers in order to maximise the potential market. Before developing a new market, companies should consider all the risks associated with the decision including the profitability of it (QuickMBA, n.d.).[15] If a company is confident about their products and believes its strengths and is enticing to the new consumers, then market development is a suitable strategy for the business.\nPenetration Pricing\n\nPenetration pricing is a marketing technique which used to gain market share by selling a new product for a price that is significantly lower than its competitors. The company begins to raise the price of the product once it has achieved a large customer base and market share. Penetration pricing is frequently used by network provider and cable or satellite services companies. Many of the providers will initially offer an unbeatable price to attract customers into switching to their service and after the discount period has ended, the price increases dramatically and some customers will be forced to stay with the provider because of contract issues (Applebaum, 1966).[16]\n\nPenetration pricing benefits from the influence of word-of-mouth advertising, allowing customers to spread the words of how affordable the products are prior to business increasing the prices. It will also discourage and disadvantage competitors who are not willing to undersell and losing sales to others. However, businesses have to ensure they have enough capital to stay in surplus before the price is raised up again.\nConstruction\n\nMarket penetration can be defined as the proportion of people in the target who bought (at least once in the period) a specific brand or a category of goods. Two key measures of a product’s 'popularity' are penetration rate and penetration share. The penetration rate (also called penetration, brand penetration or market penetration as appropriate) is the percentage of the relevant population that has purchased a given brand or category at least once in the time period under study. A brand’s penetration share, in contrast to penetration rate, is determined by comparing that brand’s customer population to the number of customers for its category in the relevant market as a whole. Here again, to be considered a customer, one must have purchased the brand or category at least once during the period.[2]\nMethodologies\nThe penetration that brands and products have can be recorded by companies such as Kantar Worldpanel \nand ACNielsen who offer panel measurement services to calculate this and other consumer measures. In these cases penetration is given as a percentage of a country's households who have bought that particular brand or product at least once within a defined period of time.", "skillName": "MarketPenetration."}
{"id": 190, "category": "ProductDevelopment", "skillText": "In diffusion of innovation theory, a pro-innovation bias is the belief that an innovation should be adopted by whole society without the need of its alteration.[1][2] The innovation's \"champion\" has such strong bias in favor of the innovation, that he/she may not see its limitations or weaknesses and continues to promote it nonetheless.[3]\n\nContents\n\n    1 Example\n    2 See also\n    3 References\n    4 Further reading\n\nExample\nMain article: Atomic Age\n\nA feeling of nuclear optimism emerged in the 1950s in which it was believed that all power generators in the future would be atomic in nature. The atomic bomb would render all conventional explosives obsolete and nuclear power plants would do the same for power sources such as coal and oil. There was a general feeling that everything would use a nuclear power source of some sort, in a positive and productive way, from irradiating food to preserve it, to the development of nuclear medicine. There would be an age of peace and plenty in which atomic energy would \"provide the power needed to desalinate water for the thirsty, irrigate the deserts for the hungry, and fuel interstellar travel deep into outer space\".[4] This use would render the Atomic Age as significant a step in technological progress as the first smelting of Bronze, of Iron, or the commencement of the Industrial Revolution.\n\nRoger Smith, then chairman of General Motors, said in 1986: \"By the turn of the century, we will live in a paperless society.\"[5] In the late 20th century, there were many predictions of this kind.[6] This transformation has so far not taken place.\nSee also\n\n    Status quo bias\n    Wishful thinking", "skillName": "Pro-innovation_bias."}
{"id": 191, "category": "ProductDevelopment", "skillText": "In marketing, a product is anything that can be offered to a market that might satisfy a want or need.[1] In retailing, products are called merchandise. In manufacturing, products are bought as raw materials and sold as finished goods. A service is another common product type.\n\nCommodities are usually raw materials such as metals and agricultural products, but a commodity can also be anything widely available in the open market. In project management, products are the formal definition of the project deliverables that make up or contribute to delivering the objectives of the project. In insurance, the policies are considered products offered for sale by the insurance company that created the contract. In economics and commerce, products belong to a broader category of goods. The economic meaning of product was first used by political economist Adam Smith.[citation needed]\n\nA related concept is that of a sub-product, a secondary but useful result of a production process.\n\nDangerous products, particularly physical ones, that cause injuries to consumers or bystanders may be subject to product liability.\n\nContents\n\n    1 Product classification\n        1.1 By use\n        1.2 By association\n        1.3 National and international product classifications\n    2 Product model\n    3 Literature\n    4 See also\n    5 References\n    6 External links\n\nProduct classification\n\nA product can be classified as tangible or intangible. A tangible product is a physical object that can be perceived by touch such as a building, vehicle, gadget, or clothing. An intangible product is a product that can only be perceived indirectly such as an insurance policy. Services can be broadly classified under intangible products which can be durable or non durable.\nBy use\n\nIn its online product catalog, retailer Sears, Roebuck and Company divides its products into \"departments\", then presents products to potential shoppers according to (1) function or (2) brand.[2] Each product has a Sears item-number and a manufacturer's model-number. Sears uses the departments and product groupings with the intention of helping customers browse products by function or brand within a traditional department-store structure.[3]\nBy association\n\nA product line is \"a group of products that are closely related, either because they function in a similar manner, are sold to the same customer groups, are marketed through the same types of outlets, or fall within given price ranges.\"[4] Many businesses offer a range of product lines which may be unique to a single organization or may be common across the business's industry. In 2002 the US Census compiled revenue figures for the finance and insurance industry by various product lines such as \"accident, health and medical insurance premiums\" and \"income from secured consumer loans\".[5] Within the insurance industry, product lines are indicated by the type of risk coverage, such as auto insurance, commercial insurance and life insurance.[6]\nNational and international product classifications\n\nVarious classification systems for products have been developed for economic statistical purposes. The NAFTA signatories are working on a system that classifies products called NAPCS as a companion to North American Industry Classification System (NAICS).[7] The European Union uses a \"Classification of Products by Activity\" among other product classifications.[8] The United Nations also classifies products for international economic activity reporting.[9]\n\nThe Aspinwall Classification System [10][11] classifies and rates products based on five variables:\n\n    Replacement rate (How frequently is the product repurchased?)\n    Gross margin (How much profit is obtained from each product?)\n    Buyer goal adjustment (How flexible are the buyers' purchasing habits with regard to this product?)\n    Duration of product satisfaction (How long will the product produce benefits for the user?)\n    Duration of buyer search behavior (How long will consumers shop for the product?)\n\nThe National Institute of Governmental Purchasing (NIGP)[12] developed a commodity and services classification system for use by state and local governments, the NIGP Code.[13] The NIGP Code is used by 33 states within the United States as well as thousands of cities, counties and political subdivisions. The NIGP Code is a hierarchical schema consisting of a 3 digit class, 5 digit class-item, 7 digit class-item-group and an 11 digit class-item-group-detail.[14] Applications of the NIGP Code include vendor registration, inventory item identification, contract item management, spend analysis and strategic sourcing.\n\nProduct model\n\nA manufacturer usually provides an identifier for each particular type of product they make, known as a model, model variant, or model number. For example, Dyson Ltd, a manufacturer of appliances (mainly vacuum cleaners), requires customers to identify their model in the support section of the website.[15] Brand and model can be used together to identify products in the market. The model number is not necessarily the same as the manufacturer part number (MPN).[16]\n\nBecause of the huge amount of similar products in the automotive industry, there is a special kind of defining a car with options (marks, attributes), that represent the characteristics features of the vehicle. A model of a car is defined by some basic options like body, engine, gear box and axles. The variants of a model are built by some additional options like color, seats, wheels, mirrors, trims, entertainment and assistant systems etc. Options, that exclude each other (pairwise) build an option-family. That means, that you can choose only one option by each family and you have to choose exactly one option. This kind of product definition fulfill the requirements of an ideal Boolean Algebra and can be helpful to construct a product configurator.[17] Sometimes, a set of options (car features) are combined to an automotive package and are offered by a lower price. A consistent car definition is essential for the production planning and control in the automotive industry, to generate a master production schedule,[18] which is the fundamental for the enterprise resource planning.\n\nIn addition, a specific unit of a product is usually (and has to be) identified by a serial number, which is necessary to distinguish products with the same product definition. In the case of automotive products it's called the Vehicle Identification Number VIN, an international standardized format.\nLiterature\n\n    Herlyn, W.: PPS im Automobilbau - Produktionsprogrammplanung und -steuerung von Fahrzeugen und Aggregaten. Hanser Verlag, München, 2012 - ISBN 978-3-446-41370-2\n    Stark, John (2015). Product Lifecycle Management: Volume 1: 21st Century Paradigm for Product Realisation. Springer. ISBN 978-3-319-17439-6.\n\nSee also\n\n    Builder's plate\n    List of fastest-selling products\n    Manufacturer part number\n    Product teardown", "skillName": "Product."}
{"id": 192, "category": "ProductDevelopment", "skillText": "Requirements management is the process of documenting, analyzing, tracing, prioritizing and agreeing on requirements and then controlling change and communicating to relevant stakeholders. It is a continuous process throughout a project. A requirement is a capability to which a project outcome (product or service) should conform.\n\nContents\n\n    1 Overview\n    2 Traceability\n    3 Requirements activities\n        3.1 Investigation\n        3.2 Feasibility\n        3.3 Design\n        3.4 Construction and test\n        3.5 Requirements Change Management\n        3.6 Release\n    4 See also\n    5 References\n    6 Further reading\n    7 External links\n\nOverview\n\nThe purpose of requirements management is to ensure that an organization documents, verifies, and meets the needs and expectations of its customers and internal or external stakeholders.[1] Requirements management begins with the analysis and elicitation of the objectives and constraints of the organization. Requirements management further includes supporting planning for requirements, integrating requirements and the organization for working with them (attributes for requirements), as well as relationships with other information delivering against requirements, and changes for these.\n\nThe traceability thus established is used in managing requirements to report back fulfilment of company and stakeholder interests in terms of compliance, completeness, coverage, and consistency. Traceabilities also support change management as part of requirements management in understanding the impacts of changes through requirements or other related elements (e.g., functional impacts through relations to functional architecture), and facilitating introducing these changes.[2]\n\nRequirements management involves communication between the project team members and stakeholders, and adjustment to requirements changes throughout the course of the project.[3] To prevent one class of requirements from overriding another, constant communication among members of the development team is critical. For example, in software development for internal applications, the business has such strong needs that it may ignore user requirements, or believe that in creating use cases, the user requirements are being taken care of.\nTraceability\nMain article: Requirements traceability\n\nRequirements traceability is concerned with documenting the life of a requirement. It should be possible to trace back to the origin of each requirement and every change made to the requirement should therefore be documented in order to achieve traceability. Even the use of the requirement after the implemented features have been deployed and used should be traceable.[4]\n\nRequirements come from different sources, like the business person ordering the product, the marketing manager and the actual user. These people all have different requirements for the product. Using requirements traceability, an implemented feature can be traced back to the person or group that wanted it during the requirements elicitation. This can, for example, be used during the development process to prioritize the requirement, determining how valuable the requirement is to a specific user. It can also be used after the deployment when user studies show that a feature is not used, to see why it was required in the first place.\nRequirements activities\n\tThis article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2010) (Learn how and when to remove this template message)\n\nAt each stage in a development process, there are key requirements management activities and methods. To illustrate, consider a standard five-phase development process with Investigation, Feasibility, Design, Construction and Test, and Release stages.\nInvestigation\n\nIn Investigation, the first three classes of requirements are gathered from the users, from the business and from the development team. In each area, similar questions are asked; what are the goals, what are the constraints, what are the current tools or processes in place, and so on. Only when these requirements are well understood can functional requirements be developed.\n\nIn the common case, requirements cannot be fully defined at the beginning of the project. Some requirements will change, either because they simply weren’t extracted, or because internal or external forces at work affect the project in mid-cycle.\n\nThe deliverable from the Investigation stage is a requirements document that has been approved by all members of the team. Later, in the thick of development, this document will be critical in preventing scope creep or unnecessary changes. As the system develops, each new feature opens a world of new possibilities, so the requirements specification anchors the team to the original vision and permits a controlled discussion of scope change.[citation needed]\n\nWhile many organizations still use only documents to manage requirements, others manage their requirements baselines using software tools. These tools allow requirements to be managed in a database, and usually have functions to automate traceability (e.g., by allowing electronic links to be created between parent and child requirements, or between test cases and requirements), electronic baseline creation, version control, and change management. Usually such tools contain an export function that allows a specification document to be created by exporting the requirements data into a standard document application.[citation needed]\nFeasibility\n\nIn the Feasibility stage, costs of the requirements are determined. For user requirements, the current cost of work is compared to the future projected costs once the new system is in place. Questions such as these are asked: “What are data entry errors costing us now?” Or “What is the cost of scrap due to operator error with the current interface?” Actually, the need for the new tool is often recognized as these questions come to the attention of financial people in the organization.\n\nBusiness costs would include, “What department has the budget for this?” “What is the expected rate of return on the new product in the marketplace?” “What’s the internal rate of return in reducing costs of training and support if we make a new, easier-to-use system?”\n\nTechnical costs are related to software development costs and hardware costs. “Do we have the right people to create the tool?” “Do we need new equipment to support expanded software roles?” This last question is an important type. The team must inquire into whether the newest automated tools will add sufficient processing power to shift some of the burden from the user to the system in order to save people time.\n\nThe question also points out a fundamental point about requirements management. A human and a tool form a system, and this realization is especially important if the tool is a computer or a new application on a computer. The human mind excels in parallel processing and interpretation of trends with insufficient data. The CPU excels in serial processing and accurate mathematical computation. The overarching goal of the requirements management effort for a software project would thus be to make sure the work being automated gets assigned to the proper processor. For instance, “Don’t make the human remember where she is in the interface. Make the interface report the human’s location in the system at all times.” Or “Don’t make the human enter the same data in two screens. Make the system store the data and fill in the second screen as needed.”\n\nThe deliverable from the Feasibility stage is the budget and schedule for the project.\nDesign\n\nAssuming that costs are accurately determined and benefits to be gained are sufficiently large, the project can proceed to the Design stage. In Design, the main requirements management activity is comparing the results of the design against the requirements document to make sure that work is staying in scope.\n\nAgain, flexibility is paramount to success. Here’s a classic story of scope change in mid-stream that actually worked well. Ford auto designers in the early ‘80s were expecting gasoline prices to hit $3.18 per gallon by the end of the decade. Midway through the design of the Ford Taurus, prices had centered to around $1.50 a gallon. The design team decided they could build a larger, more comfortable, and more powerful car if the gas prices stayed low, so they redesigned the car. The Taurus launch set nationwide sales records when the new car came out, primarily because it was so roomy and comfortable to drive.\n\nIn most cases, however, departing from the original requirements to that degree does not work. So the requirements document becomes a critical tool that helps the team make decisions about design changes.[5]\nConstruction and test\n\nIn the construction and testing stage, the main activity of requirements management is to make sure that work and cost stay within schedule and budget, and that the emerging tool does in fact meet requirements. A main tool used in this stage is prototype construction and iterative testing. For a software application, the user interface can be created on paper and tested with potential users while the framework of the software is being built. Results of these tests are recorded in a user interface design guide and handed off to the design team when they are ready to develop the interface. This saves their time and makes their jobs much easier.\nRequirements Change Management\n\nHardly would any software development project be completed without some changes being asked of the project. The changes can stem from changes in the environment in which the finished product is envisaged to be used, business changes, regulation changes, errors in the original definition of requirements,limitations in technology, changes in the security environment and so on. The activities of Requirements Change Management include receiving the change requests from the stakeholders, recording the received change requests, analyzing and determining the desirability and process of implementation, implementation of the change request, quality assurance for the implementation and closing the change request. Then the data of change requests be compiled, analyzed and appropriate metrics are derived and dovetailed into the organizational knowledge repository.[6]\nRelease\n\nRequirements management does not end with product release. From that point on, the data coming in about the application’s acceptability is gathered and fed into the Investigation phase of the next generation or release. Thus the process begins again.\nSee also\n\n    Requirement\n    Requirements engineering\n    Requirements analysis\n    Requirements traceability\n    Requirements Engineering Specialist Group\n    Process area (CMMI):\n        Requirements Development (RD)\n        Requirements Management (REQM)\n    Product requirements document", "skillName": "RequirementsManagement."}
{"id": 193, "category": "Computer_Programming", "skillText": "C-- (pronounced \"see minus minus\") is a C-like programming language. Its creators, functional programming researchers Simon Peyton Jones and Norman Ramsey, designed it to be generated mainly by compilers for very high-level languages rather than written by human programmers. Unlike many other intermediate languages, its representation is plain ASCII text, not bytecode or another binary format.\n\nContents\n\n    1 Design\n    2 Type system\n    3 See also\n    4 References\n    5 External links\n\nDesign\n\nC-- is a \"portable assembly language\", designed to ease the task of implementing a compiler which produces high quality machine code. This is done by having the compiler generate C-- code, delegating the harder work of low-level code generation and optimisation to a C-- compiler.\n\nWork on C-- began in the late 1990s. Since writing a custom code generator is a challenge in itself, and the compiler back ends available to researchers at that time were complex and poorly documented, several projects had written compilers which generated C code (for instance, the original Modula-3 compiler). However, C is a poor choice for functional languages: it does not support tail recursion, accurate garbage collection or efficient exception handling. C-- is a simpler, tightly-defined alternative to C which does support all of these things. Its most innovative feature is a run-time interface which allows writing of portable garbage collectors, exception handling systems and other run-time features which work with any C-- compiler.\n\nThe language's syntax borrows heavily from C. It omits or changes standard C features such as variadic functions, pointer syntax, and aspects of C's type system, because they hamper certain essential features of C-- and the ease with which code-generation tools can produce it.\n\nThe name of the language is an in-joke, indicating that C-- is a reduced form of C, in the same way that C++ is basically an expanded form of C. (In C-like languages, \"--\" and \"++\" are operators meaning \"decrement\" and \"increment\".)\n\nC-- is a target platform for the Glasgow Haskell Compiler.[1] Some of C--'s developers, including Simon Peyton Jones, João Dias, and Norman Ramsey, work or have worked on the Glasgow Haskell Compiler. The GHC codebase and development are based at Microsoft Research in Cambridge, though it is not a Microsoft project.\nType system\nThe C-- type system is deliberately designed to reflect constraints imposed by hardware rather than conventions imposed by higher-level languages. In C-- a value stored in a register or memory may have only one type: bit vector. However, bit vector is a polymorphic type and may come in several widths, e.g., bits8, bits32, or bits64. In addition to the bit-vector type C-- also provides a Boolean type bool, which can be computed by expressions and used for control flow but cannot be stored in a register or in memory. As in an assembly language, any higher type discipline, such as distinctions between signed, unsigned, float, and pointer, is imposed by the C-- operators or other syntactic constructs in the language.", "skillName": "C--."}
{"id": 194, "category": "Computer_Programming", "skillText": "C (/ˈsiː/, as in the letter c) is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.\n\nC was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs,[5] and used to re-implement the Unix operating system.[6] It has since become one of the most widely used programming languages of all time,[7][8] with C compilers from various vendors available for the majority of existing computer architectures and operating systems. C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).\n\nContents\n\n    1 Design\n    2 Overview\n        2.1 Relations to other languages\n    3 History\n        3.1 Early developments\n        3.2 K&R C\n        3.3 ANSI C and ISO C\n        3.4 C99\n        3.5 C11\n        3.6 Embedded C\n    4 Syntax\n        4.1 Character set\n        4.2 Reserved words\n        4.3 Operators\n    5 \"Hello, world\" example\n    6 Data types\n        6.1 Pointers\n        6.2 Arrays\n        6.3 Array–pointer interchangeability\n    7 Memory management\n    8 Libraries\n    9 Language tools\n    10 Uses\n    11 Related languages\n    12 See also\n    13 Notes\n    14 References\n    15 Further reading\n    16 External links\n\nDesign\n\nC is an imperative (procedural) language. It was designed to be compiled using a relatively straightforward compiler, to provide low-level access to memory, to provide language constructs that map efficiently to machine instructions, and to require minimal run-time support. Therefore, C was useful for many applications that had formerly been coded in assembly language, for example in system programming.\n\nDespite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant and portably written C program can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code. The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.\nOverview\n\nLike most imperative languages in the ALGOL tradition, C has facilities for structured programming and allows lexical variable scope and recursion, while a static type system prevents many unintended operations. In C, all executable code is contained within subroutines, which are called \"functions\" (although not in the strict sense of functional programming). Function parameters are always passed by value. Pass-by-reference is simulated in C by explicitly passing pointer values. C program source text is free-format, using the semicolon as a statement terminator and curly braces for grouping blocks of statements.\n\nThe C language also exhibits the following characteristics:\n\n    There is a small, fixed number of keywords, including a full set of flow of control primitives: for, if/else, while, switch, and do/while. There is one namespace, and user-defined names are not distinguished from keywords by any kind of sigil.\n    There are a large number of arithmetical and logical operators, such as +, +=, ++, &, ~, etc.\n    More than one assignment may be performed in a single statement.\n    Function return values can be ignored when not needed.\n    Typing is static, but weakly enforced: all data has a type, but implicit conversions can be performed; for instance, characters can be used as integers.\n    Declaration syntax mimics usage context. C has no \"define\" keyword; instead, a statement beginning with the name of a type is taken as a declaration. There is no \"function\" keyword; instead, a function is indicated by the parentheses of an argument list.\n    User-defined (typedef) and compound types are possible.\n        Heterogeneous aggregate data types (struct) allow related data elements to be accessed and assigned as a unit.\n        Array indexing is a secondary notation, defined in terms of pointer arithmetic. Unlike structs, arrays are not first-class objects; they cannot be assigned or compared using single built-in operators. There is no \"array\" keyword, in use or definition; instead, square brackets indicate arrays syntactically, for example month[11].\n        Enumerated types are possible with the enum keyword. They are not tagged, and are freely interconvertible with integers.\n        Strings are not a separate data type, but are conventionally implemented as null-terminated arrays of characters.\n    Low-level access to computer memory is possible by converting machine addresses to typed pointers.\n    Procedures (subroutines not returning values) are a special case of function, with an untyped return type void.\n    Functions may not be defined within the lexical scope of other functions.\n    Function and data pointers permit ad hoc run-time polymorphism.\n    A preprocessor performs macro definition, source code file inclusion, and conditional compilation.\n    There is a basic form of modularity: files can be compiled separately and linked together, with control over which functions and data objects are visible to other files via static and extern attributes.\n    Complex functionality such as I/O, string manipulation, and mathematical functions are consistently delegated to library routines.\n\nC does not include some features found in newer, more modern high-level languages, including object orientation and garbage collection.\nRelations to other languages\n\nMany later languages have borrowed directly or indirectly from C, including C++, D, Go, Rust, Java, JavaScript, Limbo, LPC, C#, Objective-C, Perl, PHP, Python, Verilog (hardware description language),[4] and Unix's C shell. These languages have drawn many of their control structures and other basic features from C. Most of them (with Python being the most dramatic exception) are also very syntactically similar to C in general, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.\nHistory\nEarly developments\nKen Thompson (left) with Dennis Ritchie (right, the inventor of the C programming language)\n\nThe origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Ritchie and Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was developed in assembly language. The developers were considering rewriting the system using the B language, Thompson's simplified version of BCPL.[9] However B's inability to take advantage of some of the PDP-11's features, notably byte addressability, led to C.\n\nThe development of C started in 1972 on the PDP-11 Unix system[10] and first appeared in Version 2 Unix.[11] The language was not initially designed with portability in mind, but soon ran on different platforms as well: a compiler for the Honeywell 6000 was written within the first year of C's history, while an IBM System/370 port followed soon.[1][10] The name of C simply continued the alphabetic order started by B.[12]\n\nAlso in 1972, a large part of Unix was rewritten in C.[13] By 1973, with the addition of struct types, the C language had become powerful enough that most of the Unix's kernel was now in C.\n\nUnix was one of the first operating system kernels implemented in a language other than assembly. (Earlier instances include the Multics system (written in PL/I), and MCP (Master Control Program) for the Burroughs B5000 written in ALGOL in 1961.) Circa 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.[10]\nK&R C\nThe cover of the book, The C Programming Language, first edition by Brian Kernighan and Dennis Ritchie\n\nIn 1978, Brian Kernighan and Dennis Ritchie published the first edition of The C Programming Language.[1] This book, known to C programmers as \"K&R\", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as K&R C. The second edition of the book[14] covers the later ANSI C standard, described below.\n\nK&R introduced several language features:\n\n    Standard I/O library\n    long int data type\n    unsigned int data type\n    Compound assignment operators of the form =op (such as =-) were changed to the form op= (that is, -=) to remove the semantic ambiguity created by constructs such as i =- 10, which had been interpreted as i =- 10 (decrement i by 10) instead of the possibly intended i = -10 (let i be -10)\n\nEven after the publication of the 1989 ANSI standard, for many years K&R C was still considered the \"lowest common denominator\" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.\n\nIn early versions of C, only functions that returned a non-int value needed to be declared if used before the function definition; a function used without any previous declaration was assumed to return type int, if its value was used.\n\nFor example:\n\nlong some_function();\n/* int */ other_function();\n\n/* int */ calling_function()\n{\n    long test1;\n    register /* int */ test2;\n\n    test1 = some_function();\n    if (test1 > 0)\n          test2 = 0;\n    else\n          test2 = other_function();\n    return test2;\n}\n\nThe int type specifiers which are commented out could be omitted in K&R C, but are required in later standards.\n\nSince K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.\n\nIn the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC[15]) and some other vendors. These included:\n\n    void functions (i.e., functions with no return value)\n    functions returning struct or union types (rather than pointers)\n    assignment for struct data types\n    enumerated types\n\nThe large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.\nANSI C and ISO C\nMain article: ANSI C\nThe cover of the book, The C Programming Language, second edition by Brian Kernighan and Dennis Ritchie covering ANSI C\n\nDuring the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.\n\nIn 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 \"Programming Language C\". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.\n\nIn 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms \"C89\" and \"C90\" refer to the same programming language.\n\nANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.\n\nOne of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), void pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.\n\nC89 is supported by current C compilers, and most C code being written today is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.\n\nIn cases where code must be compilable by either standard-conforming or K&R C-based compilers, the __STDC__ macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.\n\nAfter the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995 Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.[citation needed]\nC99\nMain article: C99\n\nThe C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as \"C99\". It has since been amended three times by Technical Corrigenda.[16]\n\nC99 introduced several new features, including inline functions, several new data types (including long long int and a complex type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with //, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.\n\nC99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has int implicitly assumed. A standard macro __STDC_VERSION__ is defined with value 199901L to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.[17]\nC11\nMain article: C11 (C standard revision)\n\nIn 2007, work began on another revision of the C standard, informally called \"C1X\" until its official publication on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.\n\nThe C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro __STDC_VERSION__ is defined as 201112L to indicate that C11 support is available.\nEmbedded C\nMain article: Embedded C\n\nHistorically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.\n\nIn 2008, the C Standards Committee published a technical report extending the C language[18] to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.\nSyntax\nMain article: C syntax\n\nC has a formal grammar specified by the C standard.[19] Unlike languages such as FORTRAN 77, C source code is free-form which allows arbitrary use of whitespace to format code, rather than column-based or text-line-based restrictions; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters /* and */, or (since C99) following // until the end of the line. Comments delimited by /* and */ do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.[20]\n\nC source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as struct, union, and enum, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as char and int specify built-in types. Sections of code are enclosed in braces ({ and }, sometimes called \"curly brackets\") to limit the scope of declarations and to act as a single statement for control structures.\n\nAs an imperative language, C uses statements to specify actions. The most common statement is an expression statement, consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by if(-else) conditional execution and by do-while, while, and for iterative execution (looping). The for statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. break and continue can be used to leave the innermost enclosing loop statement or skip to its reinitialization. There is also a non-structured goto statement which branches directly to the designated label within the function. switch selects a case to be executed based on the value of an integer expression.\n\nExpressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next \"sequence point\"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (&&, ||, ?: and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.\n\nKernighan and Ritchie say in the Introduction of The C Programming Language: \"C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better.\"[21] The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.\nCharacter set\n\nThe basic C source character set includes the following characters:\n\n    Lowercase and uppercase letters: a–z A–Z\n    Decimal digits: 0–9\n    Graphic characters: ! \" # % & ' ( ) * + , - . / : ; < = > ? [ \\ ] ^ _ { | } ~\n    Whitespace characters: space, horizontal tab, vertical tab, form feed, newline\n\nNewline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.\n\nAdditional multibyte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multinational Unicode characters to be embedded portably within C source text by using \\uXXXX or \\UXXXXXXXX encoding (where the X denotes a hexadecimal character), although this feature is not yet widely implemented.\n\nThe basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.\nReserved words\n\nC89 has 32 reserved words, also known as keywords, which are the words that cannot be used for any purposes other than those for which they are predefined:\n\n    auto\n    break\n    case\n    char\n    const\n    continue\n    default\n    do\n\n\t\n\n    double\n    else\n    enum\n    extern\n    float\n    for\n    goto\n    if\n\n\t\n\n    int\n    long\n    register\n    return\n    short\n    signed\n    sizeof\n    static\n\n\t\n\n    struct\n    switch\n    typedef\n    union\n    unsigned\n    void\n    volatile\n    while\n\nC99 reserved five more words:\n\n    _Bool\n    _Complex\n\n\t\n\n    _Imaginary\n    inline\n\n\t\n\n    restrict\n\nC11 reserved seven more words:[22]\n\n    _Alignas\n    _Alignof\n\n\t\n\n    _Atomic\n    _Generic\n\n\t\n\n    _Noreturn\n    _Static_assert\n\n\t\n\n    _Thread_local\n\nMost of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved word called entry, but this was seldom implemented, and has now been removed as a reserved word.[23]\nOperators\nMain article: Operators in C and C++\n\nC supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:\n\n    arithmetic: +, -, *, /, %\n    assignment: =\n    augmented assignment: +=, -=, *=, /=, %=, &=, |=, ^=, <<=, >>=\n    bitwise logic: ~, &, |, ^\n    bitwise shifts: <<, >>\n    boolean logic: !, &&, ||\n    conditional evaluation: ? :\n    equality testing: ==, !=\n    calling functions: ( )\n    increment and decrement: ++, --\n    member selection: ., ->\n    object size: sizeof\n    order relations: <, <=, >, >=\n    reference and dereference: &, *, [ ]\n    sequencing: ,\n    subexpression grouping: ( )\n    type conversion: (typename)\n\nC uses the = operator, reserved in mathematics to express equality, to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. The similarity between C's operator for assignment and that for equality (==) has been criticized[by whom?] as it makes it easy to accidentally substitute one for the other. In many cases, each may be used in the context of the other without a compilation error (although some compilers produce warnings). For example, the conditional expression in if(a=b+1) is true if a is not zero after the assignment.[24] Additionally, C's operator precedence is non-intuitive, such as == binding more tightly than & and | in expressions like x & 1 == 0, which would need to be written (x & 1) == 0 to be properly evaluated.[25]\n\"Hello, world\" example\n\nThe \"hello, world\" example, which appeared in the first edition of K&R, has become the model for an introductory program in most programming textbooks, regardless of programming language. The program prints \"hello, world\" to the standard output, which is usually a terminal or screen display.\n\nThe original version was:[26]\n\nmain()\n{\n    printf(\"hello, world\\n\");\n}\n\nA standard-conforming \"hello, world\" program is:[a]\n\n#include <stdio.h>\n\nint main(void)\n{\n    printf(\"hello, world\\n\");\n}\n\nThe first line of the program contains a preprocessing directive, indicated by #include. This causes the compiler to replace that line with the entire text of the stdio.h standard header, which contains declarations for standard input and output functions such as printf. The angle brackets surrounding stdio.h indicate that stdio.h is located using a search strategy that prefers headers in the compiler's include path to other headers having the same name; double quotes are used to include local or project-specific header files.[discuss]\n\nThe next line indicates that a function named main is being defined. The main function serves a special purpose in C programs; the run-time environment calls the main function to begin program execution. The type specifier int indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the main function, is an integer. The keyword void as a parameter list indicates that this function takes no arguments.[b]\n\nThe opening curly brace indicates the beginning of the definition of the main function.\n\nThe next line calls (diverts execution to) a function named printf, which is supplied from a system library. In this call, the printf function is passed (provided with) a single argument, the address of the first character in the string literal \"hello, world\\n\". The string literal is an unnamed array with elements of type char, set up automatically by the compiler with a final 0-valued character to mark the end of the array (printf needs to know this). The \\n is an escape sequence that C translates to a newline character, which on output signifies the end of the current line. The return value of the printf function is of type int, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the printf function succeeded.) The semicolon ; terminates the statement.\n\nThe closing curly brace indicates the end of the code for the main function. According to the C99 specification and newer, the main function, unlike any other function, will implicitly return a status of 0 upon reaching the } that terminates the function. This is interpreted by the run-time system as an exit code indicating successful execution.[27]\nData types\nMain article: C variable types and declarations\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2012) (Learn how and when to remove this template message)\n\nThe type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal.[28] There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, characters, and enumerated types (enum). C99 added a boolean datatype. There are also derived types including arrays, pointers, records (struct), and untagged unions (union).\n\nC is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a type cast to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.\n\nSome find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: \"declaration reflects use\".)[29]\n\nC's usual arithmetic conversions allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.\nPointers\n\nC supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be dereferenced to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type. Pointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers. Many data types, such as trees, are commonly implemented as dynamically allocated struct objects linked together using pointers. Pointers to functions are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch) or as callbacks to be invoked by event handlers.[27]\n\nA null pointer value explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no \"next\" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a null pointer constant can be written as 0, with or without explicit casting to a pointer type, or as the NULL macro defined by several standard headers. In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.\n\nVoid pointers (void *) point to objects of unspecified type, and can therefore be used as \"generic\" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.[27]\n\nCareless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may be deallocated and reused (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.\nArrays\nSee also: C string\n\nArray types in C are traditionally of a fixed, static size specified at compile time. (The more recent C99 standard also allows a form of variable-length arrays.) However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's malloc function, and treat it as an array. C's unification of arrays and pointers means that declared arrays and these dynamically allocated simulated arrays are virtually interchangeable.\n\nSince arrays are always accessed (in effect) via pointers, array accesses are typically not checked against the underlying array size, although some compilers may provide bounds checking as an option.[30] Array bounds violations are therefore possible and rather common in carelessly written code, and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions. If bounds checking is desired, it must be done manually.\n\nC does not have a special provision for declaring multidimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting \"multidimensional array\" can be thought of as increasing in row-major order.\n\nMultidimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, since arrays are passed merely as pointers, the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this is to allocate the array with an additional \"row vector\" of pointers to the columns.)\n\nC99 introduced \"variable-length arrays\" which address some, but not all, of the issues with ordinary C arrays.\nArray–pointer interchangeability\n\nThe subscript notation x[i] (where x designates a pointer) is a syntactic sugar for *(x+i).[31] Taking advantage of the compiler's knowledge of the pointer type, the address that x + i points to is not the base address (pointed to by x) incremented by i bytes, but rather is defined to be the base address incremented by i multiplied by the size of an element that x points to. Thus, x[i] designates the i+1th element of the array.\n\nFurthermore, in most expression contexts (a notable exception is as operand of sizeof), the name of an array is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.\n\nThe size of an element can be determined by applying the operator sizeof to any dereferenced element of x, as in n = sizeof *x or n = sizeof x[0], and the number of elements in a declared array A can be determined as sizeof A / sizeof A[0]. The latter only applies to array names: variables declared with subscripts (int A[20]). Due to the semantics of C, it is not possible to determine the entire size of arrays through pointers to arrays or those created by dynamic allocation (malloc); code such as sizeof arr / sizeof arr[0] (where arr designates a pointer) will not work since the compiler assumes the size of the pointer itself is being requested.[32][33] Since array name arguments to sizeof are not converted to pointers, they do not exhibit such ambiguity. However, arrays created by dynamic allocation are accessed by pointers rather than true array variables, so they suffer from the same sizeof issues as array pointers.\n\nThus, despite this apparent equivalence between array and pointer variables, there is still a distinction to be made between them. Even though the name of an array is, in most expression contexts, converted into a pointer (to its first element), this pointer does not itself occupy any storage; the array name is not an l-value, and its address is a constant, unlike a pointer variable. Consequently, what an array \"points to\" cannot be changed, and it is impossible to assign a new address to an array name. Array contents may be copied, however, by using the memcpy function, or by accessing the individual elements.\nMemory management\n\nOne of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three distinct ways to allocate memory for objects:[27]\n\n    Static memory allocation: space for the object is provided in the binary at compile-time; these objects have an extent (or lifetime) as long as the binary which contains them is loaded into memory.\n    Automatic memory allocation: temporary objects can be stored on the stack, and this space is automatically freed and reusable after the block in which they are declared is exited.\n    Dynamic memory allocation: blocks of memory of arbitrary size can be requested at run-time using library functions such as malloc from a region of memory called the heap; these blocks persist until subsequently freed for reuse by calling the library function realloc or free\n\nThese three approaches are appropriate in different situations and have various tradeoffs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.\n\nWhere possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary.[27] Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on malloc for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)\n\nUnless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.\n\nAnother issue is that heap memory allocation has to be synchronized with its actual usage in any program in order for it to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before free() is called, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a memory leak. Conversely, it is possible for memory to be freed but continue to be referenced, leading to unpredictable results. Typically, the symptoms will appear in a portion of the program far removed from the actual error, making it difficult to track down the problem. (Such issues are ameliorated in languages with automatic garbage collection.)\nLibraries\n\nThe C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single \"archive\" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., -lm, shorthand for \"math library\").[27]\n\nThe most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation. (Implementations which target limited environments such as embedded systems may provide only a subset of the standard library.) This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, stdio.h) specify the interfaces for these and other standard library facilities.\n\nAnother common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.\n\nSince many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.[27]\nLanguage tools\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2014) (Learn how and when to remove this template message)\n\nTools have been created to help C programmers avoid some of the problems inherent in the language, such as statements with undefined behavior or statements that are not a good practice because they are likely to result in unintended behavior or run-time errors.\n\nAutomated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.[34]\n\nThere are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as array bounds checking, buffer overflow detection, serialization, and automatic garbage collection.\n\nTools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.\nUses\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2012) (Learn how and when to remove this template message)\nThe TIOBE index graph from 2002 to 2015, showing a comparison of the popularity of various programming languages[35]\n\nC is widely used for \"system programming\", including implementing operating systems and embedded system applications, due to a combination of desirable characteristics such as code portability and efficiency, ability to access specific hardware addresses, ability to pun types to match externally imposed data access requirements, and low run-time demand on system resources. C can also be used for website programming using CGI as a \"gateway\" for information between the Web application, the server, and the browser.[36] Some reasons for choosing C over interpreted languages are its speed, stability, and near-universal availability.[37]\n\nOne consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. The primary implementations of Python, Perl 5 and PHP, for example, are all written in C.\n\nDue to its thin layer of abstraction and low overhead, C allows efficient implementations of algorithms and data structures, which is useful for programs that perform a lot of computations. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica and MATLAB are completely or partially written in C.\n\nC is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, it is not necessary to develop machine-specific code generators. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, which support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--.\n\nC has also been widely used to implement end-user applications, but much of that development has shifted to newer, higher-level languages.\nRelated languages\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (February 2016) (Learn how and when to remove this template message)\n\nC has directly or indirectly influenced many later languages such as C#, D, Go, Java, JavaScript, Limbo, LPC, Perl, PHP, Python, and Unix's C shell. The most pervasive influence has been syntactical: all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models and/or large-scale program structures that differ from those of C, sometimes radically.\n\nSeveral C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.\n\nWhen object-oriented languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.\n\nThe C++ programming language was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax.[38] C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.\n\nObjective-C was originally a very \"thin\" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.\n\nIn addition to C++ and Objective-C, Ch, Cilk and Unified Parallel C are nearly supersets of C.", "skillName": "C."}
{"id": 195, "category": "Computer_Programming", "skillText": "Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding[1][2]) of algorithms in a target programming language. Source code is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms and formal logic.\n\nRelated tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.\n\nContents\n\n    1 Overview\n    2 History\n    3 Modern programming\n        3.1 Quality requirements\n        3.2 Readability of source code\n        3.3 Algorithmic complexity\n        3.4 Methodologies\n        3.5 Measuring language usage\n        3.6 Debugging\n    4 Programming languages\n    5 Programmers\n    6 See also\n    7 References\n    8 Further reading\n    9 External links\n\nOverview\n\nWithin software engineering, programming (the implementation) is regarded as one phase in a software development process.\n\nThere is an ongoing debate on the extent to which the writing of programs is an art form, a craft, or an engineering discipline.[3] In general, good programming is considered to be the measured application of all three, with the goal of producing an efficient and evolvable software solution (the criteria for \"efficient\" and \"evolvable\" vary considerably). The discipline differs from many other technical professions in that programmers, in general, do not need to be licensed or pass any standardized (or governmentally regulated) certification tests in order to call themselves \"programmers\" or even \"software engineers.\" Because the discipline covers many areas, which may or may not include critical applications, it is debatable whether licensing is required for the profession as a whole. In most cases, the discipline is self-governed by the entities which require the programming, and sometimes very strict environments are defined (e.g. United States Air Force use of AdaCore and security clearance). However, representing oneself as a \"professional software engineer\" without a license from an accredited institution is illegal in many parts of the world.\n\nAnother ongoing debate is the extent to which the programming language used in writing computer programs affects the form that the final program takes.[citation needed] This debate is analogous to that surrounding the Sapir–Whorf hypothesis[4] in linguistics and cognitive science, which postulates that a particular spoken language's nature influences the habitual thought of its speakers. Different language patterns yield different patterns of thought. This idea challenges the possibility of representing the world perfectly with language, because it acknowledges that the mechanisms of any language condition the thoughts of its speaker community.\nHistory\nSee also: History of programming languages\nAda Lovelace commenting on the work of Luigi Menabrea, created the first algorithm designed for processing by an Analytical Engine and is often recognized as history's first computer programmer.\n\nAncient cultures seemed to have no conception of computing beyond arithmetic, algebra, and geometry, occasionally devising computational systems with elements of calculus (e.g. the method of exhaustion). The only mechanical device that existed for numerical computation at the beginning of human history was the abacus, invented in Sumeria circa 2500 BC. Later, the Antikythera mechanism, invented some time around 100 BC in ancient Greece, is the first known mechanical calculator utilizing gears of various sizes and configuration to perform calculations,[5] which tracked the metonic cycle still used in lunar-to-solar calendars, and which is consistent for calculating the dates of the Olympiads.[6]\n\nThe medieval scientist Al-Jazari built programmable automata in 1206 AD. One system employed in these devices was the use of pegs and cams placed into a wooden drum at specific locations, which would sequentially trigger levers that in turn operated percussion instruments. The output of this device was a small drummer playing various rhythms and drum patterns.[7] The Jacquard loom, which Joseph Marie Jacquard developed in 1801, uses a series of pasteboard cards with holes punched in them. The hole pattern represented the pattern that the loom had to follow in weaving cloth. The loom could produce entirely different weaves using different sets of cards.\n\nCharles Babbage adopted the use of punched cards around 1830 to control his Analytical Engine. Mathematician Ada Lovelace, a friend of Babbage, between 1842 and 1843 translated an article by Italian military engineer Luigi Menabrea on the engine,[8] which she supplemented with a set of notes, simply called Notes. These notes include an algorithm to calculate a sequence of Bernoulli numbers,[9] intended to be carried out by a machine. Despite controversy over scope of her contribution, many consider this algorithm to be the first computer program.[8]\nData and instructions were once stored on external punched cards, which were kept in order and arranged in program decks.\n\nIn the 1880s, Herman Hollerith invented the recording of data on a medium that could then be read by a machine. Prior uses of machine readable media, above, had been for lists of instructions (not data) to drive programmed machines such as Jacquard looms and mechanized musical instruments. \"After some initial trials with paper tape, he settled on punched cards...\"[10] To process these punched cards, first known as \"Hollerith cards\" he invented the keypunch, sorter, and tabulator unit record machines.[11] These inventions were the foundation of the data processing industry. In 1896 he founded the Tabulating Machine Company (which later became the core of IBM). The addition of a control panel (plugboard) to his 1906 Type I Tabulator allowed it to do different jobs without having to be physically rebuilt. By the late 1940s, there were several unit record calculators, such as the IBM 602 and IBM 604, whose control panels specified a sequence (list) of operations and thus were programmable machines.\n\nThe invention of the von Neumann architecture allowed computer programs to be stored in computer memory. Early programs had to be painstakingly crafted using the instructions (elementary operations) of the particular machine, often in binary notation. Every model of computer would likely use different instructions (machine language) to do the same task. Later, assembly languages were developed that let the programmer specify each instruction in a text format, entering abbreviations for each operation code instead of a number and specifying addresses in symbolic form (e.g., ADD X, TOTAL). Entering a program in assembly language is usually more convenient, faster, and less prone to human error than using machine language, but because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages.\nWired control panel for an IBM 402 Accounting Machine\n\nThe synthesis of numerical calculation, predetermined operation and output, along with a way to organize and input instructions in a manner relatively easy for humans to conceive and produce, led to the modern development of computer programming. In 1954, FORTRAN was invented; it was the first widely used high level programming language to have a functional implementation, as opposed to just a design on paper.[12][13] (A high-level language is, in very general terms, any programming language that allows the programmer to write programs in terms that are more abstract than assembly language instructions, i.e. at a level of abstraction \"higher\" than that of an assembly language.) It allowed programmers to specify calculations by entering a formula directly (e.g. Y = X*2 + 5*X + 9). The program text, or source, is converted into machine instructions using a special program called a compiler, which translates the FORTRAN program into machine language. In fact, the name FORTRAN stands for \"Formula Translation\". Many other languages were developed, including some for commercial programming, such as COBOL. Programs were mostly still entered using punched cards or paper tape. (See computer programming in the punch card era). By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were developed that allowed changes and corrections to be made much more easily than with punched cards. (Usually, an error in punching a card meant that the card had to be discarded and a new one punched to replace it.)\n\nAs time has progressed, computers have made giant leaps in processing power, which have allowed the development of programming languages that are more abstracted from the underlying hardware. Popular programming languages of the modern era include ActionScript, C, C++, C#, Haskell, Java, JavaScript, Objective-C, Perl, PHP, Python, Ruby, Smalltalk, SQL, Visual Basic, and dozens more.[14] Although these high-level languages usually incur greater overhead, the increase in speed of modern computers has made the use of these languages much more practical than in the past. These increasingly abstracted languages are typically easier to learn and allow the programmer to develop applications much more efficiently and with less source code. However, high-level languages are still impractical for a few programs, such as those where low-level hardware control is necessary or where maximum processing speed is vital. Computer programming has become a popular career in the developed world, particularly in the United States, Europe, and Japan. Due to the high labor cost of programmers in these countries, some forms of programming have been increasingly subject to outsourcing (importing software and services from other countries, usually at a lower wage), making programming career decisions in developed countries more complicated, while increasing economic opportunities for programmers in less developed areas, particularly China and India.\nModern programming\nQuestion book-new.svg\n\tThis section relies largely or entirely upon a single source. Relevant discussion may be found on the talk page. Please help improve this article by introducing citations to additional sources. (August 2010)\nQuality requirements\n\nWhatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:\n\n    Reliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms, and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors).\n    Robustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services and network connections, user error, and unexpected power outages.\n    Usability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness and completeness of a program's user interface.\n    Portability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware and operating system, and availability of platform specific compilers (and sometimes libraries) for the language of the source code.\n    Maintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or customizations, fix bugs and security holes, or adapt it to new environments. Good practices[15] during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term.\n    Efficiency/performance: Measure of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating memory leaks.\n\nReadability of source code\n\nIn computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\n\nReadability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study[16] found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.\n\nFollowing a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[17] Some of these factors include:\n\n    Different indentation styles (whitespace)\n    Comments\n    Decomposition\n    Naming conventions for objects (such as variables, classes, procedures, etc.)\n\nVarious visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Techniques like Code refactoring can enhance readability.\nAlgorithmic complexity\n\nThe academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.\nMethodologies\n\nThe first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of differing approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.\n\nPopular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.\n\nA similar technique used for database design is Entity-Relationship Modeling (ER Modeling).\n\nImplementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.\nMeasuring language usage\nMain article: Measuring programming language popularity\n\nIt is very difficult to determine what are the most popular of modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language,[18] the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).\n\nSome languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers[19] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).\nDebugging\nThe bug from 1947 which is at the origin of a popular (but incorrect) etymology for the common term for a software defect.\nMain article: Debugging\n\nDebugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems.\n\nDebugging is often done with IDEs like Eclipse, Visual Studio, Kdevelop, NetBeans and Code::Blocks. Standalone debuggers like gdb are also used, and these often provide less of a visual environment, usually using a command line.\nProgramming languages\nMain articles: Programming language and List of programming languages\n\nDifferent programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from \"low-level\" to \"high-level\"; \"low-level\" languages are typically more machine-oriented and faster to execute, whereas \"high-level\" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in \"high-level\" languages than in \"low-level\" ones.\n\nAllen Downey, in his book How To Think Like A Computer Scientist, writes:\n\n    The details look different in different languages, but a few basic instructions appear in just about every language:\n\n        Input: Gather data from the keyboard, a file, or some other device.\n        Output: Display data on the screen or send data to a file or other device.\n        Arithmetic: Perform basic arithmetical operations like addition and multiplication.\n        Conditional Execution: Check for certain conditions and execute the appropriate sequence of statements.\n        Repetition: Perform some action repeatedly, usually with some variation.\n\nMany computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run time conventions (e.g., method of passing arguments), then these functions may be written in any other language.\nProgrammers\nMain article: Programmer\nSee also: Software developer and Software engineer\n\nComputer programmers are those who write computer software. Their jobs usually involve:\n\n    Coding\n    Debugging\n    Documentation\n    Integration\n    Maintenance\n    Requirements analysis\n    Software architecture\n    Software testing\n    Specification\n\nSee also\nBook icon \t\n\n    Book: Programming\n\n    iconComputer Science portal Computing portal Computer networking portal iconComputer programming portal \n\nMain article: Outline of computer programming\n\n    ACCU\n    Association for Computing Machinery\n    Computer networking\n    Hello world program\n    Institution of Analysts and Programmers\n    System programming\n    The Art of Computer Programming\n    Codacy", "skillName": "Computer_Programming."}
{"id": 196, "category": "Computer_Programming", "skillText": "Programming with Big Data in R (pbdR)[1] is a series of R packages and an environment for statistical computing with Big Data by using high-performance statistical computation.[2] The pbdR uses the same programming language as R with S3/S4 classes and methods which is used among statisticians and data miners for developing statistical software. The significant difference between pbdR and R code is that pbdR mainly focuses on distributed memory systems, where data are distributed across several processors and analyzed in a batch mode, while communications between processors are based on MPI that is easily used in large high-performance computing (HPC) systems. R system mainly focuses[citation needed] on single multi-core machines for data analysis via an interactive mode such as GUI interface.\n\nTwo main implementations in R using MPI are Rmpi[3] and pbdMPI of pbdR.\n\n    The pbdR built on pbdMPI uses SPMD parallelism where every processor is considered as worker and owns parts of data. The SPMD parallelism introduced in mid 1980 is particularly efficient in homogeneous computing environments for large data, for example, performing singular value decomposition on a large matrix, or performing clustering analysis on high-dimensional large data. On the other hand, there is no restriction to use manager/workers parallelism in SPMD parallelism environment.\n    The Rmpi[3] uses manager/workers parallelism where one main processor (manager) servers as the control of all other processors (workers). The manager/workers parallelism introduced around early 2000 is particularly efficient for large tasks in small clusters, for example, bootstrap method and Monte Carlo simulation in applied statistics since i.i.d. assumption is commonly used in most statistical analysis. In particular, task pull parallelism has better performance for Rmpi in heterogeneous computing environments.\n\nThe idea of SPMD parallelism is to let every processor do the same amount of work, but on different parts of a large data set. For example, a modern GPU is a large collection of slower co-processors that can simply apply the same computation on different parts of relatively smaller data, but the SPMD parallelism ends up with an efficient way to obtain final solutions (i.e. time to solution is shorter).[4] It is clear that pbdR is not only suitable for small clusters, but is also more stable for analyzing Big data and more scalable for supercomputers.[5][third-party source needed] In short, pbdR\n\n    does not like Rmpi, snow, snowfall, do-like,[clarification needed] nor parallel packages in R,\n    does not focus on interactive computing nor master/workers,\n    but is able to use both SPMD and task parallelisms.\n\nContents\n\n    1 Package design\n    2 Examples\n        2.1 Example 1\n        2.2 Example 2\n        2.3 Example 3\n    3 Further reading\n    4 References\n    5 External links\n\nPackage design\n\nProgramming with pbdR requires usage of various packages developed by pbdR core team. Packages developed are the following.\nGeneral \tI/O \tComputation \tApplication \tProfiling \tClient/Server\npbdDEMO \tpbdNCDF4 \tpbdDMAT \tpmclust \tpbdPROF \tpbdZMQ\npbdMPI \tpbdADIOS \tpbdBASE \tpbdML \tpbdPAPI \tremoter\n\t\tpbdSLAP \t\thpcvis \tpbdCS\nThe images describes how various pbdr packages are correlated.\n\nAmong these packages, pbdMPI provides wrapper functions to MPI library, and it also produces a shared library and a configuration file for MPI environments. All other packages rely on this configuration for installation and library loading that avoids difficulty of library linking and compiling. All other packages can directly use MPI functions easily.\n\n    pbdMPI --- an efficient interface to MPI either OpenMPI or MPICH2 with a focus on Single Program/Multiple Data (SPMD) parallel programming style\n    pbdSLAP --- bundles scalable dense linear algebra libraries in double precision for R, based on ScaLAPACK version 2.0.2 which includes several scalable linear algebra packages (namely BLACS, PBLAS, and ScaLAPACK).\n    pbdNCDF4 --- interface to Parallel Unidata NetCDF4 format data files\n    pbdBASE --- low-level ScaLAPACK codes and wrappers\n    pbdDMAT --- distributed matrix classes and computational methods, with a focus on linear algebra and statistics\n    pbdDEMO --- set of package demonstrations and examples, and this unifying vignette\n    pmclust --- parallel model-based clustering using pbdR\n    pbdPROF --- profiling package for MPI codes and visualization of parsed stats\n    pbdZMQ --- interface to ØMQ\n\nAmong those packages, the pbdDEMO package is a collection of 20+ package demos which offer example uses of the various pbdR packages, and contains a vignette that offers detailed explanations for the demos and provides some mathematical or statistical insight.\nExamples\nExample 1\n\nHello World! Save the following code in a file called \"demo.r\"\n\n### Initial MPI\nlibrary(pbdMPI, quiet = TRUE)\ninit()\n\ncomm.cat(\"Hello World!\\n\")\n\n### Finish\nfinalize()\n\nand use the command\n\nmpiexec -np 2 Rscript demo.r\n\nto execute the code where Rscript is one of command line executable program.\nExample 2\n\nThe following example modified from pbdMPI illustrates the basic syntax of the language of pbdR. Since pbdR is designed in SPMD, all the R scripts are stored in files and executed from the command line via mpiexec, mpirun, etc. Save the following code in a file called \"demo.r\"\n\n### Initial MPI\nlibrary(pbdMPI, quiet = TRUE)\ninit()\n.comm.size <- comm.size()\n.comm.rank <- comm.rank()\n\n### Set a vector x on all processors with different values\nN <- 5\nx <- (1:N) + N * .comm.rank\n\n### All reduce x using summation operation\ny <- allreduce(as.integer(x), op = \"sum\")\ncomm.print(y)\ny <- allreduce(as.double(x), op = \"sum\")\ncomm.print(y)\n\n### Finish\nfinalize()\n\nand use the command\n\nmpiexec -np 4 Rscript demo.r\n\nto execute the code where Rscript is one of command line executable program.\nExample 3\n\nThe following example modified from pbdDEMO illustrates the basic ddmatrix computation of pbdR which performs singular value decomposition on a given matrix. Save the following code in a file called \"demo.r\"\n\n# Initialize process grid\nlibrary(pbdDMAT, quiet=T)\nif(comm.size() != 2)\n  comm.stop(\"Exactly 2 processors are required for this demo.\")\ninit.grid()\n\n# Setup for the remainder\ncomm.set.seed(diff=TRUE)\nM <- N <- 16\nBL <- 2 # blocking --- passing single value BL assumes BLxBL blocking\ndA <- ddmatrix(\"rnorm\", nrow=M, ncol=N, mean=100, sd=10)\n\n# LA SVD\nsvd1 <- La.svd(dA)\ncomm.print(svd1$d)\n\n# Finish\nfinalize()\n\nand use the command\n\nmpiexec -np 2 Rscript demo.r\n\nto execute the code where Rscript is one of command line executable program.", "skillName": "pbdR."}
{"id": 197, "category": "Computer_Programming", "skillText": "SAS (Statistical Analysis System)[1] is a software suite developed by SAS Institute for advanced analytics, multivariate analyses, business intelligence, data management, and predictive analytics.\n\nSAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.\n\nContents\n\n    1 Technical overview and terminology\n    2 History\n        2.1 Origins\n        2.2 Development\n        2.3 Recent history\n    3 Software products\n        3.1 Comparison to other products\n    4 Adoption\n    5 See also\n    6 References\n    7 Further reading\n    8 External links\n\nTechnical overview and terminology\n\nSAS is a software suite that can mine, alter, manage and retrieve data from a variety of sources and perform statistical analysis on it.[2] SAS provides a graphical point-and-click user interface for non-technical users and more advanced options through the SAS programming language.[2] In order to use Statistical Analysis System, Data should be in an Excel table format or SAS format.[3] SAS programs have a DATA step, which retrieves and manipulates data, usually creating a SAS data set, and a PROC step, which analyzes the data.[4]\n\nEach step consists of a series of statements.[5] The DATA step has executable statements that result in the software taking an action, and declarative statements that provide instructions to read a data set or alter the data's appearance.[4] The DATA step has two phases, compilation and execution. In the compilation phase, declarative statements are processed and syntax errors are identified. Afterwards, the execution phase processes each executable statement sequentially.[6] Data sets are organized into tables with rows called \"observations\" and columns called \"variables\". Additionally, each piece of data has a descriptor and a value.[4][7]\n\nThe PROC step consists of PROC statements that call upon named procedures. Procedures perform analysis and reporting on data sets to produce statistics, analyses and graphics. There are more than 300 procedures and each one contains a substantial body of programming and statistical work.[4] PROC statements can also display results, sort data or perform other operations.[5] SAS Macros are pieces of code or variables that are coded once and referenced to perform repetitive tasks.[8]\n\nSAS data can be published in HTML, PDF, Excel and other formats using the Output Delivery System, which was first introduced in 2007.[9] The SAS Enterprise Guide is SAS' point-and-click interface. It generates code to manipulate data or perform analysis automatically and does not require SAS programming experience to use.[10]\n\nThe SAS software suite has more than 200[11] components[12][13] Some of the SAS components include:[2][12][14]\n\n    Base SAS – Basic procedures and data management\n    SAS/STAT – Statistical analysis\n    SAS/GRAPH – Graphics and presentation\n    SAS/OR – Operations research\n    SAS/ETS – Econometrics and Time Series Analysis\n    SAS/IML – Interactive matrix language\n    SAS/AF – Applications facility\n    SAS/QC – Quality control\n    SAS/INSIGHT – Data mining\n    SAS/PH – Clinical trial analysis\n    Enterprise Miner – data mining\n    Enterprise Guide - GUI based code editor & project manager\n    SAS EBI - Suite of Business Intelligence Applications\n    SAS Grid Manager - Manager of SAS grid computing environment\n\nHistory\nOrigins\n\nThe development of SAS began in 1966 after North Carolina State University re-hired Anthony Barr[15] to program his analysis of variance and regression software so that it would run on IBM System/360 computers.[16] The project was funded by the National Institute of Health[17] and was originally intended to analyze agricultural data[12][18] to improve crop yields.[19] Barr was joined by student James Goodnight, who developed the software's statistical routines, and the two became project-leaders.[15][16][20] In 1968, Barr and Goodnight integrated new multiple regression and analysis of variance routines.[21][22] In 1972, after issuing the first release of SAS, the project lost its funding.[17] According to Goodnight, this was because NIH only wanted to fund projects with medical applications.[23] Goodnight continued teaching at the university for a salary of $1 and access to mainframe computers for use with the project,[17] until it was funded by the University Statisticians of the Southern Experiment Stations the following year.[16][23] John Sall joined the project in 1973 and contributed to the software's econometrics, time series, and matrix algebra. Another early participant, Caroll G. Perkins, contributed to SAS' early programming. Jolayne W. Service and Jane T. Helwig created SAS' first documentation.[21]\n\nThe first versions of SAS were named after the year in which they were released.[24] In 1971, SAS 71 was published as a limited release.[2][25] It was used only on IBM mainframes and had the main elements of SAS programming, such as the DATA step and the most common procedures in the PROC step.[24] The following year a full version was released as SAS 72, which introduced the MERGE statement and added features for handling missing data or combining data sets.[26] In 1976, Barr, Goodnight, Sall, and Helwig removed the project from North Carolina State and incorporated it into SAS Institute, Inc.[27]\nDevelopment\n\nSAS was re-designed in SAS 76 with an open architecture that allowed for compilers and procedures. The INPUT and INFILE statements were improved so they could read most data formats used by IBM mainframes. Generating reports was also added through the PUT and FILE statements. The ability to analyze general linear models was also added[28] as was the FORMAT procedure, which allowed developers to customize the appearance of data.[24] In 1979, SAS 79 added support for the CMS operating system and introduced the DATASETS procedure. Three years later, SAS 82 introduced an early macro language and the APPEND procedure.[24]\n\nSAS version 4 had limited features, but made SAS more accessible. Version 5 introduced a complete macro language, array subscripts, and a full-screen interactive user interface called Display Manager.[24] In 1985, SAS was rewritten in the C programming language. This allowed for the SAS' Multivendor Architecture that allows the software to run on UNIX, MS-DOS, and Windows. It was previously written in PL/I, Fortran, and assembly language.[20][24]\n\nIn the 1980s and 1990s, SAS released a number of components to complement Base SAS. SAS/GRAPH, which produces graphics, was released in 1980, as well as the SAS/ETS component, which supports econometric and time series analysis. A component intended for pharmaceutical users, SAS/PH-Clinical, was released in the 1990s. The Food and Drug Administration standardized on SAS/PH-Clinical for new drug applications in 2002.[20] Vertical products like SAS Financial Management and SAS Human Capital Management (then called CFO Vision and HR Vision respectively) were also introduced.[29] JMP was developed by SAS co-founder John Sall and a team of developers to take advantage of the graphical user interface introduced in the 1984 Apple Macintosh[30] and shipped for the first time in 1989.[30] Updated versions of JMP were released continuously after 2002 with the most recent release being from 2012.[31][32][33][34][35][36][37]\n\nSAS version 6 was used throughout the 1990s and was available on a wider range of operating systems, including Macintosh, OS/2, Silicon Graphics, and Primos. SAS introduced new features through dot-releases. From 6.06 to 6.09, a user interface based on the windows paradigm was introduced and support for SQL was added.[24] Version 7 introduced the Output Delivery System (ODS) and an improved text editor. ODS was improved upon in successive releases. For example, more output options were added in version 8. The number of operating systems that were supported was reduced to UNIX, Windows and z/OS, and Linux was added.[24][38] SAS version 8 and SAS Enterprise Miner were released in 1999.[20]\nRecent history\n\nIn 2002, the Text Miner software was introduced. Text Miner analyzes text data like emails for patterns in Business Intelligence applications.[39] In 2004, SAS Version 9.0 was released, which was dubbed \"Project Mercury\" and was designed to make SAS accessible to a broader range of business users.[40][41] Version 9.0 added custom user interfaces based on the user’s role and established the point-and-click user interface of SAS Enterprise Guide as the software's primary graphical user interface (GUI).[40] The Customer Relationship Management (CRM) features were improved in 2004 with SAS Interaction Management.[42] In 2008 SAS announced Project Unity, designed to integrate data quality, data integration and master data management.[43]\n\nSAS sued World Programming, the developers of a competing implementation, World Programming System, alleging that they had infringed SAS's copyright in part by implementing the same functionality. This case was referred from the United Kingdom's High Court of Justice to the European Court of Justice on 11 August 2010.[44] In May 2012, the European Court of Justice ruled in favor of World Programming, finding that \"the functionality of a computer program and the programming language cannot be protected by copyright.\"[45]\n\nA free version was introduced for students in 2010.[46] SAS Social Media Analytics, a tool for social media monitoring, engagement and sentiment analysis, was also released that year.[47][48] SAS Rapid Predictive Modeler (RPM), which creates basic analytical models using Microsoft Excel, was introduced that same year.[48][49] JMP 9 in 2010 added a new interface for using the R programming language from JMP and an add-in for Excel.[50][51] The following year, a High Performance Computing appliance was made available in a partnership with Teradata and EMC Greenplum.[52][53] In 2011, the company released Enterprise Miner 7.1.[54] The company introduced 27 data management products from October 2013 to October 2014 and updates to 160 others.[55] At the 2015 SAS Global Forum, it announced several new products that were specialized for different industries, as well as new training software.[56]\nSoftware products\n\nAs of 2011 SAS's largest set of products is its line for customer intelligence. Numerous SAS modules for web, social media and marketing analytics may be used to profile customers and prospects, predict their behaviors and manage and optimize communications.[37][57] SAS also provides the SAS Fraud Framework. The framework's primary functionality is to monitor transactions across different applications, networks and partners and use analytics to identify anomalies that are indicative of fraud.[58][59][60][61] SAS Enterprise GRC (Governance, Risk and Compliance) provides risk modeling, scenario analysis and other functions[62][63] in order to manage and visualize risk, compliance and corporate policies.[64] There is also a SAS Enterprise Risk Management product-set designed primarily for banks and financial services organizations.[65]\n\nSAS' products for monitoring and managing the operations of IT systems are collectively referred to as SAS IT Management Solutions.[66] SAS collects data from various IT assets on performance and utilization, then creates reports and analyses.[67] SAS' Performance Management products consolidate and provide graphical displays for key performance indicators (KPIs) at the employee, department and organizational level.[68][69] The SAS Supply Chain Intelligence product suite is offered for supply chain needs, such as forecasting product demand, managing distribution and inventory and optimizing pricing.[70] There is also a \"SAS for Sustainability Management\" set of software to forecast environmental, social and economic effects and identify causal relationships between operations and an impact on the environmental or ecosystem.[71]\n\nSAS has product sets for specific industries, such as government, retail, telecommunications and aerospace and for marketing optimization or high-performance computing.[72]\nComparison to other products\nSee also: Comparison of statistical packages\n\nIn a 2005 article for the Journal of Marriage and Family comparing statistical packages from SAS and its competitors Stata and SPSS, Alan C. Acock wrote that SAS programs provide \"extraordinary range of data analysis and data management tasks,\" but were difficult to use and learn.[73] SPSS and Stata, meanwhile, were both easier to learn (with better documentation) but had less capable analytic abilities, though these could be expanded with paid (in SPSS) or free (in Stata) add-ons. Acock concluded that SAS was best for power users, while occasional users would benefit most from SPSS and Stata.[73] A comparison by the University of California, Los Angeles, gave similar results.[74]\n\nCompetitors such as Revolution Analytics and Alpine Data Labs advertise their products as considerably cheaper than SAS'. In a 2011 comparison, Doug Henschen of InformationWeek found that start-up fees for the three are similar, though he admitted that the starting fees were not necessarily the best basis for comparison.[75] SAS' business model is not weighted as heavily on initial fees for its programs, instead focusing on revenue from annual subscription fees.[76][77]\nAdoption\n\nAccording to IDC, SAS is the largest market-share holder in \"advanced analytics\" with 35.4 percent of the market as of 2013.[78] It is the fifth largest market-share holder for business intelligence (BI) software with a 6.9% share[79] and the largest independent vendor. It competes in the BI market against conglomerates, such as SAP BusinessObjects, IBM Cognos, SPSS Modeler, Oracle Hyperion, and Microsoft BI.[80] SAS has been named in the Gartner Leader's Quadrant for Data Integration Tools[81] and for Business Intelligence and Analytical Platforms.[82] A study published in 2011 in BMC Health Services Research found that SAS was used in 42.6 percent of data analyses in health service research, based on a sample of 1,139 articles drawn from three journals.[83]", "skillName": "SAS."}
{"id": 198, "category": "Computer_Programming", "skillText": "A programmer, computer programmer, developer, dev, coder, or software engineer is a person who writes computer software. The term computer programmer can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (Assembly, COBOL, C, C++, C#, Java, Lisp, Python, etc.) is often prefixed to these titles, and those who work in a Web environment often prefix their titles with Web. The term programmer can be used to refer to a software developer, Web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, or software analyst. However, members of these professions possess other software engineering skills, beyond programming; for this reason, the term programmer, or code monkey, is sometimes considered an insulting or derogatory oversimplification of these other professions. This has sparked much debate amongst developers, analysts, computer scientists, programmers, and outsiders who continue to be puzzled at the subtle differences in the definitions of these occupations.[1][2][3][4][5]\n\nContents\n\n    1 History\n    2 Nature of the work\n        2.1 Testing and debugging\n        2.2 Application versus system programming\n        2.3 Types of software\n    3 Globalization\n        3.1 Market changes in the UK\n        3.2 Market changes in the US\n    4 See also\n    5 References\n    6 Further reading\n    7 External links\n\nHistory\nAda Lovelace is considered by many as the first computer programmer.[6]\n\nBritish countess and mathematician Ada Lovelace is often considered the first computer programmer, as she was the first to publish an algorithm intended for implementation on Charles Babbage's analytical engine, in October 1842, intended for the calculation of Bernoulli numbers.[7] Because Babbage's machine was never completed to a functioning standard in her time, she never saw this algorithm run.\n\nThe first person to run a program on a functioning modern electronically based computer was computer scientist Konrad Zuse, in 1941.\n\nThe ENIAC programming team, consisting of Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were the first regularly working programmers.[8][9]\n\nInternational Programmers' Day is celebrated annually on 7 January.[10] In 2009, the government of Russia decreed a professional annual holiday known as Programmers' Day to be celebrated on 13 September (12 September in leap years). It had also been an unofficial international holiday before that.\nNature of the work\n\n    Some of this section is from the Occupational Outlook Handbook \n    , 2006–07 Edition, which is in the public domain as a work of the United States Government.\n\nComputer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming — advanced computing technologies and sophisticated new languages and programming tools — have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.\n\nProgrammers work in many settings, including corporate information technology (\"IT\") departments, big software companies, small service firms and government entities of all sizes. Many professional programmers also work for consulting companies at client sites as contractors. Licensing is not typically required to work as a programmer, although professional certifications are commonly held by programmers. Programming is widely considered a profession (although some[who?] authorities disagree on the grounds that only careers with legal licensing requirements count as a profession).\n\nProgrammers' work varies widely depending on the type of business for which they are writing programs. For example, the instructions involved in updating financial records are very different from those required to duplicate conditions on an aircraft for pilots training in a flight simulator. Simple programs can be written in a few hours, more complex ones may require more than a year of work, while others are never considered 'complete' but rather are continuously improved as long as they stay in use. In most cases, several programmers work together as a team under a senior programmer’s supervision.\nA computer programmer writing Java code\n\nProgrammers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ is widely used for both scientific and business applications. Java, C#, VB and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as Java programmers, or by the type of function they perform or environment in which they work: for example, database programmers, mainframe programmers, or Web developers.\n\nWhen making changes to the source code that programs are made up of, programmers need to make other programmers aware of the task that the routine is to perform. They do this by inserting comments in the source code so that others can understand the program more easily and by documenting their code. To save work, programmers often use libraries of basic code that can be modified or customized for a specific application. This approach yields more reliable and consistent programs and increases programmers' productivity by eliminating some routine steps.\nTesting and debugging\n\nProgrammers test a program by running it and looking for bugs (errors). As they are identified, the programmer usually makes the appropriate corrections, then rechecks the program until an acceptably low level and severity of bugs remain. This process is called testing and debugging. These are important parts of every programmer's job. Programmers may continue to fix these problems throughout the life of a program. Updating, repairing, modifying, and expanding existing programs is sometimes called maintenance programming. Programmers may contribute to user guides and online help, or they may work with technical writers to do such work.\nApplication versus system programming\n\nComputer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives.\nTypes of software\n\nProgrammers in software development companies may work directly with experts from various fields to create software – either programs designed for specific clients or packaged software for general use – ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations – even small ones – have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is for example the case in research laboratories.[citation needed]\n\nIn some organizations, particularly small ones, people commonly known as programmer analysts are responsible for both the systems analysis and the actual programming work. The transition from a mainframe environment to one that is based primarily on personal computers (PCs) has blurred the once rigid distinction between the programmer and the user. Increasingly, adept end users are taking over many of the tasks previously performed by programmers. For example, the growing use of packaged software, such as spreadsheet and database management software packages, allows users to write simple programs to access data and perform calculations.[citation needed]\n\nIn addition, the rise of the Internet has made web development a huge part of the programming field. Currently more software applications are web applications that can be used by anyone with a web browser.[citation needed] Examples of such applications include the Google search service, the Outlook.com e-mail service, and the Flickr photo-sharing service.\n\nProgramming editors, also known as source code editors, are text editors that are specifically designed for programmers or developers for writing the source code of an application or a program. Most of these editors include features useful for programmers, which may include color syntax highlighting, auto indentation, auto-complete, bracket matching, syntax check, and allows plug-ins. These features aid the users during coding, debugging and testing.[citation needed]\nGlobalization\nGlobe icon.\n\tThe examples and perspective in this article deal primarily with the United States and do not represent a worldwide view of the subject. You may improve this article, discuss the issue on the talk page, or create a new article, as appropriate. (December 2010) (Learn how and when to remove this template message)\nMarket changes in the UK\n\nAccording to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey.[11] The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.[12]\nMarket changes in the US\n\nComputer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.[citation needed]\n\nLarge companies claim there is a skills shortage with regard to programming talent. However, US programmers and unions counter that large companies are exaggerating their case in order to obtain cheaper programmers from developing countries and to avoid paying for training in very specific technologies.[13]\n\nEnrollment in computer-related degrees in US has dropped recently due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers.[14] This situation has resulted in confusion about whether the US economy is entering a \"post-information age\" and the nature of US comparative advantages.\nSee also\n\n    iconComputer programming portal \n\n    Game programmer\n    List of programmers\n    Software development process\n    Software engineering\n    System administrator", "skillName": "Programmer."}
{"id": 199, "category": "Computer_Programming", "skillText": "C#[note 2] (pronounced as see sharp) is a multi-paradigm programming language encompassing strong typing, imperative, declarative, functional, generic, object-oriented (class-based), and component-oriented programming disciplines. It was developed by Microsoft within its .NET initiative and later approved as a standard by Ecma (ECMA-334) and ISO (ISO/IEC 23270:2006). C# is one of the programming languages designed for the Common Language Infrastructure.\n\nC# is a general-purpose, object-oriented programming language.[9] Its development team is led by Anders Hejlsberg. The most recent version is C# 6.0, which was released on July 20, 2015.[10]\n\nContents\n\n    1 Design goals\n    2 History\n        2.1 Name\n        2.2 Versions\n        2.3 Features added in versions\n            2.3.1 C# 2.0\n            2.3.2 C# 3.0\n            2.3.3 C# 4.0\n            2.3.4 C# 5.0[39]\n            2.3.5 C# 6.0\n            2.3.6 C# 7.0 proposals[42]\n    3 Syntax\n    4 Distinguishing features\n        4.1 Portability\n        4.2 Typing\n        4.3 Meta programming\n        4.4 Methods and functions\n        4.5 Property\n        4.6 Namespace\n        4.7 Memory access\n        4.8 Exception\n        4.9 Polymorphism\n        4.10 Functional programming\n    5 Common type system\n        5.1 Categories of data types\n        5.2 Boxing and unboxing\n    6 Libraries\n    7 Examples\n    8 Standardization and licensing\n    9 Implementations\n    10 See also\n    11 Notes\n    12 References\n    13 Further reading\n    14 External links\n\nDesign goals\n\nThe ECMA standard lists these design goals for C#:[9]\n\n    The C# language is intended to be a simple, modern, general-purpose, object-oriented programming language.\n    The language, and implementations thereof, should provide support for software engineering principles such as strong type checking, array bounds checking, detection of attempts to use uninitialized variables, and automatic garbage collection. Software robustness, durability, and programmer productivity are important.\n    The language is intended for use in developing software components suitable for deployment in distributed environments.\n    Portability is very important for source code and programmers, especially those already familiar with C and C++.\n    Support for internationalization is very important.\n    C# is intended to be suitable for writing applications for both hosted and embedded systems, ranging from the very large that use sophisticated operating systems, down to the very small having dedicated functions.\n    Although C# applications are intended to be economical with regard to memory and processing power requirements, the language was not intended to compete directly on performance and size with C or assembly language.\n\nHistory\n\nDuring the development of the .NET Framework, the class libraries were originally written using a managed code compiler system called Simple Managed C (SMC).[11][12] In January 1999, Anders Hejlsberg formed a team to build a new language at the time called Cool, which stood for \"C-like Object Oriented Language\".[13] Microsoft had considered keeping the name \"Cool\" as the final name of the language, but chose not to do so for trademark reasons. By the time the .NET project was publicly announced at the July 2000 Professional Developers Conference, the language had been renamed C#, and the class libraries and ASP.NET runtime had been ported to C#.\n\nC#'s principal designer and lead architect at Microsoft is Anders Hejlsberg, who was previously involved with the design of Turbo Pascal, Embarcadero Delphi (formerly CodeGear Delphi, Inprise Delphi and Borland Delphi), and Visual J++. In interviews and technical papers he has stated that flaws[citation needed] in most major programming languages (e.g. C++, Java, Delphi, and Smalltalk) drove the fundamentals of the Common Language Runtime (CLR), which, in turn, drove the design of the C# language itself.\n\nJames Gosling, who created the Java programming language in 1994, and Bill Joy, a co-founder of Sun Microsystems, the originator of Java, called C# an \"imitation\" of Java; Gosling further said that \"[C# is] sort of Java with reliability, productivity and security deleted.\"[14][15] Klaus Kreft and Angelika Langer (authors of a C++ streams book) stated in a blog post that \"Java and C# are almost identical programming languages. Boring repetition that lacks innovation,\"[16] \"Hardly anybody will claim that Java or C# are revolutionary programming languages that changed the way we write programs,\" and \"C# borrowed a lot from Java - and vice versa. Now that C# supports boxing and unboxing, we'll have a very similar feature in Java.\"[17] In July 2000, Anders Hejlsberg said that C# is \"not a Java clone\" and is \"much closer to C++\" in its design.[18]\n\nSince the release of C# 2.0 in November 2005, the C# and Java languages have evolved on increasingly divergent trajectories, becoming somewhat less similar. One of the first major departures came with the addition of generics to both languages, with vastly different implementations. C# makes use of reification to provide \"first-class\" generic objects that can be used like any other class, with code generation performed at class-load time.[19] Furthermore, C# has added several major features to accommodate functional-style programming, culminating in the LINQ extensions released with C# 3.0 and its supporting framework of lambda expressions, extension methods, and anonymous types.[20] These features enable C# programmers to use functional programming techniques, such as closures, when it is advantageous to their application. The LINQ extensions and the functional imports help developers reduce the amount of \"boilerplate\" code that is included in common tasks like querying a database, parsing an xml file, or searching through a data structure, shifting the emphasis onto the actual program logic to help improve readability and maintainability.[21]\n\nC# used to have a mascot called Andy (named after Anders Hejlsberg). It was retired on January 29, 2004.[22]\n\nC# was originally submitted to the ISO subcommittee JTC 1/SC 22 for review,[23] under ISO/IEC 23270:2003,[24] was withdrawn and was then approved under ISO/IEC 23270:2006.[25]\nName\nC-sharp musical note\n\nThe name \"C sharp\" was inspired by musical notation where a sharp indicates that the written note should be made a semitone higher in pitch.[26] This is similar to the language name of C++, where \"++\" indicates that a variable should be incremented by 1. The sharp symbol also resembles a ligature of four \"+\" symbols (in a two-by-two grid), further implying that the language is an increment of C++.[27]\n\nDue to technical limitations of display (standard fonts, browsers, etc.) and the fact that the sharp symbol (U+266F ♯ MUSIC SHARP SIGN (HTML &#9839;)) is not present on the standard keyboard, the number sign (U+0023 # NUMBER SIGN (HTML &#35;)) was chosen to represent the sharp symbol in the written name of the programming language.[28] This convention is reflected in the ECMA-334 C# Language Specification.[9] However, when it is practical to do so (for example, in advertising or in box art[29]), Microsoft uses the intended musical symbol.\n\nThe \"sharp\" suffix has been used by a number of other .NET languages that are variants of existing languages, including J# (a .NET language also designed by Microsoft that is derived from Java 1.1), A# (from Ada), and the functional programming language F#.[30] The original implementation of Eiffel for .NET was called Eiffel#,[31] a name retired since the full Eiffel language is now supported. The suffix has also been used for libraries, such as Gtk# (a .NET wrapper for GTK+ and other GNOME libraries) and Cocoa# (a wrapper for Cocoa).\nVersions\nVersion \tLanguage specification \tDate \t.NET Framework \tVisual Studio\nECMA \tISO/IEC \tMicrosoft\nC# 1.0 \tDecember 2002 \n\tApril 2003 \n\tJanuary 2002 \n\tJanuary 2002 \t.NET Framework 1.0 \tVisual Studio .NET 2002\nC# 1.2 \tOctober 2003 \n\tApril 2003 \t.NET Framework 1.1 \tVisual Studio .NET 2003\nC# 2.0 \tJune 2006 \n\tSeptember 2006 \n\tSeptember 2005 \n[note 3] \tNovember 2005 \t.NET Framework 2.0 \tVisual Studio 2005\nC# 3.0 \tNone[note 4] \tAugust 2007 \n\tNovember 2007 \t\n\n.NET Framework 2.0 (Except LINQ/Query Extensions)[32]\n.NET Framework 3.0 (Except LINQ/Query Extensions)[32]\n.NET Framework 3.5\n\tVisual Studio 2008\nVisual Studio 2010\nC# 4.0 \tApril 2010 \tApril 2010 \t.NET Framework 4 \tVisual Studio 2010\nC# 5.0 \tIn Progress[33] \tNone[note 4] \tJune 2013 \n\tAugust 2012 \t.NET Framework 4.5 \tVisual Studio 2012\nVisual Studio 2013\nC# 6.0 \tNone[note 4] \tNone \tJuly 2015 \t.NET Framework 4.6 \tVisual Studio 2015\nFeatures added in versions\nC# 2.0\n\n    Generics[34]\n    Partial types[34]\n    Anonymous methods[34]\n    Iterators[34]\n    Nullable types[34]\n    Getter/setter separate accessibility[34]\n    Method group conversions (delegates)[34]\n    Co- and Contra-variance for delegates[34]\n    Static classes[34]\n    Delegate inference[34]\n\nC# 3.0\n\n    Implicitly typed local variables[35]\n    Object and collection initializers[35]\n    Auto-Implemented properties[35]\n    Anonymous types[35]\n    Extension methods[35]\n    Query expressions[35]\n    Lambda expressions[35]\n    Expression trees[35]\n    Partial methods[36]\n\nC# 4.0\n\n    Dynamic binding[37]\n    Named and optional arguments[37]\n    Tuples[38]\n    Generic co- and contravariance[37]\n    Embedded interop types (\"NoPIA\")[37]\n\nC# 5.0[39]\n\n    Asynchronous methods[40]\n    Caller info attributes[40]\n\nC# 6.0\n\n    Compiler-as-a-service (Roslyn)\n    Import of static type members into namespace[41]\n    Exception filters[41]\n    Await in catch/finally blocks[41]\n    Auto property initializers[41]\n    Default values for getter-only properties[41]\n    Expression-bodied members[41]\n    Null propagator (null-conditional operator, succinct null checking)[41]\n    String Interpolation[41]\n    nameof operator[41]\n    Dictionary initializer[41]\n\nC# 7.0 proposals[42]\n\n    Local functions\n    Pattern matching\n    Records / algebraic data types\n    Nullability tracking\n    Async streams and disposal\n    Strongly typed access to wire formats\n\nSyntax\nMain article: C Sharp syntax\nSee also: Syntax (programming languages)\n\nThe core syntax of C# language is similar to that of other C-style languages such as C, C++ and Java. In particular:\n\n    Semicolons are used to denote the end of a statement.\n    Curly brackets are used to group statements. Statements are commonly grouped into methods (functions), methods into classes, and classes into namespaces.\n    Variables are assigned using an equals sign, but compared using two consecutive equals signs.\n    Square brackets are used with arrays, both to declare them and to get a value at a given index in one of them.\n\nDistinguishing features\nSee also: Comparison of C Sharp and Java\n\nSome notable features of C# that distinguish it from C, C++, and Java where noted, are:\nPortability\n\nBy design, C# is the programming language that most directly reflects the underlying Common Language Infrastructure (CLI).[43] Most of its intrinsic types correspond to value-types implemented by the CLI framework. However, the language specification does not state the code generation requirements of the compiler: that is, it does not state that a C# compiler must target a Common Language Runtime, or generate Common Intermediate Language (CIL), or generate any other specific format. Theoretically, a C# compiler could generate machine code like traditional compilers of C++ or Fortran.\nTyping\n\nC# supports strongly typed implicit variable declarations with the keyword var, and implicitly typed arrays with the keyword new[] followed by a collection initializer.\n\nC# supports a strict Boolean data type, bool. Statements that take conditions, such as while and if, require an expression of a type that implements the true operator, such as the Boolean type. While C++ also has a Boolean type, it can be freely converted to and from integers, and expressions such as if(a) require only that a is convertible to bool, allowing a to be an int, or a pointer. C# disallows this \"integer meaning true or false\" approach, on the grounds that forcing programmers to use expressions that return exactly bool can prevent certain types of programming mistakes such as if (a = b) (use of assignment = instead of equality ==, which while not an error in C or C++, will be caught by the compiler anyway).\n\nC# is more type safe than C++. The only implicit conversions by default are those that are considered safe, such as widening of integers. This is enforced at compile-time, during JIT, and, in some cases, at runtime. No implicit conversions occur between Booleans and integers, nor between enumeration members and integers (except for literal 0, which can be implicitly converted to any enumerated type). Any user-defined conversion must be explicitly marked as explicit or implicit, unlike C++ copy constructors and conversion operators, which are both implicit by default.\n\nC# has explicit support for covariance and contravariance in generic types, unlike C++ which has some degree of support for contravariance simply through the semantics of return types on virtual methods.\n\nEnumeration members are placed in their own scope.\n\nThe C# language does not allow for global variables or functions. All methods and members must be declared within classes. Static members of public classes can substitute for global variables and functions.\n\nLocal variables cannot shadow variables of the enclosing block, unlike C and C++.\nMeta programming\n\nMeta programming via C# attributes is part of the language. Many of these attributes duplicate the functionality of GCC's and VisualC++'s platform-dependent preprocessor directives.\nMethods and functions\n\nLike C++, and unlike Java, C# programmers must use the keyword virtual to allow methods to be overridden by subclasses.\n\nExtension methods in C# allow programmers to use static methods as if they were methods from a class's method table, allowing programmers to add methods to an object that they feel should exist on that object and its derivatives.\n\nThe type dynamic allows for run-time method binding, allowing for JavaScript-like method calls and run-time object composition.\n\nC# has support for strongly-typed function pointers via the keyword delegate. Like the Qt framework's pseudo-C++ signal and slot, C# has semantics specifically surrounding publish-subscribe style events, though C# uses delegates to do so.\n\nC# offers Java-like synchronized method calls, via the attribute [MethodImpl(MethodImplOptions.Synchronized)], and has support for mutually-exclusive locks via the keyword lock.\nProperty\n\nC# provides properties as syntactic sugar for a common pattern in which a pair of methods, accessor (getter) and mutator (setter) encapsulate operations on a single attribute of a class. No redundant method signatures for the getter/setter implementations need be written, and the property may be accessed using attribute syntax rather than more verbose method calls.\nNamespace\n\nA C# namespace provides the same level of code isolation as a Java package or a C++ namespace, with very similar rules and features to a package.\nMemory access\n\nIn C#, memory address pointers can only be used within blocks specifically marked as unsafe, and programs with unsafe code need appropriate permissions to run. Most object access is done through safe object references, which always either point to a \"live\" object or have the well-defined null value; it is impossible to obtain a reference to a \"dead\" object (one that has been garbage collected), or to a random block of memory. An unsafe pointer can point to an instance of a value-type, array, string, or a block of memory allocated on a stack. Code that is not marked as unsafe can still store and manipulate pointers through the System.IntPtr type, but it cannot dereference them.\n\nManaged memory cannot be explicitly freed; instead, it is automatically garbage collected. Garbage collection addresses the problem of memory leaks by freeing the programmer of responsibility for releasing memory that is no longer needed.\nException\n\nChecked exceptions are not present in C# (in contrast to Java). This has been a conscious decision based on the issues of scalability and versionability.[44]\nPolymorphism\n\nUnlike C++, C# does not support multiple inheritance, although a class can implement any number of interfaces. This was a design decision by the language's lead architect to avoid complication and simplify architectural requirements throughout CLI. When implementing multiple interfaces that contain a method with the same signature, C# allows implementing each method depending on which interface that method is being called through, or, like Java, allows implementing the method once, and have that be the one invocation on a call through any of the class's interfaces.\n\nHowever, unlike Java, C# supports operator overloading. Only the most commonly overloaded operators in C++ may be overloaded in C#.\nFunctional programming\n\nThough primarily an imperative language, C# 2.0 offered limited support for functional programming through first-class functions and closures in the form of anonymous delegates. C# 3.0 expanded support for functional programming with the introduction of a light weight syntax for lambda expressions, extension methods (an affordance for modules), and a list comprehension syntax in the form of a \"query comprehension\" language.\nCommon type system\n\nC# has a unified type system. This unified type system is called Common Type System (CTS).[45]\n\nA unified type system implies that all types, including primitives such as integers, are subclasses of the System.Object class. For example, every type inherits a ToString() method.\nCategories of data types\n\nCTS separates data types into two categories:[45]\n\n    Reference types\n    Value types\n\nInstances of value types do not have referential identity nor referential comparison semantics - equality and inequality comparisons for value types compare the actual data values within the instances, unless the corresponding operators are overloaded. Value types are derived from System.ValueType, always have a default value, and can always be created and copied. Some other limitations on value types are that they cannot derive from each other (but can implement interfaces) and cannot have an explicit default (parameterless) constructor. Examples of value types are all primitive types, such as int (a signed 32-bit integer), float (a 32-bit IEEE floating-point number), char (a 16-bit Unicode code unit), and System.DateTime (identifies a specific point in time with nanosecond precision). Other examples are enum (enumerations) and struct (user defined structures).\n\nIn contrast, reference types have the notion of referential identity - each instance of a reference type is inherently distinct from every other instance, even if the data within both instances is the same. This is reflected in default equality and inequality comparisons for reference types, which test for referential rather than structural equality, unless the corresponding operators are overloaded (such as the case for System.String). In general, it is not always possible to create an instance of a reference type, nor to copy an existing instance, or perform a value comparison on two existing instances, though specific reference types can provide such services by exposing a public constructor or implementing a corresponding interface (such as ICloneable or IComparable). Examples of reference types are object (the ultimate base class for all other C# classes), System.String (a string of Unicode characters), and System.Array (a base class for all C# arrays).\n\nBoth type categories are extensible with user-defined types.\nBoxing and unboxing\n\nBoxing is the operation of converting a value-type object into a value of a corresponding reference type.[45] Boxing in C# is implicit.\n\nUnboxing is the operation of converting a value of a reference type (previously boxed) into a value of a value type.[45] Unboxing in C# requires an explicit type cast. A boxed object of type T can only be unboxed to a T (or a nullable T).[46]\n\nExample:\n\nint foo = 42;         // Value type.\nobject bar = foo;     // foo is boxed to bar.\nint foo2 = (int)bar;  // Unboxed back to value type.\n\nLibraries\n\nThe C# specification details a minimum set of types and class libraries that the compiler expects to have available. In practice, C# is most often used with some implementation of the Common Language Infrastructure (CLI), which is standardized as ECMA-335 Common Language Infrastructure (CLI).\nExamples\n\nThe following is a very simple C# program, a version of the classic \"Hello world\" example:\n\nusing System;\n\nclass Program\n{\n    static void Main()\n    {\n        Console.WriteLine(\"Hello, world!\");\n    }\n}\n\nThe effect is to write the following text to the output console:\n\nHello, world!\n\nEach line has a purpose:\n\nusing System;\n\nThe above line of code tells the compiler to use System as a candidate prefix for types used in the source code. In this case, when the compiler sees use of the Console type later in the source code, it tries to find a type named Console, first in the current assembly, followed by all referenced assemblies. In this case the compiler fails to find such a type, since the name of the type is actually System.Console. The compiler then attempts to find a type named System.Console by using the System prefix from the using statement, and this time it succeeds. The using statement allows the programmer to state all candidate prefixes to use during compilation instead of always using full type names.\n\nclass Program\n\nAbove is a class definition. Everything between the following pair of braces describes Program.\n\nstatic void Main()\n\nThis declares the class member method where the program begins execution. The .NET runtime calls the Main method. (Note: Main may also be called from elsewhere, like any other method, e.g. from another method of Program.) The static keyword makes the method accessible without an instance of Program. Each console application's Main entry point must be declared static. Otherwise, the program would require an instance, but any instance would require a program. To avoid that irresolvable circular dependency, C# compilers processing console applications (like that above) report an error, if there is no static Main method. The void keyword declares that Main has no return value.\n\nConsole.WriteLine(\"Hello, world!\");\n\nThis line writes the output. Console is a static class in the System namespace. It provides an interface to the standard input, output, and error streams for console applications. The program calls the Console method WriteLine, which displays on the console a line with the argument, the string \"Hello world!\".\n\nA GUI example:\n\nusing System.Windows.Forms;\n\nclass Program\n{\n    static void Main()\n    {\n        MessageBox.Show(\"Hello, world!\");\n    }\n}\n\nThis example is similar to the previous example, except that it generates a dialog box that contains the message \"Hello, world!\" instead of writing it to the console.\nStandardization and licensing\n\nIn August 2001, Microsoft Corporation, Hewlett-Packard and Intel Corporation co-sponsored the submission of specifications for C# as well as the Common Language Infrastructure (CLI) to the standards organization Ecma International. In December 2001, ECMA released ECMA-334 C# Language Specification. C# became an ISO standard in 2003 (ISO/IEC 23270:2003 - Information technology — Programming languages — C#). ECMA had previously adopted equivalent specifications as the 2nd edition of C#, in December 2002.\n\nIn June 2005, ECMA approved edition 3 of the C# specification, and updated ECMA-334. Additions included partial classes, anonymous methods, nullable types, and generics (somewhat similar to C++ templates).\n\nIn July 2005, ECMA submitted to ISO/IEC JTC 1, via the latter's Fast-Track process, the standards and related TRs. This process usually takes 6–9 months.\n\nThe C# language definition and the CLI are standardized under ISO and Ecma standards that provide reasonable and non-discriminatory licensing protection from patent claims.\n\nMicrosoft has agreed not to sue open source developers for violating patents in non-profit projects for the part of the framework that is covered by the OSP.[47] Microsoft has also agreed not to enforce patents relating to Novell products against Novell's paying customers[48] with the exception of a list of products that do not explicitly mention C#, .NET or Novell's implementation of .NET (The Mono Project).[49] However, Novell maintains that Mono does not infringe any Microsoft patents.[50] Microsoft has also made a specific agreement not to enforce patent rights related to the Moonlight browser plugin, which depends on Mono, provided it is obtained through Novell.[51]\nImplementations\n\nThe reference C# compiler is Microsoft Visual C#, which is open-source.[52]\n\nMicrosoft is leading the development of a new open-source C# compiler and set of tools, previously codenamed \"Roslyn\". The compiler, which is entirely written in managed code (C#), has been opened up and functionality surfaced as APIs. It is thus enabling developers to create refactoring and diagnostics tools.\n\nOther C# compilers exist, often including an implementation of the Common Language Infrastructure and the .NET class libraries up to .NET 2.0:\n\n    The Mono project provides an open-source C# compiler, a complete open-source implementation of the Common Language Infrastructure including the required framework libraries as they appear in the ECMA specification, and a nearly complete implementation of the Microsoft proprietary .NET class libraries up to .NET 3.5. As of Mono 2.6, no plans exist to implement WPF; WF is planned for a later release; and there are only partial implementations of LINQ to SQL and WCF.[53]\n    The DotGNU project (now discontinued) also provided an open-source C# compiler, a nearly complete implementation of the Common Language Infrastructure including the required framework libraries as they appear in the ECMA specification, and subset of some of the remaining Microsoft proprietary .NET class libraries up to .NET 2.0 (those not documented or included in the ECMA specification, but included in Microsoft's standard .NET Framework distribution).\n    Microsoft's Rotor project (currently called Shared Source Common Language Infrastructure) (licensed for educational and research use only) provides a shared source implementation of the CLR runtime and a C# compiler, and a subset of the required Common Language Infrastructure framework libraries in the ECMA specification (up to C# 2.0, and supported on Windows XP only).", "skillName": "C_Sharp."}
{"id": 200, "category": "Computer_Programming", "skillText": "Objective-C is a general-purpose, object-oriented programming language that adds Smalltalk-style messaging to the C programming language. It was the main programming language used by Apple for the OS X and iOS operating systems, and their respective application programming interfaces (APIs): Cocoa and Cocoa Touch prior to the introduction of Swift.\n\nThe programming language Objective-C was originally developed in the early 1980s. It was selected as the main language used by NeXT for its NeXTSTEP operating system, from which OS X and iOS are derived.[2] Portable Objective-C programs that do not use the Cocoa or Cocoa Touch libraries, or those using parts that may be ported or reimplemented for other systems, can also be compiled for any system supported by GNU Compiler Collection (GCC) or Clang.\n\nObjective-C source code 'implementation' program files usually have .m filename extensions, while Objective-C 'header/interface' files have .h extensions, the same as C header files. Objective-C++ files are denoted with a .mm file extension.\n\nContents\n\n    1 History\n        1.1 Popularization through NeXT\n        1.2 Apple development and Swift\n    2 Syntax\n        2.1 Messages\n        2.2 Interfaces and implementations\n            2.2.1 Interface\n            2.2.2 Implementation\n            2.2.3 Instantiation\n        2.3 Protocols\n        2.4 Dynamic typing\n        2.5 Forwarding\n            2.5.1 Example\n            2.5.2 Notes\n        2.6 Categories\n            2.6.1 Example usage of categories\n            2.6.2 Notes\n        2.7 Posing\n        2.8 #import\n    3 Other features\n    4 Language variants\n        4.1 Objective-C++\n        4.2 Objective-C 2.0\n            4.2.1 Garbage collection\n            4.2.2 Properties\n            4.2.3 Non-fragile instance variables\n            4.2.4 Fast enumeration\n            4.2.5 Class extensions\n            4.2.6 Implications for Cocoa development\n        4.3 Blocks\n        4.4 Modern Objective-C\n            4.4.1 Automatic Reference Counting\n            4.4.2 Literals\n            4.4.3 Subscripting\n        4.5 \"Modern\" Objective-C syntax (1997)\n        4.6 Portable Object Compiler\n        4.7 GEOS Objective-C\n        4.8 Clang\n    5 Library use\n    6 Analysis of the language\n        6.1 Memory management\n        6.2 Philosophical differences between Objective-C and C++\n    7 See also\n    8 References\n    9 Further reading\n    10 External links\n\nHistory\n\nObjective-C was created primarily by Brad Cox and Tom Love in the early 1980s at their company Stepstone.[3] Both had been introduced to Smalltalk while at ITT Corporation's Programming Technology Center in 1981. The earliest work on Objective-C traces back to around that time.[4] Cox was intrigued by problems of true reusability in software design and programming. He realized that a language like Smalltalk would be invaluable in building development environments for system developers at ITT. However, he and Tom Love also recognized that backward compatibility with C was critically important in ITT's telecom engineering milieu.[5]\n\nCox began writing a pre-processor for C to add some of the abilities of Smalltalk. He soon had a working implementation of an object-oriented extension to the C language, which he called \"OOPC\" for Object-Oriented Pre-Compiler.[6] Love was hired by Schlumberger Research in 1982 and had the opportunity to acquire the first commercial copy of Smalltalk-80, which further influenced the development of their brainchild.\n\nIn order to demonstrate that real progress could be made, Cox showed that making interchangeable software components really needed only a few practical changes to existing tools. Specifically, they needed to support objects in a flexible manner, come supplied with a usable set of libraries, and allow for the code (and any resources needed by the code) to be bundled into one cross-platform format.\n\nLove and Cox eventually formed a new venture, Productivity Products International (PPI), to commercialize their product, which coupled an Objective-C compiler with class libraries. In 1986, Cox published the main description of Objective-C in its original form in the book Object-Oriented Programming, An Evolutionary Approach. Although he was careful to point out that there is more to the problem of reusability than just the language, Objective-C often found itself compared feature for feature with other languages.\nPopularization through NeXT\n\nIn 1988, NeXT licensed Objective-C from StepStone (the new name of PPI, the owner of the Objective-C trademark) and extended the GCC compiler to support Objective-C. NeXT developed the AppKit and Foundation Kit libraries on which the NeXTSTEP user interface and Interface Builder were based. While the NeXT workstations failed to make a great impact in the marketplace, the tools were widely lauded in the industry. This led NeXT to drop hardware production and focus on software tools, selling NeXTSTEP (and OpenStep) as a platform for custom programming.\n\nIn order to circumvent the terms of the GPL, NeXT had originally intended to ship the Objective-C frontend separately, allowing the user to link it with GCC to produce the compiler executable. After being initially accepted by Richard M. Stallman, this plan was rejected after Stallman consulted with GNU's lawyers and NeXT agreed to make Objective-C part of GCC.[7]\n\nThe work to extend GCC was led by Steve Naroff, who joined NeXT from StepStone. The compiler changes were made available as per GPL license terms, but the runtime libraries were not, rendering the open source contribution unusable to the general public. This led to other parties developing such runtime libraries under open source license. Later, Steve Naroff was also principal contributor to work at Apple to build the Objective-C frontend to Clang.\n\nThe GNU project started work on its free software implementation of Cocoa, named GNUstep, based on the OpenStep standard.[8] Dennis Glatting wrote the first GNU Objective-C runtime in 1992. The GNU Objective-C runtime, which has been in use since 1993, is the one developed by Kresten Krab Thorup when he was a university student in Denmark.[citation needed] Thorup also worked at NeXT from 1993 to 1996.[9]\nApple development and Swift\n\nAfter acquiring NeXT in 1996, Apple Computer used OpenStep in its new operating system, Mac OS X. This included Objective-C, NeXT's Objective-C based developer tool, Project Builder, and its interface design tool, Interface Builder (both now merged into one Xcode application). Most of Apple's present-day Cocoa API is based on OpenStep interface objects, and is the most significant Objective-C environment being used for active development.\n\nAt WWDC 2014, Apple introduced a new language, Swift, which was characterized as \"Objective-C without the C\".\nSyntax\n\nObjective-C is a thin layer atop C, and is a \"strict superset\" of C, meaning that it is possible to compile any C program with an Objective-C compiler, and to freely include C language code within an Objective-C class.[10][11][12][13][14][15]\n\nObjective-C derives its object syntax from Smalltalk. All of the syntax for non-object-oriented operations (including primitive variables, pre-processing, expressions, function declarations, and function calls) are identical to those of C, while the syntax for object-oriented features is an implementation of Smalltalk-style messaging.\nMessages\n\nThe Objective-C model of object-oriented programming is based on message passing to object instances. In Objective-C one does not call a method; one sends a message. This is unlike the Simula-style programming model used by C++. The difference between these two concepts is in how the code referenced by the method or message name is executed. In a Simula-style language, the method name is in most cases bound to a section of code in the target class by the compiler. In Smalltalk and Objective-C, the target of a message is resolved at runtime, with the receiving object itself interpreting the message. A method is identified by a selector or SEL — a NUL-terminated string representing its name — and resolved to a C method pointer implementing it: an IMP.[16] A consequence of this is that the message-passing system has no type checking. The object to which the message is directed — the receiver — is not guaranteed to respond to a message, and if it does not, it simply raises an exception.[17]\n\nSending the message method to the object pointed to by the pointer obj would require the following code in C++:\n\nobj->method(argument);\n\nIn Objective-C, this is written as follows:\n\n[obj method:argument];\n\nBoth styles of programming have their strengths and weaknesses. Object-oriented programming in the Simula (C++) style allows multiple inheritance and faster execution by using compile-time binding whenever possible, but it does not support dynamic binding by default. It also forces all methods to have a corresponding implementation unless they are abstract. The Smalltalk-style programming as used in Objective-C allows messages to go unimplemented, with the method resolved to its implementation at runtime. For example, a message may be sent to a collection of objects, to which only some will be expected to respond, without fear of producing runtime errors. Message passing also does not require that an object be defined at compile time. An implementation is still required for the method to be called in the derived object. (See the dynamic typing section below for more advantages of dynamic (late) binding.)\nInterfaces and implementations\n\nObjective-C requires that the interface and implementation of a class be in separately declared code blocks. By convention, developers place the interface in a header file and the implementation in a code file. The header files, normally suffixed .h, are similar to C header files while the implementation (method) files, normally suffixed .m, can be very similar to C code files.\nInterface\n\nIn other programming languages, this is called a \"class declaration\".\n\nThe interface of a class is usually defined in a header file. A common convention is to name the header file after the name of the class, e.g. Ball.h would contain the interface for the class Ball.\n\nAn interface declaration takes the form:\n\n@interface classname : superclassname {\n // instance variables\n}\n+ classMethod1;\n+ (return_type)classMethod2;\n+ (return_type)classMethod3:(param1_type)param1_varName;\n\n- (return_type)instanceMethod1With1Parameter:(param1_type)param1_varName;\n- (return_type)instanceMethod2With2Parameters:(param1_type)param1_varName param2_callName:(param2_type)param2_varName;\n@end\n\nIn the above, plus signs denote class methods, or methods that can be called on the class itself (not on an instance), and minus signs denote instance methods, which can only be called on a particular instance of the class. Class methods also have no access to instance variables.\n\nThe code above is roughly equivalent to the following C++ interface:\n\nclass classname : public superclassname {\n protected:\n // instance variables\n\n public:\n // Class (static) functions\n static void * classMethod1();\n static return_type classMethod2();\n static return_type classMethod3(param1_type param1_varName);\n\n // Instance (member) functions\n return_type instanceMethod1With1Parameter (param1_type param1_varName);\n return_type instanceMethod2With2Parameters (param1_type param1_varName, param2_type param2_varName=default);\n};\n\nNote that instanceMethod2With2Parameters:param2_callName: demonstrates the interleaving of selector segments with argument expressions, for which there is no direct equivalent in C/C++.\n\nReturn types can be any standard C type, a pointer to a generic Objective-C object, a pointer to a specific type of object such as NSArray *, NSImage *, or NSString *, or a pointer to the class to which the method belongs (instancetype). The default return type is the generic Objective-C type id.\n\nMethod arguments begin with a name labeling the argument that is part of the method name, followed by a colon followed by the expected argument type in parentheses and the argument name. The label can be omitted.\n\n- (void)setRangeStart:(int)start end:(int)end;\n- (void)importDocumentWithName:(NSString *)name withSpecifiedPreferences:\n(Preferences *)prefs beforePage:(int)insertPage;\n\nImplementation\n\nThe interface only declares the class interface and not the methods themselves: the actual code is written in the implementation file. Implementation (method) files normally have the file extension .m, which originally signified \"messages\".[18]\n\n@implementation classname\n+ (return_type)classMethod\n{\n // implementation\n}\n- (return_type)instanceMethod\n{\n // implementation\n}\n@end\n\nMethods are written using their interface declarations. Comparing Objective-C and C:\n\n- (int)method:(int)i\n{\n return [self square_root:i];\n}\n\nint function (int i)\n{\n return square_root(i);\n}\n\nThe syntax allows pseudo-naming of arguments.\n\n- (int)changeColorToRed:(float)red green:(float)green blue:(float)blue;\n\n[myColor changeColorToRed:5.0 green:2.0 blue:6.0];\n\nInternal representations of a method vary between different implementations of Objective-C. If myColor is of the class Color, instance method -changeColorToRed:green:blue: might be internally labeled _i_Color_changeColorToRed_green_blue. The i is to refer to an instance method, with the class and then method names appended and colons changed to underscores. As the order of parameters is part of the method name, it cannot be changed to suit coding style or expression as with true named parameters.\n\nHowever, internal names of the function are rarely used directly. Generally, messages are converted to function calls defined in the Objective-C runtime library. It is not necessarily known at link time which method will be called because the class of the receiver (the object being sent the message) need not be known until runtime.\nInstantiation\n\nOnce an Objective-C class is written, it can be instantiated. This is done by first allocating an uninitialized instance of the class (an object) and then by initializing it. An object is not fully functional until both steps have been completed. These steps should be accomplished with one line of code so that there is never an allocated object that hasn't undergone initialization (and because it is unwise to keep the intermediate result since -init can return a different object than that on which it is called).\n\nInstantiation with the default, no-parameter initializer:\n\nMyObject *o = [[MyObject alloc] init];\n\nInstantiation with a custom initializer:\n\nMyObject *o = [[MyObject alloc] initWithString:myString];\n\nIn the case where no custom initialization is being performed, the \"new\" method can often be used in place of the alloc-init messages:\n\nMyObject *o = [MyObject new];\n\nAlso, some classes implement class method initializers. Like +new, they combine +alloc and -init, but unlike +new, they return an autoreleased instance. Some class method initializers take parameters:\n\nMyObject *o = [MyObject object];\nMyObject *o2 = [MyObject objectWithString:myString];\n\nThe alloc message allocates enough memory to hold all the instance variables for an object, sets all the instance variables to zero values, and turns the memory into an instance of the class; at no point during the initialization is the memory an instance of the superclass.\n\nThe init message performs the set-up of the instance upon creation. The init method is often written as follows:\n\n- (id)init {\n    self = [super init];\n    if (self) {\n        // perform initialization of object here\n    }\n    return self;\n}\n\nIn the above example, notice the id return type. This type stands for \"pointer to any object\" in Objective-C (See the Dynamic typing section).\n\nThe initializer pattern is used to assure that the object is properly initialized by its superclass before the init method performs its initialization. It performs the following actions:\n\n    self = [super init]\n\n        Sends the superclass instance an init message and assigns the result to self (pointer to the current object).\n\n    if (self)\n\n        Checks if the returned object pointer is valid before performing any initialization.\n\n    return self\n\n        Returns the value of self to the caller.\n\nA non-valid object pointer has the value nil; conditional statements like \"if\" treat nil like a null pointer, so the initialization code will not be executed if [super init] returned nil. If there is an error in initialization the init method should perform any necessary cleanup, including sending a \"release\" message to self, and return nil to indicate that initialization failed. Any checking for such errors must only be performed after having called the superclass initialization to ensure that destroying the object will be done correctly.\n\nIf a class has more than one initialization method, only one of them (the \"designated initializer\") needs to follow this pattern; others should call the designated initializer instead of the superclass initializer.\nProtocols\n\nIn other programming languages, these are called \"interfaces\".\n\nObjective-C was extended at NeXT to introduce the concept of multiple inheritance of specification, but not implementation, through the introduction of protocols. This is a pattern achievable either as an abstract multiple inherited base class in C++, or as an \"interface\" (as in Java and C#). Objective-C makes use of ad hoc protocols called informal protocols and compiler-enforced protocols called formal protocols.\n\nAn informal protocol is a list of methods that a class can opt to implement. It is specified in the documentation, since it has no presence in the language. Informal protocols are implemented as a category (see below) on NSObject and often include optional methods, which, if implemented, can change the behavior of a class. For example, a text field class might have a delegate that implements an informal protocol with an optional method for performing auto-completion of user-typed text. The text field discovers whether the delegate implements that method (via reflection) and, if so, calls the delegate's method to support the auto-complete feature.\n\nA formal protocol is similar to an interface in Java, C#, and Ada 2005. It is a list of methods that any class can declare itself to implement. Versions of Objective-C before 2.0 required that a class must implement all methods in a protocol it declares itself as adopting; the compiler will emit an error if the class does not implement every method from its declared protocols. Objective-C 2.0 added support for marking certain methods in a protocol optional, and the compiler will not enforce implementation of optional methods.\n\nA class must be declared to implement that protocol to be said to conform to it. This is detectable at runtime. Formal protocols cannot provide any implementations; they simply assure callers that classes that conform to the protocol will provide implementations. In the NeXT/Apple library, protocols are frequently used by the Distributed Objects system to represent the abilities of an object executing on a remote system.\n\nThe syntax\n\n@protocol NSLocking\n- (void)lock;\n- (void)unlock;\n@end\n\ndenotes that there is the abstract idea of locking. By stating in the class definition that the protocol is implemented,\n\n@interface NSLock : NSObject <NSLocking>\n//...\n@end\n\ninstances of NSLock claim that they will provide an implementation for the two instance methods.\nDynamic typing\n\nObjective-C, like Smalltalk, can use dynamic typing: an object can be sent a message that is not specified in its interface. This can allow for increased flexibility, as it allows an object to \"capture\" a message and send the message to a different object that can respond to the message appropriately, or likewise send the message on to another object. This behavior is known as message forwarding or delegation (see below). Alternatively, an error handler can be used in case the message cannot be forwarded. If an object does not forward a message, respond to it, or handle an error, then the system will generate a runtime exception.[19] If messages are sent to nil (the null object pointer), they will be silently ignored or raise a generic exception, depending on compiler options.\n\nStatic typing information may also optionally be added to variables. This information is then checked at compile time. In the following four statements, increasingly specific type information is provided. The statements are equivalent at runtime, but the extra information allows the compiler to warn the programmer if the passed argument does not match the type specified.\n\n- (void)setMyValue:(id)foo;\n\nIn the above statement, foo may be of any class.\n\n- (void)setMyValue:(id<NSCopying>)foo;\n\nIn the above statement, foo may be an instance of any class that conforms to the NSCopying protocol.\n\n- (void)setMyValue:(NSNumber *)foo;\n\nIn the above statement, foo must be an instance of the NSNumber class.\n\n- (void)setMyValue:(NSNumber<NSCopying> *)foo;\n\nIn the above statement, foo must be an instance of the NSNumber class, and it must conform to the NSCopying protocol.\nForwarding\n\nObjective-C permits the sending of a message to an object that may not respond. Rather than responding or simply dropping the message, an object can forward the message to an object that can respond. Forwarding can be used to simplify implementation of certain design patterns, such as the observer pattern or the proxy pattern.\n\nThe Objective-C runtime specifies a pair of methods in Object\n\n    forwarding methods:\n\n    - (retval_t)forward:(SEL)sel args:(arglist_t)args; // with GCC\n    - (id)forward:(SEL)sel args:(marg_list)args; // with NeXT/Apple systems\n\n    action methods:\n\n    - (retval_t)performv:(SEL)sel args:(arglist_t)args; // with GCC\n    - (id)performv:(SEL)sel args:(marg_list)args; // with NeXT/Apple systems\n\nAn object wishing to implement forwarding needs only to override the forwarding method with a new method to define the forwarding behavior. The action method performv:: need not be overridden, as this method merely performs an action based on the selector and arguments. Notice the SEL type, which is the type of messages in Objective-C.\n\nNote: in OpenStep, Cocoa, and GNUstep, the commonly used frameworks of Objective-C, one does not use the Object class. The - (void)forwardInvocation:(NSInvocation *)anInvocation method of the NSObject class is used to do forwarding.\nExample\n\nHere is an example of a program that demonstrates the basics of forwarding.\n\nForwarder.h\n\n# import <objc/Object.h>\n\n@interface Forwarder : Object {\n id recipient; //The object we want to forward the message to.\n}\n\n//Accessor methods.\n- (id)recipient;\n- (id)setRecipient:(id)_recipient;\n\n@end\n\nForwarder.m\n\n# import \"Forwarder.h\"\n\n@implementation Forwarder\n\n- (retval_t)forward:(SEL)sel args:(arglist_t) args {\n /*\n * Check whether the recipient actually responds to the message.\n * This may or may not be desirable, for example, if a recipient\n * in turn does not respond to the message, it might do forwarding\n * itself.\n */\n if([recipient respondsToSelector:sel]) {\n  return [recipient performv:sel args:args];\n } else {\n  return [self error:\"Recipient does not respond\"];\n }\n}\n\n- (id)setRecipient:(id)_recipient {\n [recipient autorelease];\n recipient = [_recipient retain];\n return self;\n}\n\n- (id) recipient {\n return recipient;\n}\n@end\n\nRecipient.h\n\n# import <objc/Object.h>\n\n// A simple Recipient object.\n@interface Recipient : Object\n- (id)hello;\n@end\n\nRecipient.m\n\n# import \"Recipient.h\"\n\n@implementation Recipient\n\n- (id)hello {\n printf(\"Recipient says hello!\\n\");\n\n return self;\n}\n\n@end\n\nmain.m\n\n# import \"Forwarder.h\"\n# import \"Recipient.h\"\n\nint main(void) {\n Forwarder *forwarder = [Forwarder new];\n Recipient *recipient = [Recipient new];\n\n [forwarder setRecipient:recipient]; //Set the recipient.\n /*\n * Observe forwarder does not respond to a hello message! It will\n * be forwarded. All unrecognized methods will be forwarded to\n * the recipient\n * (if the recipient responds to them, as written in the Forwarder)\n */\n [forwarder hello];\n\n [recipient release];\n [forwarder release];\n\n return 0;\n}\n\nNotes\n\nWhen compiled using gcc, the compiler reports:\n\n$ gcc -x objective-c -Wno-import Forwarder.m Recipient.m main.m -lobjc\nmain.m: In function `main':\nmain.m:12: warning: `Forwarder' does not respond to `hello'\n$\n\nThe compiler is reporting the point made earlier, that Forwarder does not respond to hello messages. In this circumstance, it is safe to ignore the warning since forwarding was implemented. Running the program produces this output:\n\n$ ./a.out\nRecipient says hello!\n\nCategories\n\nDuring the design of Objective-C, one of the main concerns was the maintainability of large code bases. Experience from the structured programming world had shown that one of the main ways to improve code was to break it down into smaller pieces. Objective-C borrowed and extended the concept of categories from Smalltalk implementations to help with this process.[20]\n\nFurthermore, the methods within a category are added to a class at run-time. Thus, categories permit the programmer to add methods to an existing class without the need to recompile that class or even have access to its source code. For example, if a system does not contain a spell checker in its String implementation, it could be added without modifying the String source code.\n\nMethods within categories become indistinguishable from the methods in a class when the program is run. A category has full access to all of the instance variables within the class, including private variables.\n\nIf a category declares a method with the same method signature as an existing method in a class, the category's method is adopted. Thus categories can not only add methods to a class, but also replace existing methods. This feature can be used to fix bugs in other classes by rewriting their methods, or to cause a global change to a class's behavior within a program. If two categories have methods with the same name (not to be confused with method signature), it is undefined which category's method is adopted.\n\nOther languages have attempted to add this feature in a variety of ways. TOM took the Objective-C system a step further and allowed for the addition of variables also. Other languages have used prototype-based solutions instead, the most notable being Self.\n\nThe C# and Visual Basic.NET languages implement superficially similar functionality in the form of extension methods, but these lack access to the private variables of the class.[21] Ruby and several other dynamic programming languages refer to the technique as \"monkey patching\".\n\nLogtalk implements a concept of categories (as first-class entities) that subsumes Objective-C categories functionality (Logtalk categories can also be used as fine-grained units of composition when defining e.g. new classes or prototypes; in particular, a Logtalk category can be virtually imported by any number of classes and prototypes).\nExample usage of categories\n\nThis example builds up an Integer class, by defining first a basic class with only accessor methods implemented, and adding two categories, Arithmetic and Display, which extend the basic class. While categories can access the base class's private data members, it is often good practice to access these private data members through the accessor methods, which helps keep categories more independent from the base class. Implementing such accessors is one typical usage of categories. Another is to use categories to add methods to the base class. However, it is not regarded as good practice to use categories for subclass overriding, also known as monkey patching. Informal protocols are implemented as a category on the base NSObject class. By convention, files containing categories that extend base classes will take the name BaseClass+ExtensionClass.h.\n\nInteger.h\n\n# import <objc/Object.h>\n\n@interface Integer : Object {\n int integer;\n}\n\n- (int) integer;\n- (id) integer: (int) _integer;\n@end\n\nInteger.m\n\n# import \"Integer.h\"\n\n@implementation Integer\n- (int) integer {\n return integer;\n}\n\n- (id) integer: (int) _integer {\n integer = _integer;\n\n return self;\n}\n@end\n\nInteger+Arithmetic.h\n\n# import \"Integer.h\"\n\n@interface Integer (Arithmetic)\n- (id) add: (Integer *) addend;\n- (id) sub: (Integer *) subtrahend;\n@end\n\nInteger+Arithmetic.m\n\n# import \"Integer+Arithmetic.h\"\n\n@implementation Integer (Arithmetic)\n- (id) add: (Integer *) addend {\n return [self integer: [self integer] + [addend integer]];\n}\n\n- (id) sub: (Integer *) subtrahend {\n return [self integer: [self integer] - [subtrahend integer]];\n}\n@end\n\nInteger+Display.h\n\n# import \"Integer.h\"\n\n@interface Integer (Display)\n- (id) showstars;\n- (id) showint;\n@end\n\nInteger+Display.m\n\n# import \"Integer+Display.h\"\n\n@implementation Integer (Display)\n- (id) showstars {\n int i, x = [self integer];\n for (i = 0; i < x; i++) {\n printf(\"*\");\n }\n printf(\"\\n\");\n\n return self;\n}\n\n- (id) showint {\n printf(\"%d\\n\", [self integer]);\n\n return self;\n}\n@end\n\nmain.m\n\n# import \"Integer.h\"\n# import \"Integer+Arithmetic.h\"\n# import \"Integer+Display.h\"\n\nint main(void) {\n Integer *num1 = [Integer new], *num2 = [Integer new];\n int x;\n\n printf(\"Enter an integer: \");\n scanf(\"%d\", &x);\n\n [num1 integer:x];\n [num1 showstars];\n\n printf(\"Enter an integer: \");\n scanf(\"%d\", &x);\n\n [num2 integer:x];\n [num2 showstars];\n\n [num1 add:num2];\n [num1 showint];\n\n return 0;\n}\n\nNotes\n\nCompilation is performed, for example, by:\n\ngcc -x objective-c main.m Integer.m Integer+Arithmetic.m Integer+Display.m -lobjc\n\nOne can experiment by leaving out the #import \"Integer+Arithmetic.h\" and [num1 add:num2] lines and omitting Integer+Arithmetic.m in compilation. The program will still run. This means that it is possible to mix-and-match added categories if needed; if a category does not need to have some ability, it can simply not be compile in.\nPosing\n\nObjective-C permits a class to wholly replace another class within a program. The replacing class is said to \"pose as\" the target class.\n\nClass posing was declared deprecated with Mac OS X v10.5, and is unavailable in the 64-bit runtime. Similar functionality can be achieved by using method swizzling in categories, that swaps one method's implementation with another's that have the same signature.\n\nFor the versions still supporting posing, all messages sent to the target class are instead received by the posing class. There are several restrictions:\n\n    A class may only pose as one of its direct or indirect superclasses.\n    The posing class must not define any new instance variables that are absent from the target class (though it may define or override methods).\n    The target class may not have received any messages prior to the posing.\n\nPosing, similarly with categories, allows global augmentation of existing classes. Posing permits two features absent from categories:\n\n    A posing class can call overridden methods through super, thus incorporating the implementation of the target class.\n    A posing class can override methods defined in categories.\n\nFor example,\n\n@interface CustomNSApplication : NSApplication\n@end\n\n@implementation CustomNSApplication\n- (void) setMainMenu: (NSMenu*) menu {\n // do something with menu\n}\n@end\n\nclass_poseAs ([CustomNSApplication class], [NSApplication class]);\n\nThis intercepts every invocation of setMainMenu to NSApplication.\n#import\n\nIn the C language, the #include pre-compile directive always causes a file's contents to be inserted into the source at that point. Objective-C has the #import directive, equivalent except that each file is included only once per compilation unit, obviating the need for include guards.\nOther features\n\nObjective-C's features often allow for flexible, and often easy, solutions to programming issues.\n\n    Delegating methods to other objects and remote invocation can be easily implemented using categories and message forwarding.\n    Swizzling of the isa pointer allows for classes to change at runtime. Typically used for debugging where freed objects are swizzled into zombie objects whose only purpose is to report an error when someone calls them. Swizzling was also used in Enterprise Objects Framework to create database faults[citation needed]. Swizzling is used today by Apple's Foundation Framework to implement Key-Value Observing.\n\nLanguage variants\nObjective-C++\n\nObjective-C++ is a language variant accepted by the front-end to the GNU Compiler Collection and Clang, which can compile source files that use a combination of C++ and Objective-C syntax. Objective-C++ adds to C++ the extensions that Objective-C adds to C. As nothing is done to unify the semantics behind the various language features, certain restrictions apply:\n\n    A C++ class cannot derive from an Objective-C class and vice versa.\n    C++ namespaces cannot be declared inside an Objective-C declaration.\n    Objective-C declarations may appear only in global scope, not inside a C++ namespace\n    Objective-C classes cannot have instance variables of C++ classes that lack a default constructor or that have one or more virtual methods,[citation needed] but pointers to C++ objects can be used as instance variables without restriction (allocate them with new in the -init method).\n    C++ \"by value\" semantics cannot be applied to Objective-C objects, which are only accessible through pointers.\n    An Objective-C declaration cannot be within a C++ template declaration and vice versa. However, Objective-C types, (e.g., Classname *) can be used as C++ template parameters.\n    Objective-C and C++ exception handling is distinct; the handlers of each cannot handle exceptions of the other type. This is mitigated in recent runtimes as Objective-C exceptions are either replaced by C++ exceptions completely (Apple runtime), or partly when Objective-C++ library is linked (GNUstep libobjc2).\n    Care must be taken since the destructor calling conventions of Objective-C and C++'s exception run-time models do not match (i.e., a C++ destructor will not be called when an Objective-C exception exits the C++ object's scope). The new 64-bit runtime resolves this by introducing interoperability with C++ exceptions in this sense.[22]\n    Objective-C blocks and C++11 lambdas are distinct entities, however a block is transparently generated on Mac OS X when passing a lambda where a block is expected.[23]\n\nObjective-C 2.0\n\nAt the 2006 Worldwide Developers Conference, Apple announced the release of \"Objective-C 2.0,\" a revision of the Objective-C language to include \"modern garbage collection, syntax enhancements,[24] runtime performance improvements,[25] and 64-bit support\". Mac OS X v10.5, released in October 2007, included an Objective-C 2.0 compiler. GCC 4.6 supports many new Objective-C features, such as declared and synthesized properties, dot syntax, fast enumeration, optional protocol methods, method/protocol/class attributes, class extensions and a new GNU Objective-C runtime API.[26]\nGarbage collection\n\nObjective-C 2.0 provided an optional conservative, generational garbage collector. When run in backwards-compatible mode, the runtime turned reference counting operations such as \"retain\" and \"release\" into no-ops. All objects were subject to garbage collection when garbage collection was enabled. Regular C pointers could be qualified with \"__strong\" to also trigger the underlying write-barrier compiler intercepts and thus participate in garbage collection.[27] A zero-ing weak subsystem was also provided such that pointers marked as \"__weak\" are set to zero when the object (or more simply, GC memory) is collected. The garbage collector does not exist on the iOS implementation of Objective-C 2.0.[28] Garbage collection in Objective-C runs on a low-priority background thread, and can halt on user events, with the intention of keeping the user experience responsive.[29]\n\nGarbage collection was deprecated in OS X v10.8 in favor of Automatic Reference Counting (ARC).[30] Objective-C on iOS 7 running on ARM64 uses 19 bits out of a 64-bit word to store the reference count, as a form of tagged pointers.[31][32]\nProperties\n\nObjective-C 2.0 introduces a new syntax to declare instance variables as properties, with optional attributes to configure the generation of accessor methods. Properties are, in a sense, public instance variables; that is, declaring an instance variable as a property provides external classes with access (possibly limited, e.g. read only) to that property. A property may be declared as \"readonly\", and may be provided with storage semantics such as assign, copy or retain. By default, properties are considered atomic, which results in a lock preventing multiple threads from accessing them at the same time. A property can be declared as nonatomic, which removes this lock.\n\n@interface Person : NSObject {\n @public\n NSString *name;\n @private\n int age;\n}\n\n@property(copy) NSString *name;\n@property(readonly) int age;\n\n-(id)initWithAge:(int)age;\n@end\n\nProperties are implemented by way of the @synthesize keyword, which generates getter (and setter, if not read-only) methods according to the property declaration. Alternatively, the getter and setter methods must be implemented explicitly, or the @dynamic keyword can be used to indicate that accessor methods will be provided by other means. When compiled using clang 3.1 or higher, all properties which are not explicitly declared with @dynamic, marked readonly or have complete user-implemented getter and setter will be automatically implicitly @synthesize'd.\n\n@implementation Person\n@synthesize name;\n\n-(id)initWithAge:(int)initAge {\n self = [super init];\n if (self) {\n age = initAge; // NOTE: direct instance variable assignment, not property setter\n }\n return self;\n}\n\n-(int)age {\n return age;\n}\n@end\n\nProperties can be accessed using the traditional message passing syntax, dot notation, or, in Key-Value Coding, by name via the \"valueForKey:\"/\"setValue:forKey:\" methods.\n\nPerson *aPerson = [[Person alloc] initWithAge: 53];\naPerson.name = @\"Steve\"; // NOTE: dot notation, uses synthesized setter,\n // equivalent to [aPerson setName: @\"Steve\"];\nNSLog(@\"Access by message (%@), dot notation(%@),\nproperty name(%@) and direct instance variable access (%@)\",\n [aPerson name], aPerson.name, [aPerson valueForKey:@\"name\"], aPerson->name);\n\nIn order to use dot notation to invoke property accessors within an instance method, the \"self\" keyword should be used:\n\n-(void) introduceMyselfWithProperties:(BOOL)useGetter {\n NSLog(@\"Hi, my name is %@.\", (useGetter ? self.name : name));\n// NOTE: getter vs. ivar access\n}\n\nA class or protocol's properties may be dynamically introspected.\n\nint i;\nint propertyCount = 0;\nobjc_property_t *propertyList = class_copyPropertyList([aPerson class], &propertyCount);\n\nfor (i = 0; i < propertyCount; i++) {\n objc_property_t *thisProperty = propertyList + i;\n const char* propertyName = property_getName(*thisProperty);\n NSLog(@\"Person has a property: '%s'\", propertyName);\n}\n\nNon-fragile instance variables\n\nObjective-C 2.0 provides non-fragile instance variables where supported by the runtime (i.e. when building code for 64-bit Mac OS X, and all iOS). Under the modern runtime, an extra layer of indirection is added to instance variable access, allowing the dynamic linker to adjust instance layout at runtime. This feature allows for two important improvements to Objective-C code:\n\n    It eliminates the fragile binary interface problem; superclasses can change sizes without affecting binary compatibility.\n    It allows instance variables that provide the backing for properties to be synthesized at runtime without them being declared in the class's interface.\n\nFast enumeration\n\nInstead of using an NSEnumerator object or indices to iterate through a collection, Objective-C 2.0 offers the fast enumeration syntax. In Objective-C 2.0, the following loops are functionally equivalent, but have different performance traits.\n\n// Using NSEnumerator\nNSEnumerator *enumerator = [thePeople objectEnumerator];\nPerson *p;\n\nwhile ((p = [enumerator nextObject]) != nil) {\n NSLog(@\"%@ is %i years old.\", [p name], [p age]);\n}\n\n// Using indexes\nfor (int i = 0; i < [thePeople count]; i++) {\n Person *p = [thePeople objectAtIndex:i];\n NSLog(@\"%@ is %i years old.\", [p name], [p age]);\n}\n\n// Using fast enumeration\nfor (Person *p in thePeople) {\n NSLog(@\"%@ is %i years old.\", [p name], [p age]);\n}\n\nFast enumeration generates more efficient code than standard enumeration because method calls to enumerate over objects are replaced by pointer arithmetic using the NSFastEnumeration protocol.[33]\nClass extensions\n\nA class extension has the same syntax as a category declaration with no category name, and the methods and properties declared in it are added directly to the main class. It is mostly used as an alternative to a category to add methods to a class without advertising them in the public headers, with the advantage that for class extensions the compiler checks that all the privately declared methods are actually implemented.[34]\nImplications for Cocoa development\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (November 2012) (Learn how and when to remove this template message)\n\nAll Objective-C applications developed for Mac OS X that make use of the above improvements for Objective-C 2.0 are incompatible with all operating systems prior to 10.5 (Leopard). Since fast enumeration does not generate exactly the same binaries as standard enumeration, its use will cause an application to crash on OS X version 10.4 or earlier.\nBlocks\nMain article: Blocks (C language extension)\n\nBlocks is a nonstandard extension for Objective-C (and C and C++) that uses special syntax to create closures. Blocks are only supported in Mac OS X 10.6 \"Snow Leopard\" or later, iOS 4 or later, and GNUstep with libobjc2 1.7 and compiling with clang 3.1 or later.[35]\n\n#include <stdio.h>\n#include <Block.h>\ntypedef int (^IntBlock)();\n\nIntBlock MakeCounter(int start, int increment) {\n\t__block int i = start;\n\t\n\treturn Block_copy( ^ {\n\t\tint ret = i;\n\t\ti += increment;\n\t\treturn ret;\n\t});\n\t\n}\n\nint main(void) {\n\tIntBlock mycounter = MakeCounter(5, 2);\n\tprintf(\"First call: %d\\n\", mycounter());\n\tprintf(\"Second call: %d\\n\", mycounter());\n\tprintf(\"Third call: %d\\n\", mycounter());\n\t\n\t/* because it was copied, it must also be released */\n\tBlock_release(mycounter);\n\t\n\treturn 0;\n}\n/* Output:\n\tFirst call: 5\n\tSecond call: 7\n\tThird call: 9\n*/\n\nModern Objective-C\nAutomatic Reference Counting\nMain article: Automatic Reference Counting\n\nAutomatic Reference Counting (ARC) is a compile-time feature that eliminates the need for programmers to manually manage retain counts using retain and release.[36] Unlike garbage collection, which occurs at run time, ARC eliminates the overhead of a separate process managing retain counts. ARC and manual memory management are not mutually exclusive; programmers can continue to use non-ARC code in ARC-enabled projects by disabling ARC for individual code files. XCode can also attempt to automatically upgrade a project to ARC.\nLiterals\n\nNeXT and Apple Obj-C runtimes have long included a short-form way to create new strings, using the literal syntax @\"a new string\", or drop to CoreFoundation constants kCFBooleanTrue and kCFBooleanFalse for NSNumber with Boolean values. Using this format saves the programmer from having to use the longer initWithString or similar methods when doing certain operations.\n\nWhen using Apple LLVM compiler 4.0 or later, arrays, dictionaries, and numbers (NSArray, NSDictionary, NSNumber classes) can also be created using literal syntax instead of methods.[37]\n\nExample without literals:\n\nNSArray *myArray = [NSArray arrayWithObjects:object1,object2,object3,nil];\nNSDictionary *myDictionary1 = [NSDictionary dictionaryWithObject:someObject forKey:@\"key\"];\nNSDictionary *myDictionary2 = [NSDictionary dictionaryWithObjectsAndKeys:object1, key1, object2, key2, nil];\nNSNumber *myNumber = [NSNumber numberWithInt:myInt];\nNSNumber *mySumNumber= [NSNumber numberWithInt:(2 + 3)];\nNSNumber *myBoolNumber = [NSNumber numberWithBool:YES];\n\nExample with literals:\n\nNSArray *myArray = @[ object1, object2, object3 ];\nNSDictionary *myDictionary1 = @{ @\"key\" : someObject };\nNSDictionary *myDictionary2 = @{ key1: object1, key2: object2 };\nNSNumber *myNumber = @(myInt);\nNSNumber *mySumNumber = @(2+3);\nNSNumber *myBoolNumber = @YES;\nNSNumber *myIntegerNumber = @8;\n\nHowever, different from string literals, which compile to constants in the executable, these literals compile to code equivalent to the above method calls. In particular, under manually reference-counted memory management, these objects are autoreleased, which requires added care when e.g., used with function-static variables or other kinds of globals.\nSubscripting\n\nWhen using Apple LLVM compiler 4.0 or later, arrays and dictionaries (NSArray and NSDictionary classes) can be manipulated using subscripting.[37] Subscripting can be used to retrieve values from indexes (array) or keys (dictionary), and with mutable objects, can also be used to set objects to indexes or keys. In code, subscripting is represented using brackets [ ].[38]\n\nExample without subscripting:\n\nid object1 = [someArray objectAtIndex:0];\nid object2 = [someDictionary objectForKey:@\"key\"];\n[someMutableArray replaceObjectAtIndex:0 withObject:object3];\n[someMutableDictionary setObject:object4 forKey:@\"key\"];\n\nExample with subscripting:\n\nid object1 = someArray[0];\nid object2 = someDictionary[@\"key\"];\nsomeMutableArray[0] = object3;\nsomeMutableDictionary[@\"key\"] = object4;\n\n\"Modern\" Objective-C syntax (1997)\n\nAfter the purchase of NeXT by Apple, attempts were made to make the language more acceptable to programmers more familiar with Java than Smalltalk. One of these attempts was introducing what was dubbed \"Modern Syntax\" for Objective-C at the time[39] (as opposed to the current, \"classic\" syntax). There was no change in behaviour, this was merely an alternative syntax. Instead of writing a method invocation like\n\n   object = [[MyClass alloc] init];\n   [object firstLabel: param1 secondLabel: param2];\n\nIt was instead written as\n\n   object = (MyClass.alloc).init;\n   object.firstLabel ( param1, param2 );\n\nSimilarly, declarations went from the form\n\n   -(void) firstLabel: (int)param1 secondLabel: (int)param2;\n\nto\n\n   -(void) firstLabel ( int param1, int param2 );\n\nThis \"modern\" syntax is no longer supported in current dialects of the Objective-C language.\nPortable Object Compiler\n\nBesides the GCC/NeXT/Apple implementation, which added several extensions to the original Stepstone implementation, another free, open-source Objective-C implementation called the Portable Object Compiler[40] also exists. The set of extensions implemented by the Portable Object Compiler differs from the GCC/NeXT/Apple implementation; in particular, it includes Smalltalk-like blocks for Objective-C, while it lacks protocols and categories, two features used extensively in OpenStep and its derivatives and relatives. Overall, POC represents an older, pre-NeXT stage in the language's evolution, roughly conformant to Brad Cox's 1991 book.\n\nIt also includes a runtime library called ObjectPak, which is based on Cox's original ICPak101 library (which in turn derives from the Smalltalk-80 class library), and is quite radically different from the OpenStep FoundationKit.\nGEOS Objective-C\n\nThe PC GEOS system used a programming language known as GEOS Objective-C or goc;[41] despite the name similarity, the two languages are similar only in overall concept and the use of keywords prefixed with an @ sign.\nClang\n\nThe Clang compiler suite, part of the LLVM project, implements Objective-C, and other languages.\nLibrary use\n\nObjective-C today is often used in tandem with a fixed library of standard objects (often known as a \"kit\" or \"framework\"), such as Cocoa, GNUstep or ObjFW. These libraries often come with the operating system: the GNUstep libraries often come with GNU/Linux based distributions and Cocoa comes with OS X. The programmer is not forced to inherit functionality from the existing base class (NSObject / OFObject). Objective-C allows for the declaration of new root classes that do not inherit any existing functionality. Originally, Objective-C based programming environments typically offered an Object class as the base class from which almost all other classes inherited. With the introduction of OpenStep, NeXT created a new base class named NSObject, which offered additional features over Object (an emphasis on using object references and reference counting instead of raw pointers, for example). Almost all classes in Cocoa inherit from NSObject.\n\nNot only did the renaming serve to differentiate the new default behavior of classes within the OpenStep API, but it allowed code that used Object—the original base class used on NeXTSTEP (and, more or less, other Objective-C class libraries)—to co-exist in the same runtime with code that used NSObject (with some limitations). The introduction of the two letter prefix also became a simplistic form of namespaces, which Objective-C lacks. Using a prefix to create an informal packaging identifier became an informal coding standard in the Objective-C community, and continues to this day.\n\nMore recently, package managers have started appearing, such as CocoaPods, which aims to be both a package manager and a repository of packages. A lot of open-source Objective-C code that was written in the last few years can now be installed using CocoaPods.\nAnalysis of the language\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2011) (Learn how and when to remove this template message)\n\nObjective-C implementations use a thin runtime system written in C, which adds little to the size of the application. In contrast, most object-oriented systems at the time that it was created used large virtual machine runtimes. Programs written in Objective-C tend to be not much larger than the size of their code and that of the libraries (which generally do not need to be included in the software distribution), in contrast to Smalltalk systems where a large amount of memory was used just to open a window. Objective-C applications tend to be larger than similar C or C++ applications because Objective-C dynamic typing does not allow methods to be stripped or inlined. Since the programmer has such freedom to delegate, forward calls, build selectors on the fly and pass them to the runtime system, the Objective-C compiler cannot assume it is safe to remove unused methods or to inline calls.\n\nLikewise, the language can be implemented atop extant C compilers (in GCC, first as a preprocessor, then as a module) rather than as a new compiler. This allows Objective-C to leverage the huge existing collection of C code, libraries, tools, etc. Existing C libraries can be wrapped in Objective-C wrappers to provide an OO-style interface. In this aspect, it is similar to GObject library and Vala language, which are widely used in development of GTK applications.\n\nAll of these practical changes lowered the barrier to entry, likely the biggest problem for the widespread acceptance of Smalltalk in the 1980s.\n\nA common criticism is that Objective-C does not have language support for namespaces. Instead, programmers are forced to add prefixes to their class names, which are traditionally shorter than namespace names and thus more prone to collisions. As of 2007, all Mac OS X classes and functions in the Cocoa programming environment are prefixed with \"NS\" (e.g. NSObject, NSButton) to identify them as belonging to the Mac OS X or iOS core; the \"NS\" derives from the names of the classes as defined during the development of NeXTSTEP.\n\nSince Objective-C is a strict superset of C, it does not treat C primitive types as first-class objects.\n\nUnlike C++, Objective-C does not support operator overloading. Also unlike C++, Objective-C allows an object to directly inherit only from one class (forbidding multiple inheritance). However, in most cases, categories and protocols may be used as alternative ways to achieve the same results.\n\nBecause Objective-C uses dynamic runtime typing and because all method calls are function calls (or, in some cases, syscalls), many common performance optimizations cannot be applied to Objective-C methods (for example: inlining, constant propagation, interprocedural optimizations, and scalar replacement of aggregates). This limits the performance of Objective-C abstractions relative to similar abstractions in languages such as C++ where such optimizations are possible.\nMemory management\n\nThe first versions of Objective-C did not support garbage collection. At the time this decision was a matter of some debate, and many people considered long \"dead times\" (when Smalltalk performed collection) to render the entire system unusable. Some 3rd party implementations have added this feature (most notably GNUstep) and Apple has implemented it as of Mac OS X v10.5.[42] However, in more recent versions of Mac OS X and iOS, garbage collection has been deprecated in favor of Automatic Reference Counting (ARC), introduced in 2011.\n\nWith ARC, the compiler inserts retain and release calls automatically into Objective-C code based on static code analysis. The automation relieves the programmer of having to write in memory management code. ARC also adds weak references to the Objective-C language.[43]\nPhilosophical differences between Objective-C and C++\n\nThe design and implementation of C++ and Objective-C represent fundamentally different approaches to extending C.\n\nIn addition to C's style of procedural programming, C++ directly supports certain forms of object-oriented programming, generic programming, and metaprogramming. C++ also comes with a large standard library that includes several container classes. Similarly, Objective-C adds object-oriented programming, dynamic typing, and reflection to C. Objective-C does not provide a standard library per se, but in most places where Objective-C is used, it is used with an OpenStep-like library such as OPENSTEP, Cocoa, or GNUstep, which provides functionality similar to C++'s standard library.\n\nOne notable difference is that Objective-C provides runtime support for reflective features, whereas C++ adds only a small amount of runtime support to C. In Objective-C, an object can be queried about its own properties, e.g., whether it will respond to a certain message. In C++, this is not possible without the use of external libraries.\n\nThe use of reflection is part of the wider distinction between dynamic (run-time) features and static (compile-time) features of a language. Although Objective-C and C++ each employ a mix of both features, Objective-C is decidedly geared toward run-time decisions while C++ is geared toward compile-time decisions. The tension between dynamic and static programming involves many of the classic trade-offs in programming: dynamic features add flexibility, static features add speed and type checking.\n\nGeneric programming and metaprogramming can be implemented in both languages using runtime polymorphism. In C++ this takes the form of virtual functions and runtime type identification, while Objective-C offers dynamic typing and reflection. Objective-C lacks compile-time polymorphism (generic functions) entirely, while C++ supports it via function overloading and templates.", "skillName": "Objective-C."}
{"id": 201, "category": "Computer_Programming", "skillText": "Ruby Ruby logo.svg\nParadigm \tMulti-paradigm: Object-oriented, imperative, functional, reflective\nDesigned by \tYukihiro Matsumoto\nDeveloper \tYukihiro Matsumoto, et al.\nFirst appeared \t1995; 21 years ago\nStable release \t2.3.1 / April 26, 2016; 2 months ago[1]\nTyping discipline \tDuck, dynamic\nScope \tLexical, sometimes dynamic\nOS \tCross-platform\nLicense \tRuby, GPLv2 or 2-clause BSD license[2][3][4]\nFilename extensions \t.rb, .rbw\nWebsite \twww.ruby-lang.org \nMajor implementations\nRuby MRI, YARV, Rubinius, MagLev, JRuby, MacRuby, RubyMotion, HotRuby, IronRuby, Mruby\nInfluenced by\nAda,[5] C++,[5] CLU,[6] Dylan,[6] Eiffel,[5] Lua, Lisp,[6] Perl,[6] Python,[6] Smalltalk[6]\nInfluenced\nCoffeescript, Clojure, Crystal, D, Elixir, Falcon, Groovy, Ioke,[7] Julia,[8] Mirah, Nu,[9] potion, Reia, Rust, Swift[10]\n\n    Ruby Programming at Wikibooks \n\nRuby is a dynamic, reflective, object-oriented, general-purpose programming language. It was designed and developed in the mid-1990s by Yukihiro \"Matz\" Matsumoto in Japan.\n\nAccording to its creator, Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, and Lisp.[11] It supports multiple programming paradigms, including functional, object-oriented, and imperative. It also has a dynamic type system and automatic memory management.\n\nContents\n\n    1 History\n        1.1 Early concept\n        1.2 The name \"Ruby\"\n        1.3 First publication\n        1.4 Early releases\n        1.5 Ruby 1.8\n        1.6 Ruby 1.9\n        1.7 Ruby 2.0\n        1.8 Ruby 2.1\n        1.9 Ruby 2.2\n        1.10 Ruby 2.3\n    2 Table of versions\n    3 Philosophy\n    4 Features\n    5 Semantics\n    6 Syntax\n    7 Differences from other languages\n    8 Interaction\n    9 Examples\n        9.1 Strings\n        9.2 Collections\n        9.3 Control structures\n        9.4 Blocks and iterators\n        9.5 Classes\n            9.5.1 Open classes\n        9.6 Exceptions\n        9.7 Metaprogramming\n        9.8 More examples\n    10 Implementations\n        10.1 Matz's Ruby Interpreter\n        10.2 Alternate implementations\n        10.3 Platform support\n    11 Repositories and libraries\n    12 See also\n    13 References\n    14 Further reading\n    15 External links\n\nHistory\nEarly concept\n\nRuby was conceived on February 24, 1993. In a 1999 post to the ruby-talk mailing list, Ruby author Yukihiro Matsumoto describes some of his early ideas about the language:[12]\n\n    I was talking with my colleague about the possibility of an object-oriented scripting language. I knew Perl (Perl4, not Perl5), but I didn't like it really, because it had the smell of a toy language (it still has). The object-oriented language seemed very promising. I knew Python then. But I didn't like it, because I didn't think it was a true object-oriented language — OO features appeared to be add-on to the language. As a language maniac and OO fan for 15 years, I really wanted a genuine object-oriented, easy-to-use scripting language. I looked for but couldn't find one. So I decided to make it.\n\nMatsumoto describes the design of Ruby as being like a simple Lisp language at its core, with an object system like that of Smalltalk, blocks inspired by higher-order functions, and practical utility like that of Perl.[13]\nThe name \"Ruby\"\n\nThe name \"Ruby\" originated during an online chat session between Matsumoto and Keiju Ishitsuka on February 24, 1993, before any code had been written for the language.[14] Initially two names were proposed: \"Coral\" and \"Ruby\". Matsumoto chose the latter in a later e-mail to Ishitsuka.[15] Matsumoto later noted a factor in choosing the name \"Ruby\" – it was the birthstone of one of his colleagues.[16][17]\nFirst publication\n\nThe first public release of Ruby 0.95 was announced on Japanese domestic newsgroups on December 21, 1995.[18][19] Subsequently three more versions of Ruby were released in two days.[14] The release coincided with the launch of the Japanese-language ruby-list mailing list, which was the first mailing list for the new language.\n\nAlready present at this stage of development were many of the features familiar in later releases of Ruby, including object-oriented design, classes with inheritance, mixins, iterators, closures, exception handling and garbage collection.[20]\nEarly releases\n\nFollowing the release of Ruby 0.95 in 1995, several stable versions of Ruby were released in the following years:\n\n    Ruby 1.0: December 25, 1996[14]\n    Ruby 1.2: December 1998\n    Ruby 1.4: August 1999\n    Ruby 1.6: September 2000\n\nIn 1997, the first article about Ruby was published on the Web. In the same year, Matsumoto was hired by netlab.jp to work on Ruby as a full-time developer.[14]\n\nIn 1998, the Ruby Application Archive was launched by Matsumoto, along with a simple English-language homepage for Ruby.[14]\n\nIn 1999, the first English language mailing list ruby-talk began, which signaled a growing interest in the language outside Japan.[21] In this same year, Matsumoto and Keiju Ishitsuka wrote the first book on Ruby, The Object-oriented Scripting Language Ruby (オブジェクト指向スクリプト言語 Ruby), which was published in Japan in October 1999. It would be followed in the early 2000s by around 20 books on Ruby published in Japanese.[14]\n\nBy 2000, Ruby was more popular than Python in Japan.[22] In September 2000, the first English language book Programming Ruby was printed, which was later freely released to the public, further widening the adoption of Ruby amongst English speakers. In early 2002, the English-language ruby-talk mailing list was receiving more messages than the Japanese-language ruby-list, demonstrating Ruby's increasing popularity in the English-speaking world.\nRuby 1.8\n\nRuby 1.8 was initially released in August 2003, was stable for a long time, and was retired June 2013.[23] Although deprecated, there is still code based on it. Ruby 1.8 is only partially compatible with Ruby 1.9.\n\nRuby 1.8 has been the subject of several industry standards. The language specifications for Ruby were developed by the Open Standards Promotion Center of the Information-Technology Promotion Agency (a Japanese government agency) for submission to the Japanese Industrial Standards Committee (JISC) and then to the International Organization for Standardization (ISO). It was accepted as a Japanese Industrial Standard (JIS X 3017) in 2011[24] and an international standard (ISO/IEC 30170) in 2012.[25]\n\nAround 2005, interest in the Ruby language surged in tandem with Ruby on Rails, a web framework written in Ruby. Rails is frequently credited with increasing awareness of Ruby.[26]\nRuby 1.9\n\nRuby 1.9 was released in December 2007. Effective with Ruby 1.9.3, released October 31, 2011,[27] Ruby switched from being dual-licensed under the Ruby License and the GPL to being dual-licensed under the Ruby License and the two-clause BSD license.[28] Adoption of 1.9 was slowed by changes from 1.8 that required many popular third party gems to be rewritten.\n\nRuby 1.9 introduces many significant changes over the 1.8 series.[29] Examples:\n\n    block local variables (variables that are local to the block in which they are declared)\n    an additional lambda syntax: f = ->(a,b) { puts a + b }\n    per-string character encodings are supported\n    new socket API (IPv6 support)\n    require_relative import security\n\nRuby 1.9 has been obsolete since February 23, 2015,[30] and it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.\nRuby 2.0\n\nRuby 2.0 added several new features, including:\n\n    method keyword arguments,\n    a new method, Module#prepend, for extending a class,\n    a new literal for creating an array of symbols,\n    new API for the lazy evaluation of Enumerables, and\n    a new convention of using #to_h to convert objects to Hashes.[31]\n\nRuby 2.0 is intended to be fully backward compatible with Ruby 1.9.3. As of the official 2.0.0 release on February 24, 2013, there were only five known (minor) incompatibilities.[32]\n\nIt has been obsolete since February 22, 2016 [1] \nand it will no longer receive bug and security fixes. Users are advised to upgrade to a more recent version.\nRuby 2.1\n\nRuby 2.1.0 was released on Christmas Day in 2013.[33] The release includes speed-ups, bugfixes, and library updates.\n\nStarting with 2.1.0, Ruby's versioning policy is more like semantic versioning.[34] Although similar, Ruby's versioning policy is not compatible with semantic versioning:\nRuby \tSemantic versioning\nMAJOR: Increased when incompatible change which can’t be released in MINOR. Reserved for special events. \tMAJOR: Increased when you make incompatible API changes.\nMINOR: increased every Christmas, may be API incompatible. \tMINOR: increased when you add functionality in a backwards-compatible manner.\nTEENY: security or bug fix which maintains API compatibility. May be increased more than 10 (such as 2.1.11), and will be released every 2–3 months. \tPATCH: increased when you make backwards-compatible bug fixes.\nPATCH: number of commits since last MINOR release (will be reset at 0 when releasing MINOR). \t-\n\nSemantic versioning also provides additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format, not available at Ruby.\nRuby 2.2\n\nRuby 2.2.0 was released on Christmas Day in 2014.[35] The release includes speed-ups, bugfixes, and library updates and removes some deprecated APIs. Most notably, Ruby 2.2.0 introduces changes to memory handling – an incremental garbage collector, support for garbage collection of symbols and the option to compile directly against jemalloc. It also contains experimental support for using vfork(2) with system() and spawn(), and added support for the Unicode 7.0 specification.\n\nFeatures that were made obsolete or removed include callcc, the DL library, Digest::HMAC, lib/rational.rb, lib/complex.rb, GServer, Logger::Application as well as various C API functions.[36]\nRuby 2.3\n\nRuby 2.3.0 was released on December 25, 2015. A few notable changes include:\n\n    The ability to mark all strings literals as frozen by default with consequently large performance increase in string operations.[37]\n    Hash comparison to allow direct checking of key/value pairs instead of just keys.\n    A new safe navigation operator &. that can ease nil handling (e.g. instead of if obj && obj.foo && obj.foo.bar, we can use if obj&.foo&.bar).\n    The did_you_mean gem is now bundled by default and required on startup to automatically suggest similar name matches on a NameError or NoMethodError.\n    Hash#dig and Array#dig to easily extract deeply nested values (e.g. given profile = { social: { wikipedia: { name: 'Foo Baz' } } }, the value Foo Baz can now be retrieved by profile.dig(:social, :wikipedia, :name)).\n    .grep_v(regexp) which will match all negative examples of a given regular expression in addition to other new features.\n\nThe 2.3 branch also includes many performance improvements, updates, and bugfixes including changes to Proc#call, Socket and IO use of exception keywords, Thread#name handling, default passive Net::FTP connections, and Rake being removed from stdlib.[38]\nTable of versions\nVersion \tLatest teeny version \tInitial release date \tEnd of support phase \tEnd of security maintenance phase\n1.8 \t1.8.7-p375[39] \t2003-08-04[40] \t2012-06[41] \t2014-07-01[42]\n1.9 \t1.9.3-p551[43] \t2007-12-25[44] \t2014-02-23[45] \t2015-02-23[46]\n2.0 \t2.0.0-p648[47] \t2013-02-24[48] \t2015-02-24[47] \t2016-02-24[47]\n2.1 \t2.1.10[49] \t2013-12-25[50] \t2016-03-30[51][52] \tTBA\n2.2 \t2.2.5[53] \t2014-12-25[54] \tTBA \tTBA\n2.3 \t2.3.1[55] \t2015-12-25[56] \tTBA \tTBA\n2.4 \t\t2016-12-25 \t\t\n3.0 \t\tTBA[57] \t\t\nLegend:\nOld version\nOlder version, still supported\nLatest version\nFuture release\nPhilosophy\nYukihiro Matsumoto, the creator of Ruby\n\nMatsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design.[58] At a Google Tech Talk in 2008 Matsumoto further stated, \"I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language.\"[59] He stresses that systems design needs to emphasize human, rather than computer, needs:[60]\n\n    Often people, especially computer engineers, focus on the machines. They think, \"By doing this, the machine will run fast. By doing this, the machine will run more effectively. By doing this, the machine will something something something.\" They are focusing on machines. But in fact we need to focus on humans, on how humans care about doing programming or operating the application of the machines. We are the masters. They are the slaves.\n\nRuby is said to follow the principle of least astonishment (POLA), meaning that the language should behave in such a way as to minimize confusion for experienced users. Matsumoto has said his primary design goal was to make a language that he himself enjoyed using, by minimizing programmer work and possible confusion. He has said that he had not applied the principle of least astonishment to the design of Ruby,[60] but nevertheless the phrase has come to be closely associated with the Ruby programming language. The phrase has itself been a source of surprise, as novice users may take it to mean that Ruby's behaviors try to closely match behaviors familiar from other languages. In a May 2005 discussion on the newsgroup comp.lang.ruby, Matsumoto attempted to distance Ruby from POLA, explaining that because any design choice will be surprising to someone, he uses a personal standard in evaluating surprise. If that personal standard remains consistent, there would be few surprises for those familiar with the standard.[61]\n\nMatsumoto defined it this way in an interview:[60]\n\n    Everyone has an individual background. Someone may come from Python, someone else may come from Perl, and they may be surprised by different aspects of the language. Then they come up to me and say, 'I was surprised by this feature of the language, so Ruby violates the principle of least surprise.' Wait. Wait. The principle of least surprise is not for you only. The principle of least surprise means principle of least my surprise. And it means the principle of least surprise after you learn Ruby very well. For example, I was a C++ programmer before I started designing Ruby. I programmed in C++ exclusively for two or three years. And after two years of C++ programming, it still surprises me.\n\nFeatures\n\n    Thoroughly object-oriented with inheritance, mixins and metaclasses[62]\n    Dynamic typing and duck typing\n    Everything is an expression (even statements) and everything is executed imperatively (even declarations)\n    Succinct and flexible syntax[63] that minimizes syntactic noise and serves as a foundation for domain-specific languages[64]\n    Dynamic reflection and alteration of objects to facilitate metaprogramming[65]\n    Lexical closures, iterators and generators, with a unique block syntax[66]\n    Literal notation for arrays, hashes, regular expressions and symbols\n    Embedding code in strings (interpolation)\n    Default arguments\n    Four levels of variable scope (global, class, instance, and local) denoted by sigils or the lack thereof\n    Garbage collection\n    First-class continuations\n    Strict boolean coercion rules (everything is true except false and nil)\n    Exception handling\n    Operator overloading\n    Built-in support for rational numbers, complex numbers and arbitrary-precision arithmetic\n    Custom dispatch behavior (through method_missing and const_missing)\n    Native threads and cooperative fibers (fibers are a 1.9/YARV feature)\n    Initial support for Unicode and multiple character encodings (no ICU support)[67]\n    Native plug-in API in C\n    Interactive Ruby Shell (a REPL)\n    Centralized package management through RubyGems\n    Implemented on all major platforms\n    Large standard library, including modules for YAML, JSON, XML, CGI, OpenSSL, HTTP, FTP, RSS, curses, zlib, and Tk[68]\n\nSemantics\n\nRuby is object-oriented: every value is an object, including classes and instances of types that many other languages designate as primitives (such as integers, booleans, and \"null\"). Variables always hold references to objects. Every function is a method and methods are always called on an object. Methods defined at the top level scope become members of the Object class. Since this class is an ancestor of every other class, such methods can be called on any object. They are also visible in all scopes, effectively serving as \"global\" procedures. Ruby supports inheritance with dynamic dispatch, mixins and singleton methods (belonging to, and defined for, a single instance rather than being defined on the class). Though Ruby does not support multiple inheritance, classes can import modules as mixins.\n\nRuby has been described as a multi-paradigm programming language: it allows procedural programming (defining functions/variables outside classes makes them part of the root, 'self' Object), with object orientation (everything is an object) or functional programming (it has anonymous functions, closures, and continuations; statements all have values, and functions return the last evaluation). It has support for introspection, reflection and metaprogramming, as well as support for interpreter-based[69] threads. Ruby features dynamic typing, and supports parametric polymorphism.\n\nAccording to the Ruby FAQ, the syntax is similar to Perl and the semantics are similar to Smalltalk but it differs greatly from Python.[70]\nSyntax\n\nThe syntax of Ruby is broadly similar to that of Perl and Python. Class and method definitions are signaled by keywords, whereas code blocks can be both defined by keywords or braces. In contrast to Perl, variables are not obligatorily prefixed with a sigil. When used, the sigil changes the semantics of scope of the variable. For practical purposes there is no distinction between expressions and statements.[71] Line breaks are significant and taken as the end of a statement; a semicolon may be equivalently used. Unlike Python, indentation is not significant.\n\nOne of the differences of Ruby compared to Python and Perl is that Ruby keeps all of its instance variables completely private to the class and only exposes them through accessor methods (attr_writer, attr_reader, etc.). Unlike the \"getter\" and \"setter\" methods of other languages like C++ or Java, accessor methods in Ruby can be created with a single line of code via metaprogramming; however, accessor methods can also be created in the traditional fashion of C++ and Java. As invocation of these methods does not require the use of parentheses, it is trivial to change an instance variable into a full function, without modifying a single line of code or having to do any refactoring achieving similar functionality to C# and VB.NET property members.\n\nPython's property descriptors are similar, but come with a tradeoff in the development process. If one begins in Python by using a publicly exposed instance variable, and later changes the implementation to use a private instance variable exposed through a property descriptor, code internal to the class may need to be adjusted to use the private variable rather than the public property. Ruby’s design forces all instance variables to be private, but also provides a simple way to declare set and get methods. This is in keeping with the idea that in Ruby, one never directly accesses the internal members of a class from outside the class; rather, one passes a message to the class and receives a response.\n\nSee the Examples section below for samples of code demonstrating Ruby syntax.\nDifferences from other languages\n\nSome features that differ notably from languages such as C or Perl:\n\n    The language syntax is sensitive to the capitalization of identifiers, in all cases treating capitalized variables as constants. Class and module names are constants and refer to objects derived from Class and Module.\n    The sigils $ and @ do not indicate variable data type as in Perl, but rather function as scope resolution operators.\n    Floating point literals must have digits on both sides of the decimal point: neither .5 nor 2. are valid floating point literals, but 0.5 and 2.0 are.\n\n    (In Ruby, integer literals are objects that can have methods apply to them, so requiring a digit after a decimal point helps to clarify whether 1.e5 should be parsed analogously to 1.to_f or as the exponential-format floating literal 1.0e5. The reason for requiring a digit before the decimal point is less clear; it might relate either to method invocation again, or perhaps to the .. and ... operators, for example in the fragment 0.1...3.)\n\n    Boolean non-boolean datatypes are permitted in boolean contexts (unlike in e.g. Smalltalk and Java), but their mapping to boolean values differs markedly from some other languages: 0 and \"empty\" (e.g. empty list, string or associative array) all evaluate to true, thus changing the meaning of some common idioms in related or similar languages such as Lisp, Perl and Python.\n\n    A consequence of this rule is that Ruby methods by convention — for example, regular-expression searches — return numbers, strings, lists, or other non-false values on success, but nil on failure.\n\n    Versions prior to 1.9 use plain integers to represent single characters, much like C. This may cause surprises when slicing strings: \"abc\"[0] yields 97 (the ASCII code of the first character in the string); to obtain \"a\" use \"abc\"[0,1] (a substring of length 1) or \"abc\"[0].chr.\n    The notation statement until expression does not run the statement if the expression is already true. (The behavior is like Perl, but unlike other languages' equivalent statements, e.g. do { statement } while (!(expression)); in C/C++/...). This is because statement until expression is actually syntactic sugar over until expression; statement; end, the equivalent of which in C/C++ is while (!(expression)) { statement; }, just as statement if expression is equivalent to if (expression) { statement; }. However, the notation begin statement end until expression in Ruby will in fact run the statement once even if the expression is already true, acting similar to the do-while of other languages. (Matsumoto has expressed a desire to remove the special behavior of begin statement end until expression,[72] but it still exists as of Ruby 2.0.)\n    Because constants are references to objects, changing what a constant refers to generates a warning, but modifying the object itself does not. For example, Greeting << \" world!\" if Greeting == \"Hello\" does not generate an error or warning. This is similar to final variables in Java or a const pointer to a non-const object in C++.\n    Ruby provides the functionality to freeze \n    an object.\n    The usual conjunctive and disjunctive operators for conditional expressions have the same precedence, so and does not bind tighter than or in Ruby, a behaviour similar to languages such as APL, Ada, VHDL, Mathematica, zkl and others. However, Ruby also has C-like operators || and && that work as in C-like languages.\n\nA list of so-called gotchas may be found in Hal Fulton's book The Ruby Way, 2nd ed (ISBN 0-672-32884-4), Section 1.5. A similar list in the 1st edition pertained to an older version of Ruby (version 1.6), some problems of which have been fixed in the meantime. For example, retry now works with while, until, and for, as well as with iterators.\nInteraction\nSee also: Interactive Ruby Shell\n\nThe Ruby official distribution also includes irb, an interactive command-line interpreter that can be used to test code quickly. The following code fragment represents a sample session using irb:\n\n$ irb\nirb(main):001:0> puts \"Hello, World\"\nHello, World\n => nil\nirb(main):002:0> 1+2\n => 3\n\nExamples\n\tThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2014) (Learn how and when to remove this template message)\n\nThe following examples can be run in a Ruby shell such as Interactive Ruby Shell, or saved in a file and run from the command line by typing ruby <filename>.\n\nClassic Hello world example:\n\nputs \"Hello World!\"\n\nSome basic Ruby code:\n\n# Everything, including a literal, is an object, so this works:\n-199.abs                                                 # => 199\n\"ice is nice\".length                                     # => 11\n\"ruby is cool.\".index(\"u\")                               # => 1\n\"Nice Day Isn't It?\".downcase.split(\"\").uniq.sort.join   # => \" '?acdeinsty\"\n\nInput:\n\nprint \"Please type name >\"\nname = gets.chomp\nputs \"Hello #{name}.\"\n\nConversions:\n\nputs \"Give me a number\"\nnumber = gets.chomp\nputs number.to_i\noutput_number = number.to_i + 1\nputs output_number.to_s + ' is a bigger number.'\n\nStrings\n\nThere are a variety of ways to define strings in Ruby.\n\nThe following assignments are equivalent:\n\na = \"\\nThis is a double-quoted string\\n\"\na = %Q{\\nThis is a double-quoted string\\n}\na = %{\\nThis is a double-quoted string\\n}\na = %/\\nThis is a double-quoted string\\n/\na = <<-BLOCK\n\nThis is a double-quoted string\nBLOCK\n\nStrings support variable interpolation:\n\nvar = 3.14159\n\"pi is #{var}\"\n=> \"pi is 3.14159\"\n\nThe following assignments are equivalent and produce raw strings:\n\na = 'This is a single-quoted string'\na = %q{This is a single-quoted string}\n\nCollections\n\nConstructing and using an array:\n\na = [1, 'hi', 3.14, 1, 2, [4, 5]]\n\na[2]             # => 3.14\na.[](2)          # => 3.14\na.reverse        # => [[4, 5], 2, 1, 3.14, 'hi', 1]\na.flatten.uniq   # => [1, 'hi', 3.14, 2, 4, 5]\n\nConstructing and using an associative array (in Ruby, called a hash):\n\nhash = Hash.new # equivalent to hash = {}\nhash = { :water => 'wet', :fire => 'hot' } # makes the previous line redundant as we are now\n                                           # assigning hash to a new, separate hash object\nputs hash[:fire] # prints \"hot\"\n\nhash.each_pair do |key, value|   # or: hash.each do |key, value|\n  puts \"#{key} is #{value}\"\nend\n# returns {:water=>\"wet\", :fire=>\"hot\"} and prints:\n# water is wet\n# fire is hot\n\nhash.delete :water                            # deletes the pair :water => 'wet' and returns \"wet\"\nhash.delete_if {|key,value| value == 'hot'}   # deletes the pair :fire => 'hot' and returns {}\n\nControl structures\n\nIf statement:\n\n# Generate a random number and print whether it's even or odd.\nif rand(100) % 2 == 0\n  puts \"It's even\"\nelse\n  puts \"It's odd\"\nend\n\nBlocks and iterators\n\nThe two syntaxes for creating a code block:\n\n{ puts \"Hello, World!\" } # note the braces\n# or:\ndo\n  puts \"Hello, World!\"\nend\n\nA code block can be passed to a method as an optional block argument. Many built-in methods have such arguments:\n\nFile.open('file.txt', 'w') do |file| # 'w' denotes \"write mode\"\n  file.puts 'Wrote some text.'\nend                                  # file is automatically closed here\n\nFile.readlines('file.txt').each do |line|\n  puts line\nend\n# => Wrote some text.\n\nParameter-passing a block to be a closure:\n\n# In an object instance variable (denoted with '@'), remember a block.\ndef remember(&a_block)\n  @block = a_block\nend\n\n# Invoke the preceding method, giving it a block that takes a name.\nremember {|name| puts \"Hello, #{name}!\"}\n\n# Call the closure (note that this happens not to close over any free variables):\n@block.call(\"Jon\")   # => \"Hello, Jon!\"\n\nCreating an anonymous function:\n\nproc {|arg| puts arg}\nProc.new {|arg| puts arg}\nlambda {|arg| puts arg}\n->(arg) {puts arg}         # introduced in Ruby 1.9\n\nReturning closures from a method:\n\ndef create_set_and_get(initial_value=0) # note the default value of 0\n  closure_value = initial_value\n  [ Proc.new {|x| closure_value = x}, Proc.new { closure_value } ]\nend\n\nsetter, getter = create_set_and_get  # returns two values\nsetter.call(21)\ngetter.call      # => 21\n\n# Parameter variables can also be used as a binding for the closure,\n# so the preceding can be rewritten as:\n\ndef create_set_and_get(closure_value=0)\n  [ proc {|x| closure_value = x } , proc { closure_value } ]\nend\n\nYielding the flow of program control to a block that was provided at calling time:\n\ndef use_hello\n  yield \"hello\"\nend\n\n# Invoke the preceding method, passing it a block.\nuse_hello {|string| puts string}  # => 'hello'\n\nIterating over enumerations and arrays using blocks:\n\narray = [1, 'hi', 3.14]\narray.each {|item| puts item }\n# prints:\n# 1\n# 'hi'\n# 3.14\n\narray.each_index {|index| puts \"#{index}: #{array[index]}\" }\n# prints:\n# 0: 1\n# 1: 'hi'\n# 2: 3.14\n\n# The following uses a (a..b) Range\n(3..6).each {|num| puts num }\n# prints:\n# 3\n# 4\n# 5\n# 6\n\n# The following uses a (a...b) Range\n(3...6).each {|num| puts num }\n# prints:\n# 3\n# 4\n# 5\n\nA method such as inject can accept both a parameter and a block. The inject method iterates over each member of a list, performing some function on it while retaining an aggregate. This is analogous to the foldl function in functional programming languages. For example:\n\n[1,3,5].inject(10) {|sum, element| sum + element}   # => 19\n\nOn the first pass, the block receives 10 (the argument to inject) as sum, and 1 (the first element of the array) as element. This returns 11, which then becomes sum on the next pass. It is added to 3 to get 14, which is then added to 5 on the third pass, to finally return 19.\n\nUsing an enumeration and a block to square the numbers 1 to 10 (using a range):\n\n(1..10).collect {|x| x*x}  # => [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\nOr invoke a method on each item (map is a synonym for collect):\n\n(1..5).map(&:to_f)  # => [1.0, 2.0, 3.0, 4.0, 5.0]\n\nClasses\n\nThe following code defines a class named Person. In addition to initialize, the usual constructor to create new objects, it has two methods: one to override the <=> comparison operator (so Array#sort can sort by age) and the other to override the to_s method (so Kernel#puts can format its output). Here, attr_reader is an example of metaprogramming in Ruby: attr_accessor defines getter and setter methods of instance variables, but attr_reader only getter methods. The last evaluated statement in a method is its return value, allowing the omission of an explicit return statement.\n\nclass Person\n  attr_reader :name, :age\n  def initialize(name, age)\n    @name, @age = name, age\n  end\n  def <=>(person) # the comparison operator for sorting\n    @age <=> person.age\n  end\n  def to_s\n    \"#{@name} (#{@age})\"\n  end\nend\n\ngroup = [\n  Person.new(\"Bob\", 33),\n  Person.new(\"Chris\", 16),\n  Person.new(\"Ash\", 23)\n]\n\nputs group.sort.reverse\n\nThe preceding code prints three names in reverse age order:\n\nBob (33)\nAsh (23)\nChris (16)\n\nPerson is a constant and is a reference to a Class object.\nOpen classes\n\nIn Ruby, classes are never closed: methods can always be added to an existing class. This applies to all classes, including the standard, built-in classes. All that is needed to do is open up a class definition for an existing class, and the new contents specified will be added to the existing contents. A simple example of adding a new method to the standard library's Time class:\n\n# re-open Ruby's Time class\nclass Time\n  def yesterday\n    self - 86400\n  end\nend\n\ntoday = Time.now               # => 2013-09-03 16:09:37 +0300\nyesterday = today.yesterday    # => 2013-09-02 16:09:37 +0300\n\nAdding methods to previously defined classes is often called monkey-patching. If performed recklessly, the practice can lead to both behavior collisions with subsequent unexpected results and code scalability problems.\nExceptions\n\nAn exception is raised with a raise call:\n\nraise\n\nAn optional message can be added to the exception:\n\nraise \"This is a message\"\n\nExceptions can also be specified by the programmer:\n\nraise ArgumentError, \"Illegal arguments!\"\n\nAlternatively, an exception instance can be passed to the raise method:\n\nraise ArgumentError.new(\"Illegal arguments!\")\n\nThis last construct is useful when raising an instance of a custom exception class featuring a constructor that takes more than one argument:\n\nclass ParseError < Exception\n  def initialize input, line, pos\n    super \"Could not parse '#{input}' at line #{line}, position #{pos}\"\n  end\nend\n\nraise ParseError.new(\"Foo\", 3, 9)\n\nExceptions are handled by the rescue clause. Such a clause can catch exceptions that inherit from StandardError. Other flow control keywords that can be used when handling exceptions are else and ensure:\n\nbegin\n  # do something\nrescue\n  # handle exception\nelse\n  # do this if no exception was raised\nensure\n  # do this whether or not an exception was raised\nend\n\nIt is a common mistake to attempt to catch all exceptions with a simple rescue clause. To catch all exceptions one must write:\n\nbegin\n  # do something\nrescue Exception\n  # Exception handling code here.\n  # Don't write only \"rescue\"; that only catches StandardError, a subclass of Exception.\nend\n\nOr catch particular exceptions:\n\nbegin\n  # do something\nrescue RuntimeError\n  # handle only RuntimeError and its subclasses\nend\n\nIt is also possible to specify that the exception object be made available to the handler clause:\n\nbegin\n  # do something\nrescue RuntimeError => e\n  # handling, possibly involving e, such as \"puts e.to_s\"\nend\n\nAlternatively, the most recent exception is stored in the magic global $!.\n\nSeveral exceptions can also be caught:\n\nbegin\n  # do something\nrescue RuntimeError, Timeout::Error => e\n  # handling, possibly involving e\nend\n\nMetaprogramming\n\tThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2014) (Learn how and when to remove this template message)\n\nRuby code can programmatically modify, at runtime, aspects of its own structure that would be fixed in more rigid languages, such as class and method definitions. This sort of metaprogramming can be used to write more concise code and effectively extend the language.\n\nFor example, the following Ruby code generates new methods for the built-in String class, based on a list of colors. The methods wrap the contents of the string with an HTML tag styled with the respective color.\n\nCOLORS = { black:   \"000\",\n           red:     \"f00\",\n           green:   \"0f0\",\n           yellow:  \"ff0\",\n           blue:    \"00f\",\n           magenta: \"f0f\",\n           cyan:    \"0ff\",\n           white:   \"fff\" }\n\nclass String\n  COLORS.each do |color,code|\n    define_method \"in_#{color}\" do\n      \"<span style=\\\"color: ##{code}\\\">#{self}</span>\"\n    end\n  end\nend\n\nThe generated methods could then be used like this:\n\n\"Hello, World!\".in_blue\n => \"<span style=\\\"color: #00f\\\">Hello, World!</span>\"\n\nTo implement the equivalent in many other languages, the programmer would have to write each method (in_black, in_red, in_green, etc.) separately.\n\nSome other possible uses for Ruby metaprogramming include:\n\n    intercepting and modifying method calls\n    implementing new inheritance models\n    dynamically generating classes from parameters\n    automatic object serialization\n    interactive help and debugging\n\nMore examples\n\nMore sample Ruby code is available as algorithms in the following article:\n\n    Exponentiating by squaring\n\nImplementations\nSee also: Ruby MRI § Operating systems\nMatz's Ruby Interpreter\n\nThe official Ruby interpreter often referred to as the Matz's Ruby Interpreter or MRI. This implementation is written in C and uses its own Ruby-specific virtual machine.\n\nThe standardized and retired Ruby 1.8 implementation was written in C, as a single-pass interpreted language.[23]\n\nStarting with Ruby 1.9, and continuing with Ruby 2.x and above, the official Ruby interpreter has been YARV (\"Yet Another Ruby VM\"), and this implementation has superseded the slower virtual machine used in previous releases of MRI.\nAlternate implementations\n\nAs of 2010, there are a number of alternative implementations of Ruby, including JRuby, Rubinius, MagLev, IronRuby, MacRuby (and its iOS counterpart, RubyMotion), mruby, HotRuby, Topaz and Opal. Each takes a different approach, with IronRuby, JRuby, MacRuby and Rubinius providing just-in-time compilation and MacRuby and mruby also providing ahead-of-time compilation.\n\nRuby has two major alternate implementations:\n\n    JRuby, a Java implementation that runs on the Java virtual machine. JRuby currently targets Ruby 2.2,\n    Rubinius, a C++ bytecode virtual machine that uses LLVM to compile to machine code at runtime. The bytecode compiler and most core classes are written in pure Ruby. Rubinius currently targets Ruby 2.1,\n\nOther Ruby implementations include:\n\n    MagLev, a Smalltalk implementation that runs on GemTalk Systems' GemStone/S VM\n    mruby, an implementation designed to be embedded into C code, in a similar vein to Lua. It is currently being developed by Yukihiro Matsumoto and others\n    Opal \n    , a web-based interpreter that compiles Ruby to JavaScript\n    RGSS, or Ruby Game Scripting System, a proprietary implementation that is used by the RPG Maker series of software for game design and modification of the RPG Maker engine\n\nOther now defunct Ruby implementations were:\n\n    MacRuby, an OS X implementation on the Objective-C runtime\n    IronRuby an implementation on the .NET Framework\n    Cardinal, an implementation for the Parrot virtual machine\n    Ruby Enterprise Edition \n    , often shortened to ree, an implementation optimized to handle large-scale Ruby on Rails projects\n\nThe maturity of Ruby implementations tends to be measured by their ability to run the Ruby on Rails (Rails) framework, because it is complex to implement and uses many Ruby-specific features. The point when a particular implementation achieves this goal is called \"the Rails singularity\". The reference implementation (MRI), JRuby, and Rubinius[73] are all able to run Rails unmodified in a production environment. IronRuby[74][75] is starting to be able to run Rails test cases, but is still far from being production-ready.\nPlatform support\n\nMatsumoto originally did Ruby development on the 4.3BSD-based Sony NEWS-OS 3.x, but later migrated his work to SunOS 4.x, and finally to Linux.[76][77]\n\nBy 1999, Ruby was known to work across many different operating systems, including NEWS-OS, SunOS, AIX, SVR4, Solaris, NEC UP-UX, NeXTSTEP, BSD, Linux, Mac OS, DOS, Windows, and BeOS.[78]\n\nModern Ruby versions and implementations are available on many operating systems, such as Linux, BSD, Solaris, AIX, OS X, Windows, Windows Phone,[79] Windows CE, Symbian OS, BeOS, and IBM i.\nRepositories and libraries\n\nRubyGems is Ruby's package manager. A Ruby package is called a \"gem\" and can easily be installed via the command line. Most gems are libraries, though a few exist that are applications, such as IDEs.[80] There are over 70,000 Ruby gems hosted on RubyGems.org \n.\n\nMany new and existing Ruby libraries are hosted on GitHub, a service that offers version control repository hosting for Git.", "skillName": "Ruby."}
{"id": 202, "category": "Computer_Programming", "skillText": "C++ (pronounced as cee plus plus, /ˈsiː plʌs plʌs/) is a general-purpose programming language. It has imperative, object-oriented and generic programming features, while also providing facilities for low-level memory manipulation.\n\nIt was designed with a bias toward system programming and embedded, resource-constrained and large systems, with performance, efficiency and flexibility of use as its design highlights.[5] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[5] including desktop applications, servers (e.g. e-commerce, web search or SQL servers), and performance-critical applications (e.g. telephone switches or space probes).[6] C++ is a compiled language, with implementations of it available on many platforms and provided by various organizations, including the Free Software Foundation (FSF's GCC), LLVM, Microsoft, Intel and IBM.\n\nC++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2014 as ISO/IEC 14882:2014 (informally known as C++14).[7] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, ISO/IEC 14882:2003, standard. The current C++14 standard supersedes these and C++11, with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Bjarne Stroustrup at Bell Labs since 1979, as an extension of the C language as he wanted an efficient and flexible language similar to C, which also provided high-level features for program organization.\n\nMany other programming languages have been influenced by C++, including C#, Java, and newer versions of C (after 1998).\n\nContents\n\n    1 History\n        1.1 Etymology\n        1.2 Philosophy\n        1.3 Standardization\n    2 Language\n        2.1 Object storage\n            2.1.1 Static storage duration objects\n            2.1.2 Thread storage duration objects\n            2.1.3 Automatic storage duration objects\n            2.1.4 Dynamic storage duration objects\n        2.2 Templates\n        2.3 Objects\n            2.3.1 Encapsulation\n            2.3.2 Inheritance\n        2.4 Operators and operator overloading\n        2.5 Polymorphism\n            2.5.1 Static polymorphism\n            2.5.2 Dynamic polymorphism\n                2.5.2.1 Inheritance\n                2.5.2.2 Virtual member functions\n        2.6 Lambda expressions\n        2.7 Exception handling\n    3 Standard library\n    4 Compatibility\n        4.1 With C\n    5 Criticism\n    6 See also\n    7 References\n    8 Further reading\n    9 External links\n\nHistory\nBjarne Stroustrup, the creator of C++\n\nIn 1979, Bjarne Stroustrup, a Danish computer scientist, began work on the predecessor to C++, \"C with Classes\".[8] The motivation for creating a new language originated from Stroustrup's experience in programming for his Ph.D. thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his Ph.D. experience, Stroustrup set out to enhance the C language with Simula-like features.[9] C was chosen because it was general-purpose, fast, portable and widely used. As well as C and Simula's influences, other languages also influenced C++, including ALGOL 68, Ada, CLU and ML.\n\nInitially, Stroustrup's \"C with Classes\" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining and default arguments.[10]\n\nIn 1983, C with Classes was renamed to C++ (\"++\" being the increment operator in C), adding new features that included virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL style single-line comments with two forward slashes (//). Furthermore, it included the development of a standalone compiler for C++, Cfront.\n\nIn 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[11] The first commercial implementation of C++ was released in October of the same year.[8]\n\nIn 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[12] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a boolean type.\n\nAfter the 2.0 update, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update, released in December 2014, various new additions are planned for 2017.\nEtymology\n\nAccording to Stroustrup: \"the name signifies the evolutionary nature of the changes from C\".[13] This name is credited to Rick Mascitti (mid-1983)[10] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name comes from C's \"++\" operator (which increments the value of a variable) and a common naming convention of using \"+\" to indicate an enhanced computer program.\n\nDuring C++'s development period, the language had been referred to as \"new C\" and \"C with Classes\"[10][14] before acquiring its final name.\nPhilosophy\n\nThroughout C++'s life, its development and evolution has been informally governed by a set of rules that its evolution should follow:[9]\n\n    It must be driven by actual problems and its features should be useful immediately in real world programs.\n    Every feature should be implementable (with a reasonably obvious way to do so).\n    Programmers should be free to pick their own programming style, and that style should be fully supported by C++.\n    Allowing a useful feature is more important than preventing every possible misuse of C++.\n    It should provide facilities for organising programs into well-defined separate parts, and provide facilities for combining separately developed parts.\n    No implicit violations of the type system (but allow explicit violations; that is, those explicitly requested by the programmer).\n    User-created types need to have the same support and performance as built-in types.\n    Unused features should not negatively impact created executables (e.g. in lower performance).\n    There should be no language beneath C++ (except assembly language).\n    C++ should work alongside other existing programming languages, rather than fostering its own separate and incompatible programming environment.\n    If the programmer's intent is unknown, allow the programmer to specify it by providing manual control.\n\nStandardization\nYear \tC++ Standard \tInformal name\n1998 \tISO/IEC 14882:1998[15] \tC++98\n2003 \tISO/IEC 14882:2003[16] \tC++03\n2011 \tISO/IEC 14882:2011[7] \tC++11\n2014 \tISO/IEC 14882:2014[17] \tC++14\n2017 \tto be determined \tC++17\n\nC++ is standardized by an ISO working group known as JTC1/SC22/WG21. So far, it has published four revisions of the C++ standard and is currently working on the next revision, C++17.\n\nIn 1998, the ISO working group standardized C++ for the first time as ISO/IEC 14882:1998, which is informally known as C++98. In 2003, it published a new version of the C++ standard called ISO/IEC 14882:2003, which fixed problems identified in C++98.\n\nThe next major revision of the standard was informally referred to as \"C++0x\", but it was not released until 2011.[18] C++11 (14882:2011) included many additions to both the core language and the standard library.[7]\n\nIn 2014, C++14 (also known as C++1y) was released as a small extension to C++11, featuring mainly bug fixes and small improvements.[19] The Draft International Standard ballot procedures completed in mid-August 2014.[20]\n\nAfter C++14, a major revision, informally known as C++17 or C++1z, is planned for 2017,[19] which is almost feature-complete.[21]\n\nAs part of the standardization process, ISO also publishes technical reports and specifications:\n\n    ISO/IEC TR 18015:2006[22] on the use of C++ in embedded systems and on performance implications of C++ language and library features,\n    ISO/IEC TR 19768:2007[23] (also known as the C++ Technical Report 1) on library extensions mostly integrated into C++11,\n    ISO/IEC TR 29124:2010[24] on special mathematical functions,\n    ISO/IEC TR 24733:2011[25] on decimal floating point arithmetic,\n    ISO/IEC TS 18822:2015[26] on the standard filesystem library,\n    ISO/IEC TS 19570:2015[27] on parallel versions of the standard library algorithms,\n    ISO/IEC TS 19841:2015[28] on software transactional memory,\n    ISO/IEC TS 19568:2015[29] on a new set of library extensions, some of which are already integrated into C++17,\n    ISO/IEC TS 19217:2015[30] on the C++ Concepts\n\nMore technical specifications are in development and pending approval, including concurrency library extensions, a networking standard library, ranges, and modules.[31]\nLanguage\n\nThe C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as \"a light-weight abstraction programming language [designed] for building and using efficient and elegant abstractions\";[5] and \"offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages\".[32]\n\nC++ inherits most of C's syntax. The following is Bjarne Stroustrup's version of the Hello world program that uses the C++ Standard Library stream facility to write a message to standard output:[33][34]\n\n#include <iostream>\n\nint main()\n{\n\tstd::cout << \"Hello, world!\\n\";\n}\n\nWithin functions that define a non-void return type, failure to return a value before control reaches the end of the function results in undefined behaviour (compilers typically provide the means to issue a diagnostic in such a case).[35] The sole exception to this rule is the main function, which implicitly returns a value of zero.[36]\nObject storage\n\nAs in C, C++ supports four types of memory management: static storage duration objects, thread storage duration objects, automatic storage duration objects, and dynamic storage duration objects.[37]\nStatic storage duration objects\n\nStatic storage duration objects are created before main() is entered (see exceptions below) and destroyed in reverse order of creation after main() exits. The exact order of creation is not specified by the standard (though there are some rules defined below) to allow implementations some freedom in how to organize their implementation. More formally, objects of this type have a lifespan that \"shall last for the duration of the program\".[38]\n\nStatic storage duration objects are initialized in two phases. First, \"static initialization\" is performed, and only after all static initialization is performed, \"dynamic initialization\" is performed. In static initialization, all objects are first initialized with zeros; after that, all objects that have a constant initialization phase are initialized with the constant expression (i.e. variables initialized with a literal or constexpr). Though it is not specified in the standard, the static initialization phase can be completed at compile time and saved in the data partition of the executable. Dynamic initialization involves all object initialization done via a constructor or function call (unless the function is marked with constexpr, in C++11). The dynamic initialization order is defined as the order of declaration within the compilation unit (i.e. the same file). No guarantees are provided about the order of initialization between compilation units.\nThread storage duration objects\n\nVariables of this type are very similar to Static Storage duration objects. The main difference is the creation time is just prior to thread creation and destruction is done after the thread has been joined.[39]\nAutomatic storage duration objects\n\nThe most common variable types in C++ are local variables inside a function or block, and temporary variables.[40] The common feature about automatic variables is that they have a lifetime that is limited to the scope of the variable. They are created and potentially initialized at the point of declaration (see below for details) and destroyed in the reverse order of creation when the scope is left.\n\nLocal variables are created as the point of execution passes the declaration point. If the variable has a constructor or initializer this is used to define the initial state of the object. Local variables are destroyed when the local block or function that they are declared in is closed. C++ destructors for local variables are called at the end of the object lifetime, allowing a discipline for automatic resource management termed RAII, which is widely used in C++.\n\nMember variables are created when the parent object is created. Array members are initialized from 0 to the last member of the array in order. Member variables are destroyed when the parent object is destroyed in the reverse order of creation. i.e. If the parent is an \"automatic object\" then it will be destroyed when it goes out of scope which triggers the destruction of all its members.\n\nTemporary variables are created as the result of expression evaluation and are destroyed when the statement containing the expression has been fully evaluated (usually at the ; at the end of a statement).\nDynamic storage duration objects\nMain article: new and delete (C++)\n\nThese objects have a dynamic lifespan and are created with a call to new and destroyed explicitly with a call to delete.[41]\nTemplates\nSee also: Template metaprogramming and Generic programming\n\nC++ templates enable generic programming. C++ supports function, class, alias and variable templates. Templates may be parameterized by types, compile-time constants, and other templates. Templates are implemented by instantiation at compile-time. To instantiate a template, compilers substitute specific arguments for a template's parameters to generate a concrete function or class instance. Some substitutions are not possible; these are eliminated by an overload resolution policy described by the phrase \"Substitution failure is not an error\" (SFINAE). Templates are a powerful tool that can be used for generic programming, template metaprogramming, and code optimization, but this power implies a cost. Template use may increase code size, because each template instantiation produces a copy of the template code: one for each set of template arguments, however, this is the same or smaller amount of code that would be generated if the code was written by hand.[42] This is in contrast to run-time generics seen in other languages (e.g., Java) where at compile-time the type is erased and a single template body is preserved.\n\nTemplates are different from macros: while both of these compile-time language features enable conditional compilation, templates are not restricted to lexical substitution. Templates are aware of the semantics and type system of their companion language, as well as all compile-time type definitions, and can perform high-level operations including programmatic flow control based on evaluation of strictly type-checked parameters. Macros are capable of conditional control over compilation based on predetermined criteria, but cannot instantiate new types, recurse, or perform type evaluation and in effect are limited to pre-compilation text-substitution and text-inclusion/exclusion. In other words, macros can control compilation flow based on pre-defined symbols but cannot, unlike templates, independently instantiate new symbols. Templates are a tool for static polymorphism (see below) and generic programming.\n\nIn addition, templates are a compile time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.\n\nIn summary, a template is a compile-time parameterized function or class written without knowledge of the specific arguments used to instantiate it. After instantiation, the resulting code is equivalent to code written specifically for the passed arguments. In this manner, templates provide a way to decouple generic, broadly applicable aspects of functions and classes (encoded in templates) from specific aspects (encoded in template parameters) without sacrificing performance due to abstraction.\nObjects\nMain article: C++ classes\n\nC++ introduces object-oriented programming (OOP) features to C. It offers classes, which provide the four features commonly present in OOP (and some non-OOP) languages: abstraction, encapsulation, inheritance, and polymorphism. One distinguishing feature of C++ classes compared to classes in other programming languages is support for deterministic destructors, which in turn provide support for the Resource Acquisition is Initialization (RAII) concept.\nEncapsulation\n\nEncapsulation is the hiding of information to ensure that data structures and operators are used as intended and to make the usage model more obvious to the developer. C++ provides the ability to define classes and functions as its primary encapsulation mechanisms. Within a class, members can be declared as either public, protected, or private to explicitly enforce encapsulation. A public member of the class is accessible to any function. A private member is accessible only to functions that are members of that class and to functions and classes explicitly granted access permission by the class (\"friends\"). A protected member is accessible to members of classes that inherit from the class in addition to the class itself and any friends.\n\nThe OO principle is that all of the functions (and only the functions) that access the internal representation of a type should be encapsulated within the type definition. C++ supports this (via member functions and friend functions), but does not enforce it: the programmer can declare parts or all of the representation of a type to be public, and is allowed to make public entities that are not part of the representation of the type. Therefore, C++ supports not just OO programming, but other weaker decomposition paradigms, like modular programming.\n\nIt is generally considered good practice to make all data private or protected, and to make public only those functions that are part of a minimal interface for users of the class. This can hide the details of data implementation, allowing the designer to later fundamentally change the implementation without changing the interface in any way.[43][44]\nInheritance\n\nInheritance allows one data type to acquire properties of other data types. Inheritance from a base class may be declared as public, protected, or private. This access specifier determines whether unrelated and derived classes can access the inherited public and protected members of the base class. Only public inheritance corresponds to what is usually meant by \"inheritance\". The other two forms are much less frequently used. If the access specifier is omitted, a \"class\" inherits privately, while a \"struct\" inherits publicly. Base classes may be declared as virtual; this is called virtual inheritance. Virtual inheritance ensures that only one instance of a base class exists in the inheritance graph, avoiding some of the ambiguity problems of multiple inheritance.\n\nMultiple inheritance is a C++ feature not found in most other languages, allowing a class to be derived from more than one base class; this allows for more elaborate inheritance relationships. For example, a \"Flying Cat\" class can inherit from both \"Cat\" and \"Flying Mammal\". Some other languages, such as C# or Java, accomplish something similar (although more limited) by allowing inheritance of multiple interfaces while restricting the number of base classes to one (interfaces, unlike classes, provide only declarations of member functions, no implementation or member data). An interface as in C# and Java can be defined in C++ as a class containing only pure virtual functions, often known as an abstract base class or \"ABC\". The member functions of such an abstract base class are normally explicitly defined in the derived class, not inherited implicitly. C++ virtual inheritance exhibits an ambiguity resolution feature called dominance.\nOperators and operator overloading\nOperators that cannot be overloaded Operator \tSymbol\nScope resolution operator \t::\nConditional operator \t?:\ndot operator \t.\nMember selection operator \t.*\n\"sizeof\" operator \tsizeof\n\"typeid\" operator \ttypeid\nMain article: Operators in C and C++\n\nC++ provides more than 35 operators, covering basic arithmetic, bit manipulation, indirection, comparisons, logical operations and others. Almost all operators can be overloaded for user-defined types, with a few notable exceptions such as member access (. and .*) as well as the conditional operator. The rich set of overloadable operators is central to making user-defined types in C++ seem like built-in types.\n\nOverloadable operators are also an essential part of many advanced C++ programming techniques, such as smart pointers. Overloading an operator does not change the precedence of calculations involving the operator, nor does it change the number of operands that the operator uses (any operand may however be ignored by the operator, though it will be evaluated prior to execution). Overloaded \"&&\" and \"||\" operators lose their short-circuit evaluation property.\nPolymorphism\nSee also: Polymorphism in object-oriented programming\n\nPolymorphism enables one common interface for many implementations, and for objects to act differently under different circumstances.\n\nC++ supports several kinds of static (resolved at compile-time) and dynamic (resolved at run-time) polymorphisms, supported by the language features described above. Compile-time polymorphism does not allow for certain run-time decisions, while runtime polymorphism typically incurs a performance penalty.\nStatic polymorphism\nSee also: Parametric polymorphism and ad hoc polymorphism\n\nFunction overloading allows programs to declare multiple functions having the same name but with different arguments (i.e. ad hoc polymorphism). The functions are distinguished by the number or types of their formal parameters. Thus, the same function name can refer to different functions depending on the context in which it is used. The type returned by the function is not used to distinguish overloaded functions and would result in a compile-time error message.\n\nWhen declaring a function, a programmer can specify for one or more parameters a default value. Doing so allows the parameters with defaults to optionally be omitted when the function is called, in which case the default arguments will be used. When a function is called with fewer arguments than there are declared parameters, explicit arguments are matched to parameters in left-to-right order, with any unmatched parameters at the end of the parameter list being assigned their default arguments. In many cases, specifying default arguments in a single function declaration is preferable to providing overloaded function definitions with different numbers of parameters.\n\nTemplates in C++ provide a sophisticated mechanism for writing generic, polymorphic code (i.e. parametric polymorphism). In particular, through the Curiously Recurring Template Pattern, it's possible to implement a form of static polymorphism that closely mimics the syntax for overriding virtual functions. Because C++ templates are type-aware and Turing-complete, they can also be used to let the compiler resolve recursive conditionals and generate substantial programs through template metaprogramming. Contrary to some opinion, template code will not generate a bulk code after compilation with the proper compiler settings.[42]\nDynamic polymorphism\nInheritance\nSee also: Subtyping\n\nVariable pointers and references to a base class type in C++ can also refer to objects of any derived classes of that type. This allows arrays and other kinds of containers to hold pointers to objects of differing types (references cannot be directly held in containers). This enables dynamic (run-time) polymorphism, where the referred objects can behave differently depending on their (actual, derived) types\n\nC++ also provides the dynamic_cast operator, which allows code to safely attempt conversion of an object, via a base reference/pointer, to a more derived type: downcasting. The attempt is necessary as often one does not know which derived type is referenced. (Upcasting, conversion to a more general type, can always be checked/performed at compile-time via static_cast, as ancestral classes are specified in the derived class's interface, visible to all callers.) dynamic_cast relies on run-time type information (RTTI), metadata in the program that enables differentiating types and their relationships. If a dynamic_cast to a pointer fails, the result is the nullptr constant, whereas if the destination is a reference (which cannot be null), the cast throws an exception. Objects known to be of a certain derived type can be cast to that with static_cast, bypassing RTTI and the safe runtime type-checking of dynamic_cast, so this should be used only if the programmer is very confident the cast is, and will always be, valid.\nVirtual member functions\n\nOrdinarily, when a function in a derived class overrides a function in a base class, the function to call is determined by the type of the object. A given function is overridden when there exists no difference in the number or type of parameters between two or more definitions of that function. Hence, at compile time, it may not be possible to determine the type of the object and therefore the correct function to call, given only a base class pointer; the decision is therefore put off until runtime. This is called dynamic dispatch. Virtual member functions or methods[45] allow the most specific implementation of the function to be called, according to the actual run-time type of the object. In C++ implementations, this is commonly done using virtual function tables. If the object type is known, this may be bypassed by prepending a fully qualified class name before the function call, but in general calls to virtual functions are resolved at run time.\n\nIn addition to standard member functions, operator overloads and destructors can be virtual. As a rule of thumb, if any function in the class is virtual, the destructor should be as well. As the type of an object at its creation is known at compile time, constructors, and by extension copy constructors, cannot be virtual. Nonetheless a situation may arise where a copy of an object needs to be created when a pointer to a derived object is passed as a pointer to a base object. In such a case, a common solution is to create a clone() (or similar) virtual function that creates and returns a copy of the derived class when called.\n\nA member function can also be made \"pure virtual\" by appending it with = 0 after the closing parenthesis and before the semicolon. A class containing a pure virtual function is called an abstract data type. Objects cannot be created from abstract data types; they can only be derived from. Any derived class inherits the virtual function as pure and must provide a non-pure definition of it (and all other pure virtual functions) before objects of the derived class can be created. A program that attempts to create an object of a class with a pure virtual member function or inherited pure virtual member function is ill-formed.\nLambda expressions\n\nC++ provides support for anonymous functions, which are also known as lambda expressions and have the following form:\n\n[capture](parameters) -> return_type { function_body }\n\nThe [capture] list supports the definition of closures. Such lambda expressions are defined in the standard as syntactic sugar for an unnamed function object. An example lambda function may be defined as follows:\n\n[](int x, int y) -> int { return x + y; }\n\nException handling\n\nException handling is used to communicate the existence of a runtime problem or error from where it was detected to where the issue can be handled.[46] It permits this to be done in a uniform manner and separately from the main code, while detecting all errors.[47] Should an error occur, an exception is thrown (raised), which is then caught by the nearest suitable exception handler. The exception causes the current scope to be exited, and also each outer scope (propagation) until a suitable handler is found, calling in turn the destructors of any objects in these exited scopes.[48] At the same time, an exception is presented as an object carrying the data about the detected problem.[49]\n\nThe exception-causing code is placed inside a try block. The exceptions are handled in separate catch blocks (the handlers); each try block can have multiple exception handlers, as it is visible in the example below.[50]\n\n#include <iostream>\n#include <vector>\n#include <stdexcept>\n\nint main() {\n    try {\n        std::vector<int> vec{3,4,3,1};\n        int i{vec.at(4)}; // Throws an exception, std::out_of_range (indexing for vec is from 0-3 not 1-4)\n    }\n\n    // An exception handler, catches std::out_of_range, which is thrown by vec.at(4)\n    catch (std::out_of_range& e) {\n        std::cerr << \"Accessing a non-existent element: \" << e.what() << '\\n';\n    }\n\n    // To catch any other standard library exceptions (they derive from std::exception)\n    catch (std::exception& e) {\n        std::cerr << \"Exception thrown: \" << e.what() << '\\n';\n    }\n\n    // Catch any unrecognised exceptions (i.e. those which don't derive from std::exception)\n    catch (...) {\n        std::cerr << \"Some fatal error\\n\";\n    }\n}\n\nIt is also possible to raise exceptions purposefully, using the throw keyword; these exceptions are handled in the usual way. In some cases, exceptions cannot be used due to technical reasons. One such example is a critical component of an embedded system, where every operation must be guaranteed to complete within a specified amount of time. This cannot be determined with exceptions as no tools exist to determine the minimum time required for an exception to be handled.[51]\nStandard library\nMain article: C++ Standard Library\n\nThe C++ standard consists of two parts: the core language and the standard library. C++ programmers expect the latter on every major implementation of C++; it includes vectors, lists, maps, algorithms (find, for_each, binary_search, random_shuffle, etc.), sets, queues, stacks, arrays, tuples, input/output facilities (iostream, for reading from and writing to the console and files), smart pointers for automatic memory management, regular expression support, multi-threading library, atomics support (allowing a variable to be read or written to by at most one thread at a time without any external synchronisation), time utilities (measurement, getting current time, etc.), a system for converting error reporting that doesn't use C++ exceptions into C++ exceptions, a random number generator and a slightly modified version of the C standard library (to make it comply with the C++ type system).\n\nA large part of the C++ library is based on the Standard Template Library (STL). Useful tools provided by the STL include containers as the collections of objects (such as vectors and lists), iterators that provide array-like access to containers, and algorithms that perform operations such as searching and sorting.\n\nFurthermore, (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore, using templates it is possible to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the #include directive to include a standard header. C++ provides 105 standard headers, of which 27 are deprecated.\n\nThe standard incorporates the STL that was originally designed by Alexander Stepanov, who experimented with generic algorithms and containers for many years. When he started with C++, he finally found a language where it was possible to create generic algorithms (e.g., STL sort) that perform even better than, for example, the C standard library qsort, thanks to C++ features like using inlining and compile-time binding instead of function pointers. The standard does not refer to it as \"STL\", as it is merely a part of the standard library, but the term is still widely used to distinguish it from the rest of the standard library (input/output streams, internationalization, diagnostics, the C library subset, etc.).[52]\n\nMost C++ compilers, and all major ones, provide a standards conforming implementation of the C++ standard library.\nCompatibility\n\nTo give compiler vendors greater freedom, the C++ standards committee decided not to dictate the implementation of name mangling, exception handling, and other implementation-specific features. The downside of this decision is that object code produced by different compilers is expected to be incompatible. There were, however, attempts to standardize compilers for particular machines or operating systems (for example C++ ABI),[53] though they seem to be largely abandoned now.\nWith C\nFor more details on this topic, see Compatibility of C and C++.\n\nC++ is often considered to be a superset of C, but this is not strictly true.[54] Most C code can easily be made to compile correctly in C++, but there are a few differences that cause some valid C code to be invalid or behave differently in C++. For example, C allows implicit conversion from void* to other pointer types, but C++ does not (for type safety reasons). Also, C++ defines many new keywords, such as new and class, which may be used as identifiers (for example, variable names) in a C program.\n\nSome incompatibilities have been removed by the 1999 revision of the C standard (C99), which now supports C++ features such as line comments (//), and declarations mixed with code. On the other hand, C99 introduced a number of new features that C++ did not support, were incompatible or redundant in C++, such as variable-length arrays, native complex-number types (however, the std::complex class in the C++ standard library provides similar functionality, although not code compatible), designated initializers, compound literals, and the restrict keyword.[55] Some of the C99-introduced features were included in the subsequent version of the C++ standard, C++11 (out of those which were not redundant).[56][57][58] However, the C++11 standard introduces new incompatibilities, such as disallowing assignment of a string literal to a character pointer, which remains valid C.\n\nTo intermix C and C++ code, any function declaration or definition that is to be called from/used both in C and C++ must be declared with C linkage by placing it within an extern \"C\" {/*...*/} block. Such a function may not rely on features depending on name mangling (i.e., function overloading).\nCriticism\nMain article: Criticism of C++\n\nDespite its widespread adoption, many programmers have criticized the C++ language, including Linus Torvalds,[59] Richard Stallman,[60] and Ken Thompson.[61] Issues include a lack of reflection or garbage collection, slow compilation times, and verbose error messages, particularly from template metaprogramming.[62] Because C++ introduces an additional memory footprint on programs due to internally generated vtables and constructors,[citation needed] programmers such as Torvalds prefer C over C++ for low-level, performance-critical and portable system software.[59]\n\nTo avoid the problems that exist in C++, and to increase productivity,[63] some people suggest alternative languages newer than C++, such as D, Go, Rust and Vala.[64]", "skillName": "C++."}
{"id": 203, "category": "Computer_Programming", "skillText": "Java (programming language)\nJava is a general-purpose computer programming language that is concurrent, class-based, object-oriented,[14] and specifically designed to have as few implementation dependencies as possible. It is intended to let application developers \"write once, run anywhere\" (WORA),[15] meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.[16] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of computer architecture. As of 2016, Java is one of the most popular programming languages in use,[17][18][19][20] particularly for client-server web applications, with a reported 9 million developers.[21] Java was originally developed by James Gosling at Sun Microsystems (which has since been acquired by Oracle Corporation) and released in 1995 as a core component of Sun Microsystems' Java platform. The language derives much of its syntax from C and C++, but it has fewer low-level facilities than either of them.\n\nThe original and reference implementation Java compilers, virtual machines, and class libraries were originally released by Sun under proprietary licences. As of May 2007, in compliance with the specifications of the Java Community Process, Sun relicensed most of its Java technologies under the GNU General Public License. Others have also developed alternative implementations of these Sun technologies, such as the GNU Compiler for Java (bytecode compiler), GNU Classpath (standard libraries), and IcedTea-Web (browser plugin for applets).\n\nThe latest version is Java 8, which is the only version currently supported for free by Oracle, although earlier versions are supported both by Oracle and other companies on a commercial basis.\n\nDuke, the Java mascot\n\nJames Gosling, the creator of Java (2008)\n\nThe TIOBE programming language popularity index graph from 2002 to 2015. Over the course of a decade Java (blue) and C (black) competing for the top position.\nJames Gosling, Mike Sheridan, and Patrick Naughton initiated the Java language project in June 1991.[22] Java was originally designed for interactive television, but it was too advanced for the digital cable television industry at the time.[23] The language was initially called Oak after an oak tree that stood outside Gosling's office. Later the project went by the name Green and was finally renamed Java, from Java coffee.[24] Gosling designed Java with a C/C++-style syntax that system and application programmers would find familiar.[25]\n\nSun Microsystems released the first public implementation as Java 1.0 in 1995.[26] It promised \"Write Once, Run Anywhere\" (WORA), providing no-cost run-times on popular platforms. Fairly secure and featuring configurable security, it allowed network- and file-access restrictions. Major web browsers soon incorporated the ability to run Java applets within web pages, and Java quickly became popular, while mostly outside of browsers, that wasn't the original plan. In January 2016, Oracle announced that Java runtime environments based on JDK 9 will discontinue the browser plugin.[27] The Java 1.0 compiler was re-written in Java by Arthur van Hoff to comply strictly with the Java 1.0 language specification.[28] With the advent of Java 2 (released initially as J2SE 1.2 in December 1998 � 1999), new versions had multiple configurations built for different types of platforms. J2EE included technologies and APIs for enterprise applications typically run in server environments, while J2ME featured APIs optimized for mobile applications. The desktop version was renamed J2SE. In 2006, for marketing purposes, Sun renamed new J2 versions as Java EE, Java ME, and Java SE, respectively.\n\nIn 1997, Sun Microsystems approached the ISO/IEC JTC 1 standards body and later the Ecma International to formalize Java, but it soon withdrew from the process.[29][30][31] Java remains a de facto standard, controlled through the Java Community Process.[32] At one time, Sun made most of its Java implementations available without charge, despite their proprietary software status. Sun generated revenue from Java through the selling of licenses for specialized products such as the Java Enterprise System.\n\nOn November 13, 2006, Sun released much of its Java virtual machine (JVM) as free and open-source software, (FOSS), under the terms of the GNU General Public License (GPL). On May 8, 2007, Sun finished the process, making all of its JVM's core code available under free software/open-source distribution terms, aside from a small portion of code to which Sun did not hold the copyright.[33]\n\nSun's vice-president Rich Green said that Sun's ideal role with regard to Java was as an \"evangelist\".[34] Following Oracle Corporation's acquisition of Sun Microsystems in 2009�10, Oracle has described itself as the \"steward of Java technology with a relentless commitment to fostering a community of participation and transparency\".[35] This did not prevent Oracle from filing a lawsuit against Google shortly after that for using Java inside the Android SDK (see Google section below). Java software runs on everything from laptops to data centers, game consoles to scientific supercomputers.[36] On April 2, 2010, James Gosling resigned from Oracle.[37]\n\nPrinciples\nThere were five primary goals in the creation of the Java language:[16]\n\nIt must be \"simple, object-oriented, and familiar\".\nIt must be \"robust and secure\".\nIt must be \"architecture-neutral and portable\".\nIt must execute with \"high performance\".\nIt must be \"interpreted, threaded, and dynamic\".\nVersions\nMain article: Java version history\nAs of 2015, only Java 8 is supported (\"publicly\"). Major release versions of Java, along with their release dates:\n\nJDK 1.0 (January 21, 1996)\nJDK 1.1 (February 19, 1997)\nJ2SE 1.2 (December 8, 1998)\nJ2SE 1.3 (May 8, 2000)\nJ2SE 1.4 (February 6, 2002)\nJ2SE 5.0 (September 30, 2004)\nJava SE 6 (December 11, 2006)\nJava SE 7 (July 28, 2011)\nJava SE 8 (March 18, 2014)\nPractices\nJava platform\nMain articles: Java (software platform) and Java virtual machine\n\nJava Control Panel, version 7\nOne design goal of Java is portability, which means that programs written for the Java platform must run similarly on any combination of hardware and operating system with adequate runtime support. This is achieved by compiling the Java language code to an intermediate representation called Java bytecode, instead of directly to architecture-specific machine code. Java bytecode instructions are analogous to machine code, but they are intended to be executed by a virtual machine (VM) written specifically for the host hardware. End users commonly use a Java Runtime Environment (JRE) installed on their own machine for standalone Java applications, or in a web browser for Java applets.\n\nStandard libraries provide a generic way to access host-specific features such as graphics, threading, and networking.\n\nThe use of universal bytecode makes porting simple. However, the overhead of interpreting bytecode into machine instructions makes interpreted programs almost always run more slowly than native executables. However, just-in-time (JIT) compilers that compile bytecodes to machine code during runtime were introduced from an early stage. Java itself is platform-independent, and is adapted to the particular platform it is to run on by a Java virtual machine for it, which translates the Java bytecode into the platform's machine language.[38]\n\nImplementations\nSee also: Free Java implementations\nOracle Corporation is the current owner of the official implementation of the Java SE platform, following their acquisition of Sun Microsystems on January 27, 2010. This implementation is based on the original implementation of Java by Sun. The Oracle implementation is available for Microsoft Windows (still works for XP, while only later versions currently \"publicly\" supported), Mac OS X, Linux and Solaris. Because Java lacks any formal standardization recognized by Ecma International, ISO/IEC, ANSI, or other third-party standards organization, the Oracle implementation is the de facto standard.\n\nThe Oracle implementation is packaged into two different distributions: The Java Runtime Environment (JRE) which contains the parts of the Java SE platform required to run Java programs and is intended for end users, and the Java Development Kit (JDK), which is intended for software developers and includes development tools such as the Java compiler, Javadoc, Jar, and a debugger.\n\nOpenJDK is another notable Java SE implementation that is licensed under the GNU GPL. The implementation started when Sun began releasing the Java source code under the GPL. As of Java SE 7, OpenJDK is the official Java reference implementation.\n\nThe goal of Java is to make all implementations of Java compatible. Historically, Sun's trademark license for usage of the Java brand insists that all implementations be \"compatible\". This resulted in a legal dispute with Microsoft after Sun claimed that the Microsoft implementation did not support RMI or JNI and had added platform-specific features of their own. Sun sued in 1997, and in 2001 won a settlement of US$20 million, as well as a court order enforcing the terms of the license from Sun.[39] As a result, Microsoft no longer ships Java with Windows.\n\nPlatform-independent Java is essential to Java EE, and an even more rigorous validation is required to certify an implementation. This environment enables portable server-side applications.\n\nPerformance\nMain article: Java performance\nPrograms written in Java have a reputation for being slower and requiring more memory than those written in C++.[40][41] However, Java programs' execution speed improved significantly with the introduction of just-in-time compilation in 1997/1998 for Java 1.1,[42] the addition of language features supporting better code analysis (such as inner classes, the StringBuilder class, optional assertions, etc.), and optimizations in the Java virtual machine, such as HotSpot becoming the default for Sun's JVM in 2000. With Java 1.5, the performance was improved with the addition of the java.util.concurrent package, including Lock free implementations of the ConcurrentMaps and other multi-core collections, and it was improved further Java 1.6.\n\nSome platforms offer direct hardware support for Java; there are microcontrollers that can run Java in hardware instead of a software Java virtual machine, and ARM based processors can have hardware support for executing Java bytecode through their Jazelle option (while its support is mostly dropped in current implementations of ARM).\n\nAutomatic memory management\nJava uses an automatic garbage collector to manage memory in the object lifecycle. The programmer determines when objects are created, and the Java runtime is responsible for recovering the memory once objects are no longer in use. Once no references to an object remain, the unreachable memory becomes eligible to be freed automatically by the garbage collector. Something similar to a memory leak may still occur if a programmer's code holds a reference to an object that is no longer needed, typically when objects that are no longer needed are stored in containers that are still in use. If methods for a nonexistent object are called, a \"null pointer exception\" is thrown.[43][44]\n\nOne of the ideas behind Java's automatic memory management model is that programmers can be spared the burden of having to perform manual memory management. In some languages, memory for the creation of objects is implicitly allocated on the stack, or explicitly allocated and deallocated from the heap. In the latter case the responsibility of managing memory resides with the programmer. If the program does not deallocate an object, a memory leak occurs. If the program attempts to access or deallocate memory that has already been deallocated, the result is undefined and difficult to predict, and the program is likely to become unstable and/or crash. This can be partially remedied by the use of smart pointers, but these add overhead and complexity. Note that garbage collection does not prevent \"logical\" memory leaks, i.e., those where the memory is still referenced but never used.\n\nGarbage collection may happen at any time. Ideally, it will occur when a program is idle. It is guaranteed to be triggered if there is insufficient free memory on the heap to allocate a new object; this can cause a program to stall momentarily. Explicit memory management is not possible in Java.\n\nJava does not support C/C++ style pointer arithmetic, where object addresses and unsigned integers (usually long integers) can be used interchangeably. This allows the garbage collector to relocate referenced objects and ensures type safety and security.\n\nAs in C++ and some other object-oriented languages, variables of Java's primitive data types are either stored directly in fields (for objects) or on the stack (for methods) rather than on the heap, as is commonly true for non-primitive data types (but see escape analysis). This was a conscious decision by Java's designers for performance reasons.\n\nJava contains multiple types of garbage collectors. By default,[citation needed] HotSpot uses the parallel scavenge garbage collector. However, there are also several other garbage collectors that can be used to manage the heap. For 90% of applications in Java, the Concurrent Mark-Sweep (CMS) garbage collector is sufficient.[45] Oracle aims to replace CMS with the Garbage-First collector (G1).[46]\n\nSyntax\nMain article: Java syntax\nThe syntax of Java is largely influenced by C++. Unlike C++, which combines the syntax for structured, generic, and object-oriented programming, Java was built almost exclusively as an object-oriented language.[16] All code is written inside classes, and every data item is an object, with the exception of the primitive data types, i.e. integers, floating-point numbers, boolean values, and characters, which are not objects for performance reasons. Java reuses some popular aspects of C++ (such as printf() method).\n\nUnlike C++, Java does not support operator overloading[47] or multiple inheritance for classes, though multiple inheritance is supported for interfaces.[48] This simplifies the language and aids in preventing potential errors and anti-pattern design.[citation needed]\n\nJava uses comments similar to those of C++. There are three different styles of comments: a single line style marked with two slashes (//), a multiple line style opened with /* and closed with */, and the Javadoc commenting style opened with /** and closed with */. The Javadoc style of commenting allows the user to run the Javadoc executable to create documentation for the program.\n\nExample:\n\n// This is an example of a single line comment using two slashes\n\n/* This is an example of a multiple line comment using the slash and asterisk.\n This type of comment can be used to hold a lot of information or deactivate\n code, but it is very important to remember to close the comment. */\n\npackage fibsandlies;\nimport java.util.HashMap;\n\n/**\n * This is an example of a Javadoc comment; Javadoc can compile documentation\n * from this text. Javadoc comments must immediately precede the class, method, or field being documented.\n */\npublic class FibCalculator extends Fibonacci implements Calculator {\n    private static Map<Integer, Integer> memoized = new HashMap<Integer, Integer>();\n\n    /*\n     * The main method written as follows is used by the JVM as a starting point for the program.\n     */\n    public static void main(String[] args) {\n        memoized.put(1, 1);\n        memoized.put(2, 1);\n        System.out.println(fibonacci(12)); //Get the 12th Fibonacci number and print to console\n    }\n\n    /**\n     * An example of a method written in Java, wrapped in a class.\n     * Given a non-negative number FIBINDEX, returns\n     * the Nth Fibonacci number, where N equals FIBINDEX.\n     * @param fibIndex The index of the Fibonacci number\n     * @return The Fibonacci number\n     */\n    public static int fibonacci(int fibIndex) {\n        if (memoized.containsKey(fibIndex)) {\n            return memoized.get(fibIndex);\n        } else {\n            int answer = fibonacci(fibIndex - 1) + fibonacci(fibIndex - 2);\n            memoized.put(fibIndex, answer);\n            return answer;\n        }\n    }\n}\nExamples\n\"Hello, world!\" program\nThe traditional \"Hello, world!\" program can be written in Java as:[49]\n\nclass HelloWorldApp {\n    public static void main(String[] args) {\n        System.out.println(\"Hello World!\"); // Prints the string to the console.\n    }\n}\nSource files must be named after the public class they contain, appending the suffix .java, for example, HelloWorldApp.java. It must first be compiled into bytecode, using a Java compiler, producing a file named HelloWorldApp.class. Only then can it be executed, or \"launched\". The Java source file may only contain one public class, but it can contain multiple classes with other than public access and any number of public inner classes. When the source file contains multiple classes, make one class \"public\" and name the source file with that public class name.\n\nA class that is not declared public may be stored in any .java file. The compiler will generate a class file for each class defined in the source file. The name of the class file is the name of the class, with .class appended. For class file generation, anonymous classes are treated as if their name were the concatenation of the name of their enclosing class, a $, and an integer.\n\nThe keyword public denotes that a method can be called from code in other classes, or that a class may be used by classes outside the class hierarchy. The class hierarchy is related to the name of the directory in which the .java file is located. This is called an access level modifier. Other access level modifiers include the keywords private and protected.\n\nThe keyword static in front of a method indicates a static method, which is associated only with the class and not with any specific instance of that class. Only static methods can be invoked without a reference to an object. Static methods cannot access any class members that are not also static. Methods that are not designated static are instance methods, and require a specific instance of a class to operate.\n\nThe keyword void indicates that the main method does not return any value to the caller. If a Java program is to exit with an error code, it must call System.exit() explicitly.\n\nThe method name \"main\" is not a keyword in the Java language. It is simply the name of the method the Java launcher calls to pass control to the program. Java classes that run in managed environments such as applets and Enterprise JavaBeans do not use or need a main() method. A Java program may contain multiple classes that have main methods, which means that the VM needs to be explicitly told which class to launch from.\n\nThe main method must accept an array of String objects. By convention, it is referenced as args although any other legal identifier name can be used. Since Java 5, the main method can also use variable arguments, in the form of public static void main(String... args), allowing the main method to be invoked with an arbitrary number of String arguments. The effect of this alternate declaration is semantically identical (the args parameter is still an array of String objects), but it allows an alternative syntax for creating and passing the array.\n\nThe Java launcher launches Java by loading a given class (specified on the command line or as an attribute in a JAR) and starting its public static void main(String[]) method. Stand-alone programs must declare this method explicitly. The String[] args parameter is an array of String objects containing any arguments passed to the class. The parameters to main are often passed by means of a command line.\n\nPrinting is part of a Java standard library: The System class defines a public static field called out. The out object is an instance of the PrintStream class and provides many methods for printing data to standard out, including println(String) which also appends a new line to the passed string.\n\nThe string \"Hello World!\" is automatically converted to a String object by the compiler.\n\nComprehensive example\n\n[hide]This section has multiple issues. Please help improve it or discuss these issues on the talk page. (Learn how and when to remove these template messages)\nThis section does not cite any sources. (May 2013)\nThis section contains instructions, advice, or how-to content. (May 2013)\n// OddEven.java\nimport javax.swing.JOptionPane;\n\npublic class OddEven {\n\n    private int userInput; // a whole number(\"int\" means integer)\n\n    /**\n     * This is the constructor method. It gets called when an object of the OddEven type\n     * is being created.\n     */\n    public OddEven() {\n        /*\n         * In most Java programs constructors can initialize objects with default values, or create\n         * other objects that this object might use to perform its functions. In some Java programs, the\n         * constructor may simply be an empty function if nothing needs to be initialized prior to the\n         * functioning of the object. In this program's case, an empty constructor would suffice.\n         * A constructor must exist; however, if the user doesn't put one in then the compiler\n         * will create an empty one.\n         */\n    }\n\n    /**\n     * This is the main method. It gets called when this class is run through a Java interpreter.\n     * @param args command line arguments (unused)\n     */\n    public static void main(final String[] args) {\n       /*\n        * This line of code creates a new instance of this class called \"number\" (also known as an\n        * Object) and initializes it by calling the constructor. The next line of code calls\n        * the \"showDialog()\" method, which brings up a prompt to ask you for a number.\n        */\n       OddEven number = new OddEven();\n       number.showDialog();\n    }\n\n    public void showDialog() {\n        /*\n         * \"try\" makes sure nothing goes wrong. If something does,\n         * the interpreter skips to \"catch\" to see what it should do.\n         */\n        try {\n            /*\n             * The code below brings up a JOptionPane, which is a dialog box\n             * The String returned by the \"showInputDialog()\" method is converted into\n             * an integer, making the program treat it as a number instead of a word.\n             * After that, this method calls a second method, calculate() that will\n             * display either \"Even\" or \"Odd.\"\n             */\n            userInput = Integer.parseInt(JOptionPane.showInputDialog(\"Please enter a number.\"));\n            calculate();\n        } catch (final NumberFormatException e) {\n            /*\n             * Getting in the catch block means that there was a problem with the format of\n             * the number. Probably some letters were typed in instead of a number.\n             */\n            System.err.println(\"ERROR: Invalid input. Please type in a numerical value.\");\n        }\n    }\n\n    /**\n     * When this gets called, it sends a message to the interpreter.\n     * The interpreter usually shows it on the command prompt (For Windows users)\n     * or the terminal (For *nix users).(Assuming it's open)\n     */\n    private void calculate() {\n        if ((userInput % 2) == 0) {\n            JOptionPane.showMessageDialog(null, \"Even\");\n        } else {\n            JOptionPane.showMessageDialog(null, \"Odd\");\n        }\n    }\n}\nThe import statement imports the JOptionPane class from the javax.swing package.\nThe OddEven class declares a single private field of type int named userInput. Every instance of the OddEven class has its own copy of the userInput field. The private declaration means that no other class can access (read or write) the userInput field.\nOddEven() is a public constructor. Constructors have the same name as the enclosing class they are declared in, and unlike a method, have no return type. A constructor is used to initialize an object that is a newly created instance of the class.\nThe calculate() method is declared without the static keyword. This means that the method is invoked using a specific instance of the OddEven class. (The reference used to invoke the method is passed as an undeclared parameter of type OddEven named this.) The method tests the expression userInput % 2 == 0 using the if keyword to see if the remainder of dividing the userInput field belonging to the instance of the class by two is zero. If this expression is true, then it prints Even; if this expression is false it prints Odd. (The calculate method can be equivalently accessed as this.calculate and the userInput field can be equivalently accessed as this.userInput, which both explicitly use the undeclared this parameter.)\nOddEven number = new OddEven(); declares a local object reference variable in the main method named number. This variable can hold a reference to an object of type OddEven. The declaration initializes number by first creating an instance of the OddEven class, using the new keyword and the OddEven() constructor, and then assigning this instance to the variable.\nThe statement number.showDialog(); calls the calculate method. The instance of OddEven object referenced by the number local variable is used to invoke the method and passed as the undeclared this parameter to the calculate method.\nuserInput = Integer.parseInt(JOptionPane.showInputDialog(\"Please Enter A Number\")); is a statement that converts the type of String to the primitive data type int by using a utility function in the primitive wrapper class Integer.\nSpecial classes\n\nThis section contains instructions, advice, or how-to content. The purpose of Wikipedia is to present facts, not to train. Please help improve this article either by rewriting the how-to content or by moving it to Wikiversity, Wikibooks or Wikivoyage. (January 2012)\nApplet\nMain article: Java applet\nJava applets are programs that are embedded in other applications, typically in a Web page displayed in a web browser.\n\n// Hello.java\nimport javax.swing.JApplet;\nimport java.awt.Graphics;\n\npublic class Hello extends JApplet {\n    public void paintComponent(final Graphics g) {\n        g.drawString(\"Hello, world!\", 65, 95);\n    }\n}\nThe import statements direct the Java compiler to include the javax.swing.JApplet and java.awt.Graphics classes in the compilation. The import statement allows these classes to be referenced in the source code using the simple class name (i.e. JApplet) instead of the fully qualified class name (FQCN, i.e. javax.swing.JApplet).\n\nThe Hello class extends (subclasses) the JApplet (Java Applet) class; the JApplet class provides the framework for the host application to display and control the lifecycle of the applet. The JApplet class is a JComponent (Java Graphical Component) which provides the applet with the capability to display a graphical user interface (GUI) and respond to user events.\n\nThe Hello class overrides the paintComponent(Graphics) method (additionally indicated with the annotation, supported as of JDK 1.5, Override) inherited from the Container superclass to provide the code to display the applet. The paintComponent() method is passed a Graphics object that contains the graphic context used to display the applet. The paintComponent() method calls the graphic context drawString(String, int, int) method to display the \"Hello, world!\" string at a pixel offset of (65, 95) from the upper-left corner in the applet's display.\n\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n\"http://www.w3.org/TR/html4/strict.dtd\">\n<!-- Hello.html -->\n<html>\n    <head>\n        <title>Hello World Applet</title>\n    </head>\n    <body>\n        <applet code=\"Hello.class\" width=\"200\" height=\"200\">\n        </applet>\n    </body>\n</html>\nAn applet is placed in an HTML document using the <applet> HTML element. The applet tag has three attributes set: code=\"Hello\" specifies the name of the JApplet class and width=\"200\" height=\"200\" sets the pixel width and height of the applet. Applets may also be embedded in HTML using either the object or embed element,[50] although support for these elements by web browsers is inconsistent.[51] However, the applet tag is deprecated, so the object tag is preferred where supported.\n\nThe host application, typically a Web browser, instantiates the Hello applet and creates an AppletContext for the applet. Once the applet has initialized itself, it is added to the AWT display hierarchy. The paintComponent() method is called by the AWT event dispatching thread whenever the display needs the applet to draw itself.\n\nServlet\nMain article: Java Servlet\nJava Servlet technology provides Web developers with a simple, consistent mechanism for extending the functionality of a Web server and for accessing existing business systems. Servlets are server-side Java EE components that generate responses (typically HTML pages) to requests (typically HTTP requests) from clients. A servlet can almost be thought of as an applet that runs on the server side�without a face.\n\n// Hello.java\nimport java.io.*;\nimport javax.servlet.*;\n\npublic class Hello extends GenericServlet {\n    public void service(final ServletRequest request, final ServletResponse response)\n    throws ServletException, IOException {\n        response.setContentType(\"text/html\");\n        final PrintWriter pw = response.getWriter();\n        try {\n            pw.println(\"Hello, world!\");\n        } finally {\n            pw.close();\n        }\n    }\n}\nThe import statements direct the Java compiler to include all the public classes and interfaces from the java.io and javax.servlet packages in the compilation. Packages make Java well suited for large scale applications.\n\nThe Hello class extends the GenericServlet class; the GenericServlet class provides the interface for the server to forward requests to the servlet and control the servlet's lifecycle.\n\nThe Hello class overrides the service(ServletRequest, ServletResponse) method defined by the Servlet interface to provide the code for the service request handler. The service() method is passed: a ServletRequest object that contains the request from the client and a ServletResponse object used to create the response returned to the client. The service() method declares that it throws the exceptions ServletException and IOException if a problem prevents it from responding to the request.\n\nThe setContentType(String) method in the response object is called to set the MIME content type of the returned data to \"text/html\". The getWriter() method in the response returns a PrintWriter object that is used to write the data that is sent to the client. The println(String) method is called to write the \"Hello, world!\" string to the response and then the close() method is called to close the print writer, which causes the data that has been written to the stream to be returned to the client.\n\nJavaServer Pages\nMain article: JavaServer Pages\nJavaServer Pages (JSP) are server-side Java EE components that generate responses, typically HTML pages, to HTTP requests from clients. JSPs embed Java code in an HTML page by using the special delimiters <% and %>. A JSP is compiled to a Java servlet, a Java application in its own right, the first time it is accessed. After that, the generated servlet creates the response.\n\nSwing application\nMain article: Swing (Java)\nSwing is a graphical user interface library for the Java SE platform. It is possible to specify a different look and feel through the pluggable look and feel system of Swing. Clones of Windows, GTK+ and Motif are supplied by Sun. Apple also provides an Aqua look and feel for Mac OS X. Where prior implementations of these looks and feels may have been considered lacking, Swing in Java SE 6 addresses this problem by using more native GUI widget drawing routines of the underlying platforms.\n\nThis example Swing application creates a single window with \"Hello, world!\" inside:\n\n// Hello.java (Java SE 5)\nimport javax.swing.*;\n\npublic class Hello extends JFrame {\n    public Hello() {\n        super(\"hello\");\n        super.setDefaultCloseOperation(WindowConstants.EXIT_ON_CLOSE);\n        super.add(new JLabel(\"Hello, world!\"));\n        super.pack();\n        super.setVisible(true);\n    }\n\n    public static void main(final String[] args) {\n        new Hello();\n    }\n}\nThe first import includes all the public classes and interfaces from the javax.swing package.\n\nThe Hello class extends the JFrame class; the JFrame class implements a window with a title bar and a close control.\n\nThe Hello() constructor initializes the frame by first calling the superclass constructor, passing the parameter \"hello\", which is used as the window's title. It then calls the setDefaultCloseOperation(int) method inherited from JFrame to set the default operation when the close control on the title bar is selected to WindowConstants.EXIT_ON_CLOSE � this causes the JFrame to be disposed of when the frame is closed (as opposed to merely hidden), which allows the Java virtual machine to exit and the program to terminate. Next, a JLabel is created for the string \"Hello, world!\" and the add(Component) method inherited from the Container superclass is called to add the label to the frame. The pack() method inherited from the Window superclass is called to size the window and lay out its contents.\n\nThe main() method is called by the Java virtual machine when the program starts. It instantiates a new Hello frame and causes it to be displayed by calling the setVisible(boolean) method inherited from the Component superclass with the boolean parameter true. Once the frame is displayed, exiting the main method does not cause the program to terminate because the AWT event dispatching thread remains active until all of the Swing top-level windows have been disposed.\n\nGenerics\nMain article: Generics in Java\nIn 2004, generics were added to the Java language, as part of J2SE 5.0. Prior to the introduction of generics, each variable declaration had to be of a specific type. For container classes, for example, this is a problem because there is no easy way to create a container that accepts only specific types of objects. Either the container operates on all subtypes of a class or interface, usually Object, or a different container class has to be created for each contained class. Generics allow compile-time type checking without having to create many container classes, each containing almost identical code. In addition to enabling more efficient code, certain runtime exceptions are converted to compile-time errors, a characteristic known as type safety.\n\nCriticism\nMain article: Criticism of Java\nCriticisms directed at Java include the implementation of generics,[52] speed,[53] the handling of unsigned numbers,[54] the implementation of floating-point arithmetic,[55] and a history of security vulnerabilities in the primary Java VM implementation HotSpot.[56]\n\nUse on unofficial software platforms\nThe Java programming language requires the presence of a software platform in order for compiled programs to be executed. A well-known unofficial Java-like software platform is the Android software platform, which allows the use of Java 6 and some Java 7 features, uses a different standard library (Apache Harmony reimplementation), different bytecode language and different virtual machine, and is designed for low-memory devices such as smartphones and tablet computers.\n\n\nThe Android operating system makes extensive use of Java-related technology.\nGoogle\nSee also: Oracle America, Inc. v. Google, Inc.\nThe Java language is a key pillar in Android, an open source mobile operating system. Although Android, built on the Linux kernel, was written largely in C, the Android SDK uses the Java language as the basis for Android applications. However, Android does not use the standard Java virtual machine, instead using Java bytecode as an intermediate step which is transformed into Dalvik bytecode. Depending on the Android version, this is then either interpreted by the Dalvik virtual machine, or compiled into native code by the Android Runtime.\n\nAndroid also does not provide the full Java SE standard library, although the Android class library does include an independent implementation of a large subset of it. This led to a legal dispute between Oracle and Google. On May 7, 2012, a San Francisco jury found that if APIs could be copyrighted, then Google had infringed Oracle's copyrights by the use of Java in Android devices.[57] District Judge William Haskell Alsup ruled on May 31, 2012, that APIs cannot be copyrighted,[58] but this was reversed by the United States Court of Appeals for the Federal Circuit in May 2014.[59][60][61]\n\nClass libraries\n\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (December 2014) (Learn how and when to remove this template message)\nMain article: Java Class Library\nThe Java Class Library is the standard library, developed to support application development in Java. It is controlled by Sun Microsystems in cooperation with others through the Java Community Process program. Companies or individuals participating in this process can influence the design and development of the APIs. This process has been a subject of controversy.[when?] The class library contains features such as:\n\nThe core libraries, which include:\nIO/NIO\nNetworking\nReflection\nConcurrency\nGenerics\nScripting/Compiler\nFunctional Programming (Lambda, Streaming)\nCollection libraries that implement data structures such as lists, dictionaries, trees, sets, queues and double-ended queue, or stacks[62]\nXML Processing (Parsing, Transforming, Validating) libraries\nSecurity[63]\nInternationalization and localization libraries[64]\nThe integration libraries, which allow the application writer to communicate with external systems. These libraries include:\nThe Java Database Connectivity (JDBC) API for database access\nJava Naming and Directory Interface (JNDI) for lookup and discovery\nRMI and CORBA for distributed application development\nJMX for managing and monitoring applications\nUser interface libraries, which include:\nThe (heavyweight, or native) Abstract Window Toolkit (AWT), which provides GUI components, the means for laying out those components and the means for handling events from those components\nThe (lightweight) Swing libraries, which are built on AWT but provide (non-native) implementations of the AWT widgetry\nAPIs for audio capture, processing, and playback\nJavaFX\nA platform dependent implementation of the Java virtual machine that is the means by which the bytecodes of the Java libraries and third party applications are executed\nPlugins, which enable applets to be run in web browsers\nJava Web Start, which allows Java applications to be efficiently distributed to end users across the Internet\nLicensing and documentation\nDocumentation\nMain article: Javadoc\nJavadoc is a comprehensive documentation system, created by Sun Microsystems, used by many Java developers[by whom?]. It provides developers with an organized system for documenting their code. Javadoc comments have an extra asterisk at the beginning, i.e. the delimiters are /** and */, whereas the normal multi-line comments in Java are set off with the delimiters /* and */.[65]\n\nEditions\nSee also: Free Java implementations � Class library\nJava editions\nWave.svg\nJava Card\nMicro Edition (ME)\nStandard Edition (SE)\nEnterprise Edition (EE)\nJavaFX (Merged to Java SE 8)\nPersonalJava (discontinued)\nv t e\nSun has defined and supports four editions of Java targeting different application environments and segmented many of its APIs so that they belong to one of the platforms. The platforms are:\n\nJava Card for smartcards.[66]\nJava Platform, Micro Edition (Java ME) � targeting environments with limited resources.[67]\nJava Platform, Standard Edition (Java SE) � targeting workstation environments.[68]\nJava Platform, Enterprise Edition (Java EE) � targeting large distributed enterprise or Internet environments.[69]\nThe classes in the Java APIs are organized into separate groups called packages. Each package contains a set of related interfaces, classes and exceptions. Refer to the separate platforms for a description of the packages available.[relevant to this section? � discuss]\n\nSun also provided an edition called PersonalJava that has been superseded by later, standards-based Java ME configuration-profile pairings.\n\nSee also\nPortal icon\tJava portal\nPortal icon\tComputer programming portal\nBook icon\nBook: Programming for Students\nDalvik  used in old Android versions, replaced by non-JIT Android Runtime\nJavaOne\nJavapedia\nList of Java virtual machines\nList of Java APIs\nList of JVM languages\nGraal (compiler), a project aiming to implement a high performance Java dynamic compiler and interpreter\nSpring Framework\nComparison of Java with other languages\nComparison of programming languages\nComparison of Java and C++\nComparison of C# and Java", "skillName": "Java."}
{"id": 204, "category": "Computer_Programming", "skillText": "Objective-J is a programming language developed as part of the Cappuccino web development framework. Its syntax is nearly identical to the Objective-C syntax and it shares with JavaScript the same relationship that Objective-C has with the C programming language: that of being a strict, but small, superset; adding traditional inheritance and Smalltalk/Objective-C style dynamic dispatch. Pure JavaScript, being a prototype-based language, already has a notion of object orientation and inheritance, but Objective-J adds the use of class-based programming to JavaScript.\n\nPrograms written in Objective-J need to be preprocessed before being run by a web browser's JavaScript virtual machine. This step can occur in the web browser at runtime or by a compiler which translates Objective-J programs into pure JavaScript code. The Objective-J compiler is written in JavaScript; consequently, deploying Objective-J programs does not require a web browser plug-in. Objective-J can be compiled and run on Node.js.\n\nContents\n\n    1 Applications\n        1.1 Applications designed using the Cappuccino Framework[1]\n    2 Syntax\n    3 Memory management\n    4 See also\n    5 References\n    6 External links\n\nApplications\n\nThe first widely known use of Objective-J was in the Cappuccino-based web application 280 Slides, which was developed by 280 North itself. Even though Objective-J can be used (and has been designed) independently from the Cappuccino framework, Objective-J has primarily been invented to support web development in Cappuccino.\nApplications designed using the Cappuccino Framework[1]\n\n    Mockingbird \n    PicEngine \n    GithubIssues \n    TimeTable \n    Enstore \n    (until October 2013, they rewrote it using Ember [2])\n    Almost At \n    Akshell - Online JavaScript Web-App IDE \n    Archipel Project - Virtual machine orchestrator \n    Spot Specific - Mobile App SDK and IDE \n\nSyntax\n\nObjective-J is a superset of JavaScript, which means that any valid JavaScript code is also valid Objective-J code.\n\nThe following example shows the definition and implementation in Objective-J of a class named Address; this class extends the root object CPObject, which plays a role similar to the Objective-C's NSObject. This example differs from traditional Objective-C in that the root object reflects the underlying Cappuccino framework as opposed to Cocoa, Objective-J does not use pointers and, as such, type definitions do not contain asterisk characters. Instance variables are always defined in the @implementation.\n\n@implementation Address : CPObject\n{\n  CPString name;\n  CPString city;\n}\n\n- (id)initWithName:(CPString)aName city:(CPString)aCity\n{\n  self = [super init];\n\n  name = aName;\n  city = aCity;\n\n  return self;\n}\n\n- (void)setName:(CPString)aName\n{\n  name = aName;\n}\n\n- (CPString)name\n{\n  return name;\n}\n\n+ (id)newAddressWithName:(CPString)aName city:(CPString)aCity\n{\n  return [[self alloc] initWithName:aName city:aCity];\n}\n\n@end\n\nAs with Objective-C, class method definitions and instance method definitions start with '+' (plus) and '-' (dash), respectively.\nMemory management\n\nObjective-C uses ARC (Automatic Reference Counting) for deallocating unused objects. In Objective-J, objects are automatically deallocated by JavaScript's Garbage Collector.", "skillName": "Objective-J."}
{"id": 205, "category": "Computer_Programming", "skillText": "PHP is a server-side scripting language designed for web development but also used as a general-purpose programming language. Originally created by Rasmus Lerdorf in 1994,[4] the PHP reference implementation is now produced by The PHP Group.[5] PHP originally stood for Personal Home Page,[4] but it now stands for the recursive backronym PHP: Hypertext Preprocessor.[6]\n\nPHP code may be embedded into HTML code, or it can be used in combination with various web template systems, web content management systems and web frameworks. PHP code is usually processed by a PHP interpreter implemented as a module in the web server or as a Common Gateway Interface (CGI) executable. The web server combines the results of the interpreted and executed PHP code, which may be any type of data, including images, with the generated web page. PHP code may also be executed with a command-line interface (CLI) and can be used to implement standalone graphical applications.[7]\n\nThe standard PHP interpreter, powered by the Zend Engine, is free software released under the PHP License. PHP has been widely ported and can be deployed on most web servers on almost every operating system and platform, free of charge.[8]\n\nThe PHP language evolved without a written formal specification or standard until 2014, leaving the canonical PHP interpreter as a de facto standard. Since 2014 work has gone on to create a formal PHP specification.[9]\n\nDuring the 2010s there have been increased efforts towards standardisation and code sharing in PHP applications by projects such as PHP-FIG \nin the form of PSR-initiatives \nas well as Composer dependency manager and the Packagist repository \n.\n\nContents\n\n    1 History\n        1.1 Early history\n        1.2 PHP 3 and 4\n        1.3 PHP 5\n        1.4 PHP 6 and Unicode\n        1.5 PHP 7\n        1.6 Release history\n    2 Mascot\n    3 Syntax\n        3.1 Data types\n        3.2 Functions\n        3.3 PHP Objects\n    4 Implementations\n    5 Licensing\n    6 Development and community\n    7 Installation and configuration\n    8 Use\n    9 Security\n    10 See also\n    11 References\n    12 Further reading\n    13 External links\n\nHistory\nEarly history\nRasmus Lerdorf, who wrote the original Common Gateway Interface (CGI) component, together with Andi Gutmans and Zeev Suraski, who rewrote the parser that formed PHP 3.\n\nPHP development began in 1995 when Rasmus Lerdorf wrote several Common Gateway Interface (CGI) programs in C,[10][11][12] which he used to maintain his personal homepage. He extended them to work with web forms and to communicate with databases, and called this implementation \"Personal Home Page/Forms Interpreter\" or PHP/FI.\n\nPHP/FI could be used to build simple, dynamic web applications. To accelerate bug reporting and improve the code, Lerdorf initially announced the release of PHP/FI as \"Personal Home Page Tools (PHP Tools) version 1.0\" on the Usenet discussion group comp.infosystems.www.authoring.cgi on June 8, 1995.[13][14] This release already had the basic functionality that PHP has as of 2013. This included Perl-like variables, form handling, and the ability to embed HTML. The syntax resembled that of Perl but was simpler, more limited and less consistent.[5]\n\nEarly PHP was not intended to be a new programming language, and grew organically, with Lerdorf noting in retrospect: \"I don’t know how to stop it, there was never any intent to write a programming language […] I have absolutely no idea how to write a programming language, I just kept adding the next logical step on the way.\"[15] A development team began to form and, after months of work and beta testing, officially released PHP/FI 2 in November 1997.\n\nThe fact that PHP was not originally designed but instead was developed organically has led to inconsistent naming of functions and inconsistent ordering of their parameters.[16] In some cases, the function names were chosen to match the lower-level libraries which PHP was \"wrapping\",[17] while in some very early versions of PHP the length of the function names was used internally as a hash function, so names were chosen to improve the distribution of hash values.[18]\nPHP 3 and 4\nThis is an example of custom php code on a computer screen.\n\nZeev Suraski and Andi Gutmans rewrote the parser in 1997 and formed the base of PHP 3, changing the language's name to the recursive acronym PHP: Hypertext Preprocessor.[5][19] Afterwards, public testing of PHP 3 began, and the official launch came in June 1998. Suraski and Gutmans then started a new rewrite of PHP's core, producing the Zend Engine in 1999.[20] They also founded Zend Technologies in Ramat Gan, Israel.[5]\n\nOn May 22, 2000, PHP 4, powered by the Zend Engine 1.0, was released.[5] As of August 2008 this branch reached version 4.4.9. PHP 4 is no longer under development nor will any security updates be released.[21][22]\nPHP 5\n\nOn July 14, 2004, PHP 5 was released, powered by the new Zend Engine II.[5] PHP 5 included new features such as improved support for object-oriented programming, the PHP Data Objects (PDO) extension (which defines a lightweight and consistent interface for accessing databases), and numerous performance enhancements.[23] In 2008 PHP 5 became the only stable version under development. Late static binding had been missing from PHP and was added in version 5.3.[24][25]\n\nMany high-profile open-source projects ceased to support PHP 4 in new code as of February 5, 2008, because of the GoPHP5 initiative,[26] provided by a consortium of PHP developers promoting the transition from PHP 4 to PHP 5.[27][28]\n\nOver time, PHP interpreters became available on most existing 32-bit and 64-bit operating systems, either by building them from the PHP source code, or by using pre-built binaries.[29] For the PHP versions 5.3 and 5.4, the only available Microsoft Windows binary distributions were 32-bit x86 builds,[30][31] requiring Windows 32-bit compatibility mode while using Internet Information Services (IIS) on a 64-bit Windows platform. PHP version 5.5 made the 64-bit x86-64 builds available for Microsoft Windows.[32]\nPHP 6 and Unicode\n\nPHP received mixed reviews due to lacking native Unicode support at the core language level.[33][34] In 2005, a project headed by Andrei Zmievski was initiated to bring native Unicode support throughout PHP, by embedding the International Components for Unicode (ICU) library, and representing text strings as UTF-16 internally.[35] Since this would cause major changes both to the internals of the language and to user code, it was planned to release this as version 6.0 of the language, along with other major features then in development.[36]\n\nHowever, a shortage of developers who understood the necessary changes, and performance problems arising from conversion to and from UTF-16, which is rarely used in a web context, led to delays in the project.[37] As a result, a PHP 5.3 release was created in 2009, with many non-Unicode features back-ported from PHP 6, notably namespaces. In March 2010, the project in its current form was officially abandoned, and a PHP 5.4 release was prepared containing most remaining non-Unicode features from PHP 6, such as traits and closure re-binding.[38] Initial hopes were that a new plan would be formed for Unicode integration, but as of 2014 none have been adopted.\nPHP 7\n\nDuring 2014 and 2015, a new major PHP version was developed, which was numbered PHP 7. The numbering of this version involved some debate.[39] While the PHP 6 Unicode experiment had never been released, several articles and book titles referenced the PHP 6 name, which might have caused confusion if a new release were to reuse the name.[40] After a vote, the name PHP 7 was chosen.[41]\n\nThe foundation of PHP 7 is a PHP branch that was originally dubbed PHP next generation (phpng). It was authored by Dmitry Stogov, Xinchen Hui and Nikita Popov,[42] and aimed to optimize PHP performance by refactoring the Zend Engine while retaining near-complete language compatibility.[43] As of 14 July 2014, WordPress-based benchmarks, which served as the main benchmark suite for the phpng project, showed an almost 100% increase in performance. Changes from phpng are also expected to make it easier to improve performance in the future, as more compact data structures and other changes are seen as better suited for a successful migration to a just-in-time (JIT) compiler.[44] Because of the significant changes, the reworked Zend Engine is called Zend Engine 3, succeeding Zend Engine 2 used in PHP 5.[45]\n\nBecause of major internal changes in phpng, it must receive a new major version number of PHP, rather than a minor PHP 5 release, according to PHP's release process.[46] Major versions of PHP are allowed to break backward-compatibility of code and therefore PHP 7 presented an opportunity for other improvements beyond phpng that require backward-compatibility breaks. In particular, it involved the following changes:\n\n    Many fatal- or recoverable-level legacy PHP error mechanisms were replaced with modern object-oriented exceptions[47]\n    The syntax for variable dereferencing was reworked to be internally more consistent and complete, allowing the use of the operators ->, [], (), {}, and :: with arbitrary meaningful left-hand-side expressions[48]\n    Support for legacy PHP 4-style constructor methods was deprecated[49]\n    The behavior of the foreach statement was changed to be more predictable[50]\n    Constructors for the few classes built-in to PHP which returned null upon failure were changed to throw an exception instead, for consistency[51]\n    Several unmaintained or deprecated server application programming interfaces (SAPIs) and extensions were removed from the PHP core, most notably the legacy mysql extension[52]\n    The behavior of the list() operator was changed to remove support for strings[53]\n    Support for legacy ASP-style PHP code delimiters (<% and %>, <script language=php> and </script>) was removed[54]\n    An oversight allowing a switch statement to have multiple default clauses was fixed[55]\n    Support for hexadecimal number support in some implicit conversions from strings to number types was removed[56]\n    The left-shift and right-shift operators were changed to behave more consistently across platforms[57]\n    Conversions between integers and floating point numbers were tightened and implemented more consistently across platforms[57][58]\n\nPHP 7 also included new language features. Most notably, it introduces return type declarations for functions,[59] which complement the existing parameter type declarations, and support for the scalar types (integer, float, string, and boolean) in parameter and return type declarations.[60]\nRelease history\nKey Color \tMeaning \tDevelopment\nRed \tOld release \tNo development\nYellow \tStable release \tSecurity fixes\nGreen \tStable release \tBug and security fixes\nBlue \tFuture release \tNew features\nVersion \tRelease date \tSupported until[61] \tNotes\n1.0 \t8 June 1995 \t\tOfficially called \"Personal Home Page Tools (PHP Tools)\". This is the first use of the name \"PHP\".[5]\n2.0 \t1 November 1997 \t\tOfficially called \"PHP/FI 2.0\". This is the first release that could actually be characterised as PHP, being a standalone language with many features that have endured to the present day.\n3.0 \t6 June 1998 \t20 October 2000[61] \tDevelopment moves from one person to multiple developers. Zeev Suraski and Andi Gutmans rewrite the base for this version.[5]\n4.0 \t22 May 2000 \t23 June 2001[61] \tAdded more advanced two-stage parse/execute tag-parsing system called the Zend engine.[62]\n4.1 \t10 December 2001 \t12 March 2002[61] \tIntroduced \"superglobals\" ($_GET, $_POST, $_SESSION, etc.)[62]\n4.2 \t22 April 2002 \t6 September 2002[61] \tDisabled register_globals by default. Data received over the network is not inserted directly into the global namespace anymore, closing possible security holes in applications.[62]\n4.3 \t27 December 2002 \t31 March 2005[61] \tIntroduced the command-line interface (CLI), to supplement the CGI.[62][63]\n4.4 \t11 July 2005 \t7 August 2008[61] \tFixed a memory corruption bug, which required breaking binary compatibility with extensions compiled against PHP version 4.3.x.[64]\n5.0 \t13 July 2004 \t5 September 2005[61] \tZend Engine II with a new object model.[65]\n5.1 \t24 November 2005 \t24 August 2006[61] \tPerformance improvements with introduction of compiler variables in re-engineered PHP Engine.[65] Added PHP Data Objects (PDO) as a consistent interface for accessing databases.[66]\n5.2 \t2 November 2006 \t6 January 2011[61] \tEnabled the filter extension by default. Native JSON support.[65]\n5.3 \t30 June 2009 \t14 August 2014[61] \tNamespace support; late static bindings, jump label (limited goto), closures, PHP archives (phar), garbage collection for circular references, improved Windows support, sqlite3, mysqlnd as a replacement for libmysql as underlying library for the extensions that work with MySQL, fileinfo as a replacement for mime_magic for better MIME support, the Internationalization extension, and deprecation of ereg extension.\n5.4 \t1 March 2012 \t3 September 2015[61] \tTrait support, short array syntax support. Removed items: register_globals, safe_mode, allow_call_time_pass_reference, session_register(), session_unregister() and session_is_registered(). Built-in web server.[67] Several improvements to existing features, performance and reduced memory requirements.\n5.5 \t20 June 2013 \t10 July 2016[68] \tSupport for generators, finally blocks for exceptions handling, OpCache (based on Zend Optimizer+) bundled in official distribution.[69]\n5.6 \t28 August 2014 \t31 December 2018[68] \tConstant scalar expressions, variadic functions, argument unpacking, new exponentiation operator, extensions of the use statement for functions and constants, new phpdbg debugger as a SAPI module, and other smaller improvements.[70]\n6.x \tNot released \tN/A \tAbandoned version of PHP that planned to include native Unicode support.[71][72]\n7.0 \t3 December 2015[2] \t3 December 2018[46] \tZend Engine 3 (performance improvements[44] and 64-bit integer support on Windows[73]), uniform variable syntax,[48] AST-based compilation process,[74] added Closure::call(),[75] bitwise shift consistency across platforms,[76] ?? (null coalesce) operator,[77] Unicode codepoint escape syntax,[78] return type declarations,[59] scalar type (integer, float, string and boolean) declarations,[60] <=> \"spaceship\" three-way comparison operator,[79] generator delegation,[80] anonymous classes,[81] simpler and more consistently available CSPRNG API,[82] replacement of many remaining internal PHP \"errors\" with the more modern exceptions,[47] and shorthand syntax for importing multiple items from a namespace.[83]\n7.1 \tNovember 2016[84] \t3 years after release[46] \tvoid return type,[85] class constant visibility modifiers,[86] nullable types[87]\n\nBeginning on June 28, 2011, the PHP Group implemented a timeline for the release of new versions of PHP.[46] Under this system, at least one release should occur every month. Once per year, a minor release should occur which may include new features. Every minor release should at least be supported for two years with security and bug fixes, followed by at least one year of only security fixes, for a total of a three-year release process for every minor release. No new features, unless small and self-contained, are to be introduced into a minor release during the three-year release process.\nMascot\nThe elePHPant, PHP mascot.\n\nThe mascot of the PHP project is the elePHPant, a blue elephant with the PHP logo on its side, designed by Vincent Pontier[88] in 1998.[89] The elePHPant is sometimes differently colored when in plush toy form.\nSyntax\nMain article: PHP syntax and semantics\n\nThe following \"Hello, World!\" program is written in PHP code embedded in an HTML document:\n\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>PHP Test</title>\n    </head>\n    <body>\n        <?php echo '<p>Hello World</p>'; ?>\n    </body>\n</html>\n\nHowever, as no requirement exists for PHP code to be embedded in HTML, the simplest version of Hello, World! may be written like this, with the closing tag omitted as preferred in files containing pure PHP code[90]\n\n <?='Hello world';\n\nThe PHP interpreter only executes PHP code within its delimiters. Anything outside its delimiters is not processed by PHP, although non-PHP text is still subject to control structures described in PHP code. The most common delimiters are <?php to open and ?> to close PHP sections. The shortened form <? also exists. This short delimiter makes script files less portable, since support for them can be disabled in the local PHP configuration and it is therefore discouraged.[91][92] However, there is no recommendation against the use of the echo short tag <?=.[93] Prior to PHP 5.4.0, this short syntax for echo() only works with the short_open_tag configuration setting enabled, while for PHP 5.4.0 and later it is always available.[91][94][95] The purpose of all these delimiters is to separate PHP code from non-PHP content, such as JavaScript code or HTML markup.[96]\n\nThe first form of delimiters, <?php and ?>, in XHTML and other XML documents, creates correctly formed XML processing instructions.[97] This means that the resulting mixture of PHP code and other markup in the server-side file is itself well-formed XML.\n\nVariables are prefixed with a dollar symbol, and a type does not need to be specified in advance. PHP 5 introduced type hinting that allows functions to force their parameters to be objects of a specific class, arrays, interfaces or callback functions. However, before PHP 7.0, type hints could not be used with scalar types such as integer or string.[60]\n\nUnlike function and class names, variable names are case sensitive. Both double-quoted (\"\") and heredoc strings provide the ability to interpolate a variable's value into the string.[98] PHP treats newlines as whitespace in the manner of a free-form language, and statements are terminated by a semicolon.[99] PHP has three types of comment syntax: /* */ marks block and inline comments; // as well as # are used for one-line comments.[100] The echo statement is one of several facilities PHP provides to output text, e.g., to a web browser.\n\nIn terms of keywords and language syntax, PHP is similar to the C style syntax. if conditions, for and while loops, and function returns are similar in syntax to languages such as C, C++, C#, Java and Perl.\nData types\n\nPHP stores integers in a platform-dependent range, either a 64-bit or 32-bit signed integer equivalent to the C-language long type. Unsigned integers are converted to signed values in certain situations; this behavior is different from that of other programming languages.[101] Integer variables can be assigned using decimal (positive and negative), octal, hexadecimal, and binary notations.\n\nFloating point numbers are also stored in a platform-specific range. They can be specified using floating point notation, or two forms of scientific notation.[102] PHP has a native Boolean type that is similar to the native Boolean types in Java and C++. Using the Boolean type conversion rules, non-zero values are interpreted as true and zero as false, as in Perl and C++.[102]\n\nThe null data type represents a variable that has no value; NULL is the only allowed value for this data type.[102]\n\nVariables of the \"resource\" type represent references to resources from external sources. These are typically created by functions from a particular extension, and can only be processed by functions from the same extension; examples include file, image, and database resources.[102]\n\nArrays can contain elements of any type that PHP can handle, including resources, objects, and even other arrays. Order is preserved in lists of values and in hashes with both keys and values, and the two can be intermingled.[102] PHP also supports strings, which can be used with single quotes, double quotes, nowdoc or heredoc syntax.[103]\n\nThe Standard PHP Library (SPL) attempts to solve standard problems and implements efficient data access interfaces and classes.[104]\nFunctions\n\nPHP defines a large array of functions in the core language and many are also available in various extensions; these functions are well documented in the online PHP documentation.[105] However, the built-in library has a wide variety of naming conventions and associated inconsistencies, as described under history above.\n\nCustom functions may be defined by the developer, e.g.:\n\nfunction myAge($birthYear) {                                  // defines a function, this one is named \"myAge\"\n    $yearsOld = date('Y') - $birthYear;                       // calculates the age\n    return $yearsOld . ' year' . ($yearsOld != 1 ? 's' : ''); // returns the age in a descriptive form\n}\n\necho 'I am currently ' . myAge(1981) . ' old.';               // outputs the text concatenated\n                                                              // with the return value of myAge()\n// As the result of this syntax, myAge() is called.\n\nIn 2016, the output of the above sample program is 'I am currently 35 years old.'\n\nIn lieu of function pointers, functions in PHP can be referenced by a string containing their name. In this manner, normal PHP functions can be used, for example, as callbacks or within function tables.[106] User-defined functions may be created at any time without being prototyped.[105][106] Functions may be defined inside code blocks, permitting a run-time decision as to whether or not a function should be defined. There is a function_exists function that determines whether a function with a given name has already been defined. Function calls must use parentheses, with the exception of zero-argument class constructor functions called with the PHP operator new, in which case parentheses are optional.\n\nUntil PHP 5.3, support for anonymous functions and closures did not exist in PHP. While create_function() exists since PHP 4.0.1, it is merely a thin wrapper around eval() that allows normal PHP functions to be created during program execution.[107] PHP 5.3 added syntax to define an anonymous function or \"closure\"[108] which can capture variables from the surrounding scope:\n\nfunction getAdder($x) {\n    return function($y) use ($x) {\n        return $x + $y;\n    };\n}\n\n$adder = getAdder(8);\necho $adder(2); // prints \"10\"\n\nIn the example above, getAdder() function creates a closure using passed argument $x (the keyword use imports a variable from the lexical context), which takes an additional argument $y, and returns the created closure to the caller. Such a function is a first-class object, meaning that it can be stored in a variable, passed as a parameter to other functions, etc.[109]\n\nUnusually for a dynamically typed language, PHP supports type declarations on function parameters, which are enforced at runtime. This has been supported for classes and interfaces since PHP 5.0, for arrays since PHP 5.1, for \"callables\" since PHP 5.4, and scalar (integer, float, string and boolean) types since PHP 7.0.[60] PHP 7.0 also has type declarations for function return types, expressed by placing the type name after the list of parameters, preceded by a colon.[59] For example, the getAdder function from the earlier example could be annotated with types like so in PHP 7:\n\nfunction getAdder(int $x): \\Closure {\n    return function(int $y) use ($x) : int {\n        return $x + $y;\n    };\n}\n\n$adder = getAdder(8);\necho $adder(2);        // prints \"10\"\necho $adder(null);     // throws an exception because an incorrect type was passed\n$adder = getAdder([]); // would also throw an exception\n\nBy default, scalar type declarations follow weak typing principles. So, for example, if a parameter's type is int, PHP would allow not only integers, but also convertible numeric strings, floats or booleans to be passed to that function, and would convert them.[60] However, PHP 7 has a \"strict typing\" mode which, when used, disallows such conversions for function calls and returns within a file.[60]\nPHP Objects\n\nBasic object-oriented programming functionality was added in PHP 3 and improved in PHP 4.[5] This allowed for PHP to gain further abstraction, making creative tasks easier for programmers using the language. Object handling was completely rewritten for PHP 5, expanding the feature set and enhancing performance.[110] In previous versions of PHP, objects were handled like value types.[110] The drawback of this method was that code had to make heavy use of PHP's \"reference\" variables if it wanted to modify an object it was passed rather than creating a copy of it. In the new approach, objects are referenced by handle, and not by value.\n\nPHP 5 introduced private and protected member variables and methods, along with abstract classes, final classes, abstract methods, and final methods. It also introduced a standard way of declaring constructors and destructors, similar to that of other object-oriented languages such as C++, and a standard exception handling model. Furthermore, PHP 5 added interfaces and allowed for multiple interfaces to be implemented. There are special interfaces that allow objects to interact with the runtime system. Objects implementing ArrayAccess can be used with array syntax and objects implementing Iterator or IteratorAggregate can be used with the foreach language construct. There is no virtual table feature in the engine, so static variables are bound with a name instead of a reference at compile time.[111]\n\nIf the developer creates a copy of an object using the reserved word clone, the Zend engine will check whether a __clone() method has been defined. If not, it will call a default __clone() which will copy the object's properties. If a __clone() method is defined, then it will be responsible for setting the necessary properties in the created object. For convenience, the engine will supply a function that imports the properties of the source object, so the programmer can start with a by-value replica of the source object and only override properties that need to be changed.[112]\n\nThe following is a basic example of object-oriented programming in PHP:\n\nclass Person\n{\n    public $firstName;\n    public $lastName;\n\n    public function __construct($firstName, $lastName = '') { // optional second argument\n        $this->firstName = $firstName;\n        $this->lastName  = $lastName;\n    }\n\n    public function greet() {\n        return 'Hello, my name is ' . $this->firstName .\n               (($this->lastName != '') ? (' ' . $this->lastName) : '') . '.';\n    }\n\n    public static function staticGreet($firstName, $lastName) {\n        return 'Hello, my name is ' . $firstName . ' ' . $lastName . '.';\n    }\n}\n\n$he    = new Person('John', 'Smith');\n$she   = new Person('Sally', 'Davis');\n$other = new Person('iAmine');\n\necho $he->greet(); // prints \"Hello, my name is John Smith.\"\necho '<br />';\n\necho $she->greet(); // prints \"Hello, my name is Sally Davis.\"\necho '<br />';\n\necho $other->greet(); // prints \"Hello, my name is iAmine.\"\necho '<br />';\n\necho Person::staticGreet('Jane', 'Doe'); // prints \"Hello, my name is Jane Doe.\"\n\nThe visibility of PHP properties and methods is defined using the keywords public, private, and protected. The default is public, if only var is used; var is a synonym for public. Items declared public can be accessed everywhere. protected limits access to inherited classes (and to the class that defines the item). private limits visibility only to the class that defines the item.[113] Objects of the same type have access to each other's private and protected members even though they are not the same instance. PHP's member visibility features have sometimes been described as \"highly useful.\"[114] However, they have also sometimes been described as \"at best irrelevant and at worst positively harmful.\"[115]\nImplementations\n\nThe original, only complete and most widely used PHP implementation is powered by the Zend Engine and known simply as PHP. To disambiguate it from other implementations, it is sometimes unofficially referred to as \"Zend PHP\". The Zend Engine compiles PHP source code on-the-fly into an internal format that it can execute, thus it works as an interpreter.[116][117] It is also the \"reference implementation\" of PHP, as PHP has no formal specification, and so the semantics of Zend PHP define the semantics of PHP itself. Due to the complex and nuanced semantics of PHP, defined by how Zend works, it is difficult for competing implementations to offer complete compatibility.\n\nPHP's single-request-per-script-execution model, and the fact the Zend Engine is an interpreter, leads to inefficiency; as a result, various products have been developed to help improve PHP performance. In order to speed up execution time and not have to compile the PHP source code every time the web page is accessed, PHP scripts can also be deployed in the PHP engine's internal format by using an opcode cache, which works by caching the compiled form of a PHP script (opcodes) in shared memory to avoid the overhead of parsing and compiling the code every time the script runs. An opcode cache, Zend Opcache, is built into PHP since version 5.5.[118] Another example of a widely used opcode cache is the Alternative PHP Cache (APC), which is available as a PECL extension.[119]\n\nWhile Zend PHP is still the most popular implementation, several other implementations have been developed. Some of these are compilers or support JIT compilation, and hence offer performance benefits over Zend PHP at the expense of lacking full PHP compatibility. Alternative implementations include the following:\n\n    HipHop Virtual Machine (HHVM) – developed at Facebook and available as open source, it converts PHP code into a high-level bytecode (commonly known as an intermediate language), which is then translated into x86-64 machine code dynamically at runtime by a just-in-time (JIT) compiler, resulting in up to 6× performance improvements.[120]\n    Parrot – a virtual machine designed to run dynamic languages efficiently; Pipp transforms the PHP source code into the Parrot intermediate representation, which is then translated into the Parrot's bytecode and executed by the virtual machine.\n    Phalanger – compiles PHP into Common Intermediate Language (CIL) bytecode\n    HipHop – developed at Facebook and available as open source, it transforms the PHP scripts into C++ code and then compiles the resulting code, reducing the server load up to 50%. In early 2013, Facebook deprecated it in favor of HHVM due to multiple reasons, including deployment difficulties and lack of support for the whole PHP language, including the create_function() and eval() constructs.[121]\n\nLicensing\n\nPHP is free software released under the PHP License, which stipulates that:[122]\n\n    Products derived from this software may not be called \"PHP\", nor may \"PHP\" appear in their name, without prior written permission from group@php.net. You may indicate that your software works in conjunction with PHP by saying \"Foo for PHP\" instead of calling it \"PHP Foo\" or \"phpfoo\".\n\nThis restriction on use of \"PHP\" makes the PHP License incompatible with the General Public License (GPL), while the Zend License is incompatible due to an advertising clause similar to that of the original BSD license.[123]\nDevelopment and community\n\nPHP includes various free and open-source libraries in its source distribution, or uses them in resulting PHP binary builds. PHP is fundamentally an Internet-aware system with built-in modules for accessing File Transfer Protocol (FTP) servers and many database servers, including PostgreSQL, MySQL, Microsoft SQL Server and SQLite (which is an embedded database), LDAP servers, and others. Numerous functions familiar to C programmers, such as those in the stdio family, are available in standard PHP builds.[124]\n\nPHP allows developers to write extensions in C to add functionality to the PHP language. PHP extensions can be compiled statically into PHP or loaded dynamically at runtime. Numerous extensions have been written to add support for the Windows API, process management on Unix-like operating systems, multibyte strings (Unicode), cURL, and several popular compression formats. Other PHP features made available through extensions include integration with IRC, dynamic generation of images and Adobe Flash content, PHP Data Objects (PDO) as an abstraction layer used for accessing databases,[125][126][127][128][129][130][131] and even speech synthesis. Some of the language's core functions, such as those dealing with strings and arrays, are also implemented as extensions.[132] The PHP Extension Community Library (PECL) project is a repository for extensions to the PHP language.[133]\n\nSome other projects, such as Zephir, provide the ability for PHP extensions to be created in a high-level language and compiled into native PHP extensions. Such an approach, instead of writing PHP extensions directly in C, simplifies the development of extensions and reduces the time required for programming and testing.[134]\n\nThe PHP Group consists of ten people (as of 2015): Thies C. Arntzen, Stig Bakken, Shane Caraveo, Andi Gutmans, Rasmus Lerdorf, Sam Ruby, Sascha Schumann, Zeev Suraski, Jim Winstead, Andrei Zmievski.[135]\n\nZend Technologies provides a PHP Certification based on PHP 5.5[136] exam for programmers to become certified PHP developers.\nInstallation and configuration\n\nThere are two primary ways for adding support for PHP to a web server – as a native web server module, or as a CGI executable. PHP has a direct module interface called Server Application Programming Interface (SAPI), which is supported by many web servers including Apache HTTP Server, Microsoft IIS, Netscape (now defunct) and iPlanet. Some other web servers, such as OmniHTTPd, support the Internet Server Application Programming Interface (ISAPI), which is a Microsoft's web server module interface. If PHP has no module support for a web server, it can always be used as a Common Gateway Interface (CGI) or FastCGI processor; in that case, the web server is configured to use PHP's CGI executable to process all requests to PHP files.[137]\n\nPHP-FPM (FastCGI Process Manager) is an alternative FastCGI implementation for PHP, bundled with the official PHP distribution since version 5.3.3.[138] When compared to the older FastCGI implementation, it contains some additional features, mostly useful for heavily loaded web servers.[139]\n\nWhen using PHP for command-line scripting, a PHP command-line interface (CLI) executable is needed. PHP supports a CLI SAPI as of PHP 4.3.0.[140] The main focus of this SAPI is developing shell applications using PHP. There are quite a few differences between the CLI SAPI and other SAPIs, although they do share many of the same behaviors.[141]\n\nPHP has a direct module interface called SAPI for different web servers;[142] in case of PHP 5 and Apache 2.0 on Windows, it is provided in form of a DLL file called php5apache2.dll,[143] which is a module that, among other functions, provides an interface between PHP and the web server, implemented in a form that the server understands. This form is what is known as a SAPI.\n\nThere are different kinds of SAPIs for various web server extensions. For example, in addition to those listed above, other SAPIs for the PHP language include the Common Gateway Interface (CGI) and command-line interface (CLI).[142][144]\n\nPHP can also be used for writing desktop graphical user interface (GUI) applications, by using the PHP-GTK extension. PHP-GTK is not included in the official PHP distribution,[137] and as an extension it can be used only with PHP versions 5.1.0 and newer. The most common way of installing PHP-GTK is compiling it from the source code.[145]\n\nWhen PHP is installed and used in cloud environments, software development kits (SDKs) are provided for using cloud-specific features. For example:\n\n    Amazon Web Services provides the AWS SDK for PHP[146]\n    Windows Azure can be used with the Windows Azure SDK for PHP.[147]\n\nNumerous configuration options are supported, affecting both core PHP features and extensions.[148][149] Configuration file php.ini is searched for in different locations, depending on the way PHP is used.[150] The configuration file is split into various sections,[151] while some of the configuration options can be also set within the web server configuration.[152]\nUse\nA broad overview of the LAMP software bundle, displayed here together with Squid.\n\nPHP is a general-purpose scripting language that is especially suited to server-side web development, in which case PHP generally runs on a web server. Any PHP code in a requested file is executed by the PHP runtime, usually to create dynamic web page content or dynamic images used on websites or elsewhere.[153] It can also be used for command-line scripting and client-side graphical user interface (GUI) applications. PHP can be deployed on most web servers, many operating systems and platforms, and can be used with many relational database management systems (RDBMS). Most web hosting providers support PHP for use by their clients. It is available free of charge, and the PHP Group provides the complete source code for users to build, customize and extend for their own use.[8]\nDynamic web page: example of server-side scripting (PHP and MySQL).\n\nPHP acts primarily as a filter,[154] taking input from a file or stream containing text and/or PHP instructions and outputting another stream of data. Most commonly the output will be HTML, although it could be JSON, XML or binary data such as image or audio formats. Since PHP 4, the PHP parser compiles input to produce bytecode for processing by the Zend Engine, giving improved performance over its interpreter predecessor.[155]\n\nOriginally designed to create dynamic web pages, PHP now focuses mainly on server-side scripting,[156] and it is similar to other server-side scripting languages that provide dynamic content from a web server to a client, such as Microsoft's ASP.NET, Sun Microsystems' JavaServer Pages,[157] and mod_perl. PHP has also attracted the development of many software frameworks that provide building blocks and a design structure to promote rapid application development (RAD). Some of these include PRADO, CakePHP, Symfony, CodeIgniter, Laravel, Yii Framework, Phalcon and Zend Framework, offering features similar to other web frameworks.\n\nThe LAMP architecture has become popular in the web industry as a way of deploying web applications.[158] PHP is commonly used as the P in this bundle alongside Linux, Apache and MySQL, although the P may also refer to Python, Perl, or some mix of the three. Similar packages, WAMP and MAMP, are also available for Windows and OS X, with the first letter standing for the respective operating system. Although both PHP and Apache are provided as part of the Mac OS X base install, users of these packages seek a simpler installation mechanism that can be more easily kept up to date.\n\nAs of April 2007, over 20 million Internet domains had web services hosted on servers with PHP installed and mod_php was recorded as the most popular Apache HTTP Server module.[159] As of October 2010, PHP was used as the server-side programming language on 75% of all websites whose server-side programming language was known[160] (as of February 2014, the percentage had reached 82%[161]), and PHP was the most-used open source software within enterprises.[162] Web content management systems written in PHP include MediaWiki,[163] Joomla,[164] eZ Publish, eZ Platform, SilverStripe,[165] WordPress,[166] Drupal,[167] Moodle,[168] the user-facing portion of Facebook,[169] and Digg.[170]\n\nFor specific and more advanced usage scenarios, PHP offers a well defined and documented way for writing custom extensions in C or C++.[171][172][173][174][175][176][177] Besides extending the language itself in form of additional libraries, extensions are providing a way for improving execution speed where it is critical and there is room for improvements by using a true compiled language.[178][179] PHP also offers well defined ways for embedding itself into other software projects. That way PHP can be easily used as an internal scripting language for another project, also providing tight interfacing with the project's specific internal data structures.[180]\n\nPHP received mixed reviews due to lacking support for multithreading at the core language level,[181] though using threads is made possible by the \"pthreads\" PECL extension.[182][183]\n\nAs of January 2013, PHP was used in more than 240 million websites (39% of those sampled) and was installed on 2.1 million web servers.[184]\nSecurity\n\nIn 2013, 9% of all vulnerabilities listed by the National Vulnerability Database were linked to PHP;[185] historically, about 30% of all vulnerabilities listed since 1996 in this database are linked to PHP. Technical security flaws of the language itself or of its core libraries are not frequent (22 in 2009, about 1% of the total although PHP applies to about 20% of programs listed).[186] Recognizing that programmers make mistakes, some languages include taint checking to automatically detect the lack of input validation which induces many issues. Such a feature is being developed for PHP,[187] but its inclusion into a release has been rejected several times in the past.[188][189]\n\nThere are advanced protection patches such as Suhosin and Hardening-Patch, especially designed for web hosting environments.[190]\n\nThere are certain language features and configuration parameters (primarily the default values for such runtime settings) that make PHP applications prone to security issues. Among these, magic_quotes_gpc and register_globals[191] configuration directives are the best known; the latter made any URL parameters become PHP variables, opening a path for serious security vulnerabilities by allowing an attacker to set the value of any uninitialized global variable and interfere with the execution of a PHP script. Support for \"magic quotes\" and \"register globals\" has been deprecated as of PHP 5.3.0, and removed as of PHP 5.4.0.[192]\n\nAnother example for the runtime settings vulnerability comes from failing to disable PHP execution (via engine configuration directive)[193] for the directory where uploaded images are stored; leaving the default settings can result in execution of malicious PHP code embedded within the uploaded images.[194][195][196] Also, leaving enabled the dynamic loading of PHP extensions (via enable_dl configuration directive)[197] in a shared web hosting environment can lead to security issues.[198][199]\n\nAlso, implied type conversions that result in incompatible values being treated as identical against the programmer's intent can lead to security issues. For example, the result of the comparison 0e1234 == 0 comparison is true because the first compared value is treated as scientific notation having the value (0×101234), i.e. zero. This feature resulted in authentication vulnerabilities in Simple Machines Forum,[200] Typo3[201] and phpBB[202] when MD5 password hashes were compared. Instead, either the function strcmp or the identity operator (===) should be used; 0e1234 === 0 results in false.[203]\n\nIn a 2013 analysis of over 170,000 website defacements, published by Zone-H, the most frequently (53%) used technique was exploitation of file inclusion vulnerability, mostly related to insecure usage of the PHP functions include, require, and allow_url_fopen.[204][205]", "skillName": "PHP."}
{"id": 206, "category": "Computer_Programming", "skillText": "System programming (or systems programming) is the activity of programming computer system software. The primary distinguishing characteristic of systems programming when compared to application programming is that application programming aims to produce software which provides services to the user directly (e.g. word processor), whereas systems programming aims to produce software and software platforms which provide services to other software, are performance constrained, or both (e.g. operating systems, computational science applications, game engines and AAA video games, industrial automation, and software as a service applications).[1]\n\nSystem programming requires a great degree of hardware awareness. Its goal is to achieve efficient use of available resources, either because the software itself is performance critical (AAA video games) or because even small efficiency improvements directly transform into significant monetary savings for the service provider (cloud based word processors).\n\nContents\n\n    1 Overview\n    2 History\n    3 Alternate usage\n    4 See also\n    5 References\n    6 Further reading\n\nOverview\n\nThe following attributes characterize systems programming:\n\n    The programmer will make assumptions about the hardware and other properties of the system that the program runs on, and will often exploit those properties, for example by using an algorithm that is known to be efficient when used with specific hardware.\n    Usually a low-level programming language or programming language dialect is used that:\n        can operate in resource-constrained environments\n        is very efficient and has little runtime overhead\n        has a small runtime library, or none at all\n        allows for direct and \"raw\" control over memory access and control flow\n        lets the programmer write parts of the program directly in assembly language\n    Often systems programs cannot be run in a debugger. Running the program in a simulated environment can sometimes be used to reduce this problem.\n\nSystems programming is sufficiently different from application programming that programmers tend to specialize in one or the other.[citation needed]\n\nIn system programming, often limited programming facilities are available. The use of automatic garbage collection is not common and debugging is sometimes hard to do. The runtime library, if available at all, is usually far less powerful, and does less error checking. Because of those limitations, monitoring and logging are often used; operating systems may have extremely elaborate logging subsystems.\n\nImplementing certain parts in operating systems and networking requires systems programming, for example implementing Paging (Virtual Memory) or a device driver for an operating system.\nHistory\n\nOriginally systems programmers invariably wrote in assembly language. Experiments with hardware support in high level languages in the late 1960s led to such languages as PL/S, BLISS, BCPL, and extended ALGOL for Burroughs large systems. Forth also has applications as a systems language. In the 1980s, C became ubiquitous, aided by the growth of Unix. More recently C++ has seen some use, for instance a subset of it is used in the I/O Kit drivers of Mac OS X.[2]\nAlternate usage\n\nFor historical reasons, some organizations use the term systems programmer to describe a job function which would be more accurately termed systems administrator. This is particularly true in organizations whose computer resources have historically been dominated by mainframes, although the term is even used to describe job functions which do not involve mainframes. This usage arose because administration of IBM mainframes often involved the writing of custom assembler code (IBM's Basic Assembly Language (BAL)), which integrated with the operating system such as OS/MVS, DOS/VSE or VM/CMS. Indeed, some IBM software products had substantial code contributions from customer programming staff. This type of programming is progressively less common, but the term systems programmer is still the de facto job title for staff directly administering IBM mainframes.\nSee also\n\n    Ousterhout's dichotomy\n    System programming language\n    Scripting language", "skillName": "System_programming."}
{"id": 207, "category": "Computer_Programming", "skillText": "Scala (programming language)\nScala source code is intended to be compiled to Java bytecode, so that the resulting executable code runs on a Java virtual machine. Java libraries may be used directly in Scala code and vice versa (language interoperability).[11] Like Java, Scala is object-oriented, and uses a curly-brace syntax reminiscent of the C programming language. Unlike Java, Scala has many features of functional programming languages like Scheme, Standard ML and Haskell, including currying, type inference, immutability, lazy evaluation, and pattern matching. It also has an advanced type system supporting algebraic data types, covariance and contravariance, higher-order types (but not higher-rank types), and anonymous types. Other features of Scala not present in Java include operator overloading, optional parameters, named parameters, raw strings, and no checked exceptions.\n\nThe name Scala is a portmanteau of \"scalable\" and \"language\", signifying that it is designed to grow with the demands of its users.[12]\n\nThe design of Scala started in 2001 at the �cole Polytechnique F�d�rale de Lausanne (EPFL) by Martin Odersky. It followed on from work on Funnel, a programming language combining ideas from functional programming and Petri nets.[13] Odersky had previously worked on Generic Java and javac, Sun's Java compiler.[13]\n\nAfter an internal release in late 2003, Scala was released publicly in early 2004 on the Java platform,[14] and on the .NET platform in June 2004.[8][13][15] A second version (v2.0) followed in March 2006.[8] The .NET support was officially dropped in 2012.[16]\n\nAlthough Scala had extensive support for functional programming from its inception, Java remained a purely object oriented language until the inclusion of lambda expressions with Java 8 in 2014.\n\nOn 17 January 2011 the Scala team won a five-year research grant of over �2.3 million from the European Research Council.[17] On 12 May 2011, Odersky and collaborators launched Typesafe Inc., a company to provide commercial support, training, and services for Scala. Typesafe received a $3 million investment in 2011 from Greylock Partners.[18][19][20][21]\n\nPlatforms and license\nScala runs on the Java platform (Java Virtual Machine) and is compatible with existing Java programs.[14] As Android applications are typically written in Java and translated from Java bytecode into Dalvik bytecode (which may be further translated to native machine code during installation) when packaged, Scala's Java compatibility makes it well suited to Android development, particularly when a functional approach is preferred.[22] Scala also can compile to JavaScript, making it possible to write Scala programs that can run in web browsers.[23]\n\nThe Scala software distribution, including compiler and libraries, is released under a BSD license.[24]\n\nExamples\n\"Hello World\" example\nThe Hello World program written in Scala has this form:\n\n object HelloWorld extends App {\n   println(\"Hello, World!\")\n }\nUnlike the stand-alone Hello World application for Java, there is no class declaration and nothing is declared to be static; a singleton object created with the object keyword is used instead.\n\nWith the program saved in a file named HelloWorld.scala, it can be compiled from the command line:\n\n$ scalac HelloWorld.scala\nTo run it:\n\n$ scala HelloWorld\n(You may need to use the \"-cp\" option to set the classpath like in Java).\n\nThis is analogous to the process for compiling and running Java code. Indeed, Scala's compilation and execution model is identical to that of Java, making it compatible with Java build tools such as Ant.\n\nA shorter version of the \"Hello World\" Scala program is:\n\nprintln(\"Hello, World!\")\nScala includes interactive shell and scripting support.[25] Saved in a file named HelloWorld2.scala, this can be run as a script without prior compilation using:\n\n$ scala HelloWorld2.scala\nCommands can also be entered directly into the Scala interpreter, using the option -e:\n\n$ scala -e 'println(\"Hello, World!\")'\nFinally, commands can be entered interactively in the REPL:\n\n$ scala\nWelcome to Scala version 2.10.3 (OpenJDK 64-Bit Server VM, Java 1.7.0_51).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> println(\"Hello, World!\")\nHello, World!\n\nscala>\nBasic example\nThe following example shows the differences between Java and Scala syntax:\n\n// Java:\nint mathFunction(int num) {\n    int numSquare = num*num;\n    return (int) (Math.cbrt(numSquare) +\n      Math.log(numSquare));\n}\n// Scala: Direct conversion from Java\n\n// no import needed; scala.math\n// already imported as `math`\ndef mathFunction(num: Int): Int = {\n  var numSquare: Int = num*num\n  return (math.cbrt(numSquare) + math.log(numSquare)).\n    asInstanceOf[Int]\n}\n// Scala: More idiomatic\n// Uses type inference, omits `return` statement,\n// uses `toInt` method, declares numSquare immutable\n\nimport math._\ndef intRoot23(num: Int) = {\n  val numSquare = num*num\n  (cbrt(numSquare) + log(numSquare)).toInt\n}\nSome syntactic differences in this code are:\n\nScala does not require semicolons to end statements.\nValue types are capitalized: Int, Double, Boolean instead of int, double, boolean.\nParameter and return types follow, as in Pascal, rather than precede as in C.\nMethods must be preceded by def.\nLocal or class variables must be preceded by val (indicates an immutable variable) or var (indicates a mutable variable).\nThe return operator is unnecessary in a function (although allowed); the value of the last executed statement or expression is normally the function's value.\nInstead of the Java cast operator (Type) foo, Scala uses foo.asInstanceOf[Type], or a specialized function such as toDouble or toInt.\nInstead of Java's import foo.*;, Scala uses import foo._.\nFunction or method foo() can also be called as just foo; method thread.send(signo) can also be called as just thread send signo; and method foo.toString() can also be called as just foo toString.\nThese syntactic relaxations are designed to allow support for domain-specific languages.\n\nSome other basic syntactic differences:\n\nArray references are written like function calls, e.g. array(i) rather than array[i]. (Internally in Scala, both arrays and functions are conceptualized as kinds of mathematical mappings from one object to another.)\nGeneric types are written as e.g. List[String] rather than Java's List<String>.\nInstead of the pseudo-type void, Scala has the actual singleton class Unit (see below).\nExample with classes\nThe following example contrasts the definition of classes in Java and Scala.\n\n// Java:\npublic class Point {\n  private final double x, y;\n\n  public Point(final double x, final double y) {\n    this.x = x;\n    this.y = y;\n  }\n\n  public Point(\n    final double x, final double y,\n    final boolean addToGrid\n  ) {\n    this(x, y);\n\n    if (addToGrid)\n      grid.add(this);\n  }\n\n  public Point() {\n    this(0.0, 0.0);\n  }\n\n  public double getX() {\n    return x;\n  }\n\n  public double getY() {\n    return y;\n  }\n\n  double distanceToPoint(final Point other) {\n    return distanceBetweenPoints(x, y,\n      other.x, other.y);\n  }\n\n  private static Grid grid = new Grid();\n\n  static double distanceBetweenPoints(\n      final double x1, final double y1,\n      final double x2, final double y2\n  ) {\n    return Math.hypot(x1 - x2, y1 - y2);\n  }\n}\n// Scala\nclass Point(\n    val x: Double, val y: Double,\n    addToGrid: Boolean = false\n) {\n  import Point._\n\n  if (addToGrid)\n    grid.add(this)\n\n  def this() = this(0.0, 0.0)\n\n  def distanceToPoint(other: Point) =\n    distanceBetweenPoints(x, y, other.x, other.y)\n}\n\nobject Point {\n  private val grid = new Grid()\n\n  def distanceBetweenPoints(x1: Double, y1: Double,\n      x2: Double, y2: Double) = {\n    math.hypot(x1 - x2, y1 - y2)\n  }\n}\nThe above code shows some of the conceptual differences between Java and Scala's handling of classes:\n\nScala has no static variables or methods. Instead, it has singleton objects, which are essentially classes with only one object in the class. Singleton objects are declared using object instead of class. It is common to place static variables and methods in a singleton object with the same name as the class name, which is then known as a companion object.[14] (The underlying class for the singleton object has a $ appended. Hence, for class Foo with companion object object Foo, under the hood there's a class Foo$ containing the companion object's code, and a single object of this class is created, using the singleton pattern.)\nIn place of constructor parameters, Scala has class parameters, which are placed on the class itself, similar to parameters to a function. When declared with a val or var modifier, fields are also defined with the same name, and automatically initialized from the class parameters. (Under the hood, external access to public fields always goes through accessor (getter) and mutator (setter) methods, which are automatically created. The accessor function has the same name as the field, which is why it's unnecessary in the above example to explicitly declare accessor methods.) Note that alternative constructors can also be declared, as in Java. Code that would go into the default constructor (other than initializing the member variables) goes directly at class level.\nDefault visibility in Scala is public.\nFeatures (with reference to Java)\nScala has the same compilation model as Java and C#, namely separate compilation and dynamic class loading, so that Scala code can call Java libraries, or .NET libraries in the .NET implementation.\n\nScala's operational characteristics are the same as Java's. The Scala compiler generates byte code that is nearly identical to that generated by the Java compiler.[14] In fact, Scala code can be decompiled to readable Java code, with the exception of certain constructor operations. To the JVM, Scala code and Java code are indistinguishable. The only difference is a single extra runtime library, scala-library.jar.[26]\n\nScala adds a large number of features compared with Java, and has some fundamental differences in its underlying model of expressions and types, which make the language theoretically cleaner and eliminate a number of \"corner cases\" in Java. From the Scala perspective, this is practically important because a number of additional features in Scala are also available in C#. Examples include:\n\nSyntactic flexibility\nAs mentioned above, Scala has a good deal of syntactic flexibility, compared with Java. The following are some examples:\n\nSemicolons are unnecessary; lines are automatically joined if they begin or end with a token that cannot normally come in this position, or if there are unclosed parentheses or brackets.\nAny method can be used as an infix operator, e.g. \"%d apples\".format(num) and \"%d apples\" format num are equivalent. In fact, arithmetic operators like + and << are treated just like any other methods, since function names are allowed to consist of sequences of arbitrary symbols (with a few exceptions made for things like parens, brackets and braces that must be handled specially); the only special treatment that such symbol-named methods undergo concerns the handling of precedence.\nMethods apply and update have syntactic short forms. foo()�where foo is a value (singleton object or class instance)�is short for foo.apply(), and foo() = 42 is short for foo.update(42). Similarly, foo(42) is short for foo.apply(42), and foo(4) = 2 is short for foo.update(4, 2). This is used for collection classes and extends to many other cases, such as STM cells.\nScala distinguishes between no-parens (def foo = 42) and empty-parens (def foo() = 42) methods. When calling an empty-parens method, the parentheses may be omitted, which is useful when calling into Java libraries that do not know this distinction, e.g., using foo.toString instead of foo.toString(). By convention, a method should be defined with empty-parens when it performs side effects.\nMethod names ending in colon (:) expect the argument on the left-hand-side and the receiver on the right-hand-side. For example, the 4 :: 2 :: Nil is the same as Nil.::(2).::(4), the first form corresponding visually to the result (a list with first element 4 and second element 2).\nClass body variables can be transparently implemented as separate getter and setter methods. For trait FooLike { var bar: Int }, an implementation may be object Foo extends FooLike { private var x = 0; def bar = x; def bar_=(value: Int) { x = value }} } }. The call site will still be able to use a concise foo.bar = 42.\nThe use of curly braces instead of parentheses is allowed in method calls. This allows pure library implementations of new control structures.[27] For example, breakable { ... if (...) break() ... } looks as if breakable was a language defined keyword, but really is just a method taking a thunk argument. Methods that take thunks or functions often place these in a second parameter list, allowing to mix parentheses and curly braces syntax: Vector.fill(4) { math.random } is the same as Vector.fill(4)(math.random). The curly braces variant allows the expression to span multiple lines.\nFor-expressions (explained further down) can accommodate any type that defines monadic methods such as map, flatMap and filter.\nBy themselves, these may seem like questionable choices, but collectively they serve the purpose of allowing domain-specific languages to be defined in Scala without needing to extend the compiler. For example, Erlang's special syntax for sending a message to an actor, i.e. actor ! message can be (and is) implemented in a Scala library without needing language extensions.\n\nUnified type system\nJava makes a sharp distinction between primitive types (e.g. int and boolean) and reference types (any class). Only reference types are part of the inheritance scheme, deriving from java.lang.Object. In Scala, however, all types inherit from a top-level class Any, whose immediate children are AnyVal (value types, such as Int and Boolean) and AnyRef (reference types, as in Java). This means that the Java distinction between primitive types and boxed types (e.g. int vs. Integer) is not present in Scala; boxing and unboxing is completely transparent to the user. Scala 2.10 allows for new value types to be defined by the user.\n\nFor-expressions\nInstead of the Java \"foreach\" loops for looping through an iterator, Scala has a much more powerful concept of for-expressions. These are similar to list comprehensions in languages such as Haskell, or a combination of list comprehensions and generator expressions in Python. For-expressions using the yield keyword allow a new collection to be generated by iterating over an existing one, returning a new collection of the same type. They are translated by the compiler into a series of map, flatMap and filter calls. Where yield is not used, the code approximates to an imperative-style loop, by translating to foreach.\n\nA simple example is:\n\nval s = for (x <- 1 to 25 if x*x > 50) yield 2*x\nThe result of running it is the following vector:\n\nVector(16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50)\n(Note that the expression 1 to 25 is not special syntax. The method to is rather defined in the standard Scala library as an extension method on integers, using a technique known as implicit conversions[28] that allows new methods to be added to existing types.)\n\nA more complex example of iterating over a map is:\n\n// Given a map specifying Twitter users mentioned in a set of tweets,\n// and number of times each user was mentioned, look up the users\n// in a map of known politicians, and return a new map giving only the\n// Democratic politicians (as objects, rather than strings).\nval dem_mentions = for {\n    (mention, times) <- mentions\n    account          <- accounts.get(mention)\n    if account.party == \"Democratic\"\n  } yield (account, times)\nExpression (mention, times) <- mentions is an example of pattern matching (see below). Iterating over a map returns a set of key-value tuples, and pattern-matching allows the tuples to easily be destructured into separate variables for the key and value. Similarly, the result of the comprehension also returns key-value tuples, which are automatically built back up into a map because the source object (from the variable mentions) is a map. Note that if mentions instead held a list, set, array or other collection of tuples, exactly the same code above would yield a new collection of the same type.\n\nFunctional tendencies\nWhile supporting all of the object-oriented features available in Java (and in fact, augmenting them in various ways), Scala also provides a large number of capabilities that are normally found only in functional programming languages. Together, these features allow Scala programs to be written in an almost completely functional style, and also allow functional and object-oriented styles to be mixed.\n\nExamples are:\n\nNo distinction between statements and expressions\nType inference\nAnonymous functions with capturing semantics (i.e. closures)\nImmutable variables and objects\nLazy evaluation\nDelimited continuations (since 2.8)\nHigher-order functions\nNested functions\nCurrying\nPattern matching\nAlgebraic data types (through \"case classes\")\nTuples\nEverything is an expression\n\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2013) (Learn how and when to remove this template message)\nUnlike C or Java, but similar to languages such as Lisp, Scala makes no distinction between statements and expressions. All statements are in fact expressions that evaluate to some value. Functions that would be declared as returning void in C or Java, and statements like while that logically do not return a value, are in Scala considered to return the type Unit, which is a singleton type, with only one object of that type. Functions and operators that never return at all (e.g. the throw operator or a function that always exits non-locally using an exception) logically have return type Nothing, a special type containing no objects; that is, a bottom type, i.e. a subclass of every possible type. (This in turn makes type Nothing compatible with every type, allowing type inference to function correctly.)\n\nSimilarly, an if-then-else \"statement\" is actually an expression, which produces a value, i.e. the result of evaluating one of the two branches. This means that such a block of code can be inserted wherever an expression is desired, obviating the need for a ternary operator in Scala:\n\n// Java:\nint hexDigit = x >= 10 ? x + 'A' - 10 : x + '0';\n// Scala:\nval hexDigit = if (x >= 10) x + 'A' - 10 else x + '0'\nFor similar reasons, return statements are unnecessary in Scala, and in fact are discouraged. As in Lisp, the last expression in a block of code is the value of that block of code, and if the block of code is the body of a function, it will be returned by the function.\n\nTo make it clear that all expressions are functions, even methods that return Unit are written with an equals sign\n\ndef printValue(x: String): Unit = {\n  println(\"I ate a %s\".format(x))\n}\nor equivalently (with type inference, and omitting the unnecessary braces):\n\ndef printValue(x: String) = println(\"I ate a %s\" format x)\nType inference\nDue to type inference, the type of variables, function return values, and many other expressions can typically be omitted, as the compiler can deduce it. Examples are val x = \"foo\" (for an immutable, constant variable or immutable object) or var x = 1.5 (for a variable whose value can later be changed). Type inference in Scala is essentially local, in contrast to the more global Hindley-Milner algorithm used in Haskell, ML and other more purely functional languages. This is done to facilitate object-oriented programming. The result is that certain types still need to be declared (most notably, function parameters, and the return types of recursive functions), e.g.\n\ndef formatApples(x: Int) = \"I ate %d apples\".format(x)\nor (with a return type declared for a recursive function)\n\ndef factorial(x: Int): Int =\n  if (x == 0)\n    1\n  else\n    x*factorial(x - 1)\nAnonymous functions\nIn Scala, functions are objects, and a convenient syntax exists for specifying anonymous functions. An example is the expression x => x < 2, which specifies a function with a single parameter, that compares its argument to see if it is less than 2. It is equivalent to the Lisp form (lambda (x) (< x 2)). Note that neither the type of x nor the return type need be explicitly specified, and can generally be inferred by type inference; but they can be explicitly specified, e.g. as (x: Int) => x < 2 or even (x: Int) => (x < 2): Boolean.\n\nAnonymous functions behave as true closures in that they automatically capture any variables that are lexically available in the environment of the enclosing function. Those variables will be available even after the enclosing function returns, and unlike in the case of Java's \"anonymous inner classes\" do not need to be declared as final. (It is even possible to modify such variables if they are mutable, and the modified value will be available the next time the anonymous function is called.)\n\nAn even shorter form of anonymous function uses placeholder variables: For example, the following:\n\nlist map { x => sqrt(x) }\ncan be written more concisely as\n\nlist map { sqrt(_) }\nor even\n\nlist map sqrt\nImmutability\nScala enforces a distinction between immutable (unmodifiable, read-only) variables, whose value cannot be changed once assigned, and mutable variables, which can be changed. A similar distinction is made between immutable and mutable objects. The distinction must be made when a variable is declared: Immutable variables are declared with val while mutable variables use var. Similarly, all of the collection objects (container types) in Scala, e.g. linked lists, arrays, sets and hash tables, are available in mutable and immutable variants, with the immutable variant considered the more basic and default implementation. The immutable variants are \"persistent\" data types in that they create a new object that encloses the old object and adds the new member(s); this is similar to how linked lists are built up in Lisp, where elements are prepended by creating a new \"cons\" cell with a pointer to the new element (the \"head\") and the old list (the \"tail\"). This allows for very easy concurrency � no locks are needed as no shared objects are ever modified. Immutable structures are also constructed efficiently, in the sense that modified instances reuses most of old instance data and unused/unreferenced parts are collected by GC.[29]\n\nLazy (non-strict) evaluation\nEvaluation is strict (\"eager\") by default. In other words, Scala evaluates expressions as soon as they are available, rather than as needed. However, you can declare a variable non-strict (\"lazy\") with the lazy keyword, meaning that the code to produce the variable's value will not be evaluated until the first time the variable is referenced. Non-strict collections of various types also exist (such as the type Stream, a non-strict linked list), and any collection can be made non-strict with the view method. Non-strict collections provide a good semantic fit to things like server-produced data, where the evaluation of the code to generate later elements of a list (that in turn triggers a request to a server, possibly located somewhere else on the web) only happens when the elements are actually needed.\n\nTail recursion\nFunctional programming languages commonly provide tail call optimization to allow for extensive use of recursion without stack overflow problems. Limitations in Java bytecode complicate tail call optimization on the JVM. In general, a function that calls itself with a tail call can be optimized, but mutually recursive functions cannot. Trampolines have been suggested as a workaround.[30] Trampoline support has been provided by the Scala library with the object scala.util.control.TailCalls since Scala 2.8.0 (released July 14, 2010).[31]\n\nCase classes and pattern matching\nScala has built-in support for pattern matching, which can be thought of as a more sophisticated, extensible version of a switch statement, where arbitrary data types can be matched (rather than just simple types like integers, booleans and strings), including arbitrary nesting. A special type of class known as a case class is provided, which includes automatic support for pattern matching and can be used to model the algebraic data types used in many functional programming languages. (From the perspective of Scala, a case class is simply a normal class for which the compiler automatically adds certain behaviors that could also be provided manually�e.g. definitions of methods providing for deep comparisons and hashing, and destructuring a case class on its constructor parameters during pattern matching.)\n\nAn example of a definition of the quicksort algorithm using pattern matching is as follows:\n\ndef qsort(list: List[Int]): List[Int] = list match {\n  case Nil => Nil\n  case pivot :: tail =>\n    val (smaller, rest) = tail.partition(_ < pivot)\n    qsort(smaller) ::: pivot :: qsort(rest)\n}\nThe idea here is that we partition a list into the elements less than a pivot and the elements not less, recursively sort each part, and paste the results together with the pivot in between. This uses the same divide-and-conquer strategy of mergesort and other fast sorting algorithms.\n\nThe match operator is used to do pattern matching on the object stored in list. Each case expression is tried in turn to see if it will match, and the first match determines the result. In this case, Nil only matches the literal object Nil, but pivot :: tail matches a non-empty list, and simultaneously destructures the list according to the pattern given. In this case, the associated code will have access to a local variable named pivot holding the head of the list, and another variable tail holding the tail of the list. Note that these variables are read-only, and are semantically very similar to variable bindings established using the let operator in Lisp and Scheme.\n\nPattern matching also happens in local variable declarations. In this case, the return value of the call to tail.partition is a tuple � in this case, two lists. (Tuples differ from other types of containers, e.g. lists, in that they are always of fixed size and the elements can be of differing types � although here they are both the same.) Pattern matching is the easiest way of fetching the two parts of the tuple.\n\nThe form _ < pivot is a declaration of an anonymous function with a placeholder variable; see the section above on anonymous functions.\n\nThe list operators :: (which adds an element onto the beginning of a list, similar to cons in Lisp and Scheme) and ::: (which appends two lists together, similar to append in Lisp and Scheme) both appear. Despite appearances, there is nothing \"built-in\" about either of these operators. As specified above, any string of symbols can serve as function name, and a method applied to an object can be written \"infix\"-style without the period or parentheses. The line above as written:\n\nqsort(smaller) ::: pivot :: qsort(rest)\ncould also be written as follows:\n\nqsort(rest).::(pivot).:::(qsort(smaller))\nin more standard method-call notation. (Methods that end with a colon are right-associative and bind to the object to the right.)\n\nPartial functions\nIn the pattern-matching example above, the body of the match operator is a partial function, which consists of a series of case expressions, with the first matching expression prevailing, similar to the body of a switch statement. Partial functions are also used in the exception-handling portion of a try statement:\n\ntry {\n  ...\n} catch {\n  case nfe:NumberFormatException => { println(nfe); List(0) }\n  case _ => Nil\n}\nFinally, a partial function can be used by itself, and the result of calling it is equivalent to doing a match over it. For example, the previous code for quicksort can be written as follows:\n\nval qsort: List[Int] => List[Int] = {\n  case Nil => Nil\n  case pivot :: tail =>\n    val (smaller, rest) = tail.partition(_ < pivot)\n    qsort(smaller) ::: pivot :: qsort(rest)\n}\nHere a read-only variable is declared whose type is a function from lists of integers to lists of integers, and bind it to a partial function. (Note that the single parameter of the partial function is never explicitly declared or named.) However, we can still call this variable exactly as if it were a normal function:\n\nscala> qsort(List(6,2,5,9))\nres32: List[Int] = List(2, 5, 6, 9)\nObject-oriented extensions\nScala is a pure object-oriented language in the sense that every value is an object. Data types and behaviors of objects are described by classes and traits. Class abstractions are extended by subclassing and by a flexible mixin-based composition mechanism to avoid the problems of multiple inheritance.\n\nTraits are Scala's replacement for Java's interfaces. Interfaces in Java versions under 8 are highly restricted, able only to contain abstract function declarations. This has led to criticism that providing convenience methods in interfaces is awkward (the same methods must be reimplemented in every implementation), and extending a published interface in a backwards-compatible way is impossible. Traits are similar to mixin classes in that they have nearly all the power of a regular abstract class, lacking only class parameters (Scala's equivalent to Java's constructor parameters), since traits are always mixed in with a class. The super operator behaves specially in traits, allowing traits to be chained using composition in addition to inheritance. The following example is a simple window system:\n\nabstract class Window {\n  // abstract\n  def draw()\n}\n\nclass SimpleWindow extends Window {\n  def draw() {\n    println(\"in SimpleWindow\")\n    // draw a basic window\n  }\n}\n\ntrait WindowDecoration extends Window { }\n\ntrait HorizontalScrollbarDecoration extends WindowDecoration {\n  // \"abstract override\" is needed here in order for \"super()\" to work because the parent\n  // function is abstract. If it were concrete, regular \"override\" would be enough.\n  abstract override def draw() {\n    println(\"in HorizontalScrollbarDecoration\")\n    super.draw()\n    // now draw a horizontal scrollbar\n  }\n}\n\ntrait VerticalScrollbarDecoration extends WindowDecoration {\n  abstract override def draw() {\n    println(\"in VerticalScrollbarDecoration\")\n    super.draw()\n    // now draw a vertical scrollbar\n  }\n}\n\ntrait TitleDecoration extends WindowDecoration {\n  abstract override def draw() {\n    println(\"in TitleDecoration\")\n    super.draw()\n    // now draw the title bar\n  }\n}\nA variable may be declared as follows:\n\nval mywin = new SimpleWindow with VerticalScrollbarDecoration with HorizontalScrollbarDecoration with TitleDecoration\nThe result of calling mywin.draw() is\n\nin TitleDecoration\nin HorizontalScrollbarDecoration\nin VerticalScrollbarDecoration\nin SimpleWindow\nIn other words, the call to draw first executed the code in TitleDecoration (the last trait mixed in), then (through the super() calls) threaded back through the other mixed-in traits and eventually to the code in Window itself, even though none of the traits inherited from one another. This is similar to the decorator pattern, but is more concise and less error-prone, as it doesn't require explicitly encapsulating the parent window, explicitly forwarding functions whose implementation isn't changed, or relying on run-time initialization of entity relationships. In other languages, a similar effect could be achieved at compile-time with a long linear chain of implementation inheritance, but with the disadvantage compared to Scala that one linear inheritance chain would have to be declared for each possible combination of the mix-ins.\n\nExpressive type system\nScala is equipped with an expressive static type system that enforces the safe and coherent use of abstractions. In particular, the type system supports:\n\nClasses and abstract types as object members\nStructural types\nPath-dependent types\nCompound types\nExplicitly typed self references\nGeneric classes\nPolymorphic methods\nUpper and lower type bounds\nVariance\nAnnotation\nViews\nScala is able to infer types by usage. This makes most static type declarations optional. Static types need not be explicitly declared unless a compiler error indicates the need. In practice, some static type declarations are included for the sake of code clarity.\n\nType enrichment\nA common technique in Scala, known as \"enrich my library\"[32] (originally termed as \"pimp my library\" by Martin Odersky in 2006;[28] though concerns were raised about this phrasing due to its negative connotation[33] and immaturity[34]), allows new methods to be used as if they were added to existing types. This is similar to the C# concept of extension methods but more powerful, because the technique is not limited to adding methods and can, for instance, be used to implement new interfaces. In Scala, this technique involves declaring an implicit conversion from the type \"receiving\" the method to a new type (typically, a class) that wraps the original type and provides the additional method. If a method cannot be found for a given type, the compiler automatically searches for any applicable implicit conversions to types that provide the method in question.\n\nThis technique allows new methods to be added to an existing class using an add-on library such that only code that imports the add-on library gets the new functionality, and all other code is unaffected.\n\nThe following example shows the enrichment of type Int with methods isEven and isOdd:\n\nobject MyExtensions {\n  implicit class IntPredicates(i: Int) {\n    def isEven = i % 2 == 0\n    def isOdd  = !isEven\n  }\n}\n\nimport MyExtensions._  // bring implicit enrichment into scope\n4.isEven  // -> true\nImporting the members of MyExtensions brings the implicit conversion to extension class IntPredicates into scope.[35]\n\nConcurrency\nScala standard library includes support for the actor model, in addition to the standard Java concurrency APIs. Typesafe provides a platform[36] that includes Akka,[37] a separate open source framework that provides actor-based concurrency. Akka actors may be distributed or combined with software transactional memory (\"transactors\"). Alternative CSP implementations for channel-based message passing are Communicating Scala Objects,[38] or simply via JCSP.\n\nAn Actor is like a thread instance with a mailbox. It can be created by system.actorOf, overriding the receive method to receive messages and using the ! (exclamation point) method to send a message.[39] The following example shows an EchoServer that can receive messages and then print them.\n\nval echoServer = actor(new Act {\n  become {\n    case msg => println(\"echo \" + msg)\n  }\n})\nechoServer ! \"hi\"\nScala also comes with built-in support for data-parallel programming in the form of Parallel Collections[40] integrated into its Standard Library since version 2.9.0.\n\nThe following example shows how to use Parallel Collections to improve performance.[41]\n\nval urls = List(\"http://scala-lang.org\",  \"https://github.com/scala/scala\")\n\ndef fromURL(url: String) = scala.io.Source.fromURL(url)\n  .getLines().mkString(\"\\n\")\n\nval t = System.currentTimeMillis()\nurls.par.map(fromURL(_))\nprintln(\"time: \" + (System.currentTimeMillis - t) + \"ms\")\nBesides actor support and data-parallelism, Scala also supports asynchronous programming with Futures and Promises, software transactional memory, and event streams.[42]\n\nCluster computing\nThe most well-known open source cluster computing solution, written in Scala, is Apache Spark. Additionally, Apache Kafka, the publish-subscribe message queue popular with Spark and other stream processing technologies, is written in Scala.\n\nTesting\nThere are several ways to test code in Scala:\n\nScalaTest supports multiple testing styles and can integrate with Java-based testing frameworks[43]\nScalaCheck, a library similar to Haskell's QuickCheck[44]\nspecs2, a library for writing executable software specifications[45]\nScalaMock provides support for testing high-order and curried functions[46]\nJUnit or TestNG, two popular testing frameworks written in Java\nVersions\nVersion\tReleased\tFeatures\tStatus\tNotes\n2.0[47]\t12-Mar-2006\t_\t_\t_\n2.1.8[48]\t23-Aug-2006\t_\t_\t_\n2.3.0[49]\t23-Nov-2006\t_\t_\t_\n2.4.0[50]\t09-Mar-2007\t_\t_\t_\n2.5.0[51]\t02-May-2007\t_\t_\t_\n2.6.0[52]\t27-Jul-2007\t_\t_\t_\n2.7.0[53]\t07-Feb-2008\t_\t_\t_\n2.8.0[54]\t14-Jul-2010\tRevision the common, uniform, and all-encompassing framework for collection types.\t_\t_\n2.9.0[55]\t12-May-2011\t_\t_\t_\n2.10[56]\t04-Jan-2013\nValue Classes[57]\nImplicit Classes[58]\nString Interpolation[59]\nFutures and Promises[60]\nDynamic and applyDynamic[61]\nDependent method types:\ndef identity(x: AnyRef): x.type = x // the return type says we return exactly what we got\nNew ByteCode emitter based on ASM:\nCan target JDK 1.5, 1.6 and 1.7\nEmits 1.6 bytecode by default\nOld 1.5 backend is deprecated\nA new Pattern Matcher: rewritten from scratch to generate more robust code (no more exponential blow-up!)\ncode generation and analyses are now independent (the latter can be turned off with -Xno-patmat-analysis)\nScaladoc Improvements\nImplicits (-implicits flag)\nDiagrams (-diagrams flag, requires graphviz)\nGroups (-groups)\nModularized Language features[62]\nParallel Collections[63] are now configurable with custom thread pools\nAkka Actors now part of the distribution\nscala.actors have been deprecated and the akka implementation is now included in the distribution.\nPerformance Improvements\nFaster inliner\nRange#sum is now O(1)\nUpdate of ForkJoin library\nFixes in immutable TreeSet/TreeMap\nImprovements to PartialFunctions\nAddition of ??? and NotImplementedError\nAddition of IsTraversableOnce + IsTraversableLike type classes for extension methods\nDeprecations and cleanup\nFloating point and octal literal syntax deprecation\nRemoved scala.dbc\nExperimental features\n\nScala Reflection[64]\nMacros[65]\n_\t_\n2.10.2[66]\t06-Jun-2013\t_\t_\t_\n2.10.3[67]\t01-Oct-2013\t_\t_\t_\n2.10.4[68]\t18-Mar-2014\t_\t_\t_\n2.10.5[69]\t05-Mar-2015\t_\t_\t_\n2.11.0[70]\t21-Apr-2014\t_\t_\t_\n2.11.1[71]\t20-May-2014\t_\t_\t_\n2.11.2[72]\t22-Jul-2014\t_\t_\t_\n2.11.4[73]\t31-Oct-2014\t_\t_\t_\n2.11.5[74]\t08-Jan-2015\t_\t_\t_\n2.11.6[75]\t05-Mar-2015\t_\t_\t_\n2.11.7[76]\t23-Jun-2015\t_\t_\t_\n2.11.8[77]\t8-Mar-2016\t_\tCurrent\t_\nComparison with other JVM languages\nScala is often compared with Groovy and Clojure, two other programming languages also using the JVM. Substantial differences between these languages are found in the type system, in the extent to which each language supports object-oriented and functional programming, and in the similarity of their syntax to the syntax of Java.\n\nScala is statically typed, while both Groovy and Clojure are dynamically typed. This makes the type system more complex and difficult to understand but allows almost all type errors to be caught at compile-time and can result in significantly faster execution. By contrast, dynamic typing requires more testing to ensure program correctness and is generally slower in order to allow greater programming flexibility and simplicity. In regard to speed differences, current versions of Groovy and Clojure allow for optional type annotations to help programs avoid the overhead of dynamic typing in cases where types are practically static. This overhead is further reduced when using recent versions of the JVM, which has been enhanced with an \"invoke dynamic\" instruction for methods that are defined with dynamically typed arguments. These advances reduce the speed gap between static and dynamic typing, although a statically typed language, like Scala, is still the preferred choice when execution efficiency is very important.\n\nIn regard to programming paradigms, Scala inherits the object-oriented model of Java and extends it in various ways. Groovy, while also strongly object-oriented is more focused in reducing verbosity. In Clojure, object-oriented programming is deemphasised with functional programming being the primary strength of the language. Scala also has many functional programming facilities, including features found in advanced functional languages like Haskell, and tries to be agnostic between the two paradigms, letting the developer choose between the two paradigms or, more frequently, some combination thereof.\n\nIn regard to syntax similarity with Java, Scala inherits a lot of Java's syntax, as is the case with Groovy. Clojure on the other hand follows the Lisp syntax, which is different in both appearance and philosophy. However, learning Scala is also considered difficult because of its many advanced features. This is not the case with Groovy, despite the fact that it is also a feature-rich language, mainly because it was designed to be primarily a scripting language.[citation needed]\n\nAdoption\nLanguage rankings\nScala was voted the most popular JVM scripting language at the 2012 JavaOne conference.[14]\n\nAs of 2013, all JVM-based derivatives (Scala/Groovy/Clojure) are significantly less popular than the original Java language itself, which is usually ranked first or second,[78][79][80] and which is also simultaneously evolving over time.\n\nThe RedMonk Programming Language Rankings, as of June 2015 placed Scala 14th, based on a position in terms of number of GitHub projects and in terms of number of questions tagged on Stack Overflow.[78] (Groovy and Clojure were both in 19th place.)[78] Here, Scala is shown clearly behind a first-tier group of languages (including, C, Python, PHP, Ruby, etc.), but leading a second-tier group.\n\nAnother measure, the Popularity of Programming Language Index[81] which tracks searches for language tutorials ranked Scala 16th in March 2016 with a small upward trend, making it the most popular JVM-based language after Java.\n\n\nTIOBE Scala since 2006�2015\nAs of January 2016, the TIOBE index[79] of programming language popularity shows Scala in 30th place (as measured by internet search engine rankings and similar publication-counting), but�as mentioned under \"Bugs & Change Requests\"�TIOBE is aware of issues with its methodology of using search terms which might not be commonly used in some programming language communities. In this ranking Scala is ahead of functional languages Haskell (39th), Erlang (35rd) and Clojure (>50), but below Java (1st).\n\nThe ThoughtWorks Technology Radar, which is an opinion based half-yearly report of a group of senior technologists,[82] recommends Scala adoption in its languages and frameworks category.[83]\n\nAccording to Indeed.com Job Trends, Scala demand has been rapidly increasing since 2010, trending ahead of Clojure and Groovy.[84]\n\nCompanies\nIn April 2009, Twitter announced that it had switched large portions of its backend from Ruby to Scala and intended to convert the rest.[85]\nGilt uses Scala and Play Framework.[86]\nFoursquare uses Scala and Lift.[87]\nSpinGo uses Scala and Akka.[88]\nCoursera uses Scala and Play Framework.[89]\nApple Inc. uses Scala in certain teams, along with Java and the Play framework.[90][91]\nThe Guardian newspaper's high-traffic website guardian.co.uk[92] announced in April 2011 that it was switching from Java to Scala,[93][94]\nThe New York Times revealed in 2014 that its internal content management system Blackbeard is built using Scala, Akka and Play.[95]\nThe Huffington Post newspaper started to employ Scala as part of its contents delivery system Athena in 2013.[96]\nSwiss bank UBS approved Scala for general production usage.[97]\nThe BitGold platform was built entirely on Scala and Play Framework.[98]\nLinkedIn uses the Scalatra microframework to power its Signal API.[99]\nMeetup uses Unfiltered toolkit for real-time APIs.[100]\nRemember the Milk uses Unfiltered toolkit, Scala and Akka for public API and real time updates.[101]\nVerizon seeking to make \"a next generation framework\" using Scala.[102]\nLeadIQ was built entirely on Scala, Akka and Play Framework.[103]\nAirbnb develops open source machine learning software \"Aerosolve\", written in Java and Scala.[104]\nZalando moved its technology stack from Java to Scala and Play.[105]\nSoundCloud uses Scala for its back-end, employing technologies such as Finagle (micro services),[106] Scalding and Spark (data processing).[107]\nDatabricks uses Scala for the Apache Spark Big Data platform.\nMorgan Stanley uses Scala extensively in their finance and asset-related projects.[108]\nThere are teams within Google/Alphabet Inc. that use Scala, mostly due to acquisitions such as Firebase[109] and Nest.[110]\nx.ai uses Scala for their AI-driven Personal Assistant.[111]\nCriticism\nIn March 2015, former VP of the Platform Engineering group at Twitter Raffi Krikorian, stated he would not have chosen Scala in 2011 due to its learning curve.[112] The same month, LinkedIn SVP Kevin Scott stated their decision to \"minimize [their] dependence on Scala.\"[113] In November 2011, Yammer moved away from Scala for reasons that included the learning curve for new team members and incompatibility from one version of the Scala compiler to the next.[114]", "skillName": "Scala."}
{"id": 208, "category": "Computer_Programming", "skillText": "PL/SQL (Procedural Language/Structured Query Language) is Oracle Corporation's procedural extension for SQL and the Oracle relational database. PL/SQL is available in Oracle Database (since version 7), TimesTen in-memory database (since version 11.2.1), and IBM DB2 (since version 9.7).[1] Oracle Corporation usually extends PL/SQL functionality with each successive release of the Oracle Database.\n\nPL/SQL includes procedural language elements such as conditions and loops. It allows declaration of constants and variables, procedures and functions, types and variables of those types, and triggers. It can handle exceptions (runtime errors). Arrays are supported involving the use of PL/SQL collections. Implementations from version 8 of Oracle Database onwards have included features associated with object-orientation. One can create PL/SQL units such as procedures, functions, packages, types, and triggers, which are stored in the database for reuse by applications that use any of the Oracle Database programmatic interfaces.\n\nContents\n\n    1 Similar languages\n    2 PL/SQL program unit\n        2.1 PL/SQL anonymous block\n        2.2 Function\n        2.3 Procedure\n        2.4 Package\n        2.5 Trigger\n            2.5.1 Purpose of triggers\n    3 Data types\n        3.1 Numeric variables\n        3.2 Character variables\n        3.3 Date variables\n        3.4 Exceptions\n        3.5 Datatypes for specific columns\n    4 Conditional statements\n    5 Array handling\n        5.1 Associative arrays (index-by tables)\n        5.2 Nested tables\n        5.3 Varrays (variable-size arrays)\n    6 Cursors\n    7 Looping\n        7.1 LOOP statements\n        7.2 FOR loops\n        7.3 Cursor FOR loops\n    8 Dynamic SQL\n    9 See also\n    10 References\n    11 Further reading\n    12 External links\n\nSimilar languages\n\nPL/SQL works analogously to the embedded procedural languages associated with other relational databases. For example, Sybase ASE and Microsoft SQL Server have Transact-SQL, PostgreSQL has PL/pgSQL (which emulates PL/SQL to an extent), and IBM DB2 includes SQL Procedural Language,[2] which conforms to the ISO SQL’s SQL/PSM standard.\n\nThe designers of PL/SQL modeled its syntax on that of Ada. Both Ada and PL/SQL have Pascal as a common ancestor, and so PL/SQL also resembles Pascal in several aspects. However, the structure of a PL/SQL package does not resemble the basic Object Pascal program structure as implemented by a Borland Delphi or Free Pascal unit. Programmers can define public and private global data-types, constants and static variables in a PL/SQL package.[3]\n\nPL/SQL also allows for the definition of classes and instantiating these as objects in PL/SQL code. This resembles usage in object-oriented programming languages like Object Pascal, C++ and Java. PL/SQL refers to a class as an \"Abstract Data Type\" (ADT) or \"User Defined Type\" (UDT), and defines it as an Oracle SQL data-type as opposed to a PL/SQL user-defined type, allowing its use in both the Oracle SQL Engine and the Oracle PL/SQL engine. The constructor and methods of an Abstract Data Type are written in PL/SQL. The resulting Abstract Data Type can operate as an object class in PL/SQL. Such objects can also persist as column values in Oracle database tables.\n\nPL/SQL is fundamentally distinct from Transact-SQL, despite superficial similarities. Porting code from one to the other usually involves non-trivial work, not only due to the differences in the feature sets of the two languages,[4] but also due to the very significant differences in the way Oracle and SQL Server deal with concurrency and locking. There are software tools available that claim to facilitate porting including Oracle Translation Scratch Editor,[5] CEITON MSSQL/Oracle Compiler [6] and SwisSQL.[7]\n\nThe StepSqlite product is a PL/SQL compiler for the popular small database SQLite which supports a subset of PL/SQL [syntax. Oracle's Berkeley DB 11g R2 release added support for SQL based on the popular SQLite API by including a version of SQLite in Berkeley DB.[8] Consequently, StepSqlite can also be used as a third-party tool to run PL/SQL code on Berkeley DB.[9]\nPL/SQL program unit\n\nA PL/SQL program unit is one of the following: PL/SQL anonymous block, procedure, function, package specification, package body, trigger, type specification, type body, library. Program units are the PL/SQL source code that is compiled, developed and ultimately executed on the database.\nPL/SQL anonymous block\n\nThe basic unit of a PL/SQL source program is the block, which groups together related declarations and statements. A PL/SQL block is defined by the keywords DECLARE, BEGIN, EXCEPTION, and END. These keywords divide the block into a declarative part, an executable part, and an exception-handling part. The declaration section is optional and may be used to define and initialize constants and variables. If a variable is not initialized then it defaults to NULL value. The optional exception-handling part is used to handle run time errors. Only the executable part is required. A block can have a label.\n\nFor example:\n\n<<label>>   -- this is optional\ndeclare\n-- this section is optional\n  number1 number(2);\n  number2 number1%type    := 17;             -- value default\n  text1   varchar2(12) := 'Hello world';\n  text2   date         := SYSDATE;        -- current date and time\nbegin\n-- this section is mandatory, must contain at least one executable statement\n  SELECT street_number\n    INTO number1\n    FROM address\n    WHERE name = 'INU';\nexception\n-- this section is optional\n   WHEN OTHERS THEN\n     DBMS_OUTPUT.PUT_LINE('Error Code is ' || to_char(sqlcode )  );\n     DBMS_OUTPUT.PUT_LINE('Error Message is ' || sqlerrm   );\nend;\n\nThe symbol := functions as an assignment operator to store a value in a variable.\n\nBlocks can be nested i.e. because a block is an executable statement, it can appear in another block wherever an executable statement is allowed. A block can be submitted to an interactive tool (such as SQL*Plus) or embed it in an Oracle Precompiler or OCI program. The interactive tool or program runs the block once. The block is not stored in the database, and for that reason, it is called an anonymous block (even if it has a label).\nFunction\n\nThe purpose of a PL/SQL function is generally to compute and return a single value. This returned value may be a single scalar value (such as a number, date or character string) or a single collection (such as a nested table or varray). User-defined functions supplement the built-in functions provided by Oracle Corporation.\n\nThe PL/SQL function has the form:\n\nCREATE OR REPLACE FUNCTION <function_name> [(input/output variable declarations)] RETURN return_type\n[AUTHID <CURRENT_USER | DEFINER>] <IS|AS>   -- heading part\namount number;   -- declaration block\nBEGIN   -- executable part\n\t<PL/SQL block with return statement>\n        RETURN <return_value>;\n[Exception\n\tnone]\n        RETURN <return_value>;\nEND;\n\nPipelined table functions return collections[10] and take the form:\n\nCREATE OR REPLACE FUNCTION <function_name> [(input/output variable declarations)] RETURN return_type\n[AUTHID <CURRENT_USER | DEFINER>] [<AGGREGATE | PIPELINED>] <IS|USING>\n\t[declaration block]\nBEGIN\n\t<PL/SQL block with return statement>\n        PIPE ROW <return type>;\n        RETURN;\n[Exception\n\texception block]\n        PIPE ROW <return type>;\n        RETURN;\nEND;\n\nA function should only use the default IN type of parameter. The only out value from the function should be the value it returns.\nProcedure\n\nProcedures are similar to functions, in that they are named program units that can be invoked repeatedly. The primary difference is that functions can be used in a SQL statement whereas procedures cannot. Another difference is that the procedure can return multiple values whereas a function should only return a single value.\n\nThe procedure begins with a mandatory heading part to hold the procedure name and optionally the procedure parameter list. Next are the declarative, executable and exception-handling parts, as in the PL/SQL Anonymous Block. Here is an example of a simple procedure.\n\nCREATE PROCEDURE create_email_address ( -- Procedure heading part begins\n    name1 VARCHAR2,\n    name2 VARCHAR2,\n    company VARCHAR2,\n    email OUT VARCHAR2\n) -- Procedure heading part ends\nAS\n-- Declarative part begins (optional)\nerror_message VARCHAR2(30) := 'Email address is too long.';\nBEGIN -- Executable part begins (mandatory)\n    email := name1 || '.' || name2 || '@' || company;\nEXCEPTION -- Exception-handling part begins (optional)\nWHEN VALUE_ERROR THEN\n    DBMS_OUTPUT.PUT_LINE(error_message);\nEND create_email_address;\n\nThe example above shows a standalone procedure - this type of procedure is created and stored in a database schema using the CREATE PROCEDURE statement. A procedure may also be created in a PL/SQL package - this is called a Package Procedure. A procedure created in a PL/SQL anonymous block is called a nested procedure. The standalone or package procedures are stored in the database and so are referred to as 'stored procedures'.\n\nThere are three types of parameters: IN, OUT and IN OUT.\n\n    An IN parameter is used as input only. An IN parameter is passed by reference, though it can be changed by the inactive program.\n    An OUT parameter is initially NULL. The program assigns the parameter a value and that value is returned to the calling program.\n    An IN OUT parameter may or may not have an initial value. That initial value may or may not be modified by the called program. Any changes made to the parameter are returned to the calling program by default by copying but - with the NOCOPY hint - may be passed by reference.\n\nPackage\n\nPackages are groups of conceptually linked functions, procedures, variables, PL/SQL table and record TYPE statements, constants, cursors etc. The use of packages promotes re-use of code. Packages are composed of the package specification and an optional package body. The specification is the interface to the application; it declares the types, variables, constants, exceptions, cursors, and subprograms available. The body fully defines cursors and subprograms, and so implements the specification. Two advantages of packages are:\n\n    Modular approach, encapsulation/hiding of business logic, security, performance improvement, re-usability. They support object-oriented programming features like function overloading and encapsulation.\n    Using package variables one can declare session level (scoped) variables, since variables declared in the package specification have a session scope.\n\nTrigger\nMain article: Database trigger\n\nA database trigger is like a stored procedure that Oracle Database invokes automatically whenever a specified event occurs. It is a named PL/SQL unit that is stored in the database and can be invoked repeatedly. Unlike a stored procedure, you can enable and disable a trigger, but you cannot explicitly invoke it. While a trigger is enabled, the database automatically invokes it—that is, the trigger fires—whenever its triggering event occurs. While a trigger is disabled, it does not fire.\n\nYou create a trigger with the CREATE TRIGGER statement. You specify the triggering event in terms of triggering statements, and the item they act on. The trigger is said to be created on or defined on the item—which is either a table, a view, a schema, or the database. You also specify the timing point, which determines whether the trigger fires before or after the triggering statement runs and whether it fires for each row that the triggering statement affects.\n\nIf the trigger is created on a table or view, then the triggering event is composed of DML statements, and the trigger is called a DML trigger. If the trigger is created on a schema or the database, then the triggering event is composed of either DDL or database operation statements, and the trigger is called a system trigger.\n\nAn INSTEAD OF trigger is either: A DML trigger created on a view or a system trigger defined on a CREATE statement. The database fires the INSTEAD OF trigger instead of running the triggering statement.\nPurpose of triggers\n\nTriggers can be written for the following purposes:\n\n    Generating some derived column values automatically\n    Enforcing referential integrity\n    Event logging and storing information on table access\n    Auditing\n    Synchronous replication of tables\n    Imposing security authorizations\n    Preventing invalid transactions\n\nData types\n\nThe major datatypes in PL/SQL include NUMBER, CHAR, VARCHAR2, DATE and TIMESTAMP.\nNumeric variables\n\nvariable_name number([P, S]) := 0;\n\nTo define a numeric variable, the programmer appends the variable type NUMBER to the name definition. To specify the (optional) precision (P) and the (optional) scale (S), one can further append these in round brackets, separated by a comma. (\"Precision\" in this context refers to the number of digits the variable can hold, and \"scale\" refers to the number of digits that can follow the decimal point.)\n\nA selection of other datatypes for numeric variables would include: binary_float, binary_double, dec, decimal, double precision, float, integer, int, numeric, real, smallint, binary_integer.\nCharacter variables\n\nvariable_name varchar2(20) := 'Text';\n\ne.g.: address varchar2(20) := 'lake view road';\n\nTo define a character variable, the programmer normally appends the variable type VARCHAR2 to the name definition. There follows in brackets the maximum number of characters the variable can store.\n\nOther datatypes for character variables include: varchar, char, long, raw, long raw, nchar, nchar2, clob, blob, bfile\nDate variables\n\nvariable_name date := to_date('01-01-2005 14:20:23', 'DD-MM-YYYY hh24:mi:ss');\n\nDate variables can contain date and time. The time may be left out, but there is no way to define a variable that only contains the time. There is no DATETIME type. And there is a TIME type. But there is no TIMESTAMP type that can contain fine grained timestamp up to millisecond or nanosecond. Oracle Datatypes \n\nThe TO_DATE function can be used to convert strings to date values. The function converts the first quoted string into a date, using as a definition the second quoted string, for example:\n\n to_date('31-12-2004', 'dd-mm-yyyy')\n\nor\n\n to_date ('31-Dec-2004', 'dd-mon-yyyy', 'NLS_DATE_LANGUAGE = American')\n\nTo convert the dates to strings one uses the function TO_CHAR (date_string, format_string).\n\nPL/SQL also supports the use of ANSI date and interval literals.[11] The following clause gives an 18-month range:\n\nWHERE dateField BETWEEN DATE '2004-12-30' - INTERVAL '1-6' YEAR TO MONTH\n    AND DATE '2004-12-30'\n\nExceptions\n\nExceptions—errors during code execution—are of two types: user defined and predefined.\n\nUser-defined exceptions are always raised explicitly by the programmers, using the RAISE or RAISE_APPLICATION_ERROR commands, in any situation where they determine it is impossible for normal execution to continue. The RAISE command has the syntax:\n\nRAISE <exception name>;\n\nOracle Corporation has predefined several exceptions like NO_DATA_FOUND, TOO_MANY_ROWS, etc. Each exception has an SQL error number and SQL error message associated with it. Programmers can access these by using the SQLCODE and SQLERRM functions.\nDatatypes for specific columns\n\nVariable_name Table_name.Column_name%type;\n\nThis syntax defines a variable of the type of the referenced column on the referenced tables.\n\nProgrammers specify user-defined datatypes with the syntax:\n\ntype data_type is record (field_1 type_1 := xyz, field_2 type_2 := xyz, ..., field_n type_n := xyz);\n\nFor example:\n\ndeclare\n    type t_address is  record (\n        name address.name%type,\n        street address.street%type,\n        street_number address.street_number%type,\n        postcode address.postcode%type);\n    v_address t_address;\nbegin\n    select name, street, street_number, postcode into v_address from address where rownum = 1;\nend;\n\nThis sample program defines its own datatype, called t_address, which contains the fields name, street, street_number and postcode.\n\nSo according to the example, we are able to copy the data from the database to the fields in the program.\n\nUsing this datatype the programmer has defined a variable called v_address and loaded it with data from the ADDRESS table.\n\nProgrammers can address individual attributes in such a structure by means of the dot-notation, thus: \"v_address.street := 'High Street';\"\nConditional statements\n\nThe following code segment shows the IF-THEN-ELSIF construct. The ELSIF and ELSE parts are optional so it is possible to create simpler IF-THEN or, IF-THEN-ELSE constructs.\n\nIF x = 1 THEN\n   sequence_of_statements_1;\nELSIF x = 2 THEN\n   sequence_of_statements_2;\nELSIF x = 3 THEN\n   sequence_of_statements_3;\nELSIF x = 4 THEN\n   sequence_of_statements_4;\nELSIF x = 5 THEN\n   sequence_of_statements_5;\nELSE\n   sequence_of_statements_N;\nEND IF;\n\nThe CASE statement simplifies some large IF-THEN-ELSE structures.\n\nCASE\n   WHEN x = 1 THEN sequence_of_statements_1;\n   WHEN x = 2 THEN sequence_of_statements_2;\n   WHEN x = 3 THEN sequence_of_statements_3;\n   WHEN x = 4 THEN sequence_of_statements_4;\n   WHEN x = 5 THEN sequence_of_statements_5;\n   ELSE sequence_of_statements_N;\nEND CASE;\n\nCASE statement can be used with predefined selector:\n\nCASE x\n   WHEN 1 THEN sequence_of_statements_1;\n   WHEN 2 THEN sequence_of_statements_2;\n   WHEN 3 THEN sequence_of_statements_3;\n   WHEN 4 THEN sequence_of_statements_4;\n   WHEN 5 THEN sequence_of_statements_5;\n   ELSE sequence_of_statements_N;\nEND CASE;\n\nArray handling\n\nPL/SQL refers to arrays as \"collections\". The language offers three types of collections:\n\n    Associative arrays (Index-by tables)\n    Nested tables\n    Varrays (variable-size arrays)\n\nProgrammers must specify an upper limit for varrays, but need not for index-by tables or for nested tables. The language includes several collection methods used to manipulate collection elements: for example FIRST, LAST, NEXT, PRIOR, EXTEND, TRIM, DELETE, etc. Index-by tables can be used to simulate associative arrays, as in this example of a memo function for Ackermann's function in PL/SQL.\nAssociative arrays (index-by tables)\n\nWith index-by tables, the array can be indexed by numbers or strings. It parallels a Java map, which comprises key-value pairs. There is only one dimension and it is unbounded.\nNested tables\n\nWith nested tables the programmer needs to understand what is nested. Here, a new type is created that may be composed of a number of components. That type can then be used to make a column in a table, and nested within that column are those components.\nVarrays (variable-size arrays)\n\nWith Varrays you need to understand that the word \"variable\" in the phrase \"variable-size arrays\" doesn't apply to the size of the array in the way you might think that it would. The size the array is declared with is in fact fixed. The number of elements in the array is variable up to the declared size. Arguably then, variable-sized arrays aren't that variable in size.\nCursors\n\nA cursor is a mechanism, pointer to a private SQL area that stores information coming from a SELECT or data manipulation language (DML) statement (INSERT, UPDATE, DELETE, or MERGE). A cursor holds the rows (one or more) returned by a SQL statement. The set of rows the cursor holds is referred to as the active set.[12]\n\nA cursor can be explicit or implicit. In a FOR loop, an explicit cursor shall be used if the query will be reused, otherwise an implicit cursor is preferred. If using a cursor inside a loop, use a FETCH is recommended when needing to bulk collect or when needing dynamic SQL.\nLooping\n\nAs a procedural language by definition, PL/SQL provides several iteration constructs, including basic LOOP statements, WHILE loops, FOR loops, and Cursor FOR loops. Since Oracle 7.3 the REF CURSOR type was introduced to allow recordsets to be returned from stored procedures and functions. Oracle 9i introduced the predefined SYS_REFCURSOR type, meaning we no longer have to define our own REF CURSOR types.\nLOOP statements\n\n<<parent_loop>>\nLOOP\n\tstatements\n\n\t<<child_loop>>\n\tloop\n\t\tstatements\n\t\texit parent_loop when <condition>; -- Terminates both loops\n\t\texit when <condition>; -- Returns control to parent_loop\n\tend loop child_loop;\n        if <condition> then\n           continue; -- continue to next iteration\n        end if;\n\n\texit when <condition>;\nEND LOOP parent_loop;\n\n[13]\n\nLoops can be terminated by using the EXIT keyword, or by raising an exception.\nFOR loops\n\ndeclare\n    var number;\nbegin\n     /*N.B. for loop variables in pl/sql are new declarations, with scope only inside the loop */\n     for var in 0 .. 10 loop\n          dbms_output.put_line(var);\n     end loop;\n\n     if (var is null) then\n          dbms_output.put_line('var is null');\n     else\n          dbms_output.put_line('var is not null');\n     end if;\nend;\n\nOutput:\n\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n 10\n var is null\n\nCursor FOR loops\n\nFOR RecordIndex IN (SELECT person_code FROM people_table)\nLOOP\n  DBMS_OUTPUT.PUT_LINE(RecordIndex.person_code);\nEND LOOP;\n\nCursor-for loops automatically open a cursor, read in their data and close the cursor again.\n\nAs an alternative, the PL/SQL programmer can pre-define the cursor's SELECT-statement in advance to (for example) allow re-use or make the code more understandable (especially useful in the case of long or complex queries).\n\nDECLARE\n  CURSOR cursor_person IS\n    SELECT person_code FROM people_table;\nBEGIN\n  FOR RecordIndex IN cursor_person\n  LOOP\n    DBMS_OUTPUT.PUT_LINE(recordIndex.person_code);\n  END LOOP;\nEND;\n\nThe concept of the person_code within the FOR-loop gets expressed with dot-notation (\".\"):\n\nRecordIndex.person_code\n\nDynamic SQL\n\nWhile programmers can readily embed Data Manipulation Language (DML) statements directly into PL/SQL code using straightforward SQL statements, Data Definition Language (DDL) requires more complex \"Dynamic SQL\" statements in the PL/SQL code. However, DML statements underpin the majority of PL/SQL code in typical software applications.\n\nIn the case of PL/SQL dynamic SQL, early versions of the Oracle Database required the use of a complicated Oracle DBMS_SQL package library. More recent versions have however introduced a simpler \"Native Dynamic SQL\", along with an associated EXECUTE IMMEDIATE syntax.", "skillName": "PL-SQL."}
{"id": 209, "category": "Computer_Programming", "skillText": "Julia (programming language)\nJulia is a high-level dynamic programming language designed to address the requirements of high-performance numerical and scientific computing while also being effective for general-purpose programming,[11][12][13][14] web use[15][16] or as a specification language.[17] Distinctive aspects of Julia's design include a type system with parametric types in a fully dynamic programming language and multiple dispatch as its core programming paradigm. It allows concurrent, parallel and distributed computing, and direct calling of C and Fortran libraries without glue code. Julia is garbage-collected,[18] uses eager evaluation and includes efficient libraries for floating-point calculations, linear algebra, random number generation, fast Fourier transforms and regular expression matching.\n\nAccording to the official website, the main features of the language are:\n\nMultiple dispatch: providing ability to define function behavior across many combinations of argument types\nDynamic type system: types for documentation, optimization, and dispatch\nGood performance, approaching that of statically-typed languages like C\nBuilt-in package manager\nLisp-like macros and other metaprogramming facilities\nCall Python functions: use the PyCall package[a]\nCall C functions directly: no wrappers or special APIs\nPowerful shell-like capabilities for managing other processes\nDesigned for parallelism and distributed computation\nCoroutines: lightweight \"green\" threading\nUser-defined types are as fast and compact as built-ins\nAutomatic generation of efficient, specialized code for different argument types\nElegant and extensible conversions and promotions for numeric and other types\nEfficient support for Unicode, including but not limited to UTF-8\nMultiple dispatch (also known as multimethods in Lisp) is a generalization of single dispatch � the polymorphic mechanism used in common object oriented (OO) languages � that uses inheritance. In Julia, all concrete types are subtypes of abstract types, directly or indirectly subtypes of the \"Any\" type, which is the top of the type hierarchy. Concrete types can not be subtyped, but composition is used over inheritance, that is used by traditional object-oriented languages (see also Inheritance vs subtyping).\n\nJulia draws significant inspiration from various dialects of Lisp, including Scheme and Common Lisp, and it shares many features with Dylan (such as an ALGOL-like free-form infix syntax rather than a Lisp-like prefix syntax, while in Julia \"everything\"[22] is an expression) � also a multiple-dispatch-oriented dynamic language � and Fortress, another numerical programming language with multiple dispatch and a sophisticated parametric type system. While CLOS adds multiple dispatch to Common Lisp, the addition is opt-in: only user-defined functions explicitly declared to be generic can be extended with new multimethods.\n\nIn Julia, Dylan and Fortress, on the other hand, this extensibility is the default, and the system's built-in functions are all generic and extensible. In Dylan, multiple dispatch is as fundamental as it is in Julia: all user-defined functions and even basic built-in operations like + are generic. Dylan's type system, however, does not fully support parametric types, which are more typical of the ML lineage of languages. By default, CLOS does not allow for dispatch on Common Lisp's parametric types; such extended dispatch semantics can only be added as an extension through the CLOS Metaobject Protocol. By convergent design, Fortress also features multiple dispatch on parametric types; unlike Julia, however, Fortress is statically rather than dynamically typed, with separate compilation and execution phases. The language features are summarized in the following table:\n\nLanguage\tType system\tGeneric functions\tParametric types\nJulia\tdynamic\tdefault\tyes\nCommon Lisp\tdynamic\topt-in\tyes (but no dispatch)\nDylan\tdynamic\tdefault\tpartial (no dispatch)\nFortress\tstatic\tdefault\tyes\nJulia's syntactic macros (used for metaprogramming), like Lisp macros, are more powerful and different from text-substitution macros used in the preprocessor of some other languages such as C, because they work at the level of abstract syntax trees (ASTs). Julia's macro system is hygienic, but also supports deliberate capture when desired (like for anaphoric macros) using the esc construct.\n\nInteraction\nThe Julia official distribution includes an interactive session shell, called Julia's REPL, which can be used to experiment and test code quickly.[23] The following fragment represents a sample session on the REPL:[24]\n\njulia> p(x) = 2x^2 + 1; f(x, y) = 1 + 2p(x)y\njulia> println(\"Hello world!\", \" I'm on cloud \", f(0, 4), \" as Julia supports recognizable syntax!\")\nHello world! I'm on cloud 9 as Julia supports recognizable syntax!\nThe REPL gives user access to the system shell and to help mode, by pressing ; or ? after the prompt (preceding each command), respectively. The REPL also keeps the history of commands, even between sessions. For other examples, see the Julia documentation,[25] which gives code that can be tested inside the Julia's interactive section or saved into a file with a .jl extension and run from the command line by typing (for example):[26]\n\n$ julia <filename>\nJulia is also supported by Jupyter, an online interactive \"notebooks\" environment (project Jupyter is a multi-language extension, that \"evolved\", from the IPython command shell; now includes IJulia). See for other ways in the next section.\n\nTo use Julia with other languages\nJulia's ccall keyword is used to call C-exported or Fortran shared library functions individually.\n\nJulia has Unicode support, with UTF-8 used for source code and e.g. optionally allowing common math symbols for many operators, such as ? for the in operator. For strings UTF-8, UTF-16 and UTF-32 (and ASCII) are fully supported encodings.\n\nJulia has packages supporting markup languages such as HTML, and also for HTTP), XML, JSON and BSON.\n\nImplementation\nJulia's core is implemented in C and C++ (the LLVM dependency is in C++), its parser in Scheme (\"femtolisp\"), and the LLVM compiler framework is used for just-in-time (JIT) generation of 64-bit or 32-bit optimized machine code (i.e. not for VM[27]) depending on the platform Julia runs on. With some exceptions (e.g., libuv), the standard library is implemented in Julia itself. The most notable aspect of Julia's implementation is its speed, which is often within a factor of two relative to fully optimized C code (and thus often an order of magnitude faster than Python or R).[28] Development of Julia began in 2009 and an open-source version was publicized in February 2012.[4][29]\n\nJulia, the 0.4.x line, is on a monthly release schedule where bugs are fixed and some new features from 0.5-dev are backported.[30]\n\nCurrent and future platforms\nWhile Julia uses JIT[31] (MCJIT[32] from LLVM) � Julia generates native machine code, directly, the first time a function is run (not bytecode to run on a VM, as e.g. with Java/Dalvik).\n\nCurrent support is for newer x86 or older i386 processors and in 0.4.x: 32-bit ARM architecture (\"Experimental and early support\"[33] with \"work in progress - several tests are known to fail, and backtraces are not available\"[34] with alpha support for Raspberry Pi 1/2[35][36] but \"[on ARMv7] Samsung Chromebook [..] Julia starts up just fine\"[37]), 64-bit ARMv8[38] and PowerPC being worked on.[39][40]\n\nJulia2C source-to-source compiler\nA Julia2C source-to-source compiler from Intel Labs is available.[41] This source-to-source compiler is a fork of Julia, that implements the same Julia language syntax, which emits C code (for compatibility with more CPUs) instead of native machine code, for functions or whole programs. The compiler is also meant to allow analyzing code at a higher level than C", "skillName": "Julia."}
{"id": 210, "category": "Computer_Programming", "skillText": "Python (programming language)\nPython is a widely used high-level, general-purpose, interpreted, dynamic programming language.[23][24] Its design philosophy emphasizes code readability, and its syntax allows programmers to express concepts in fewer lines of code than possible in languages such as C++ or Java.[25][26] The language provides constructs intended to enable clear programs on both a small and large scale.[27]\n\nPython supports multiple programming paradigms, including object-oriented, imperative and functional programming or procedural styles. It features a dynamic type system and automatic memory management and has a large and comprehensive standard library.[28]\n\nPython interpreters are available for many operating systems, allowing Python code to run on a wide variety of systems. Using third-party tools, such as Py2exe or Pyinstaller,[29] Python code can be packaged into stand-alone executable programs for some of the most popular operating systems, so Python-based software can be distributed to, and used on, those environments with no need to install a Python interpreter.\n\nCPython, the reference implementation of Python, is free and open-source software and has a community-based development model, as do nearly all of its variant implementations. CPython is managed by the non-profit Python Software Foundation.\n\nGuido van Rossum, the creator of Python\nMain article: History of Python\nPython was conceived in the late 1980s,[30] and its implementation began in December 1989[31] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC language (itself inspired by SETL)[32] capable of exception handling and interfacing with the operating system Amoeba.[7] Van Rossum is Python's principal author, and his continuing central role in deciding the direction of Python is reflected in the title given to him by the Python community, benevolent dictator for life (BDFL).\n\nAbout the origin of Python, Van Rossum wrote in 1996:[33]\n\nOver six years ago, in December 1989, I was looking for a \"hobby\" programming project that would keep me occupied during the week around Christmas. My office ... would be closed, but I had a home computer, and not much else on my hands. I decided to write an interpreter for the new scripting language I had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers. I chose Python as a working title for the project, being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus).\n\nPython 2.0 was released on 16 October 2000 and had many major new features, including a cycle-detecting garbage collector and support for Unicode. With this release the development process was changed and became more transparent and community-backed.[34]\n\nPython 3.0 (which early in its development was commonly referred to as Python 3000 or py3k), a major, backwards-incompatible release, was released on 3 December 2008[35] after a long period of testing. Many of its major features have been backported to the backwards-compatible Python 2.6.x[36] and 2.7.x version series.\n\nFeatures and philosophy\nPython is a multi-paradigm programming language: object-oriented programming and structured programming are fully supported, and many language features support functional programming and aspect-oriented programming (including by metaprogramming[37] and metaobjects (magic methods).[38] Many other paradigms are supported via extensions, including design by contract[39][40] and logic programming.[41]\n\nPython uses dynamic typing and a mix of reference counting and a cycle-detecting garbage collector for memory management. An important feature of Python is dynamic name resolution (late binding), which binds method and variable names during program execution.\n\nThe design of Python offers some support for functional programming in the Lisp tradition. The language has map(), reduce() and filter() functions; list comprehensions, dictionaries, and sets; and generator expressions.[42] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.[43]\n\nThe core philosophy of the language is summarized by the document The Zen of Python (PEP 20), which includes aphorisms such as:[44]\n\nBeautiful is better than ugly\nExplicit is better than implicit\nSimple is better than complex\nComplex is better than complicated\nReadability counts\nRather than requiring all desired functionality to be built into the language's core, Python was designed to be highly extensible. Python can also be embedded in existing applications that need a programmable interface. This design of a small core language with a large standard library and an easily extensible interpreter was intended by Van Rossum from the start because of his frustrations with ABC, which espoused the opposite mindset.[30]\n\nWhile offering choice in coding methodology, the Python philosophy rejects exuberant syntax, such as in Perl, in favor of a sparser, less-cluttered grammar. As Alex Martelli put it: \"To describe something as clever is not considered a compliment in the Python culture.\"[45] Python's philosophy rejects the Perl \"there is more than one way to do it\" approach to language design in favor of \"there should be one�and preferably only one�obvious way to do it\".[44]\n\nPython's developers strive to avoid premature optimization, and moreover, reject patches to non-critical parts of CPython that would offer a marginal increase in speed at the cost of clarity.[46] When speed is important, a Python programmer can move time-critical functions to extension modules written in languages such as C, or try using PyPy, a just-in-time compiler. Cython is also available, which translates a Python script into C and makes direct C-level API calls into the Python interpreter.\n\nAn important goal of Python's developers is making it fun to use. This is reflected in the origin of the name, which comes from Monty Python,[47] and in an occasionally playful approach to tutorials and reference materials, such as using examples that refer to spam and eggs instead of the standard foo and bar.[48][49]\n\nA common neologism in the Python community is pythonic, which can have a wide range of meanings related to program style. To say that code is pythonic is to say that it uses Python idioms well, that it is natural or shows fluency in the language, that it conforms with Python's minimalist philosophy and emphasis on readability. In contrast, code that is difficult to understand or reads like a rough transcription from another programming language is called unpythonic.\n\nUsers and admirers of Python, especially those considered knowledgeable or experienced, are often referred to as Pythonists, Pythonistas, and Pythoneers.[50][51]\n\nSyntax and semantics\nMain article: Python syntax and semantics\nPython is intended to be a highly readable language. It is designed to have an uncluttered visual layout, often using English keywords where other languages use punctuation. Further, Python has fewer syntactic exceptions and special cases than C or Pascal.[52]\n\nIndentation\nMain article: Python syntax and semantics � Indentation\nPython uses whitespace indentation, rather than curly braces or keywords, to delimit blocks; this feature is also termed the off-side rule. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block.[53]\n\nStatements and control flow\nPython's statements include (among others):\n\nThe assignment statement (token '=', the equals sign). This operates differently than in traditional imperative programming languages, and this fundamental mechanism (including the nature of Python's version of variables) illuminates many other features of the language. Assignment in C, e.g., x = 2, translates to \"typed variable name x receives a copy of numeric value 2\". The (right-hand) value is copied into an allocated storage location for which the (left-hand) variable name is the symbolic address. The memory allocated to the variable is large enough (potentially quite large) for the declared type. In the simplest case of Python assignment, using the same example, x = 2, translates to \"(generic) name x receives a reference to a separate, dynamically allocated object of numeric (int) type of value 2.\" This is termed binding the name to the object. Since the name's storage location doesn't contain the indicated value, it is improper to call it a variable. Names may be subsequently rebound at any time to objects of greatly varying types, including strings, procedures, complex objects with data and methods, etc. Successive assignments of a common value to multiple names, e.g., x = 2; y = 2; z = 2 result in allocating storage to (at most) three names and one numeric object, to which all three names are bound. Since a name is a generic reference holder it is unreasonable to associate a fixed data type with it. However at a given time a name will be bound to some object, which will have a type; thus there is dynamic typing.\nThe if statement, which conditionally executes a block of code, along with else and elif (a contraction of else-if).\nThe for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block.\nThe while statement, which executes a block of code as long as its condition is true.\nThe try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses; it also ensures that clean-up code in a finally block will always be run regardless of how the block exits.\nThe class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming.\nThe def statement, which defines a function or method.\nThe with statement (from Python 2.5), which encloses a code block within a context manager (for example, acquiring a lock before the block of code is run and releasing the lock afterwards, or opening a file and then closing it), allowing Resource Acquisition Is Initialization (RAII)-like behavior.\nThe pass statement, which serves as a NOP. It is syntactically needed to create an empty code block.\nThe assert statement, used during debugging to check for conditions that ought to apply.\nThe yield statement, which returns a value from a generator function. From Python 2.5, yield is also an operator. This form is used to implement coroutines.\nThe import statement, which is used to import modules whose functions or variables can be used in the current program.\nThe print statement was changed to the print() function in Python 3.[54]\nPython does not support tail call optimization or first-class continuations, and, according to Guido van Rossum, it never will.[55][56] However, better support for coroutine-like functionality is provided in 2.5, by extending Python's generators.[57] Before 2.5, generators were lazy iterators; information was passed unidirectionally out of the generator. As of Python 2.5, it is possible to pass information back into a generator function, and as of Python 3.3, the information can be passed through multiple stack levels.[58]\n\nExpressions\nSome Python expressions are similar to languages such as C and Java, while some are not:\n\nAddition, subtraction, and multiplication are the same, but the behavior of division differs (see Mathematics for details). Python also added the ** operator for exponentiation.\nAs of Python 3.5, it supports matrix multiplication directly with the @ operator, versus C and Java, which implement these as library functions. Earlier versions of Python also used methods instead of an infix operator.[59][60]\nIn Python, == compares by value, versus Java, which compares numerics by value[61] and objects by reference.[62] (Value comparisons in Java on objects can be performed with the equals() method.) Python's is operator may be used to compare object identities (comparison by reference). In Python, comparisons may be chained, for example a <= b <= c.\nPython uses the words and, or, not for its boolean operators rather than the symbolic &&, ||, ! used in Java and C.\nPython has a type of expression termed a list comprehension. Python 2.4 extended list comprehensions into a more general expression termed a generator expression.[42]\nAnonymous functions are implemented using lambda expressions; however, these are limited in that the body can only be one expression.\nConditional expressions in Python are written as x if c else y[63] (different in order of operands from the c ? x : y operator common to many other languages).\nPython makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). Tuples are written as (1, 2, 3), are immutable and thus can be used as the keys of dictionaries, provided all elements of the tuple are immutable. The parentheses around the tuple are optional in some contexts. Tuples can appear on the left side of an equal sign; hence a statement like x, y = y, x can be used to swap two variables.\nPython has a \"string format\" operator %. This functions analogous to printf format strings in C, e.g. \"spam=%s eggs=%d\" % (\"blah\", 2) evaluates to \"spam=blah eggs=2\". In Python 3 and 2.6+, this was supplemented by the format() method of the str class, e.g. \"spam={0} eggs={1}\".format(\"blah\", 2).\nPython has various kinds of string literals:\nStrings delimited by single or double quote marks. Unlike in Unix shells, Perl and Perl-influenced languages, single quote marks and double quote marks function identically. Both kinds of string use the backslash (\\) as an escape character and there is no implicit string interpolation such as \"$spam\".\nTriple-quoted strings, which begin and end with a series of three single or double quote marks. They may span multiple lines and function like here documents in shells, Perl and Ruby.\nRaw string varieties, denoted by prefixing the string literal with an r. No escape sequences are interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. Compare \"@-quoting\" in C#.\nPython has array index and array slicing expressions on lists, denoted as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The third slice parameter, called step or stride, allows elements to be skipped and reversed. Slice indexes may be omitted, for example a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.\nIn Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This leads to duplicating some functionality. For example:\n\nList comprehensions vs. for-loops\nConditional expressions vs. if blocks\nThe eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former is for expressions, the latter is for statements.\nStatements cannot be a part of an expression, so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. A particular case of this is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement. This has the advantage of avoiding a classic C error of mistaking an assignment operator = for an equality operator == in conditions: if (c = 1) { ... } is valid C code but if c = 1: ... causes a syntax error in Python.\n\nMethods\nMethods on objects are functions attached to the object's class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, or Ruby).[64]\n\nTyping\nPython uses duck typing and has typed objects but untyped variable names. Type constraints are not checked at compile time; rather, operations on an object may fail, signifying that the given object is not of a suitable type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) rather than silently attempting to make sense of them.\n\nPython allows programmers to define their own types using classes, which are most often used for object-oriented programming. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.\n\nBefore version 3.0, Python had two kinds of classes: old-style and new-style.[65] Old-style classes were eliminated in Python 3.0, making all classes new-style. In versions between 2.2 and 3.0, both kinds of classes could be used. The syntax of both styles is the same, the difference being whether the class object is inherited from, directly or indirectly (all new-style classes inherit from object and are instances of type).\n\nSummary of Python 3's built-in types\nType\tMutable\tDescription\tSyntax example\nstr\tImmutable\tA character string: sequence of Unicode codepoints\t'Wikipedia'\n\"Wikipedia\"\n\"\"\"Spanning\nmultiple\nlines\"\"\"\nbytearray\tMutable\tSequence of bytes\tbytearray(b'Some ASCII')\nbytearray(b\"Some ASCII\")\nbytearray([119, 105, 107, 105])\nbytes\tImmutable\tSequence of bytes\tb'Some ASCII'\nb\"Some ASCII\"\nbytes([119, 105, 107, 105])\nlist\tMutable\tList, can contain mixed types\t[4.0, 'string', True]\ntuple\tImmutable\tCan contain mixed types\t(4.0, 'string', True)\nset\tMutable\tUnordered set, contains no duplicates; can contain mixed types if hashable\t{4.0, 'string', True}\nfrozenset\tImmutable\tUnordered set, contains no duplicates; can contain mixed types if hashable\tfrozenset([4.0, 'string', True])\ndict\tMutable\tAssociative array (or dictionary) of key and value pairs; can contain mixed types (keys and values), keys must be a hashable type\t{'key1': 1.0, 3: False}\nint\tImmutable\tInteger of unlimited magnitude[66]\t42\nfloat\tImmutable\tFloating point number, system-defined precision\t3.1415927\ncomplex\tImmutable\tComplex number with real and imaginary parts\t3+2.7j\nbool\tImmutable\tBoolean value\tTrue\nFalse\nellipsis\t\tAn ellipsis placeholder to be used as an index in NumPy arrays\t...\nMathematics\nPython has the usual C arithmetic operators (+, -, *, /, %). It also has ** for exponentiation, e.g. 5**3 == 125 and 9**0.5 == 3.0, and a new matrix multiply @ operator is included in version 3.5.[67]\n\nThe behavior of division has changed significantly over time:[68]\n\nPython 2.1 and earlier use the C division behavior. The / operator is integer division if both operands are integers, and floating-point division otherwise. Integer division rounds towards 0, e.g. 7 / 3 == 2 and -7 / 3 == -2.\nPython 2.2 changes integer division to round towards negative infinity, e.g. 7 / 3 == 2 and -7 / 3 == -3. The floor division // operator is introduced. So 7 // 3 == 2, -7 // 3 == -3, 7.5 // 3 == 2.0 and -7.5 // 3 == -3.0. Adding from __future__ import division causes a module to use Python 3.0 rules for division (see next).\nPython 3.0 changes / to be always floating-point division. In Python terms, the pre-3.0 / is classic division, the version-3.0 / is real division, and // is floor division.\nRounding towards negative infinity, though different from most languages, adds consistency. For instance, it means that the equation (a+b) // b == a // b + 1 is always true. It also means that the equation b * (a // b) + a % b == a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a % b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.[69]\n\nPython provides a round function for rounding a float to the nearest integer. For tie-breaking, versions before 3 use round-away-from-zero: round(0.5) is 1.0, round(-0.5) is -1.0.[70] Python 3 uses round to even: round(1.5) is 2, round(2.5) is 2.[71]\n\nPython allows boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.[72][page needed]\n\nPython has extensive built-in support for arbitrary precision arithmetic. Integers are transparently switched from the machine-supported maximum fixed-precision (usually 32 or 64 bits), belonging to the python type int, to arbitrary precision, belonging to the python type long, where needed. The latter have an \"L\" suffix in their textual representation.[73] The Decimal type/class in module decimal (since version 2.4) provides decimal floating point numbers to arbitrary precision and several rounding modes.[74] The Fraction type in module fractions (since version 2.6) provides arbitrary precision for rational numbers.[75]\n\nDue to Python's extensive mathematics library, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.\n\nLibraries\nPython has a large standard library, commonly cited as one of Python's greatest strengths,[76] providing tools suited to many tasks. This is deliberate and has been described as a \"batteries included\"[28] Python philosophy. For Internet-facing applications, many standard formats and protocols (such as MIME and HTTP) are supported. Modules for creating graphical user interfaces, connecting to relational databases, pseudorandom number generators, arithmetic with arbitrary precision decimals,[77] manipulating regular expressions, and doing unit testing are also included.\n\nSome parts of the standard library are covered by specifications (for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333[78]), but most modules are not. They are specified by their code, internal documentation, and test suite (if supplied). However, because most of the standard library is cross-platform Python code, only a few modules need altering or rewriting for variant implementations.\n\nThe standard library is not needed to run Python or embed it in an application. For example, Blender 2.49 omits most of the standard library.\n\nAs of January 2016, the Python Package Index, the official repository of third-party software for Python, contains more than 72,000 packages offering a wide range of functionality, including:\n\ngraphical user interfaces, web frameworks, multimedia, databases, networking and communications\ntest frameworks, automation and web scraping, documentation tools, system administration\nscientific computing, text processing, image processing\nDevelopment environments\nSee also: Comparison of integrated development environments � Python\nMost Python implementations (including CPython) can function as a command line interpreter, for which the user enters statements sequentially and receives the results immediately (read�eval�print loop (REPL)). In short, Python acts as a command-line interface or shell.\n\nOther shells add abilities beyond those in the basic interpreter, including IDLE and IPython. While generally following the visual style of the Python shell, they implement features like auto-completion, session state retention, and syntax highlighting.\n\nIn addition to standard desktop integrated development environments (Python IDEs), there are also web browser-based IDEs, Sage (intended for developing science and math-related Python programs), and a browser-based IDE and hosting environment, PythonAnywhere.\n\nImplementations\nSee also: List of Python software � Python implementations\nThe main Python implementation, named CPython, is written in C meeting the C89 standard.[79] It compiles Python programs into intermediate bytecode,[80] which is executed by the virtual machine.[81] CPython is distributed with a large standard library written in a mixture of C and Python. It is available in versions for many platforms, including Windows and most modern Unix-like systems. CPython was intended from almost its very conception to be cross-platform.[82]\n\nPyPy is a fast, compliant[83] interpreter of Python 2.7 and 3.2. Its just-in-time compiler brings a significant speed improvement over CPython.[84] A version taking advantage of multi-core processors using software transactional memory is being created.[85]\n\nStackless Python is a significant fork of CPython that implements microthreads; it does not use the C memory stack, thus allowing massively concurrent programs. PyPy also has a stackless version.[86]\n\nMicroPython is a lean, fast Python 3 variant that is optimised to run on microcontrollers.\n\nOther just-in-time compilers have been developed in the past, but are now unsupported:\n\nGoogle began a project named Unladen Swallow in 2009 with the aims of speeding up the Python interpreter by 5 times, by using the LLVM, and of improving its multithreading ability to scale to thousands of cores.[87]\nPsyco is a just-in-time specialising compiler that integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialised for certain data types and is faster than standard Python code.\nIn 2005, Nokia released a Python interpreter for the Series 60 mobile phones named PyS60. It includes many of the modules from the CPython implementations and some added modules to integrate with the Symbian operating system. This project has been kept up to date to run on all variants of the S60 platform and there are several third party modules available. The Nokia N900 also supports Python with GTK widget libraries, with the feature that programs can be both written and run on the target device.[88]\n\nThere are several compilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\n\nJython compiles into Java byte code, which can then be executed by every Java virtual machine implementation. This also enables the use of Java class library functions from the Python program.\nIronPython follows a similar approach in order to run Python programs on the .NET Common Language Runtime.\nThe RPython language can be compiled to C, Java bytecode, or Common Intermediate Language, and is used to build the PyPy interpreter of Python.\nPyjamas compiles Python to JavaScript.\nShed Skin compiles Python to C++.\nCython and Pyrex compile to C.\nA performance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy '13.[89]\n\nDevelopment\nPython's development is conducted largely through the Python Enhancement Proposal (PEP) process. The PEP process is the primary mechanism for proposing major new features, for collecting community input on an issue, and for documenting the design decisions that have gone into Python.[90] Outstanding PEPs are reviewed and commented upon by the Python community and by Van Rossum, the Python project's benevolent dictator for life.[90]\n\nEnhancement of the language goes along with development of the CPython reference implementation. The mailing list python-dev is the primary forum for discussion about the language's development; specific issues are discussed in the Roundup bug tracker maintained at python.org.[91] Development takes place on a self-hosted source code repository running Mercurial.[92]\n\nCPython's public releases come in three types, distinguished by which part of the version number is incremented:\n\nBackwards-incompatible versions, where code is expected to break and must be manually ported. The first part of the version number is incremented. These releases happen infrequently�for example, version 3.0 was released 8 years after 2.0.\nMajor or \"feature\" releases, which are largely compatible but introduce new features. The second part of the version number is incremented. These releases are scheduled to occur roughly every 18 months, and each major version is supported by bugfixes for several years after its release.[93]\nBugfix releases, which introduce no new features but fix bugs. The third and final part of the version number is incremented. These releases are made whenever a sufficient number of bugs have been fixed upstream since the last release, or roughly every 3 months. Security vulnerabilities are also patched in bugfix releases.[94]\nMany alpha, beta, and release-candidates are also released as previews, and for testing before final releases. Although there is a rough schedule for each release, this is often pushed back if the code is not ready. The development team monitors the state of the code by running the large unit test suite during development, and using the BuildBot continuous integration system.[95]\n\nThe community of Python developers has also contributed over 72,000 software modules (as of January 2016) to the Python Package Index (PyPI), the official repository of third-party libraries for Python.\n\nThe major academic conference on Python is named PyCon. There are special mentoring programmes like the Pyladies.\n\nNaming\nPython's name is derived from the television series Monty Python's Flying Circus,[96] and it is common to use Monty Python references in example code.[97] For example, the metasyntactic variables often used in Python literature are spam and eggs, instead of the traditional foo and bar.[97][98] Also, the official Python documentation often contains various obscure Monty Python references.\n\nThe prefix Py- is used to show that something is related to Python. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of SDL to Python (commonly used to create games); PyS60, an implementation for the Symbian S60 operating system; PyQt and PyGTK, which bind Qt and GTK, respectively, to Python; and PyPy, a Python implementation originally written in Python.\n\nUses\nMain article: List of Python software\nSince 2003, Python has consistently ranked in the top ten most popular programming languages as measured by the TIOBE Programming Community Index. As of January 2016, it is in position five.[99] It was ranked as Programming Language of the Year for the year 2007 and 2010.[23] It is the third most popular language whose grammatical syntax is not predominantly based on C, e.g. C++, Objective-C (note, C# and Java only have partial syntactic similarity to C, such as the use of curly braces, and are closer in similarity to each other than C).\n\nAn empirical study found scripting languages (such as Python) more productive than conventional languages (such as C and Java) for a programming problem involving string manipulation and search in a dictionary. Memory consumption was often \"better than Java and not much worse than C or C++\".[100]\n\nLarge organizations that make use of Python include Google,[101] Yahoo!,[102] CERN,[103] NASA,[104] and some smaller ones like ILM,[105] and ITA.[106] The social news networking site, Reddit, is written entirely in Python.\n\nPython can serve as a scripting language for web applications, e.g., via mod_wsgi for the Apache web server.[107] With Web Server Gateway Interface, a standard API has evolved to facilitate these applications. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle and Zope support developers in the design and maintenance of complex applications. Pyjamas and IronPython can be used to develop the client-side of Ajax-based applications. SQLAlchemy can be used as data mapper to a relational database. Twisted is a framework to program communications between computers, and is used (for example) by Dropbox.\n\nLibraries like NumPy, SciPy and Matplotlib allow the effective use of Python in scientific computing,[108][109] with specialized libraries such as BioPython and Astropy providing domain-specific functionality. Sage is a mathematical software with a \"notebook\" programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus.\n\nPython has been successfully embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modeler like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP,[110] Inkscape, Scribus and Paint Shop Pro,[111] and musical notation program or scorewriter capella. GNU Debugger uses Python as a pretty printer to show complex structures such as C++ containers. Esri promotes Python as the best choice for writing scripts in ArcGIS.[112] It has also been used in several video games,[113][114] and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go.[115]\n\nPython has been used in artificial intelligence tasks.[116][117][118][119] As a scripting language with module architecture, simple syntax and rich text processing tools, Python is often used for natural language processing tasks.[120]\n\nMany operating systems include Python as a standard component; the language ships with most Linux distributions, AmigaOS 4, FreeBSD, NetBSD, OpenBSD and OS X, and can be used from the terminal. Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora use the Anaconda installer. Gentoo Linux uses Python in its package management system, Portage.\n\nPython has also seen extensive use in the information security industry, including in exploit development.[121][122]\n\nMost of the Sugar software for the One Laptop per Child XO, now developed at Sugar Labs, is written in Python.[123]\n\nThe Raspberry Pi single-board computer project has adopted Python as its main user-programming language.\n\nLibreOffice includes Python and intends to replace Java with Python. Python Scripting Provider is a core feature[124] since Version 4.0 from 7 February 2013.\n\nLanguages influenced by Python\nPython's design and philosophy have influenced several programming languages, including:\n\nBoo uses indentation, a similar syntax, and a similar object model. However, Boo uses static typing (and optional duck typing) and is closely integrated with the .NET Framework.[125]\nCobra uses indentation and a similar syntax. Cobra's \"Acknowledgements\" document lists Python first among languages that influenced it.[126] However, Cobra directly supports design-by-contract, unit tests, and optional static typing.[127]\nECMAScript borrowed iterators, generators, and list comprehensions from Python.[128]\nGo is described as incorporating the \"development speed of working in a dynamic language like Python\".[129]\nGroovy was motivated by the desire to bring the Python design philosophy to Java.[130]\nJulia was designed \"with true macros [.. and to be] as usable for general programming as Python [and] should be as fast as C\".[20] Calling to or from Julia is possible; to with PyCall.jl and a Python package jyjulia allows calling, in the other direction, from Python.\nOCaml has an optional syntax, named twt (The Whitespace Thing), inspired by Python and Haskell.[131]\nRuby's creator, Yukihiro Matsumoto, has said: \"I wanted a scripting language that was more powerful than Perl, and more object-oriented than Python. That's why I decided to design my own language.\"[132]\nCoffeeScript is a programming language that cross-compiles to JavaScript; it has Python-inspired syntax.\nSwift is a programming language invented by Apple; it has some Python-inspired syntax.[133]\nPython's development practices have also been emulated by other languages. The practice of requiring a document describing the rationale for, and issues surrounding, a change to the language (in Python's case, a PEP) is also used in Tcl[134] and Erlang[135] because of Python's influence.\n\nPython has been awarded a TIOBE Programming Language of the Year award twice (in 2007 and 2010), which is given to the language with the greatest growth in popularity over the course of a year, as measured by the TIOBE index.[136]", "skillName": "Python."}
{"id": 211, "category": "Computer_Programming", "skillText": "Software architecture refers to the fundamental structures of a software system, the discipline of creating such structures, and the documentation of these structures. These structures are needed to reason about the software system. Each structure comprises software elements, relations among them, and properties of both elements and relations,[1] along with rationale for the introduction and configuration of each element. The architecture of a software system is a metaphor, analogous to the architecture of a building.[2]\n\nSoftware architecture is about making fundamental structural choices which are costly to change once implemented. Software architecture choices, also called architectural decisions, include specific structural options from possibilities in the design of software. For example, the systems that controlled the space shuttle launch vehicle had the requirement of being very fast and very reliable. Therefore, an appropriate real-time computing language would need to be chosen. Additionally, to satisfy the need for reliability the choice could be made to have multiple redundant and independently produced copies of the program, and to run these copies on independent hardware while cross-checking results.\n\nDocumenting software architecture facilitates communication between stakeholders, captures decisions about the architecture design, and allows reuse of design components between projects.[3]:pp.29–35\n\nContents\n\n    1 Scope\n    2 Characteristics\n    3 Motivation\n    4 History\n    5 Architecture activities\n        5.1 Architecture supporting activities\n    6 Software architecture topics\n        6.1 Software architecture description\n            6.1.1 Architecture description languages\n            6.1.2 Architecture viewpoints\n            6.1.3 Architecture frameworks\n        6.2 Architecture design methods\n            6.2.1 Software architecture and agile development\n        6.3 Architectural styles and patterns\n        6.4 Software architecture erosion\n        6.5 Software architecture recovery\n    7 Related fields\n        7.1 Design\n        7.2 Requirements Engineering\n        7.3 Other types of 'architecture'\n    8 See also\n    9 References\n    10 Further reading\n    11 External links\n\nScope\n\nOpinions vary as to the scope of software architectures:[4]\n\n    Overall, macroscopic system structure;[5] this refers to architecture as a higher level abstraction of a software system that consists of a collection of computational components together with connectors that describe the interaction between these components.\n    The important stuff—whatever that is;[6] this refers to the fact that software architects should concern themselves with those decisions that have high impact on the system and its stakeholders.\n    That which is fundamental to understanding a system in its environment;[7] in this definition, the environment is characterized by stakeholder concerns, technical constraints, and various dimensions of project context.[8]\n    Things that people perceive as hard to change;[6] since designing the architecture often takes place at the beginning of a software system's lifecycle, the architect should focus on decisions that \"have to\" be right the first time. Following this line of thought, architectural design issues may become non-architectural once their irreversibility can be overcome (see \"Software architecture and agile development\" below).\n    A set of architectural design decisions;[9] software architecture should not be considered merely a set of models or structures, but should include the decisions that lead to these particular structures, and the rationale behind them (e.g., justifications, answers to \"why\" questions)). This insight has led to substantial research into software architecture knowledge management.[10]\n\nThere is no sharp distinction between software architecture versus design and requirements engineering (see Related fields below). They are all part of a \"chain of intentionality\" from high-level intentions to low-level details.[11](p18) This duality is also referred to as the \"twin peaks\" of software engineering.[citation needed]\nCharacteristics\n\nSoftware architecture exhibits the following:\n\nMultitude of stakeholders: software systems have to cater to a variety of stakeholders such as business managers, application owners, developers, end users and infrastructure operators. These stakeholders all have their own concerns with respect to the system. Balancing these concerns and demonstrating how they are addressed is part of designing the system.[3]:pp.29–31 This implies that architecture involves dealing with a broad variety of concerns and stakeholders, and has a multidisciplinary nature. Software architect require non-technicals skills such as communication and negotiation competencies.\n\nSeparation of concerns: the established way for architects to reduce complexity is to separate the concerns that drive the design. Architecture documentation shows that all stakeholder concerns are addressed by modeling and describing the architecture from separate points of view associated with the various stakeholder concerns.[12] These separate descriptions are called architectural views (see for example the 4+1 Architectural View Model).\n\nQuality-driven: classic software design approaches (e.g. Jackson Structured Programming) were driven by required functionality and the flow of data through the system, but the current insight[3]:pp.26–28 is that the architecture of a software system is more closely related to its quality attributes such as fault-tolerance, backward compatibility, extensibility, reliability, maintainability, availability, security, usability, and other such –ilities. Stakeholder concerns often translate into requirements and constraints on these quality attributes, which are variously called non-functional requirements, extra-functional requirements, behavioral requirements, or quality attribute requirements.\n\nRecurring styles: like building architecture, the software architecture discipline has developed standard ways to address recurring concerns. These \"standard ways\" are called by various names at various levels of abstraction. Common terms for recurring solutions are architectural style, principle,[11]:pp.273–277 tactic,[3]:pp.70–72 reference architecture[13][14] and architectural pattern.[3]:pp.203–205\n\nConceptual integrity: a term introduced by Fred Brooks in The Mythical Man-Month to denote the idea that the architecture of a software system represents an overall vision of what it should do and how it should do it. This vision should be separated from its implementation. The architect assumes the role of \"keeper of the vision\", making sure that additions to the system are in line with the architecture, hence preserving conceptual integrity.[15]:pp.41–50\nMotivation\n\nSoftware architecture is an \"intellectually graspable\" abstraction of a complex system.[3]:pp.5–6 This abstraction provides a number of benefits:\n\n    It gives a basis for analysis of software systems' behavior before the system has been built.[2] The ability to verify that a future software system fulfills its stakeholders' needs without actually having to build it represents substantial cost-saving and risk-mitigation.[16] A number of techniques have been developed in academia and practice to perform such analyses, for instance ATAM, ARID and TARA.\n    It provides a basis for re-use of elements and decisions.[2][3]:p.35 A complete software architecture or parts of it, like individual architectural strategies and decisions, can be re-used across multiple systems whose stakeholders require similar quality attributes or functionality, saving design costs and mitigating the risk of design mistakes (assuming that the project contexts[17] match).\n    It supports early design decisions that impact a system's development, deployment, and maintenance life.[3]:p.31 Getting the early, high-impact decisions right is important to prevent schedule and budget overruns. On the other hand, a principle of lean software development is to defer decisions until the last responsible moment (M. and T. Poppendieck); however, it is not always clear when the this moment for a particular subset of decisions has come.\n    It facilitates communication with stakeholders, contributing to a system that better fulfills their needs.[3]:p.29–31 Communicating about complex systems from the point of view of stakeholders helps them understand the consequences of their stated requirements and the design decisions based on them. Architecture gives the ability to communicate about design decisions before the system is implemented, when they are still relatively easy to adapt.\n    It helps in risk management. Software architecture helps to reduce risks and chance of failure.[11](p18)\n    It enables cost reduction. Software architecture is a means to manage risk and costs in complex IT projects.[18]\n\nHistory\n\nThe comparison between software design and (civil) architecture was first drawn in the late 1960s ,[19] but the term software architecture became prevalent only in the 1990s.[20] The field of computer science had encountered problems associated with complexity since its formation.[21] Earlier problems of complexity were solved by developers by choosing the right data structures, developing algorithms, and by applying the concept of separation of concerns. Although the term \"software architecture\" is relatively new to the industry, the fundamental principles of the field have been applied sporadically by software engineering pioneers since the mid-1980s. Early attempts to capture and explain software architecture of a system were imprecise and disorganized, often characterized by a set of box-and-line diagrams. [22]\n\nSoftware architecture as a concept has its origins in the research of Edsger Dijkstra in 1968 and David Parnas in the early 1970s. These scientists emphasized that the structure of a software system matters and getting the structure right is critical. During the 1990s there was a concerted effort to define and codify fundamental aspects of the discipline, with research work concentrating on architectural styles (patterns), architecture description languages, and architecture documentation.[23] Research institutions have played a prominent role in furthering software architecture as a discipline. For instance, Mary Shaw and David Garlan of Carnegie Mellon wrote a book titled Software Architecture: Perspectives on an Emerging Discipline in 1996, which promoted software architecture concepts such as components, connectors, and styles.\n\nIEEE 1471-2000, Recommended Practice for Architecture Description of Software-Intensive Systems, was the first formal standard in the area of software architecture. It was adopted in 2007 by ISO as ISO/IEC 42010:2007. In November 2011, IEEE 1471–2000 was superseded by ISO/IEC/IEEE 42010:2011, Systems and software engineering — Architecture description (jointly published by IEEE and ISO).[12] While in IEEE 1471, software architecture was about the architecture of \"software-intensive systems\", defined as \"any system where software contributes essential influences to the design, construction, deployment, and evolution of the system as a whole\", the 2011 edition goes a step further by including the ISO/IEC 15288 and ISO/IEC 12207 definitions of a system, which embrace not only hardware and software, but also \"humans, processes, procedures, facilities, materials and naturally occurring entities\".\nArchitecture activities\n\nThere are many activities that a software architect performs. A software architect typically works with project managers, discusses architecturally significant requirements with stakeholders, designs a software architecture, evaluates a design, communicates with designers and stakeholders, documents the architectural design and more.[24] There are four core activities in software architecture design.[25] These core architecture activities are performed iteratively and at different stages of the initial software development life-cycle, as well as over the evolution of a system.\n\nArchitectural Analysis is the process of understanding the environment in which a proposed system or systems will operate and determining the requirements for the system. The input or requirements to the analysis activity can come from any number of stakeholders and include items such as:\n\n    what the system will do when operational (the functional requirements)\n    how well the system will perform runtime non-functional requirements such as reliability, operability, performance efficiency, security, compatibility defined in ISO/IEC 25010:2011 standard [26]\n    development-time non-functional requirements such as maintainability and transferability defined in ISO 25010:2011 standard [26]\n    business requirements and environmental contexts of a system that may change over time, such as legal, social, financial, competitive, and technology concerns [27]\n\nThe outputs of the analysis activity are those requirements that have a measurable impact on a software system’s architecture, called architecturally significant requirements.[28]\n\nArchitectural Synthesis or design is the process of creating an architecture. Given the architecturally significant requirements determined by the analysis, the current state of the design and the results of any evaluation activities, the design is created and improved. See [3]:pp.311–326[25] for a discussion of various techniques for improving a current design.\n\nArchitecture Evaluation is the process of determining how well the current design or a portion of it satisfies the requirements derived during analysis. An evaluation can occur whenever an architect is considering a design decision, it can occur after some portion of the design has been completed, it can occur after the final design has been completed or it can occur after the system has been constructed. Some of the available software architecture evaluation techniques include Architecture Tradeoff Analysis Method (ATAM) and TARA.[29] Frameworks for comparing the techniques are discussed in [16] and.[30]\n\nArchitecture Evolution is the process of maintaining and adapting an existing software architecture to meet requirement and environmental changes. As software architecture provides a fundamental structure of a software system, its evolution and maintenance would necessarily impact its fundamental structure. As such, architecture evolution is concerned with adding new functionality as well as maintaining existing functionality and system behaviour, for instance, via architectural refactoring.[31]\n\nArchitecture requires critical supporting activities. These supporting activities take place throughout the core software architecture process. They include knowledge management and communication, design reasoning and decision making, and documentation.\nArchitecture supporting activities\n\nSoftware architecture supporting activities are carried out during core software architecture activities. These supporting activities assist a software architect to carry out analysis, synthesis, evaluation and evolution. For instance, an architect has to gather knowledge, make decisions and document during the analysis phase.\n\n    Knowledge Management and Communication is the activity of exploring and managing knowledge that is essential to designing a software architecture. A software architect does not work in isolation. They get inputs, functional and non-functional requirements and design contexts, from various stakeholders; and provides outputs to stakeholders. Software architecture knowledge is often tacit and is retained in the heads of stakeholders. Software Architecture Knowledge Management (AKM) is about finding, communicating, and retaining knowledge. As software architecture design issues are intricate and interdependent, a knowledge gap in design reasoning can lead to incorrect software architecture design.[24][32] Examples of AKM and communication activities include searching for design patterns, prototyping, asking experienced developers and architects, evaluating the designs of similar systems, sharing knowledge with other designers and stakeholders.\n    Design Reasoning and Decision Making is the activity of evaluating design decisions. This activity is fundamental to all three core software architecture activities.[9][33] It entails gathering and associating decision contexts, formulating design decision problems, finding solution options and evaluating tradeoffs before making decisions. This process occurs at different levels of decision granularity, while evaluating significant architectural requirements and software architecture decisions, and software architecture analysis, synthesis, and evaluation. Examples of reasoning activities include understanding the impacts of a requirement or a design on quality attributes, questioning the issues that a design might cause, assessing possible solution options, and evaluating the tradeoffs between solutions.\n    Documentation is the activity of recording the design generated during the software architecture process. A system design is described using several views that frequently include a static view showing the code structure of the system, a dynamic view showing the actions of the system during execution, and a deployment view showing how a system is placed on hardware for execution. Kruchten's 4+1 view suggests a description of commonly used views for documenting software architecture;[34] Documenting Software Architectures: Views and Beyond has descriptions of the kinds of notations that could be used within the view description.[1] Examples of documentation activities are writing a specification, recording a system design model, documenting a design rationale, developing a viewpoint, documenting views. Software engineering methods such as the OpenUP and architecture design methods such as The Process of Software Architecting (P. Eeles, P. Cripps) suggest artifact (a.k.a. work product) types and templates for these documentation activities; ISO/IEC/IEEE 42010:2011 is accompanied by a documentation template as well (http://www.iso-architecture.org/ieee-1471/templates/ \n    ).\n\nSoftware architecture topics\nSoftware architecture description\nMain article: Software architecture description\n\nSoftware architecture description involves the principles and practices of modeling and representing architectures, using mechanisms such as: architecture description languages, architecture viewpoints, and architecture frameworks.\nArchitecture description languages\nMain article: Architecture description language\n\nAn architecture description language (ADL) is any means of expression used to describe a software architecture (ISO/IEC/IEEE 42010). Many special-purpose ADLs have been developed since the 1990s, including ArchiMate, AADL (SAE standard), Wright (developed by Carnegie Mellon), Acme (developed by Carnegie Mellon), xADL (developed by UCI), Darwin (developed by Imperial College London), DAOP-ADL (developed by University of Málaga), SBC-ADL (developed by National Sun Yat-Sen University), and ByADL (University of L'Aquila, Italy).\n\nADLs have not yet succeeded on a broad scale in practice; UML, often profiled, and Informal Rich Pictures (IPRs) a.k.a. box-and-line diagrams dominate. Usage of UML has been criticized by some thought leaders, but successes have also been reported. Simon Brown's Context, Containers, Components, Classes (C4) model is a recent adiditon to the architect's notation toolbox: https://www.voxxed.com/blog/2014/10/simple-sketches-for-diagramming-your-software-architecture/ \n.\n\nAccording to Gregor Hohpe, architects should stop drawing diagrams, in whatever notation, and start communicating: http://www.enterpriseintegrationpatterns.com/ramblings/94_37things.html \n.\nArchitecture viewpoints\nMain article: View model\n4+1 Architectural View Model.\n\nSoftware architecture descriptions are commonly organized into views, which are analogous to the different types of blueprints made in building architecture. Each view addresses a set of system concerns, following the conventions of its viewpoint, where a viewpoint is a specification that describes the notations, modeling and analysis techniques to use in a view that express the architecture in question from the perspective of a given set of stakeholders and their concerns (ISO/IEC/IEEE 42010). The viewpoint specifies not only the concerns framed (i.e., to be addressed) but the presentation, model kinds used, conventions used and any consistency (correspondence) rules to keep a view consistent with other views.\n\nPopular viewpoint models include the 4+1 views on software architecture, the viewpoints and perspectices catalog by Nick Rozanski and Eoin Woods, and the IBM ADS viewpoint model by Phlippe Spaas et al.\nArchitecture frameworks\nMain article: Architecture framework\n\nAn architecture framework captures the \"conventions, principles and practices for the description of architectures established within a specific domain of application and/or community of stakeholders\" (ISO/IEC/IEEE 42010). A framework is usually implemented in terms of one or more viewpoints or ADLs.\nArchitecture design methods\n\nMethods define process models (activities performed by roles) and specify artifacts to be created and delivered; they may also sugegst technqiues and practices that assist practitioners when performing the activities and producing the artifacts defined in the method. Five such methods are compared and consolidated in.[25]\nSoftware architecture and agile development\nMain article: Agile development\n\nThe importance of architecture was stated in the early works on Agile; for instance, Ken Schwaber's original Scrum paper from OOPSLA '97 has the notion of a pregame, in which a high-level system architecture is established (http://www.jeffsutherland.org/oopsla/schwapub.pdf \n). However, there are also concerns that software architecture leads to too much Big Design Up Front, especially among proponents of Agile software development. A number of methods have been developed to balance the trade-offs of up-front design and agility,[35] including the agile method DSDM which mandates a \"Foundations\" phase during which \"just enough\" architectural foundations are laid. IEEE Software devoted a special issue[36] to the interaction between agility and architecture. P. Kruchten, one of the creators of the Unified Process (UP) and the original 4+1 views on software architecture, summarizes the synergetic relationship in a December 2013 blog post called \"Agile architecture\".[37]\nArchitectural styles and patterns\nMain article: Software Architecture styles and patterns\n\nAn architectural pattern is a general, reusable solution to a commonly occurring problem in software architecture within a given context. Architectural patterns are often documented as software design patterns.\n\nFollowing traditional building architecture, a 'software architectural style' is a specific method of construction, characterized by the features that make it notable\" (Architectural style). \"An architectural style defines: a family of systems in terms of a pattern of structural organization; a vocabulary of components and connectors, with constraints on how they can be combined.\"[38] \"Architectural styles are reusable 'packages' of design decisions and constraints that are applied to an architecture to induce chosen desirable qualities.\"[39]\n\nThere are many recognized architectural patterns and styles, among them:\n\n    Blackboard\n    Client-server (2-tier, 3-tier, n-tier, cloud computing exhibit this style)\n    Component-based\n    Data-centric\n    Event-driven (or Implicit invocation)\n    Layered (or Multilayered architecture)\n    Monolithic application\n    Peer-to-peer (P2P)\n    Pipes and filters\n    Plug-ins\n    Representational state transfer (REST)\n    Rule-based\n    Service-oriented architecture and microservices as its implementation approach\n    Shared nothing architecture\n    Space-based architecture\n\nSome software architects treat architectural patterns and architectural styles as the same,[40] Others treat styles as compositions of patterns combined with architectural principles that jointly address a particular intent. These two positions have in common that both patterns and styles are idioms for architects to use; they \"provide a common language\"[40] or \"vocabulary\"[38] with which to describe classes of systems.\nSoftware architecture erosion\n\nSoftware architecture erosion (or \"decay\") refers to the gap observed between the planned and actual architecture of a software system as realized in its implementation.[41] Software architecture erosion occurs when implementation decisions either do not fully achieve the architecture-as-planned or otherwise violate constraints or principles of that architecture.[2] The gap between planned and actual architectures is sometimes understood in terms of the notion of technical debt.\n\nAs an example, consider a strictly layered system, where each layer can only use services provided by the layer immediately below it. Any source code component that does not observe this constraint represents an architecture violation. If not corrected, such violations can transform the architecture into a monolithic block, with adverse effects on understandability, maintainability, and evolvability.\n\nVarious approaches have been proposed to address erosion. \"These approaches, which include tools, techniques and processes, are primarily classified into three generic categories that attempt to minimise, prevent and repair architecture erosion. Within these broad categories, each approach is further broken down reflecting the high-level strategies adopted to tackle erosion. These are: process-oriented architecture conformance, architecture evolution management, architecture design enforcement, architecture to implementation linkage, self-adaptation and architecture restoration techniques consisting of recovery, discovery and reconciliation.\"[42]\n\nThere are two major techniques to detect architectural violations: reflexion models and domain-specific languages. Reflexion model (RM) techniques compare a high-level model provided by the system's architects with the source code implementation. Examples of commercial RM-based tools include the Bauhaus Suite \n(developed by Axivion), SAVE \n(developed by Fraunhofer IESE) and Structure-101 \n(developed by Headway Software). There are also domain-specific languages with focus on specifying and checking architectural constraints, including .QL (developed by Semmle Limited) and DCL \n(from Federal University of Minas Gerais).\nSoftware architecture recovery\nMain article: Software architecture recovery\n\nSoftware architecture recovery (or reconstruction, or reverse engineering) includes the methods, techniques and processes to uncover a software system's architecture from available information, including its implementation and documentation. Architecture recovery is often necessary to make informed decisions in the face of obsolete or out-of-date documentation and architecture erosion: implementation and maintenance decisions diverging from the envisioned architecture.[43]\nRelated fields\nDesign\nMain article: Software design\n\nArchitecture is design but not all design is architectural.[1] In practice, the architect is the one who draws the line between software architecture (architectural design) and detailed design (non-architectural design). There are no rules or guidelines that fit all cases, although there have been attempts to formalize the distinction. According to the Intension/Locality Hypothesis,[44] the distinction between architectural and detailed design is defined by the Locality Criterion,[44] according to which a statement about software design is non-local (architectural) if and only if a program that satisfies it can be expanded into a program that does not. For example, the client–server style is architectural (strategic) because a program that is built on this principle can be expanded into a program that is not client–server—for example, by adding peer-to-peer nodes.\nRequirements Engineering\nMain article: Requirements engineering\n\nRequirements engineering and software architecture can be seen as complementary approaches: while software architecture targets the 'solution space' or the 'how', requirements engineering addresses the 'problem space' or the 'what'.[45] Requirements engineering entails the elicitation, negotiation, specification, validation, documentation and management of requirements. Both requirements engineering and software architecture revolve around stakeholder concerns, needs and wishes.\n\nThere is considerable overlap between requirements engineering and software architecture, as evidenced for example by a study into five industrial software architecture methods that concludes that \"the inputs (goals, constrains, etc.) are usually ill-defined, and only get discovered or better understood as the architecture starts to emerge\" and that while \"most architectural concerns are expressed as requirements on the system, they can also include mandated design decisions\".[25] In short, the choice of required behavior given a particular problem impacts the architecture of the solution that addresses that problem, while at the same time the architectural design may impact the problem and introduce new requirements.[46] Approaches such as the Twin Peaks model [47] aim to exploit the synergistic relation between requirements and architecture.\nOther types of 'architecture'\nMain articles: Computer architecture, Systems architecture, and Enterprise architecture\n\nComputer architecture\n    Computer architecture targets the internal structure of a computer system, in terms of collaborating hardware components such as the CPU – or processor – the bus and the memory.\n\nSystems architecture\n    The term systems architecture has originally been applied to the architecture of systems that consists of both hardware and software. The main concern addressed by the systems architecture, also known as IT architecture, is then the integration of software and hardware in a complete, correctly working device. In another common – much broader – meaning, the term applies to the architecture of any complex system which may be of technical, sociotechnical or social nature.\n\nEnterprise architecture\n    The goal of enterprise architecture is to \"translate business vision and strategy into effective enterprise\".[48] Enterprise architecture frameworks, such as TOGAF and the Zachman Framework, usually distinguish between different enterprise architecture layers. Although terminology differs from framework to framework, many include at least a distinction between a business layer, an application (or information) layer, and a technology layer. Enterprise architecture addresses among others the alignment between these layers, usually in a top-down approach. Continuing the building architecture metaphor for software architecture, enterprise architecture is analogous to city-level planning.\n\nSee also\n\n    Architectural pattern (computer science)\n    Architecturally Significant Requirements\n    Attribute-driven design\n    Computer architecture\n    OpenUP\n    Solution architecture\n    Systems architecture\n    Systems design\n    Software Architecture Analysis Method", "skillName": "Software_architecture."}
{"id": 212, "category": "Computer_Programming", "skillText": "JavaScript (/ˈdʒɑːvəˌskrɪpt/[5]) is a high-level, dynamic, untyped, and interpreted programming language.[6] It has been standardized in the ECMAScript language specification.[7] Alongside HTML and CSS, it is one of the three core technologies of World Wide Web content production; the majority of websites employ it and it is supported by all modern Web browsers without plug-ins.[6] JavaScript is prototype-based with first-class functions, making it a multi-paradigm language, supporting object-oriented,[8] imperative, and functional programming styles.[6] It has an API for working with text, arrays, dates and regular expressions, but does not include any I/O, such as networking, storage, or graphics facilities, relying for these upon the host environment in which it is embedded.[7]\n\nAlthough there are strong outward similarities between JavaScript and Java, including language name, syntax, and respective standard libraries, the two are distinct languages and differ greatly in their design. JavaScript was influenced by programming languages such as Self and Scheme.[9]\n\nJavaScript is also used in environments that are not Web-based, such as PDF documents, site-specific browsers, and desktop widgets. Newer and faster JavaScript virtual machines (VMs) and platforms built upon them have also increased the popularity of JavaScript for server-side Web applications. On the client side, JavaScript has been traditionally implemented as an interpreted language, but more recent browsers perform just-in-time compilation. It is also used in game development, the creation of desktop and mobile applications, and server-side network programming with run-time environments such as Node.js.\n\nContents\n\n    1 History\n        1.1 Beginnings at Netscape\n        1.2 Server-side JavaScript\n        1.3 Adoption by Microsoft\n        1.4 Standardization\n        1.5 Later developments\n    2 Trademark\n    3 Features\n        3.1 Imperative and structured\n        3.2 Dynamic\n        3.3 Prototype-based (Object-oriented)\n        3.4 Functional\n        3.5 Delegative\n        3.6 Miscellaneous\n        3.7 Vendor-specific extensions\n    4 Syntax\n        4.1 Simple examples\n        4.2 More advanced example\n    5 Use in Web pages\n        5.1 Example script\n        5.2 Compatibility considerations\n    6 Security\n        6.1 Cross-site vulnerabilities\n            6.1.1 Misplaced trust in the client\n            6.1.2 Browser and plugin coding errors\n            6.1.3 Sandbox implementation errors\n    7 Uses outside Web pages\n        7.1 Embedded scripting language\n        7.2 Scripting engine\n        7.3 Application platform\n    8 Development tools\n    9 Version history\n    10 Related languages and features\n        10.1 Use as an intermediate language\n        10.2 JavaScript and Java\n    11 References\n    12 Further reading\n    13 External links\n\nHistory\nBeginnings at Netscape\n\nJavaScript was originally developed in 10 days in May 1995 by Brendan Eich, while he was working for Netscape Communications Corporation. Indeed, while competing with Microsoft for user adoption of Web technologies and platforms, Netscape considered their client-server offering a distributed OS with a portable version of Sun Microsystems's Java providing an environment in which applets could be run.[citation needed] Because Java was a competitor of C++ and aimed at professional programmers, Netscape also wanted a lightweight interpreted language that would complement Java by appealing to nonprofessional programmers, like Microsoft's Visual Basic (see JavaScript and Java).[10]\n\nAlthough it was developed under the name Mocha, the language was officially called LiveScript when it first shipped in beta releases of Netscape Navigator 2.0 in September 1995, but it was renamed JavaScript[11] when it was deployed in the Netscape browser version 2.0B3.[12]\n\nThe change of name from LiveScript to JavaScript roughly coincided with Netscape adding support for Java technology in its Netscape Navigator Web browser. The final choice of name caused confusion, giving the impression that the language was a spin-off of the Java programming language, and the choice has been characterized as a marketing ploy by Netscape to give JavaScript the cachet of what was then the hot new Web programming language.[13][14]\n\nThere is a common misconception that the JavaScript language was influenced by an earlier Web page scripting language developed by Nombas named C-- (not to be confused with the later C-- created in 1997).[15][16][17] Brendan Eich, however, had never heard of C-- before he created LiveScript.[18] Nombas did pitch their embedded Web page scripting to Netscape, though Web page scripting was not a new concept, as shown by ViolaWWW.[19] Nombas later switched to offering JavaScript instead of C-- in their ScriptEase product and was part of the TC39 group that standardized ECMAScript.[20][21]\nServer-side JavaScript\n\nNetscape introduced an implementation of the language for server-side scripting with Netscape Enterprise Server in December, 1995, soon after releasing JavaScript for browsers.[22][23] Since the mid-2000s, there has been a resurgence of server-side JavaScript implementations, such as Node.js.[24][25]\nAdoption by Microsoft\n\nMicrosoft Windows script technologies including VBScript and JScript were released in 1996. JScript, a reverse-engineered implementation of Netscape's JavaScript, was released on July 16, 1996 and was part of Internet Explorer 3, as well as being available server-side in Internet Information Server. IE3 also included Microsoft's first support for Cascading Style Sheets and various extensions to HTML, but in each case the implementation was noticeably different to that found in Netscape Navigator at the time.[26][27] These differences made it difficult for designers and programmers to make a single website work well in both browsers leading to the use of 'best viewed in Netscape' and 'best viewed in Internet Explorer' logos that characterized these early years of the browser wars.[28] JavaScript began to acquire a reputation for being one of the roadblocks to a cross-platform and standards-driven Web. Some developers took on the difficult task of trying to make their sites work in both major browsers, but many could not afford the time.[26] With the release of Internet Explorer 4, Microsoft introduced the concept of Dynamic HTML, but the differences in language implementations and the different and proprietary Document Object Models remained, and were obstacles to widespread take-up of JavaScript on the Web.[26]\nBrendan Eich creator of the JavaScript\nStandardization\n\nIn November 1996, Netscape announced that it had submitted JavaScript to Ecma International for consideration as an industry standard, and subsequent work resulted in the standardized version named ECMAScript. In June 1997, Ecma International published the first edition of the ECMA-262 specification. In June 1998, some modifications were made to adapt it to the ISO/IEC-16262 standard, and the second edition was released. The third edition of ECMA-262 was published on December 1999.[29]\n\nDevelopment of the fourth edition of the ECMAScript standard was never completed.[30] The fifth edition was released in December 2009. The current edition of the ECMAScript standard is 6, released in June 2015.[31]\nLater developments\n\nJavaScript has become one of the most popular programming languages on the Web. Initially, however, many professional programmers denigrated the language because its target audience consisted of Web authors and other such \"amateurs\", among other reasons.[32] The advent of Ajax returned JavaScript to the spotlight and brought more professional programming attention. The result was a proliferation of comprehensive frameworks and libraries, improved JavaScript programming practices, and increased usage of JavaScript outside Web browsers, as seen by the proliferation of server-side JavaScript platforms.\n\nIn January 2009, the CommonJS project was founded with the goal of specifying a common standard library mainly for JavaScript development outside the browser.[33]\n\nWith the rise of the single-page application and JavaScript-heavy sites, it is increasingly being used as a compile target for source-to-source compilers from both dynamic languages and static languages.\nTrademark\n\n\"JavaScript\" is a trademark of Oracle Corporation.[34] It is used under license for technology invented and implemented by Netscape Communications and current entities such as the Mozilla Foundation.[35]\nFeatures\n\nThe following features are common to all conforming ECMAScript implementations, unless explicitly specified otherwise.\nImperative and structured\n\nJavaScript supports much of the structured programming syntax from C (e.g., if statements, while loops, switch statements, do while loops, etc.). One partial exception is scoping: JavaScript originally had only function scoping with var. ECMAScript 2015 adds a let keyword for block scoping, meaning JavaScript now has both function and block scoping. Like C, JavaScript makes a distinction between expressions and statements. One syntactic difference from C is automatic semicolon insertion, which allows the semicolons that would normally terminate statements to be omitted.[36]\nDynamic\n\nTyping\n    As with most scripting languages, JavaScript is dynamically typed; a type is associated with each value, rather than just with each expression. For example, a variable that is at one time bound to a number may later be re-bound to a string.[37] JavaScript supports various ways to test the type of an object, including duck typing.[38]\nRun-time evaluation\n    JavaScript includes an eval function that can execute statements provided as strings at run-time.\n\nPrototype-based (Object-oriented)\n\nJavaScript is almost entirely object-based. In JavaScript, an object is an associative array, augmented with a prototype (see below); each string key provides the name for an object property, and there are two syntactical ways to specify such a name: dot notation (obj.x = 10) and bracket notation (obj['x'] = 10). A property may be added, rebound, or deleted at run-time. Most properties of an object (and any property that belongs to an object's prototype inheritance chain) can be enumerated using a for...in loop.\n\nJavaScript has a small number of built-in objects, including Function and Date.\n\nPrototypes\n    JavaScript uses prototypes where many other object-oriented languages use classes for inheritance.[39] It is possible to simulate many class-based features with prototypes in JavaScript.[40]\nFunctions as object constructors\n    Functions double as object constructors along with their typical role. Prefixing a function call with new will create an instance of a prototype, inheriting properties and methods from the constructor (including properties from the Object prototype).[41] ECMAScript 5 offers the Object.create method, allowing explicit creation of an instance without automatically inheriting from the Object prototype (older environments can assign the prototype to null).[42] The constructor's prototype property determines the object used for the new object's internal prototype. New methods can be added by modifying the prototype of the function used as a constructor. JavaScript's built-in constructors, such as Array or Object, also have prototypes that can be modified. While it is possible to modify the Object prototype, it is generally considered bad practice because most objects in JavaScript will inherit methods and properties from the Object prototype and they may not expect the prototype to be modified.[43]\nFunctions as methods\n    Unlike many object-oriented languages, there is no distinction between a function definition and a method definition. Rather, the distinction occurs during function calling; when a function is called as a method of an object, the function's local this keyword is bound to that object for that invocation.\n\nFunctional\n\nA function is first-class; a function is considered to be an object. As such, a function may have properties and methods, such as .call() and .bind().[44] A nested function is a function defined within another function. It is created each time the outer function is invoked. In addition, each nested function forms a lexical closure: The lexical scope of the outer function (including any constant, local variable, or argument value) becomes part of the internal state of each inner function object, even after execution of the outer function concludes.[45] JavaScript also supports anonymous functions.\nDelegative\n\nJavaScript supports implicit and explicit delegation.\n\nFunctions as Roles (Traits and Mixins)\n    JavaScript natively supports various function-based implementations of Role[46] patterns like Traits[47][48] and Mixins.[49] Such a function defines additional behavior by at least one method bound to the this keyword within its function body. A Role then has to be delegated explicitly via call or apply to objects that need to feature additional behavior that is not shared via the prototype chain.\nObject Composition and Inheritance\n    Whereas explicit function-based delegation does cover composition in JavaScript, implicit delegation already happens every time the prototype chain is walked in order to, e.g., find a method that might be related to but is not directly owned by an object. Once the method is found it gets called within this object's context. Thus inheritance in JavaScript is covered by a delegation automatism that is bound to the prototype property of constructor functions.\n\nMiscellaneous\n\nRun-time environment\n    JavaScript typically relies on a run-time environment (e.g., a Web browser) to provide objects and methods by which scripts can interact with the environment (e.g., a webpage DOM). It also relies on the run-time environment to provide the ability to include/import scripts (e.g., HTML <script> elements). This is not a language feature per se, but it is common in most JavaScript implementations.\n\n    JavaScript processes messages from a queue one at a time. Upon loading a new message, JavaScript calls a function associated with that message, which creates a call stack frame (the function's arguments and local variables). The call stack shrinks and grows based on the function's needs. Upon function completion, when the stack is empty, JavaScript proceeds to the next message in the queue. This is called the event loop, described as \"run to completion\" because each message is fully processed before the next message is considered. However, the language's concurrency model describes the event loop as non-blocking: program input/output is performed using events and callback functions. This means, for instance, that JavaScript can process a mouse click while waiting for a database query to return information.[50]\n\nVariadic functions\n    An indefinite number of parameters can be passed to a function. The function can access them through formal parameters and also through the local arguments object. Variadic functions can also be created by using the bind \n    method.\n\nArray and object literals\n    Like many scripting languages, arrays and objects (associative arrays in other languages) can each be created with a succinct shortcut syntax. In fact, these literals form the basis of the JSON data format.\n\nRegular expressions\n    JavaScript also supports regular expressions in a manner similar to Perl, which provide a concise and powerful syntax for text manipulation that is more sophisticated than the built-in string functions.[51]\n\nVendor-specific extensions\n\nJavaScript is officially managed by Mozilla Foundation, and new language features are added periodically. However, only some JavaScript engines support these new features:\n\n    property getter and setter functions (supported by WebKit, Gecko, Opera,[52] ActionScript, and Rhino)[53]\n    conditional catch clauses\n    iterator protocol (adopted from Python)\n    shallow generators-coroutines (adopted from Python)\n    array comprehensions and generator expressions (adopted from Python)\n    proper block scope via the let keyword\n    array and object destructuring (limited form of pattern matching)\n    concise function expressions (function(args) expr)\n    ECMAScript for XML (E4X), an extension that adds native XML support to ECMAScript (unsupported in Firefox since version 21[54])\n\nSyntax\nMain article: JavaScript syntax\nSimple examples\n\nVariables in JavaScript can be defined using the var keyword:[55]\n\nvar x; // defines the variable x, the special value “undefined” (not to be confused with an undefined value) is assigned to it by default\nvar y = 2; // defines the variable y and assigns the value of 2 to it\n\nNote the comments in the example above, both of which were preceded with two forward slashes.\n\nThere is no built-in I/O functionality in JavaScript; the run-time environment provides that. The ECMAScript specification in edition 5.1 mentions:[56]\n\n    … indeed, there are no provisions in this specification for input of external data or output of computed results.\n\nHowever, most runtime environments have a console object [57] that can be used to print output. Here is a minimalist Hello World program:\n\nconsole.log(\"Hello World!\");\n\nA simple recursive function:\n\nfunction factorial(n) {\n    if (n == 0) {\n        return 1;\n    }\n    return n*factorial(n - 1);\n}\n\nAnonymous function (or lambda) syntax and closure example:\n\nvar displayClosure = function() {\n    var count = 0;\n    return function () {\n        return ++count;\n    };\n}\nvar inc = displayClosure();\ninc(); // returns 1\ninc(); // returns 2\ninc(); // returns 3\n\nVariadic function demonstration (arguments is a special variable).[58]\n\nvar sum = function() {\n    var i, x = 0;\n    for (i = 0; i < arguments.length; ++i) {\n        x += arguments[i];\n    }\n    return x;\n}\nsum(1, 2, 3); // returns 6\n\nImmediately-invoked function expressions allow functions to pass around variables under their own closures.\n\nvar v;\nv = 1;\nvar getValue = (function(v) {\n  return function() {return v;};\n})(v);\n\nv = 2;\n\ngetValue(); // 1\n\nMore advanced example\n\nThis sample code displays various JavaScript features.\n\n/* Finds the lowest common multiple (LCM) of two numbers */\nfunction LCMCalculator(x, y) { // constructor function\n    var checkInt = function (x) { // inner function\n        if (x % 1 !== 0) {\n            throw new TypeError(x + \" is not an integer\"); // throw an exception\n        }\n        return x;\n    };\n    this.a = checkInt(x)\n    //   semicolons   ^^^^  are optional, a newline is enough\n    this.b = checkInt(y);\n}\n// The prototype of object instances created by a constructor is\n// that constructor's \"prototype\" property.\nLCMCalculator.prototype = { // object literal\n    constructor: LCMCalculator, // when reassigning a prototype, set the constructor property appropriately\n    gcd: function () { // method that calculates the greatest common divisor\n        // Euclidean algorithm:\n        var a = Math.abs(this.a), b = Math.abs(this.b), t;\n        if (a < b) {\n            // swap variables\n            t = b;\n            b = a;\n            a = t;\n        }\n        while (b !== 0) {\n            t = b;\n            b = a % b;\n            a = t;\n        }\n        // Only need to calculate GCD once, so \"redefine\" this method.\n        // (Actually not redefinition—it's defined on the instance itself,\n        // so that this.gcd refers to this \"redefinition\" instead of LCMCalculator.prototype.gcd.\n        // Note that this leads to a wrong result if the LCMCalculator object members \"a\" and/or \"b\" are altered afterwards.)\n        // Also, 'gcd' === \"gcd\", this['gcd'] === this.gcd\n        this['gcd'] = function () {\n            return a;\n        };\n        return a;\n    },\n    // Object property names can be specified by strings delimited by double (\") or single (') quotes.\n    lcm : function () {\n        // Variable names don't collide with object properties, e.g., |lcm| is not |this.lcm|.\n        // not using |this.a*this.b| to avoid FP precision issues\n        var lcm = this.a/this.gcd()*this.b;\n        // Only need to calculate lcm once, so \"redefine\" this method.\n        this.lcm = function () {\n            return lcm;\n        };\n        return lcm;\n    },\n    toString: function () {\n        return \"LCMCalculator: a = \" + this.a + \", b = \" + this.b;\n    }\n};\n\n// Define generic output function; this implementation only works for Web browsers\nfunction output(x) {\n    document.body.appendChild(document.createTextNode(x));\n    document.body.appendChild(document.createElement('br'));\n}\n\n// Note: Array's map() and forEach() are defined in JavaScript 1.6.\n// They are used here to demonstrate JavaScript's inherent functional nature.\n[[25, 55], [21, 56], [22, 58], [28, 56]].map(function (pair) { // array literal + mapping function\n    return new LCMCalculator(pair[0], pair[1]);\n}).sort(function (a, b) { // sort with this comparative function\n    return a.lcm() - b.lcm();\n}).forEach(function (obj) {\n    output(obj + \", gcd = \" + obj.gcd() + \", lcm = \" + obj.lcm());\n});\n\nThe following output should be displayed in the browser window.\n\nLCMCalculator: a = 28, b = 56, gcd = 28, lcm = 56\nLCMCalculator: a = 21, b = 56, gcd = 7, lcm = 168\nLCMCalculator: a = 25, b = 55, gcd = 5, lcm = 275\nLCMCalculator: a = 22, b = 58, gcd = 2, lcm = 638\n\nUse in Web pages\nSee also: Dynamic HTML and Ajax (programming)\n\nThe most common use of JavaScript is to add client-side behavior to HTML pages, a.k.a. Dynamic HTML (DHTML). Scripts are embedded in or included from HTML pages and interact with the Document Object Model (DOM) of the page. Some simple examples of this usage are:\n\n    Loading new page content or submitting data to the server via AJAX without reloading the page (for example, a social network might allow the user to post status updates without leaving the page)\n    Animation of page elements, fading them in and out, resizing them, moving them, etc.\n    Interactive content, for example games, and playing audio and video\n    Validating input values of a Web form to make sure that they are acceptable before being submitted to the server.\n    Transmitting information about the user's reading habits and browsing activities to various websites. Web pages frequently do this for Web analytics, ad tracking, personalization or other purposes.[59]\n\nBecause JavaScript code can run locally in a user's browser (rather than on a remote server), the browser can respond to user actions quickly, making an application more responsive. Furthermore, JavaScript code can detect user actions that HTML alone cannot, such as individual keystrokes. Applications such as Gmail take advantage of this: much of the user-interface logic is written in JavaScript, and JavaScript dispatches requests for information (such as the content of an e-mail message) to the server. The wider trend of Ajax programming similarly exploits this strength.\n\nA JavaScript engine (also known as JavaScript interpreter or JavaScript implementation) is an interpreter that interprets JavaScript source code and executes the script accordingly. The first JavaScript engine was created by Brendan Eich at Netscape Communications Corporation, for the Netscape Navigator Web browser. The engine, code-named SpiderMonkey, is implemented in C. It has since been updated (in JavaScript 1.5) to conform to ECMA-262 Edition 3. The Rhino engine, created primarily by Norris Boyd (formerly of Netscape; now at Google) is a JavaScript implementation in Java. Rhino, like SpiderMonkey, is ECMA-262 Edition 3 compliant.\n\nA Web browser is by far the most common host environment for JavaScript. Web browsers typically create \"host objects\" to represent the Document Object Model (DOM) in JavaScript. The Web server is another common host environment. A JavaScript Web server would typically expose host objects representing HTTP request and response objects, which a JavaScript program could then interrogate and manipulate to dynamically generate Web pages.\n\nBecause JavaScript is the only language that the most popular browsers share support for, it has become a target language for many frameworks in other languages, even though JavaScript was never intended to be such a language.[60] Despite the performance limitations inherent to its dynamic nature, the increasing speed of JavaScript engines has made the language a surprisingly feasible compilation target.\nExample script\n\nBelow is a minimal example of a standards-conforming Web page containing JavaScript (using HTML 5 syntax) and the DOM:\n\n<!DOCTYPE html>\n<html>\n    <meta charset=\"utf-8\">\n    <title>Minimal Example</title>\n    \n    <body>\n        <h1 id=\"header\">This is JavaScript</h1>\n        \n        <script>\n            document.body.appendChild(document.createTextNode('Hello World!'));\n        \n            var h1 = document.getElementById('header'); // holds a reference to the <h1> tag\n            h1 = document.getElementsByTagName('h1')[0]; // accessing the same <h1> element\n        </script>\n        \n        <noscript>Your browser either does not support JavaScript, or has it turned off.</noscript>\n    </body>\n</html>\n\nCompatibility considerations\nMain article: Web interoperability\n\nBecause JavaScript runs in widely varying environments, an important part of testing and debugging is to test and verify that the JavaScript works across multiple browsers.\n\nThe DOM interfaces for manipulating Web pages are not part of the ECMAScript standard, or of JavaScript itself. Officially, the DOM interfaces are defined by a separate standardization effort by the W3C; in practice, browser implementations differ from the standards and from each other, and not all browsers execute JavaScript.\n\nTo deal with these differences, JavaScript authors can attempt to write standards-compliant code that will also be executed correctly by most browsers; failing that, they can write code that checks for the presence of certain browser features and behaves differently if they are not available.[61] In some cases, two browsers may both implement a feature but with different behavior, and authors may find it practical to detect what browser is running and change their script's behavior to match.[62][63] Programmers may also use libraries or toolkits that take browser differences into account.\n\nFurthermore, scripts may not work for some users. For example, a user may:\n\n    use an old or rare browser with incomplete or unusual DOM support,\n    use a PDA or mobile phone browser that cannot execute JavaScript,\n    have JavaScript execution disabled as a security precaution,\n    use a speech browser due to, for example, a visual disability.\n\nTo support these users, Web authors can try to create pages that degrade gracefully on user agents (browsers) that do not support the page's JavaScript. In particular, the page should remain usable albeit without the extra features that the JavaScript would have added. An alternative approach that many find preferable is to first author content using basic technologies that work in all browsers, then enhance the content for users that have JavaScript enabled. This is known as progressive enhancement.\nSecurity\nSee also: Browser security\n\nJavaScript and the DOM provide the potential for malicious authors to deliver scripts to run on a client computer via the Web. Browser authors contain this risk using two restrictions. First, scripts run in a sandbox in which they can only perform Web-related actions, not general-purpose programming tasks like creating files. Second, scripts are constrained by the same origin policy: scripts from one Web site do not have access to information such as usernames, passwords, or cookies sent to another site. Most JavaScript-related security bugs are breaches of either the same origin policy or the sandbox.\n\nThere are subsets of general JavaScript — ADsafe, Secure ECMA Script (SES) — that provide greater level of security, especially on code created by third parties (such as advertisements).[64][65] Caja is another project for safe embedding and isolation of third-party JavaScript and HTML.\n\nContent Security Policy is the main intended method of ensuring that only trusted code is executed on a Web page.\nSee also: Content Security Policy\nCross-site vulnerabilities\nMain articles: Cross-site scripting and Cross-site request forgery\n\nA common JavaScript-related security problem is cross-site scripting, or XSS, a violation of the same-origin policy. XSS vulnerabilities occur when an attacker is able to cause a target Web site, such as an online banking website, to include a malicious script in the webpage presented to a victim. The script in this example can then access the banking application with the privileges of the victim, potentially disclosing secret information or transferring money without the victim's authorization. A solution to XSS vulnerabilities is to use HTML escaping whenever displaying untrusted data.\n\nSome browsers include partial protection against reflected XSS attacks, in which the attacker provides a URL including malicious script. However, even users of those browsers are vulnerable to other XSS attacks, such as those where the malicious code is stored in a database. Only correct design of Web applications on the server side can fully prevent XSS.\n\nXSS vulnerabilities can also occur because of implementation mistakes by browser authors.[66]\n\nAnother cross-site vulnerability is cross-site request forgery or CSRF. In CSRF, code on an attacker's site tricks the victim's browser into taking actions the user didn't intend at a target site (like transferring money at a bank). It works because, if the target site relies only on cookies to authenticate requests, then requests initiated by code on the attacker's site will carry the same legitimate login credentials as requests initiated by the user. In general, the solution to CSRF is to require an authentication value in a hidden form field, and not only in the cookies, to authenticate any request that might have lasting effects. Checking the HTTP Referrer header can also help.\n\n\"JavaScript hijacking\" is a type of CSRF attack in which a <script> tag on an attacker's site exploits a page on the victim's site that returns private information such as JSON or JavaScript. Possible solutions include:\n\n    requiring an authentication token in the POST and GET parameters for any response that returns private information\n\nMisplaced trust in the client\n\nDevelopers of client-server applications must recognize that untrusted clients may be under the control of attackers. The application author cannot assume that his JavaScript code will run as intended (or at all) because any secret embedded in the code could be extracted by a determined adversary. Some implications are:\n\n    Web site authors cannot perfectly conceal how their JavaScript operates because the raw source code must be sent to the client. The code can be obfuscated, but obfuscation can be reverse-engineered.\n    JavaScript form validation only provides convenience for users, not security. If a site verifies that the user agreed to its terms of service, or filters invalid characters out of fields that should only contain numbers, it must do so on the server, not only the client.\n    Scripts can be selectively disabled, so JavaScript can't be relied on to prevent operations such as right-clicking on an image to save it.[67]\n    It is extremely bad practice to embed sensitive information such as passwords in JavaScript because it can be extracted by an attacker.\n\nBrowser and plugin coding errors\n\nJavaScript provides an interface to a wide range of browser capabilities, some of which may have flaws such as buffer overflows. These flaws can allow attackers to write scripts that would run any code they wish on the user's system. This code is not by any means limited to another JavaScript application. For example, a buffer overrun exploit can allow an attacker to gain access to the operating system's API with superuser privileges.\n\nThese flaws have affected major browsers including Firefox,[68] Internet Explorer,[69] and Safari.[70]\n\nPlugins, such as video players, Adobe Flash, and the wide range of ActiveX controls enabled by default in Microsoft Internet Explorer, may also have flaws exploitable via JavaScript (such flaws have been exploited in the past).[71][72]\n\nIn Windows Vista, Microsoft has attempted to contain the risks of bugs such as buffer overflows by running the Internet Explorer process with limited privileges.[73] Google Chrome similarly confines its page renderers to their own \"sandbox\".\nSandbox implementation errors\n\nWeb browsers are capable of running JavaScript outside the sandbox, with the privileges necessary to, for example, create or delete files. Of course, such privileges aren't meant to be granted to code from the Web.\n\nIncorrectly granting privileges to JavaScript from the Web has played a role in vulnerabilities in both Internet Explorer[74] and Firefox.[75] In Windows XP Service Pack 2, Microsoft demoted JScript's privileges in Internet Explorer.[76]\n\nMicrosoft Windows allows JavaScript source files on a computer's hard drive to be launched as general-purpose, non-sandboxed programs (see: Windows Script Host). This makes JavaScript (like VBScript) a theoretically viable vector for a Trojan horse, although JavaScript Trojan horses are uncommon in practice.[77]\nUses outside Web pages\n\nIn addition to Web browsers and servers, JavaScript interpreters are embedded in a number of tools. Each of these applications provides its own object model that provides access to the host environment. The core JavaScript language remains mostly the same in each application.\nEmbedded scripting language\n\n    Google's Chrome extensions, Opera's extensions, Apple's Safari 5 extensions, Apple's Dashboard Widgets, Microsoft's Gadgets, Yahoo! Widgets, Google Desktop Gadgets, and Serence Klipfolio are implemented using JavaScript.\n    The MongoDB database accepts queries written in JavaScript. MongoDB and NodeJS are the core components of MEAN: a solution stack for creating Web applications using just JavaScript.\n    The Clusterpoint database accept queries written in JS/SQL, which is a combination of SQL and JavaScript. Clusterpoint has built-in computing engine that allows execution of JavaScript code right inside the distributed database.\n    Adobe's Acrobat and Adobe Reader support JavaScript in PDF files.[78]\n    Tools in the Adobe Creative Suite, including Photoshop, Illustrator, Dreamweaver, and InDesign, allow scripting through JavaScript.\n    OpenOffice.org, an office application suite, as well as its popular fork LibreOffice, allows JavaScript to be used as a scripting language.\n    The interactive music signal processing software Max/MSP released by Cycling '74, offers a JavaScript model of its environment for use by developers. It allows much more precise control than the default GUI-centric programming model.\n    Apple's Logic Pro X digital audio workstation (DAW) software can create custom MIDI effects plugins using JavaScript.[citation needed]\n    ECMAScript was included in the VRML97 standard for scripting nodes of VRML scene description files.[citation needed]\n    The Unity game engine supports a modified version of JavaScript for scripting via Mono.[79]\n    DX Studio (3D engine) uses the SpiderMonkey implementation of JavaScript for game and simulation logic.[80]\n    Maxwell Render (rendering software) provides an ECMA standard based scripting engine for tasks automation.[81]\n    Google Apps Script in Google Spreadsheets and Google Sites allows users to create custom formulas, automate repetitive tasks and also interact with other Google products such as Gmail.[82]\n    Many IRC clients, like ChatZilla or XChat, use JavaScript for their scripting abilities.[83][84]\n    RPG Maker MV uses Javascript as its scripting language.[85]\n\nScripting engine\n\n    Microsoft's Active Scripting technology supports JScript as a scripting language.[86]\n    The Java programming language introduced the javax.script package in version 6 that includes a JavaScript implementation based on Mozilla Rhino. Thus, Java applications can host scripts that access the application's variables and objects, much like Web browsers host scripts that access a webpage's Document Object Model (DOM).[87][88]\n    The Qt C++ toolkit includes a QtScript module to interpret JavaScript, analogous to Java's javax.script package.[89]\n    OS X Yosemite introduced JavaScript for Automation (JXA), which is built upon JavaScriptCore and the Open Scripting Architecture. It features an Objective-C bridge that enables entire Cocoa applications to be programmed in JavaScript.\n    Late Night Software's JavaScript OSA (a.k.a. JavaScript for OSA, or JSOSA) is a freeware alternative to AppleScript for Mac OS X. It is based on the Mozilla 1.5 JavaScript implementation, with the addition of a MacOS object for interaction with the operating system and third-party applications.[90]\n\nApplication platform\n\n    ActionScript, the programming language used in Adobe Flash, is another implementation of the ECMAScript standard.\n    Adobe Integrated Runtime is a JavaScript runtime that allows developers to create desktop applications.\n    CA, Inc.'s AutoShell cross-application scripting environment is built on the SpiderMonkey JavaScript engine. It contains preprocessor-like extensions for command definition, as well as custom classes for various system-related tasks like file I/O, operation system command invocation and redirection, and COM scripting.\n    GNOME Shell, the shell for the GNOME 3 desktop environment,[91] made JavaScript its default programming language in 2013.[92]\n    The Mozilla platform, which underlies Firefox, Thunderbird, and some other Web browsers, uses JavaScript to implement the graphical user interface (GUI) of its various products.\n    Qt Quick's markup language (available since Qt 4.7) uses JavaScript for its application logic. Its declarative syntax is also similar to JavaScript.\n    TypeScript is a programming language based on JavaScript that adds support for optional type annotations and some other language extensions such as classes, interfaces and modules. A TS-script compiles into plain JavaScript and can be executed in any JS host supporting ECMAScript 3 or higher. The compiler is itself written in TypeScript.\n    Ubuntu Touch provides a JavaScript API for its unified usability interface.\n    webOS uses the WebKit implementation of JavaScript in its SDK to allow developers to create stand-alone applications solely in JavaScript.\n    WinJS provides a special Windows Library for JavaScript functionality in Windows 8 that enables the development of Modern style (formerly Metro style) applications in HTML5 and JavaScript.\n\nDevelopment tools\n\nWithin JavaScript, access to a debugger becomes invaluable when developing large, non-trivial programs. Because there can be implementation differences between the various browsers (particularly within the Document Object Model), it is useful to have access to a debugger for each of the browsers that a Web application targets.[93]\n\nScript debuggers are integrated within Internet Explorer, Firefox, Safari, Google Chrome, Opera and Node.js[94][95][96]\n\nIn addition to the native Internet Explorer Developer Tools, three debuggers are available for Internet Explorer: Microsoft Visual Studio is the richest of the three, closely followed by Microsoft Script Editor (a component of Microsoft Office),[97] and finally the free Microsoft Script Debugger that is far more basic than the other two. The free Microsoft Visual Web Developer Express provides a limited version of the JavaScript debugging functionality in Microsoft Visual Studio. Internet Explorer has included developer tools since version 8 (reached by pressing the F12 key).\n\nIn comparison to Internet Explorer, Firefox has a more comprehensive set of developer tools, which include a debugger as well. Old versions of Firefox without these tools used a Firefox addon called Firebug, or the older Venkman debugger. Also, WebKit's Web Inspector includes a JavaScript debugger,[98] which is used in Safari. A modified version called Blink DevTools is used in Google Chrome. Node.js has node-inspector, an interactive debugger that integrates with the Blink DevTools, available in Google Chrome. Last but not least, Opera includes a set of tools called Dragonfly.[99]\n\nIn addition to the native computer software, there are online JavaScript IDEs, debugging aids are themselves written in JavaScript and built to run on the Web. An example is the program JSLint, developed by Douglas Crockford who has written extensively on the language. JSLint scans JavaScript code for conformance to a set of standards and guidelines. Many libraries for JavaScript, such as three.js, provide links to demonstration code that can be edited by users. They are also used as a pedagogical tool by institutions such as Khan Academy[100] to allow students to experience writing code in an environment where they can see the output of their programs, without needing any setup beyond a Web browser.\nVersion history\nSee also: ECMAScript § Versions, and ECMAScript § Version correspondence\n\nJavaScript was initially developed in 1996 for use in the Netscape Navigator browser. In the same year Microsoft released an implementation for Internet Explorer. This implementation was called JScript due to trademark issues. In 1997 the first standardized version of the language was released under the name ECMAScript.\n\nThe following table is based on information from multiple sources.[101][102][103]\nVersion \tRelease date \tEquivalent to \tNetscape\nNavigator \tMozilla\nFirefox \tInternet\nExplorer \tOpera \tSafari \tGoogle\nChrome\n1.0 \tMarch 1996 \t\t2.0 \t\t3.0 \t\t\t\n1.1 \tAugust 1996 \t\t3.0 \t\t\t\t\t\n1.2 \tJune 1997 \t\t4.0-4.05 \t\t\t3[104] \t\t\n1.3 \tOctober 1998 \tECMA-262 1st + 2nd edition \t4.06-4.7x \t\t4.0 \t5[105] \t\t\n1.4 \t\t\tNetscape\nServer \t\t\t6 \t\t\n1.5 \tNovember 2000 \tECMA-262 3rd edition \t6.0 \t1.0 \t5.5 (JScript 5.5),\n6 (JScript 5.6),\n7 (JScript 5.7),\n8 (JScript 5.8) \t7.0 \t3.0-5 \t1.0-10.0.666\n1.6 \tNovember 2005 \t1.5 + array extras + array and string generics + E4X \t\t1.5 \t\t\t\t\n1.7 \tOctober 2006 \t1.6 + Pythonic generators \n+ iterators + let \t\t2.0 \t\t\t\t28.0.1500.95\n1.8 \tJune 2008 \t1.7 + generator expressions + expression closures \t\t3.0 \t\t11.50 \t\t\n1.8.1 \t\t1.8 + native JSON support + minor updates \t\t3.5 \t\t\t\t\n1.8.2 \tJune 22, 2009 \t1.8.1 + minor updates \t\t3.6 \t\t\t\t\n1.8.5 \tJuly 27, 2010 \t1.8.2 + new features for ECMA-262 Edition 5 compliance. \t\t4.0 \t\t\t\t\nRelated languages and features\n\nJSON, or JavaScript Object Notation, is a general-purpose data interchange format that is defined as a subset of JavaScript's object literal syntax. Like much of JavaScript (regexps and anonymous functions as 1st class elements, closures, flexible classes, 'use strict'), JSON, except for replacing Perl's key-value operator '=>' by an RFC 822[106] inspired ':', is syntactically pure Perl.\n\njQuery is a popular JavaScript library designed to simplify DOM-oriented client-side HTML scripting along with offering cross-browser compatibility because various browsers respond differently to certain vanilla JavaScript code.\n\nUnderscore.js is a utility JavaScript library for data manipulation that is used in both client-side and server-side network applications.\n\nMozilla browsers currently support LiveConnect, a feature that allows JavaScript and Java to intercommunicate on the Web. However, Mozilla-specific support for LiveConnect is scheduled to be phased out in the future in favor of passing on the LiveConnect handling via NPAPI to the Java 1.6+ plug-in (not yet supported on the Mac as of March 2010).[107] Most browser inspection tools, such as Firebug in Firefox, include JavaScript interpreters that can act on the visible page's DOM.\n\nasm.js is a subset of JavaScript that can be run in any JavaScript engine or run faster in an ahead-of-time (AOT) compiling engine.[108]\n\nJSFuck is an esoteric programming language. Programs are written using only six different characters, but are still valid JavaScript code.\n\np5.js[109] is an object oriented JavaScript library designed for artists and designers. It is based on the ideas of the Processing project but is for the web.\nUse as an intermediate language\n\nAs JavaScript is the most widely supported client-side language that can run within a Web browser, it has become an intermediate language for other languages to target. This has included both newly created languages and ports of existing languages. Some of these include:\n\n    Oberon Script, a full implementation of the Oberon Programming Language that compiles to high-level JavaScript.[110]\n    Objective-J, a superset of JavaScript that compiles to standard JavaScript. It adds traditional inheritance and Smalltalk/Objective-C style dynamic dispatch and optional pseudo-static typing to JavaScript.\n    Processing.js, a JavaScript port of Processing, a programming language designed to write visualizations, images, and interactive content. It allows Web browsers to display animations, visual applications, games and other graphical rich content without the need for a Java applet or Flash plugin.\n    CoffeeScript, an alternate syntax for JavaScript intended to be more concise and readable. It adds features like array comprehensions (also available in JavaScript since version 1.7)[111] and pattern matching. Like Objective-J, it compiles to JavaScript. Ruby and Python have been cited as influential on CoffeeScript syntax.\n    Google Web Toolkit translates a subset of Java to JavaScript.\n    Scala, an object-oriented and functional programming language, has a Scala-to-JavaScript compiler.[112]\n    Pyjamas, a port of Google Web Toolkit to Python (translates a subset of Python to JavaScript)\n    Dart, an open-source programming language developed by Google, can be compiled to JavaScript.\n    Whalesong,[113] a Racket-to-JavaScript compiler.\n    Emscripten, a LLVM-backend for porting native libraries to JavaScript.\n    Fantom a programming language that runs on JVM, .NET and JavaScript.\n    TypeScript, a free and open-source programming language developed by Microsoft. It is a superset of JavaScript, and essentially adds optional static typing and class-based object-oriented programming to the language.\n    Haxe, an open-source high-level multiplatform programming language and compiler that can produce applications and source code for many different platforms including JavaScript.\n    ClojureScript,[114] a compiler for Clojure that targets JavaScript. It is designed to emit JavaScript code that is compatible with the advanced compilation mode of the Google Closure optimizing compiler.\n    Kotlin, a statically-typed language that also compiles to Java byte code.\n\nAs JavaScript has unusual limitations – such as no separate integer type, using floating point – languages that compile to JavaScript commonly have slightly different behavior than in other environments.\nJavaScript and Java\n\nA common misconception is that JavaScript is similar or closely related to Java. It is true that both have a C-like syntax (the C language being their most immediate common ancestor language). They also are both typically sandboxed (when used inside a browser), and JavaScript was designed with Java's syntax and standard library in mind. In particular, all Java keywords were reserved in original JavaScript, JavaScript's standard library follows Java's naming conventions, and JavaScript's Math and Date objects are based on classes from Java 1.0,[115] but the similarities end there.\n\nThe differences between the two languages are more prominent than their similarities. Java has static typing, while JavaScript's typing is dynamic. Java is loaded from compiled bytecode, while JavaScript is loaded as human-readable source code. Java's objects are class-based, while JavaScript's are prototype-based. Finally, Java did not support functional programming until Java 8, while JavaScript has done so from the beginning, being influenced by Scheme.", "skillName": "JavaScript."}
{"id": 213, "category": "Computer_Programming", "skillText": "A software engineer is a person who applies the principles of software engineering to the design, development, maintenance, testing, and evaluation of the software and systems that make computers or anything containing software work.\n\nContents\n\n    1 Overview\n        1.1 A state of the art\n    2 Education\n        2.1 Other degrees\n    3 Profession\n        3.1 Employment\n        3.2 Work\n        3.3 Impact of globalization\n        3.4 Prizes\n    4 Use of the title \"Engineer\"\n        4.1 Suitability of the term\n        4.2 Regulatory classification\n            4.2.1 Iceland\n            4.2.2 United Kingdom\n            4.2.3 New Zealand\n            4.2.4 Canada\n            4.2.5 United States\n    5 See also\n    6 References\n\nOverview\n\nPrior to the mid-1960s, software practitioners called themselves computer programmers or software developers, regardless of their actual jobs. Many people prefer to call themselves software developer and programmer, because most widely agree what these terms mean, while software engineer is still being debated. In many companies, the titles programmer and software developer were changed to software engineer, for many categories of programmers.[citation needed]\nA state of the art\n\tThis section is written like a personal reflection or opinion essay that states the Wikipedia editor's personal feelings about a topic, rather than the opinions of experts. Please help improve it by rewriting it in an encyclopedic style. (April 2013) (Learn how and when to remove this template message)\nSee also: Regulation and licensure in engineering § Title usage\n\nIn May 2015, the United States U. S. Bureau of Labor Statistics published an updated count of software engineers which has gone up from 760,840 in 2004 to 1,554,960 in 2015; in the same period there were some 1,610,480 practitioners employed in the U.S. in all other engineering disciplines combined.[1] The label software engineer is used very liberally in the corporate world. Very few of the practicing software engineers actually hold Engineering degrees from accredited universities.\nEducation\nGlobe icon.\n\tThe examples and perspective in this article may not represent a worldwide view of the subject. You may improve this article, discuss the issue on the talk page, or create a new article, as appropriate. (November 2010) (Learn how and when to remove this template message)\n\nAbout half of all practitioners today have degrees in computer science, information systems, or information technology. A small, but growing, number of practitioners have software engineering degrees. In 1987, Imperial College London introduced the first three-year software engineering Bachelor's degree in the UK and the world; in the following year, the University of Sheffield established a similar program.[2] In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States, however, it did not obtain ABET accreditation until 2003, the same time as Rice University, Clarkson University, Milwaukee School of Engineering and Mississippi State University obtained theirs.[3] In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.[citation needed]\n\nSince then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees was recently[when?] defined by the CCSE. As of 2004, in the U.S., about 50 universities offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering Master's degree was established at Seattle University in 1979. Since then graduate software engineering degrees have been made available from many more universities. Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.\n\nIn 1998, the US Naval Postgraduate School (NPS) established the first doctorate program in Software Engineering in the world.[citation needed] Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers.[4] ETS University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.[5]\nOther degrees\n\nIn business, some software engineering practitioners have MIS or computer information systems degrees. In embedded systems, some have electrical engineering, electronics engineering, computer science with emphasis in \"embedded systems\" or computer engineering degrees, because embedded software often requires a detailed understanding of hardware. In medical software, practitioners may have medical informatics, general medical, or biology degrees.[citation needed]\n\nSome practitioners have mathematics, science, engineering, or technology degrees. Some have philosophy (logic in particular) or other non-technical degrees.[citation needed]For instance, Barry Boehm earned degrees in mathematics. And, others have no degrees.[citation needed]\nProfession\nEmployment\nSee also: Software engineering demographics\n\nMost software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work on their own as consulting software engineers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations required software engineers to do many or all of them. Entry-level software engineer or associate software engineer may be best. Some companies offer software engineer as an entry-level position. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Specializations include: in industry (analysts, architects, developers, testers, technical support, managers) and in academia (educators, researchers).\n\nThere is considerable debate over the future employment prospects for Software Engineers and other IT Professionals. For example, an online futures market called the Future of IT Jobs in America \nattempts to answer whether there will be more IT jobs, including software engineers, in 2012 than there were in 2002. Possible opportunities for Advancement can be as a Software Engineer, then to a Senior Software Engineer, or straight to a Senior Software Engineer,[6] depending on skills and reputation. Services exist that are trying to better gauge the coding ability of an engineer, given not all engineers progress their abilities at the same rate, and to make it easier for both employers and employees to find a good match in terms of jobs.\nWork\n\nThis job is office-based, and most of the work is done during normal office hours, but can sometimes lead to working away and working late or during weekends, depending on where and when the client is situated. The job can also be done at home or anywhere a computer is set up. Some high-profile companies have encouraged software engineers to work for long hours; Apple's Steve Jobs set up a culture where engineers would never take holidays and work throughout weekends, yet love what they were doing.[7]\nImpact of globalization\n\nMost students in the developed world have avoided degrees related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers.[8] Although government statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected.[9][10] Often one is expected to start out as a computer programmer before being promoted to software engineer. Thus, the career path to software engineering may be rough, especially during recessions.\n\nSome career counselors suggest a student also focus on \"people skills\" and business skills rather than purely technical skills because such \"soft skills\" are allegedly more difficult to offshore. Reasonable command over reading, writing & speaking English is asked by most of employers.[11] It is the quasi-management aspects of software engineering that appear to be what has kept it from being impacted by globalization.[12]\nPrizes\n\nThere are several prizes in the field of software engineering:[13]\n\n    The CODiE awards is a yearly award issued by the Software and Information Industry Association for excellence in software development within the software industry.\n    Jolt Awards are awards in the software industry.\n    Stevens Award is a software engineering award given in memory of Wayne Stevens.\n\nUse of the title \"Engineer\"\nSuitability of the term\nMain article: Regulation and licensure in engineering\n\nMany people[who?] believe that software engineering implies a certain level of academic training, professional discipline, adherence to formal processes, and especially legal liability that often are not applied in cases of software development. A common analogy is that working in construction does not make one a civil engineer, and so writing code does not make one a software engineer. Furthermore, because computing doesn't utilize the methods of mathematical physics common to all conventional engineering disciplines, it's more appropriate to call those engaged in this occupation as software developers, computer scientists or similar.\n\nIn 1978, computer scientist E. W. Dijkstra wrote in a paper that the coining of the term software engineer was not useful since it was an inappropriate analogy, \"The existence of the mere term has been the base of a number of extremely shallow—and false—analogies, which just confuse the issue...Computers are such exceptional gadgets that there is good reason to assume that most analogies with other disciplines are too shallow to be of any positive value, are even so shallow that they are only confusing.\"[14]\n\nIn each of the last few decades, at least one radical new approach has entered the mainstream of software development (e.g. Structured Programming, Object Orientation), implying that the field is still changing too rapidly to be considered an engineering discipline. Proponents argue that the supposedly radical new approaches are evolutionary rather than revolutionary.[citation needed]\n\nIndividual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering.[15][16] Steve McConnell has said that it is not, but that it should be.[17] Donald Knuth has said that programming is an art and a science.[18] Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused[improper synthesis?] and should be considered harmful, particularly in the United States.[19]\nRegulatory classification\nIceland\n\nThe use of the title tölvunarfræðingur (e. computer scientist) is protected by law in Iceland.[20] Software engineering is taught in Computer Science departments in Icelandic universities. Icelandic law state that a permission must be obtained from the Minister of Industry when the degree was awarded abroad, prior to use of the title. The title is only awarded to those who have obtained a master's degree in Software Engineering from a recognized higher educational institution.[citation needed][dubious – discuss]\nUnited Kingdom\n\nThe U.K. has seen the alignment of the Information Technology Professional and the Engineering Professionals.[21][verification needed]\nNew Zealand\n\nIn New Zealand, IPENZ, the professional engineering organization entrusted by the New Zealand government with legal power to license and regulate chartered engineers (CPEng), recognizes software engineering as a legitimate branch of professional engineering and accepts application of software engineers to obtain chartered status provided he or she has a tertiary degree of approved subjects. Software Engineering is included but Computer Science is normally not.[22]\nCanada\n\nIn Canada the use of the job title \"Engineer\" is controlled in each province by self-regulating professional engineering organizations, often aligned with geologists and geophysicists, who are also tasked with enforcement of the governing legislation. The intent is that any individual holding themselves out as an engineer (or geologist or geophysicist) has been verified to have been educated to a certain accredited level and their professional practice is subject to a code of ethics and peer scrutiny.\n\nIT professionals with degrees in other fields (such as computer science or information systems) are restricted from using the title \"Software Engineer\", or wording \"Software Engineer\" in a title, depending on their province or territory of residence. In some instances, cases have been taken to court regarding the illegal use of the protected title \"Software Engineer\".[23]\nUnited States\n\nThe U.S. Bureau of Labor Statistics classifies computer software engineers as a subcategory of \"computer specialists\", along with occupations such as computer scientist, programmer, and network administrator.[24] The BLS classifies all other engineering disciplines, including computer hardware engineers, as \"engineers\".[25]\n\nSome of the states regulate the use of terms such as \"computer engineer\" and even \"software engineer\". These states include at least Texas[26] and Florida.[27]\n\nThere is also a new PE (Professional Engineer) exam beginning in April 2013 for Software Engineering specifically as the process of tougher regulation moves forward.[28]\nSee also\n\tWikimedia Commons has media related to Software engineers.\n\n    iconComputer science portal \n\n    Bachelor of Science in Information Technology\n    Bachelor of Software Engineering\n    Consulting software engineer\n    Software Engineering Institute", "skillName": "Software_engineer."}
{"id": 214, "category": "Computer_Programming", "skillText": "Transact-SQL (T-SQL) is Microsoft's and Sybase's proprietary extension to the SQL (Structured Query Language) used to interact with relational databases.\n\nT-SQL expands on the SQL standard to include procedural programming, local variables, various support functions for string processing, date processing, mathematics, etc. and changes to the DELETE and UPDATE statements.\n\nTransact-SQL is central to using Microsoft SQL Server. All applications that communicate with an instance of SQL Server do so by sending Transact-SQL statements to the server, regardless of the user interface of the application.\n\nContents\n\n    1 Variables\n    2 Flow control\n    3 Changes to DELETE and UPDATE statements\n    4 BULK INSERT\n    5 TRY CATCH\n    6 See also\n    7 References\n    8 External links\n\nVariables\n\nTransact-SQL provides the following statements to declare and set local variables: DECLARE, SET and SELECT.\n\nDECLARE @var1 NVARCHAR(30)    \nSET @var1 = 'Some Name'       \nSELECT @var1 = Name       \nFROM Sales.Store       \nWHERE CustomerID = 1000\n\nFlow control\n\nKeywords for flow control in Transact-SQL include BEGIN and END, BREAK, CONTINUE, GOTO, IF and ELSE, RETURN, WAITFOR, and WHILE.\n\nIF and ELSE allow conditional execution. This batch statement will print \"It is the weekend\" if the current date is a weekend day, or \"It is a weekday\" if the current date is a weekday. (Note: This code assumes that Sunday is configured as the first day of the week in the @@DATEFIRST setting.)\n\nIF DATEPART(dw, GETDATE()) = 7 OR DATEPART(dw, GETDATE()) = 1\n   PRINT 'It is the weekend.'\nELSE\n   PRINT 'It is a weekday.'\n\nBEGIN and END mark a block of statements. If more than one statement is to be controlled by the conditional in the example above, we can use BEGIN and END like this:\n\nIF DATEPART(dw, GETDATE()) = 7 OR DATEPART(dw, GETDATE()) = 1\nBEGIN\n   PRINT 'It is the weekend.'\n   PRINT 'Get some rest on the weekend!'\nEND\nELSE\nBEGIN\n   PRINT 'It is a weekday.'\n   PRINT 'Get to work on a weekday!'\nEND\n\nWAITFOR will wait for a given amount of time, or until a particular time of day. The statement can be used for delays or to block execution until the set time.\n\nRETURN is used to immediately return from a stored procedure or function.\n\nBREAK ends the enclosing WHILE loop, while CONTINUE causes the next iteration of the loop to execute. An example of a WHILE loop is given below.\n\nDECLARE @i INT\nSET @i = 0\n\nWHILE @i < 5\nBEGIN\n   PRINT 'Hello world.'\n   SET @i = @i + 1\nEND\n\nChanges to DELETE and UPDATE statements\n\nIn Transact-SQL, both the DELETE and UPDATE statements allow a FROM clause to be added, which allows joins to be included.\n\nThis example deletes all users who have been flagged with the 'Idle' flag.\n\nDELETE u\n  FROM users AS u\n  INNER JOIN user_flags AS f\n    ON u.id = f.id\n    WHERE f.name = 'idle'\n\nBULK INSERT\n\nBULK is a Transact-SQL statement that implements a bulk data-loading process, inserting multiple rows into a table, reading data from an external sequential file. Use of BULK INSERT results in better performance than processes that issue individual INSERT statements for each row to be added. Additional details are available in MSDN \n.\nTRY CATCH\n\nBeginning with SQL Server 2005,[1] Microsoft introduced additional TRY CATCH logic to support exception type behaviour. This behaviour enables developers to simplify their code and leave out @@ERROR checking after each SQL execution statement.\n\n-- begin transaction\nBEGIN TRAN\n\nBEGIN TRY\n   -- execute each statement\n   INSERT INTO MYTABLE(NAME) VALUES ('ABC')\n   INSERT INTO MYTABLE(NAME) VALUES ('123')\n\n   -- commit the transaction\n   COMMIT TRAN\nEND TRY\nBEGIN CATCH\n   -- rollback the transaction because of error\n   ROLLBACK TRAN\nEND CATCH", "skillName": "Transact-SQL."}
{"id": 215, "category": "Computer_Programming", "skillText": "A software developer is a person concerned with facets of the software development process, including the research, design, programming, and testing of computer software.\n\nOther job titles which are often used with similar meanings are programmer, software analyst, and software engineer. According to developer Eric Sink, the differences between system design, software development and programming are more apparent. Already in the current market place there can be found a segregation between programmers and developers,[dubious – discuss] being that one who implements is not the same as the one who designs the class structure or hierarchy. Even more so that developers become systems architects, those who design the multi-leveled architecture or component interactions of a large software system.[1] (see also Debate over who is a software engineer)\n\nIn a large company, there may be employees whose sole responsibility consists of only one of the phases above. In smaller development environments, a few people or even a single individual might handle the complete process.\n\nContents\n\n    1 History\n    2 See also\n    3 References\n    4 External links\n\nHistory\n\nThe word \"software\" was coined as a prank as early as 1953, but did not appear in print until the 1960s.[2] Before this time, computers were programmed either by customers, or the few commercial computer vendors of the time, such as UNIVAC and IBM. The first company founded to provide software products and services was Computer Usage Company in 1955.[3]\n\nThe software industry expanded in the early 1960s, almost immediately after computers were first sold in mass-produced quantities. Universities, government, and business customers created a demand for software. Many of these programs were written in-house by full-time staff programmers. Some were distributed freely between users of a particular machine for no charge. Others were done on a commercial basis, and other firms such as Computer Sciences Corporation (founded in 1959) started to grow. The computer/hardware makers started bundling operating systems, systems software and programming environments with their machines.[citation needed]\n\nWhen Digital Equipment Corporation (DEC) brought a relatively low-priced microcomputer to market, it brought computing within the reach of many more companies and universities worldwide, and it spawned great innovation in terms of new, powerful programming languages and methodologies. New software was built for microcomputers, so other manufacturers including IBM, followed DEC's example quickly, resulting in the IBM AS/400 amongst others.[citation needed]\n\nThe industry expanded greatly with the rise of the personal computer (\"PC\") in the mid-1970s, which brought computing to the desktop of the office worker. In the following years, it also created a growing market for games, applications, and utilities. DOS, Microsoft's first operating system product, was the dominant operating system at the time.[4]\n\nIn the early years of the 21st century, another successful business model has arisen for hosted software, called software-as-a-service, or SaaS; this was at least the third time[citation needed] this model had been attempted. From the point of view of producers of some proprietary software, SaaS reduces the concerns about unauthorized copying, since it can only be accessed through the Web, and by definition no client software is loaded onto the end user's PC.[citation needed] By 2014 the role of cloud developer had been defined; in this context, one definition of a \"developer\" in general was published:[5]\n\n    Developers make software for the world to use. The job of a developer is to crank out code -- fresh code for new products, code fixes for maintenance, code for business logic, and code for supporting libraries.\n\nSee also\n\n    Bus factor", "skillName": "Software_developer."}
{"id": 216, "category": "Computer_Programming", "skillText": "R (programming language)\nR is a programming language and software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing.[4] The R language is widely used among statisticians and data miners for developing statistical software[5] and data analysis.[6] Polls, surveys of data miners, and studies of scholarly literature databases show that R's popularity has increased substantially in recent years.[7]\n\nR is a GNU project.[8] The source code for the R software environment is written primarily in C, Fortran, and R.[9] R is freely available under the GNU General Public License, and pre-compiled binary versions are provided for various operating systems. While R has a command line interface, there are several graphical front-ends available.[10]\n\n\nR is an implementation of the S programming language combined with lexical scoping semantics inspired by Scheme.[11] S was created by John Chambers while at Bell Labs. There are some important differences, but much of the code written for S runs unaltered.[12]\n\nR was created by Ross Ihaka and Robert Gentleman[13] at the University of Auckland, New Zealand, and is currently developed by the R Development Core Team, of which Chambers is a member. R is named partly after the first names of the first two R authors and partly as a play on the name of S.[14] The project was conceived in 1992, with an initial version released in 1994 and a stable beta version in 2000.[15][16][17]\n\nStatistical features\nR and its libraries implement a wide variety of statistical and graphical techniques, including linear and nonlinear modeling, classical statistical tests, time-series analysis, classification, clustering, and others. R is easily extensible through functions and extensions, and the R community is noted for its active contributions in terms of packages. Many of R's standard functions are written in R itself, which makes it easy for users to follow the algorithmic choices made. For computationally intensive tasks, C, C++, and Fortran code can be linked and called at run time. Advanced users can write C, C++,[18] Java,[19] .NET[20] or Python code to manipulate R objects directly.[21] R is highly extensible through the use of user-submitted packages for specific functions or specific areas of study. Due to its S heritage, R has stronger object-oriented programming facilities than most statistical computing languages. Extending R is also eased by its lexical scoping rules.[22]\n\nAnother strength of R is static graphics, which can produce publication-quality graphs, including mathematical symbols. Dynamic and interactive graphics are available through additional packages.[23]\n\nR has Rd, its own LaTeX-like documentation format, which is used to supply comprehensive documentation, both on-line in a number of formats and in hard copy.[24]\n\nProgramming features\nR is an interpreted language; users typically access it through a command-line interpreter. If a user types 2+2 at the R command prompt and presses enter, the computer replies with 4, as shown below:\n\n> 2+2\n[1] 4\nLike other similar languages such as APL and MATLAB, R supports matrix arithmetic. R's data structures include vectors, matrices, arrays, data frames (similar to tables in a relational database) and lists.[25] R's extensible object system includes objects for (among others): regression models, time-series and geo-spatial coordinates. The scalar data type was never a data structure of R.[26] Instead, a scalar is represented as a vector with length one.[citation needed]\n\nR supports procedural programming with functions and, for some functions, object-oriented programming with generic functions. A generic function acts differently depending on the classes of arguments passed to it. In other words, the generic function dispatches the function (method) specific to that class of object. For example, R has a generic print function that can print almost every class of object in R with a simple print(objectname) syntax.[27]\n\nAlthough used mainly by statisticians and other practitioners requiring an environment for statistical computation and software development, R can also operate as a general matrix calculation toolbox � with performance benchmarks comparable to GNU Octave or MATLAB.[28] Arrays are stored in column-major order.[29]\n\nPackages\nThe capabilities of R are extended through user-created packages, which allow specialized statistical techniques, graphical devices (ggplot2), import/export capabilities, reporting tools (knitr, Sweave), etc. These packages are developed primarily in R, and sometimes in Java, C, C++, and Fortran.[citation needed]\n\nA core set of packages is included with the installation of R, with more than 7,801 additional packages (as of January 2016) available at the Comprehensive R Archive Network (CRAN),[30] Bioconductor, Omegahat,[31] GitHub, and other repositories.[32]\n\nThe \"Task Views\" page (subject list) on the CRAN website[33] lists a wide range of tasks (in fields such as Finance, Genetics, High Performance Computing, Machine Learning, Medical Imaging, Social Sciences and Spatial Statistics) to which R has been applied and for which packages are available. R has also been identified by the FDA as suitable for interpreting data from clinical research.[34]\n\nOther R package resources include Crantastic, a community site for rating and reviewing all CRAN packages, and R-Forge, a central platform for the collaborative development of R packages, R-related software, and projects. R-Forge also hosts many unpublished beta packages, and development versions of CRAN packages.[35][36]\n\nThe Bioconductor project provides R packages for the analysis of genomic data, such as Affymetrix and cDNA microarray object-oriented data-handling and analysis tools, and has started to provide tools for analysis of data from next-generation high-throughput sequencing methods.[37]\n\nMilestones\nThe full list of changes is maintained in the \"R News\" file at CRAN.[38] Some highlights are listed below for several major releases.[citation needed]\n\nRelease\tDate\tDescription\n0.16\t\tThis is the last alpha version developed primarily by Ihaka and Gentleman. Much of the basic functionality from the \"White Book\" (see S history) was implemented. The mailing lists commenced on April 1, 1997.\n0.49\t1997-04-23\tThis is the oldest source release which is currently available on CRAN.[39] CRAN is started on this date, with 3 mirrors that initially hosted 12 packages.[40] Alpha versions of R for Microsoft Windows and Mac OS are made available shortly after this version.[citation needed]\n0.60\t1997-12-05\tR becomes an official part of the GNU Project. The code is hosted and maintained on CVS.\n0.65.1\t1999-10-07\tFirst versions of update.packages and install.packages functions for downloading and installing packages from CRAN.[41]\n1.0\t2000-02-29\tConsidered by its developers stable enough for production use.[42]\n1.4\t2001-12-19\tS4 methods are introduced and the first version for Mac OS X is made available soon after.\n2.0\t2004-10-04\tIntroduced lazy loading, which enables fast loading of data with minimal expense of system memory.\n2.1\t2005-04-18\tSupport for UTF-8 encoding, and the beginnings of internationalization and localization for different languages.\n2.11\t2010-04-22\tSupport for Windows 64 bit systems.\n2.13\t2011-04-14\tAdding a new compiler function that allows speeding up functions by converting them to byte-code.\n2.14\t2011-10-31\tAdded mandatory namespaces for packages. Added a new parallel package.\n2.15\t2012-03-30\tNew load balancing functions. Improved serialization speed for long vectors.\n3.0\t2013-04-03\tSupport for numeric index values 231 and larger on 64 bit systems.\nInterfaces\nGraphical user interfaces\nArchitect : cross-platform open source IDE for data science based on Eclipse and StatET\nDataJoy[43] : Online R Editor focused on beginners to data science and collaboration.\nDeducer[44] : GUI for menu-driven data analysis (similar to SPSS/JMP/Minitab).\nJava GUI for R : cross-platform stand-alone R terminal and editor based on Java (also known as JGR).\nNumber Analytics - GUI for R based business analytics (similar to SPSS) working on the cloud.\nRattle GUI : cross-platform GUI based on RGtk2 and specifically designed for data mining.\nR Commander : cross-platform menu-driven GUI based on tcltk (several plug-ins to Rcmdr are also available).\nRevolution R Productivity Environment (RPE) � Revolution Analytics-provided Visual Studio-based IDE, and has plans for web based point and click interface.\nRGUI : comes with the pre-compiled version of R for Microsoft Windows.\nRKWard : extensible GUI and IDE for R.\nRStudio : cross-platform open source IDE (which can also be run on a remote Linux server).\nA special issue of the Journal of Statistical Software discusses GUIs for R.[45]\n\nEditors and IDEs\nText editors and Integrated development environments (IDEs) with some support for R include: ConTEXT, Eclipse (StatET),[46] Emacs (Emacs Speaks Statistics), LyX (modules for knitr and Sweave), Vim, jEdit,[47] Kate,[48] Revolution R Enterprise DevelopR (part of Revolution R Enterprise),[49] RStudio,[50] Sublime Text, TextMate, Atom, WinEdt (R Package RWinEdt), Tinn-R, Notepad++,[51] and Architect.[52]\n\nScripting languages\nR functionality has been made accessible from several scripting languages such as Python,[53] Perl,[54] Ruby,[55] F#[56] and Julia.[57] Scripting in R itself is possible via a front-end called littler.[58]\n\nuseR! conferences\nThe official annual gathering of R users is called \"useR!\".[59]\n\nThe first such event was useR! 2004 in May 2004, Vienna, Austria.[60] After skipping 2005, the useR conference has been held annually, usually alternating between locations in Europe and North America.[61]\n\nSubsequent conferences have included:[59]\n\nuseR! 2006, Vienna, Austria\nuseR! 2007, Ames, Iowa, USA\nuseR! 2008, Dortmund, Germany\nuseR! 2009, Rennes, France\nuseR! 2010, Gaithersburg, Maryland, USA\nuseR! 2011, Coventry, United Kingdom\nuseR! 2012, Nashville, Tennessee, USA\nuseR! 2013, Albacete, Spain\nuseR! 2014, Los Angeles, USA\nuseR! 2015, Aalborg, Denmark\nuseR! 2016, Stanford, California, USA\nR Journal\nThe R Journal is the open access, refereed journal of the R project for statistical computing. It features short to medium length articles on the use, and development of R, including packages, programing tips, CRAN news, and foundation news.\n\nComparison with SAS, SPSS, and Stata\nThe general consensus is that R compares well with other popular statistical packages, such as SAS, SPSS, and Stata.[62] In a comparison of all basic features for a statistical software R is heads up with the best of statistical software.\n\nIn January 2009, the New York Times ran an article about R gaining acceptance among data analysts and presenting a potential threat for the market share occupied by commercial statistical packages, such as SAS.[63]\n\nCommercial support for R\nWhile R is an open source project supported by the community developing it, some companies strive to provide commercial support and/or extensions for their customers. In this section, some examples of those companies are mentioned.\n\nIn 2007, Revolution Analytics was founded to provide commercial support for Revolution R, its distribution of R, which also includes components developed by the company. Major additional components include: ParallelR, the R Productivity Environment IDE, RevoScaleR (for big data analysis), RevoDeployR, web services framework, and the ability for reading and writing data in the SAS file format.[64] They also offer a distribution of R designed to comply with established IQ/OQ/PQ criteria which enables clients in the pharmaceutical sector to validate their installation of REvolution R.[65] In 2015, Microsoft Corporation completed the acquisition of Revolution Analytics.[66]\n\nFor organizations in highly regulated sectors requiring a validated version of R, Mango Solutions has developed the ValidR product which fully complies with the Food and Drug Administration guidelines for Software verification and validation.[citation needed] They also offer to validate additional packages if the customer demands it and validate customer's self written packages.[67]\n\nIn October 2011, Oracle announced the Big Data Appliance, which integrates R, Apache Hadoop, Oracle Linux, and a NoSQL database with the Exadata hardware.[68] Oracle R Enterprise[69] is now one of two components of the \"Oracle Advanced Analytics Option\"[70] (the other component is Oracle Data Mining).[citation needed]\n\nIBM offers support for in-Hadoop execution of R,[71] and provides a programming model for massively parallel in-database analytics in R.[72]\n\nOther major commercial software systems supporting connections to or integration with R include: JMP,[73] Mathematica,[74] MATLAB,[75] Spotfire,[76] SPSS,[77] STATISTICA,[78] Platform Symphony,[79] SAS,[80] Tableau,[81] Esri ArcGis,[82] and Dundas.[83]\n\nTibco offers a runtime version R as a part of Spotfire.[84]", "skillName": "R."}
{"id": 217, "category": "Computer_Programming", "skillText": "A computer network or data network is a telecommunications network which allows computers to exchange data. In computer networks, networked computing devices exchange data with each other using a data link. The connections between nodes are established using either cable media or wireless media. The best-known computer network is the Internet.\n\nNetwork computer devices that originate, route and terminate the data are called network nodes.[1] Nodes can include hosts such as personal computers, phones, servers as well as networking hardware. Two such devices can be said to be networked together when one device is able to exchange information with the other device, whether or not they have a direct connection to each other.\n\nComputer networks differ in the transmission medium used to carry their signals, communications protocols to organize network traffic, the network's size, topology and organizational intent.\n\nComputer networks support an enormous number of applications and services such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications as well as many others. In most cases, application-specific communications protocols are layered (i.e. carried as payload) over other more general communications protocols.\n\nContents\n\n    1 History\n    2 Properties\n    3 Network packet\n    4 Network topology\n        4.1 Network links\n            4.1.1 Wired technologies\n            4.1.2 Wireless technologies\n            4.1.3 Exotic technologies\n        4.2 Network nodes\n            4.2.1 Network interfaces\n            4.2.2 Repeaters and hubs\n            4.2.3 Bridges\n            4.2.4 Switches\n            4.2.5 Routers\n            4.2.6 Modems\n            4.2.7 Firewalls\n        4.3 Network structure\n            4.3.1 Common layouts\n            4.3.2 Overlay network\n    5 Communications protocols\n        5.1 IEEE 802\n            5.1.1 Ethernet\n            5.1.2 Wireless LAN\n        5.2 Internet Protocol Suite\n        5.3 SONET/SDH\n        5.4 Asynchronous Transfer Mode\n    6 Geographic scale\n    7 Organizational scope\n        7.1 Intranet\n        7.2 Extranet\n        7.3 Internetwork\n        7.4 Internet\n        7.5 Darknet\n    8 Routing\n    9 Network service\n    10 Network performance\n        10.1 Quality of service\n        10.2 Network congestion\n        10.3 Network resilience\n    11 Security\n        11.1 Network security\n        11.2 Network surveillance\n        11.3 End to end encryption\n    12 Views of networks\n    13 See also\n    14 References\n    15 Further reading\n    16 External links\n\nHistory\nSee also: History of the Internet\n\nThe chronology of significant computer-network developments includes:\n\n    In the late 1950s early networks of computers included the military radar system Semi-Automatic Ground Environment (SAGE).\n    In 1959 Anatolii Ivanovich Kitov proposed to the Central Committee of the Communist Party of the Soviet Union a detailed plan for the re-organisation of the control of the Soviet armed forces and of the Soviet economy on the basis of a network of computing centres.[2]\n    In 1960 the commercial airline reservation system semi-automatic business research environment (SABRE) went online with two connected mainframes.\n    In 1962 J.C.R. Licklider developed a working group he called the \"Intergalactic Computer Network\", a precursor to the ARPANET, at the Advanced Research Projects Agency (ARPA).\n    In 1964 researchers at Dartmouth College developed the Dartmouth Time Sharing System for distributed users of large computer systems. The same year, at Massachusetts Institute of Technology, a research group supported by General Electric and Bell Labs used a computer to route and manage telephone connections.\n    Throughout the 1960s, Leonard Kleinrock, Paul Baran, and Donald Davies independently developed network systems that used packets to transfer information between computers over a network.\n    In 1965, Thomas Marill and Lawrence G. Roberts created the first wide area network (WAN). This was an immediate precursor to the ARPANET, of which Roberts became program manager.\n    Also in 1965, Western Electric introduced the first widely used telephone switch that implemented true computer control.\n    In 1969 the University of California at Los Angeles, the Stanford Research Institute, the University of California at Santa Barbara, and the University of Utah became connected as the beginning of the ARPANET network using 50 kbit/s circuits.[3]\n    In 1972 commercial services using X.25 were deployed, and later used as an underlying infrastructure for expanding TCP/IP networks.\n    In 1973, Robert Metcalfe wrote a formal memo at Xerox PARC describing Ethernet, a networking system that was based on the Aloha network, developed in the 1960s by Norman Abramson and colleagues at the University of Hawaii. In July 1976, Robert Metcalfe and David Boggs published their paper \"Ethernet: Distributed Packet Switching for Local Computer Networks\"[4] and collaborated on several patents received in 1977 and 1978. In 1979 Robert Metcalfe pursued making Ethernet an open standard.[5]\n    In 1976 John Murphy of Datapoint Corporation created ARCNET, a token-passing network first used to share storage devices.\n    In 1995 the transmission speed capacity for Ethernet increased from 10 Mbit/s to 100 Mbit/s. By 1998, Ethernet supported transmission speeds of a Gigabit. Subsequently, higher speeds of up to 100 Gbit/s were added (as of 2016). The ability of Ethernet to scale easily (such as quickly adapting to support new fiber optic cable speeds) is a contributing factor to its continued use.[5]\n\nProperties\n\nComputer networking may be considered a branch of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of the related disciplines.\n\nA computer network facilitates interpersonal communications allowing users to communicate efficiently and easily via various means: email, instant messaging, chat rooms, telephone, video telephone calls, and video conferencing. Providing access to information on shared storage devices is an important feature of many networks. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer. Distributed computing uses computing resources across a network to accomplish tasks. A computer network may be used by computer crackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial of service attack.\nNetwork packet\nMain article: Network packet\n\nComputer communication links that do not support packets, such as traditional point-to-point telecommunication links, simply transmit data as a bit stream. However, most information in computer networks is carried in packets. A network packet is a formatted unit of data (a list of bits or bytes, usually a few tens of bytes to a few kilobytes long) carried by a packet-switched network.\n\nIn packet networks, the data is formatted into packets that are sent through the network to their destination. Once the packets arrive they are reassembled into their original message. With packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from others users, and so the cost can be shared, with relatively little interference, provided the link isn't overused.\n\nPackets consist of two kinds of data: control information, and user data(payload). The control information provides data the network needs to deliver the user data, for example: source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.\n\nOften the route a packet needs to take through a network is not immediately available. In that case the packet is queued and waits until a link is free.\nNetwork topology\nMain article: Network topology\n\nThe physical layout of a network is usually less important than the topology that connects network nodes. Most diagrams that describe a physical network are therefore topological, rather than geographic. The symbols on these diagrams usually denote network links and network nodes.\nNetwork links\n\nThe transmission media (often referred to in the literature as the physical media) used to link devices to form a computer network include electrical cable (Ethernet, HomePNA, power line communication, G.hn), optical fiber (fiber-optic communication), and radio waves (wireless networking). In the OSI model, these are defined at layers 1 and 2 — the physical layer and the data link layer.\n\nA widely adopted family of transmission media used in local area network (LAN) technology is collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3. Ethernet transmits data over both copper and fiber cables. Wireless LAN standards (e.g. those defined by IEEE 802.11) use radio waves, or others use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data.\nWired technologies\nFiber optic cables are used to transmit light from one computer/network node to another\n\nThe orders of the following wired technologies are, roughly, from slowest to fastest transmission speed.\n\n    Coaxial cable is widely used for cable television systems, office buildings, and other work-sites for local area networks. The cables consist of copper or aluminum wire surrounded by an insulating layer (typically a flexible material with a high dielectric constant), which itself is surrounded by a conductive layer. The insulation helps minimize interference and distortion. Transmission speed ranges from 200 million bits per second to more than 500 million bits per second.\n    ITU-T G.hn technology uses existing home wiring (coaxial cable, phone lines and power lines) to create a high-speed (up to 1 Gigabit/s) local area network\n    Twisted pair wire is the most widely used medium for all telecommunication. Twisted-pair cabling consist of copper wires that are twisted into pairs. Ordinary telephone wires consist of two insulated copper wires twisted into pairs. Computer network cabling (wired Ethernet as defined by IEEE 802.3) consists of 4 pairs of copper cabling that can be utilized for both voice and data transmission. The use of two wires twisted together helps to reduce crosstalk and electromagnetic induction. The transmission speed ranges from 2 million bits per second to 10 billion bits per second. Twisted pair cabling comes in two forms: unshielded twisted pair (UTP) and shielded twisted-pair (STP). Each form comes in several category ratings, designed for use in various scenarios.\n\n2007 map showing submarine optical fiber telecommunication cables around the world.\n\n    An optical fiber is a glass fiber. It carries pulses of light that represent data. Some advantages of optical fibers over metal wires are very low transmission loss and immunity from electrical interference. Optical fibers can simultaneously carry multiple wavelengths of light, which greatly increases the rate that data can be sent, and helps enable data rates of up to trillions of bits per second. Optic fibers can be used for long runs of cable carrying very high data rates, and are used for undersea cables to interconnect continents.\n\nPrice is a main factor distinguishing wired- and wireless-technology options in a business. Wireless options command a price premium that can make purchasing wired computers, printers and other devices a financial benefit. Before making the decision to purchase hard-wired technology products, a review of the restrictions and limitations of the selections is necessary. Business and employee needs may override any cost considerations.[6]\nWireless technologies\nComputers are very often connected to networks using wireless links\nMain article: Wireless network\n\n    Terrestrial microwave – Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low-gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 48 km (30 mi) apart.\n    Communications satellites – Satellites communicate via microwave radio waves, which are not deflected by the Earth's atmosphere. The satellites are stationed in space, typically in geosynchronous orbit 35,400 km (22,000 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals.\n    Cellular and PCS systems use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area has a low-power transmitter or radio relay antenna device to relay calls from one area to the next area.\n    Radio and spread spectrum technologies – Wireless local area networks use a high-frequency radio technology similar to digital cellular and a low-frequency radio technology. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wifi.\n    Free-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices.\n\nExotic technologies\n\nThere have been various attempts at transporting data over exotic media:\n\n    IP over Avian Carriers was a humorous April fool's Request for Comments, issued as RFC 1149 \n    . It was implemented in real life in 2001.[7]\n    Extending the Internet to interplanetary dimensions via radio waves, the Interplanetary Internet.[8]\n\nBoth cases have a large round-trip delay time, which gives slow two-way communication, but doesn't prevent sending large amounts of information.\nNetwork nodes\nMain article: Node (networking)\n\nApart from any physical transmission medium there may be, networks comprise additional basic system building blocks, such as network interface controller (NICs), repeaters, hubs, bridges, switches, routers, modems, and firewalls.\nNetwork interfaces\nAn ATM network interface in the form of an accessory card. A lot of network interfaces are built-in.\n\nA network interface controller (NIC) is computer hardware that provides a computer with the ability to access the transmission media, and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.\n\nThe NIC responds to traffic addressed to a network address for either the NIC or the computer as a whole.\n\nIn Ethernet networks, each network interface controller has a unique Media Access Control (MAC) address—usually stored in the controller's permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce.\nRepeaters and hubs\n\nA repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal is retransmitted at a higher power level, or to the other side of an obstruction, so that the signal can cover longer distances without degradation. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.\n\nA repeater with multiple ports is known as a hub. Repeaters work on the physical layer of the OSI model. Repeaters require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance. As a result, many network architectures limit the number of repeaters that can be used in a row, e.g., the Ethernet 5-4-3 rule.\n\nHubs have been mostly obsoleted by modern switches; but repeaters are used for long distance links, notably undersea cabling.\nBridges\n\nA network bridge connects and filters traffic between two network segments at the data link layer (layer 2) of the OSI model to form a single network. This breaks the network's collision domain but maintains a unified broadcast domain. Network segmentation breaks down a large, congested network into an aggregation of smaller, more efficient networks.\n\nBridges come in three basic types:\n\n    Local bridges: Directly connect LANs\n    Remote bridges: Can be used to create a wide area network (WAN) link between LANs. Remote bridges, where the connecting link is slower than the end networks, largely have been replaced with routers.\n    Wireless bridges: Can be used to join LANs or connect remote devices to LANs.\n\nSwitches\n\nA network switch is a device that forwards and filters OSI layer 2 datagrams (frames) between ports based on the destination MAC address in each frame.[9] A switch is distinct from a hub in that it only forwards the frames to the physical ports involved in the communication rather than all ports connected. It can be thought of as a multi-port bridge.[10] It learns to associate physical ports to MAC addresses by examining the source addresses of received frames. If an unknown destination is targeted, the switch broadcasts to all ports but the source. Switches normally have numerous ports, facilitating a star topology for devices, and cascading additional switches.\n\nMulti-layer switches are capable of routing based on layer 3 addressing or additional logical levels. The term switch is often used loosely to include devices such as routers and bridges, as well as devices that may distribute traffic based on load or based on application content (e.g., a Web URL identifier).\nRouters\nA typical home or small office router showing the ADSL telephone line and Ethernet network cable connections\n\nA router is an internetworking device that forwards packets between networks by processing the routing information included in the packet or datagram (Internet protocol information from layer 3). The routing information is often processed in conjunction with the routing table (or forwarding table). A router uses its routing table to determine where to forward packets. A destination in a routing table can include a \"null\" interface, also known as the \"black hole\" interface because data can go into it, however, no further processing is done for said data, i.e. the packets are dropped.\nModems\n\nModems (MOdulator-DEModulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Modems are commonly used for telephone lines, using a Digital Subscriber Line technology.\nFirewalls\n\nA firewall is a network device for controlling network security and access rules. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.\nNetwork structure\n\nNetwork topology is the layout or organizational hierarchy of interconnected nodes of a computer network. Different network topologies can affect throughput, but reliability is often more critical. With many technologies, such as bus networks, a single failure can cause the network to fail entirely. In general the more interconnections there are, the more robust the network is; but the more expensive it is to install.\nCommon layouts\nCommon network topologies\n\nCommon layouts are:\n\n    A bus network: all nodes are connected to a common medium along this medium. This was the layout used in the original Ethernet, called 10BASE5 and 10BASE2.\n    A star network: all nodes are connected to a special central node. This is the typical layout found in a Wireless LAN, where each wireless client connects to the central Wireless access point.\n    A ring network: each node is connected to its left and right neighbour node, such that all nodes are connected and that each node can reach each other node by traversing nodes left- or rightwards. The Fiber Distributed Data Interface (FDDI) made use of such a topology.\n    A mesh network: each node is connected to an arbitrary number of neighbours in such a way that there is at least one traversal from any node to any other.\n    A fully connected network: each node is connected to every other node in the network.\n    A tree network: nodes are arranged hierarchically.\n\nNote that the physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with FDDI, the network topology is a ring (actually two counter-rotating rings), but the physical topology is often a star, because all neighboring connections can be routed via a central physical location.\nOverlay network\nA sample overlay network\n\nAn overlay network is a virtual computer network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links. Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks. They are organized as nodes of a virtual system of links that run on top of the Internet.[11]\n\nOverlay networks have been around since the invention of networking when computer systems were connected over telephone lines using modems, before any data network existed.\n\nThe most striking example of an overlay network is the Internet itself. The Internet itself was initially built as an overlay on the telephone network.[11] Even today, each Internet node can communicate with virtually any other through an underlying mesh of sub-networks of wildly different topologies and technologies. Address resolution and routing are the means that allow mapping of a fully connected IP overlay network to its underlying network.\n\nAnother example of an overlay network is a distributed hash table, which maps keys to nodes in the network. In this case, the underlying network is an IP network, and the overlay network is a table (actually a map) indexed by keys.\n\nOverlay networks have also been proposed as a way to improve Internet routing, such as through quality of service guarantees to achieve higher-quality streaming media. Previous proposals such as IntServ, DiffServ, and IP Multicast have not seen wide acceptance largely because they require modification of all routers in the network.[citation needed] On the other hand, an overlay network can be incrementally deployed on end-hosts running the overlay protocol software, without cooperation from Internet service providers. The overlay network has no control over how packets are routed in the underlying network between two overlay nodes, but it can control, for example, the sequence of overlay nodes that a message traverses before it reaches its destination.\n\nFor example, Akamai Technologies manages an overlay network that provides reliable, efficient content delivery (a kind of multicast). Academic research includes end system multicast,[12] resilient routing and quality of service studies, among others.\nCommunications protocols\nProtocols in relation to the Internet layering scheme.\nThe TCP/IP model or Internet layering scheme and its relation to common protocols often layered on top of it.\nFigure 4. When a router is present, message flows go down through protocol layers, across to the router, up the stack inside the router and back down again and is sent on to the final destination where it climbs back up the stack\nFigure 4. Message flows (A-B) in the presence of a router (R), red flows are effective communication paths, black paths are the actual paths.\n\nA communications protocol is a set of rules for exchanging information over network links. In a protocol stack (also see the OSI model), each protocol leverages the services of the protocol below it. An important example of a protocol stack is HTTP (the World Wide Web protocol) running over TCP over IP (the Internet protocols) over IEEE 802.11 (the Wi-Fi protocol). This stack is used between the wireless router and the home user's personal computer when the user is surfing the web.\n\nWhilst the use of protocol layering is today ubiquitous across the field of computer networking, it has been historically criticized by many researchers[13] for two principal reasons. Firstly, abstracting the protocol stack in this way may cause a higher layer to duplicate functionality of a lower layer, a prime example being error recovery on both a per-link basis and an end-to-end basis.[14] Secondly, it is common that a protocol implementation at one layer may require data, state or addressing information that is only present at another layer, thus defeating the point of separating the layers in the first place. For example, TCP uses the ECN field in the IPv4 header as an indication of congestion; IP is a network layer protocol whereas TCP is a transport layer protocol.\n\nCommunication protocols have various characteristics. They may be connection-oriented or connectionless, they may use circuit mode or packet switching, and they may use hierarchical addressing or flat addressing.\n\nThere are many communication protocols, a few of which are described below.\nIEEE 802\n\nIEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks. The complete IEEE 802 protocol suite provides a diverse set of networking capabilities. The protocols have a flat addressing scheme. They operate mostly at levels 1 and 2 of the OSI model.\n\nFor example, MAC bridging (IEEE 802.1D) deals with the routing of Ethernet packets using a Spanning Tree Protocol. IEEE 802.1Q describes VLANs, and IEEE 802.1X defines a port-based Network Access Control protocol, which forms the basis for the authentication mechanisms used in VLANs (but it is also found in WLANs) – it is what the home user sees when the user has to enter a \"wireless access key\".\nEthernet\n\nEthernet, sometimes simply called LAN, is a family of protocols used in wired LANs, described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers.\nWireless LAN\n\nWireless LAN, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. It is standarized by IEEE 802.11 and shares many properties with wired Ethernet.\nInternet Protocol Suite\n\nThe Internet Protocol Suite, also called TCP/IP, is the foundation of all modern networking. It offers connection-less as well as connection-oriented services over an inherently unreliable network traversed by data-gram transmission at the Internet protocol (IP) level. At its core, the protocol suite defines the addressing, identification, and routing specifications for Internet Protocol Version 4 (IPv4) and for IPv6, the next generation of the protocol with a much enlarged addressing capability.\nSONET/SDH\n\nSynchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support real-time, uncompressed, circuit-switched voice encoded in PCM (Pulse-Code Modulation) format. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames.\nAsynchronous Transfer Mode\n\nAsynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks. It uses asynchronous time-division multiplexing and encodes data into small, fixed-sized cells. This differs from other protocols such as the Internet Protocol Suite or Ethernet that use variable sized packets or frames. ATM has similarity with both circuit and packet switched networking. This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, low-latency content such as voice and video. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins.\n\nWhile the role of ATM is diminishing in favor of next-generation networks, it still plays a role in the last mile, which is the connection between an Internet service provider and the home user.[15]\nGeographic scale\n\nA network can be characterized by its physical capacity or its organizational purpose. Use of the network, including user authorization and access rights, differ accordingly.\n\nNanoscale network\n\nA nanoscale communication network has key components implemented at the nanoscale including message carriers and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for classical communication.[16]\n\nPersonal area network\n\nA personal area network (PAN) is a computer network used for communication among computer and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and even video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters.[17] A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN.\n\nLocal area network\n\nA local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Each computer or device on the network is a node. Wired LANs are most likely based on Ethernet technology. Newer standards such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.[18]\n\nThe defining characteristics of a LAN, in contrast to a wide area network (WAN), include higher data transfer rates, limited geographic range, and lack of reliance on leased lines to provide connectivity. Current Ethernet or other IEEE 802.3 LAN technologies operate at data transfer rates up to 100 Gbit/s, standarized by IEEE in 2010.[19] Currently, 400 Gbit/s Ethernet is being developed.\n\nA LAN can be connected to a WAN using a router.\n\nHome area network\n\nA home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable TV or digital subscriber line (DSL) provider.\n\nStorage area network\n\nA storage area network (SAN) is a dedicated network that provides access to consolidated, block level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear like locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.\n\nCampus area network\n\nA campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, copper plant, Cat5 cabling, etc.) are almost entirely owned by the campus tenant / owner (an enterprise, university, government, etc.).\n\nFor example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls.\n\nBackbone network\n\nA backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or sub-networks. A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area.\n\nFor example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone. When designing a network backbone, network performance and network congestion are critical factors to take into account. Normally, the backbone network's capacity is greater than that of the individual networks connected to it.\n\nAnother example of a backbone network is the Internet backbone, which is the set of wide area networks (WANs) and core routers that tie together all networks connected to the Internet.\n\nMetropolitan area network\n\nA Metropolitan area network (MAN) is a large computer network that usually spans a city or a large campus.\n\nWide area network\n\nA wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances. A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and air waves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI reference model: the physical layer, the data link layer, and the network layer.\n\nEnterprise private network\n\nAn enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources.\n\nVirtual private network\n\nA virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network when this is the case. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features.\n\nVPN may have best-effort performance, or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider. Generally, a VPN has a topology more complex than point-to-point.\n\nGlobal area network\n\nA global area network (GAN) is a network used for supporting mobile across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off user communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs.[20]\nOrganizational scope\n\nNetworks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity.\nIntranet\n\nAn intranet is a set of networks that are under the control of a single administrative entity. The intranet uses the IP protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information. An intranet is also anything behind the router on a local area network.\nExtranet\n\nAn extranet is a network that is also under the administrative control of a single organization, but supports a limited connection to a specific external network. For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers. These other entities are not necessarily trusted from a security standpoint. Network connection to an extranet is often, but not always, implemented via WAN technology.\nInternetwork\n\nAn internetwork is the connection of multiple computer networks via a common routing technology using routers.\nInternet\nPartial map of the Internet based on the January 15, 2005 data found on opte.org \n. Each line is drawn between two nodes, representing two IP addresses. The length of the lines are indicative of the delay between those two nodes. This graph represents less than 30% of the Class C networks reachable.\n\nThe Internet is the largest example of an internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet Protocol Suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet is also the communications backbone underlying the World Wide Web (WWW).\n\nParticipants in the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet Protocol Suite and an addressing system (IP addresses) administered by the Internet Assigned Numbers Authority and address registries. Service providers and large enterprises exchange information about the reachability of their address spaces through the Border Gateway Protocol (BGP), forming a redundant worldwide mesh of transmission paths.\nDarknet\n\nA darknet is an overlay network, typically running on the internet, that is only accessible through specialized software. A darknet is an anonymizing network where connections are made only between trusted peers — sometimes called \"friends\" (F2F)[21] — using non-standard protocols and ports.\n\nDarknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference.[22]\nRouting\nRouting calculates good paths through a network for information to take. For example, from node 1 to node 6 the best routes are likely to be 1-8-7-6 or 1-8-10-6, as this has the thickest routes.\n\nRouting is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks.\n\nIn packet switched networks, routing directs packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination) through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from limited performance. The routing process usually directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router's memory, is very important for efficient routing.\n\nThere are usually multiple routes that can be taken, and to choose between them, different elements can be considered to decide which routes get installed into the routing table, such as (sorted by priority):\n\n    Prefix-Length: where longer subnet masks are preferred (independent if it is within a routing protocol or over different routing protocol)\n    Metric: where a lower metric/cost is preferred (only valid within one and the same routing protocol)\n    Administrative distance: where a lower distance is preferred (only valid between different routing protocols)\n\nMost routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.\n\nRouting, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments.\nNetwork service\n\nNetwork services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate.\n\nThe World Wide Web, E-mail,[23] printing and network file sharing are examples of well-known network services. Network services such as DNS (Domain Name System) give names for IP and MAC addresses (people remember names like “nm.lan” better than numbers like “210.121.67.18”),[24] and DHCP to ensure that the equipment on the network has a valid IP address.[25]\n\nServices are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service.\nNetwork performance\nQuality of service\n\nDepending on the installation requirements, network performance is usually measured by the quality of service of a telecommunications product. The parameters that affect this typically can include throughput, jitter, bit error rate and latency.\n\nThe following list gives examples of network performance measures for a circuit-switched network and one type of packet-switched network, viz. ATM:\n\n    Circuit-switched networks: In circuit switched networks, network performance is synonymous with the grade of service. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads.[26] Other types of performance measures can include the level of noise and echo.\n    ATM: In an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique and modem enhancements.[27]\n\nThere are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modelled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.[28]\nNetwork congestion\n\nNetwork congestion occurs when a link or node is carrying so much data that its quality of service deteriorates. Typical effects include queueing delay, packet loss or the blocking of new connections. A consequence of these latter two is that incremental increases in offered load lead either only to small increase in network throughput, or to an actual reduction in network throughput.\n\nNetwork protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in a state of network congestion—even after the initial load is reduced to a level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse.\n\nModern networks use congestion control and congestion avoidance techniques to try to avoid congestion collapse. These include: exponential backoff in protocols such as 802.11's CSMA/CA and the original Ethernet, window reduction in TCP, and fair queueing in devices such as routers. Another method to avoid the negative effects of network congestion is implementing priority schemes, so that some packets are transmitted with higher priority than others. Priority schemes do not solve network congestion by themselves, but they help to alleviate the effects of congestion for some services. An example of this is 802.1p. A third method to avoid network congestion is the explicit allocation of network resources to specific flows. One example of this is the use of Contention-Free Transmission Opportunities (CFTXOPs) in the ITU-T G.hn standard, which provides high-speed (up to 1 Gbit/s) Local area networking over existing home wires (power lines, phone lines and coaxial cables).\n\nFor the Internet RFC 2914 \naddresses the subject of congestion control in detail.\nNetwork resilience\n\nNetwork resilience is \"the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation.”[29]\nSecurity\nMain article: Computer security\nNetwork security\n\nNetwork security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources.[30] Network security is the authorization of access to data in a network, which is controlled by the network administrator. Users are assigned an ID and password that allows them access to information and programs within their authority. Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies and individuals.\nNetwork surveillance\n\nNetwork surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency.\n\nComputer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity.\n\nSurveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent/investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens.[31]\n\nHowever, many civil rights and privacy groups—such as Reporters Without Borders, the Electronic Frontier Foundation, and the American Civil Liberties Union—have expressed concern that increasing surveillance of citizens may lead to a mass surveillance society, with limited political and personal freedoms. Fears such as this have led to numerous lawsuits such as Hepting v. AT&T.[31][32] The hacktivist group Anonymous has hacked into government websites in protest of what it considers \"draconian surveillance\".[33][34]\nEnd to end encryption\n\nEnd-to-end encryption (E2EE) is a digital communications paradigm of uninterrupted protection of data traveling between two communicating parties. It involves the originating party encrypting data so only the intended recipient can decrypt it, with no dependency on third parties. End-to-end encryption prevents intermediaries, such as Internet providers or application service providers, from discovering or tampering with communications. End-to-end encryption generally protects both confidentiality and integrity.\n\nExamples of end-to-end encryption include PGP for email, OTR for instant messaging, ZRTP for telephony, and TETRA for radio.\n\nTypical server-based communications systems do not include end-to-end encryption. These systems can only guarantee protection of communications between clients and servers, not between the communicating parties themselves. Examples of non-E2EE systems are Google Talk, Yahoo Messenger, Facebook, and Dropbox. Some such systems, for example LavaBit and SecretInk, have even described themselves as offering \"end-to-end\" encryption when they do not. Some systems that normally offer end-to-end encryption have turned out to contain a back door that subverts negotiation of the encryption key between the communicating parties, for example Skype or Hushmail.\n\nThe end-to-end encryption paradigm does not directly address risks at the communications endpoints themselves, such as the technical exploitation of clients, poor quality random number generators, or key escrow. E2EE also does not address traffic analysis, which relates to things such as the identities of the end points and the times and quantities of messages that are sent.\nViews of networks\n\nUsers and network administrators typically have different views of their networks. Users can share printers and some servers from a workgroup, which usually means they are in the same geographic location and are on the same LAN, whereas a Network Administrator is responsible to keep that network up and running. A community of interest has less of a connection of being in a local area, and should be thought of as a set of arbitrarily located users who share a set of servers, and possibly also communicate via peer-to-peer technologies.\n\nNetwork administrators can see networks from both physical and logical perspectives. The physical perspective involves geographic locations, physical cabling, and the network elements (e.g., routers, bridges and application layer gateways) that interconnect via the transmission media. Logical networks, called, in the TCP/IP architecture, subnets, map onto one or more transmission media. For example, a common practice in a campus of buildings is to make a set of LAN cables in each building appear to be a common subnet, using virtual LAN (VLAN) technology.\n\nBoth users and administrators are aware, to varying extents, of the trust and scope characteristics of a network. Again using TCP/IP architectural terminology, an intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees).[35] Intranets do not have to be connected to the Internet, but generally have a limited connection. An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers).[35]\n\nUnofficially, the Internet is the set of users, enterprises, and content providers that are interconnected by Internet Service Providers (ISP). From an engineering viewpoint, the Internet is the set of subnets, and aggregates of subnets, which share the registered IP address space and exchange information about the reachability of those IP addresses using the Border Gateway Protocol. Typically, the human-readable names of servers are translated to IP addresses, transparently to users, via the directory function of the Domain Name System (DNS).\n\nOver the Internet, there can be business-to-business (B2B), business-to-consumer (B2C) and consumer-to-consumer (C2C) communications. When money or sensitive information is exchanged, the communications are apt to be protected by some form of communications security mechanism. Intranets and extranets can be securely superimposed onto the Internet, without any access by general Internet users and administrators, using secure Virtual Private Network (VPN) technology.\nSee also\n\n    Comparison of network diagram software\n    Cyberspace\n    History of the Internet\n    Network simulation\n    Network planning and design", "skillName": "Computer_network."}
