Data management comprises all the disciplines related to managing data as a valuable resource.

Contents

    1 Overview
    2 Corporate Data Quality Management
    3 Topics in Data Management
    4 Body of Knowledge
    5 Usage
    6 Integrated data management
    7 See also
    8 References
    9 External links

Overview

The official definition provided by DAMA International, the professional organization for those in the data management profession, is: "Data Resource Management is the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise." This definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower-level aspects of data management, such as relational database management.
The data lifecycle

Alternatively, the definition provided in the DAMA Data Management Body of Knowledge ([1]) is: "Data management is the development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets."[2]

The concept of "Data Management" arose in the 1980s as technology moved from sequential processing (first cards, then tape) to random access processing. Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that "Data Management" was more important than "Process Management" used arguments such as "a customer's home address is stored in 75 (or some other large number) places in our computer systems." During this period, random access processing was not competitively fast, so those suggesting "Process Management" was more important than "Data Management" used batch processing time as their primary argument. As applications moved into real-time, interactive applications, it became obvious to most practitioners that both management processes were important. If the data was not well defined, the data would be mis-used in applications. If the process wasn't well defined, it was impossible to meet user needs.
Corporate Data Quality Management

Corporate Data Quality Management (CDQM) is, according to the European Foundation for Quality Management and the Competence Center Corporate Data Quality (CC CDQ, University of St. Gallen), the whole set of activities intended to improve corporate data quality (both reactive and preventive). Main premise of CDQM is the business relevance of high-quality corporate data. CDQM comprises with following activity areas:.[3]

    Strategy for Corporate Data Quality: As CDQM is affected by various business drivers and requires involvement of multiple divisions in an organization; it must be considered a company-wide endeavor.
    Corporate Data Quality Controlling: Effective CDQM requires compliance with standards, policies, and procedures. Compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders.
    Corporate Data Quality Organization: CDQM requires clear roles and responsibilities for the use of corporate data. The CDQM organization defines tasks and privileges for decision making for CDQM.
    Corporate Data Quality Processes and Methods: In order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality, standard procedures and guidelines must be embedded in company’s daily processes.
    Data Architecture for Corporate Data Quality: The data architecture consists of the data object model - which comprises the unambiguous definition and the conceptual model of corporate data - and the data storage and distribution architecture.
    Applications for Corporate Data Quality: Software applications support the activities of Corporate Data Quality Management. Their use must be planned, monitored, managed and continuously improved.

Topics in Data Management

Topics in Data Management, grouped by the DAMA DMBOK Framework,[4] include:

    Data governance
        Data asset
        Data governance
        Data steward
    Data Architecture, Analysis and Design
        Data analysis
        Data architecture
        Data modeling
    Database Management
        Data maintenance
        Database administration
        Database management system
    Data Security Management
        Data access
        Data erasure
        Data privacy
        Data security
    Data Quality Management
        Data cleansing
        Data integrity
        Data enrichment
        Data quality
        Data quality assurance
    Reference and Master Data Management
        Data integration
        Master data management
        Reference data
    Data Warehousing and Business Intelligence Management
        Business intelligence
        Data mart
        Data mining
        Data movement (Extract, transform, load )
        Data warehouse
    Document, Record and Content Management
        Document management system
        Records management
    Meta Data Management
        Meta-data management
        Metadata
        Metadata discovery
        Metadata publishing
        Metadata registry
    Contact Data Management
        Business continuity planning
        Marketing operations
        Customer data integration
        Identity management
        Identity theft
        Data theft
        ERP software
        CRM software
        Address (geography)
        Postal code
        Email address
        Telephone number

Body of Knowledge

The DAMA Guide to the Data Management Body of Knowledge" (DAMA-DMBOK Guide), under the guidance of a new DAMA-DMBOK Editorial Board. This publication is available from April 5, 2009.
Usage

In modern management usage, one can easily discern a trend away from the term "data" in composite expressions to the term "information" or even "knowledge" when talking in a non-technical context. Thus there exists not only data management, but also information management and knowledge management. This is a misleading trend as it obscures that traditional data are managed or somehow processed on second looks.[citation needed] The distinction between data and derived values can be seen in the information ladder.[citation needed] While data can exist as such, "information" and "knowledge" are always in the "eye" (or rather the brain) of the beholder and can only be measured in relative units.

Several organisations have established a data management centre (DMC)[5] for their operations.
Integrated data management

Integrated data management (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its lifetime.[6][7][8][9] IDM's purpose is to:

    Produce enterprise-ready applications faster
    Improve data access, speed iterative testing
    Empower collaboration between architects, developers and DBAs
    Consistently achieve service level targets
    Automate and simplify operations
    Provide contextual intelligence across the solution stack
    Support business growth
    Accommodate new initiatives without expanding infrastructure
    Simplify application upgrades, consolidation and retirement
    Facilitate alignment, consistency and governance
    Define business policies and standards up front; share, extend, and apply throughout the lifecycle

See also

    Open data
    Information architecture
    Information management
    Enterprise architecture
    Information design
    Information system
    Controlled vocabulary
    Data curation
    Data retention
    Data governance
    Data quality
    Data modeling
    Data management plan
    Information lifecycle management
    Computer data storage
    Data proliferation
    Digital preservation
    Digital perpetuation
    Document management
    Enterprise content management
    Hierarchical storage management
    Information repository
    Records management
    System integration


Data curation is a term used to indicate management activities related to organization and integration of data collected from various sources, annotation of the data, and publication and presentation of the data such that the value of the data is maintained over time, and the data remains available for reuse and preservation. Data curation includes "all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data".[1] In science, data curation may indicate the process of extraction of important information from scientific texts, such as research articles by experts, to be converted into an electronic format, such as an entry of a biological database.[2] The term is also used in the humanities, where increasing cultural and scholarly data from digital humanities projects requires the expertise and analytical practices of data curation.[3] In broad terms, curation means a range of activities and processes done to create, manage, maintain, and validate a component.[4]

Contents

    1 Definition and practice
    2 Projects and studies
    3 See also
    4 References
    5 External links

Definition and practice

According to the University of Illinois' Graduate School of Library and Information Science, "Data curation is the active and on-going management of data through its lifecycle of interest and usefulness to scholarship, science, and education; curation activities enable data discovery and retrieval, maintain quality, add value, and provide for re-use over time."[5]

Deep background on data libraries appeared in a 1982 issue of the Illinois journal, Library Trends.[6] For historical background on the data archive movement, see "Social Scientific Information Needs for Numeric Data: The Evolution of the International Data Archive Infrastructure."[7]

This term is sometimes used in context of biological databases, where specific biological information is firstly obtained from a range of research articles and then stored within a specific category of database. For instance, information about anti-depressant drugs can be obtained from various sources and, after checking whether they are available as a database or not, they are saved under a drug's database's anti-depressive category. Enterprises are also utilizing data curation within their operational and strategic processes to ensure data quality and accuracy.[8]
Projects and studies

The Dissemination Information Packages (DIPS) for Information Reuse (DIPIR) project is studying research data produced and used by quantitative social scientists, archaeologists, and zoologists. The intended audience is researchers who use secondary data and the digital curators, digital repository managers, data center staff, and others who collect, manage, and store digital information.[9]
See also

    Biocurator
    Data archaeology
    Data degradation
    Data format management
    Data governance
    Data management
    Data stewardship
    Data wrangling, low-level activities to parse and reformat data
    Informationist, an individual with extensive industry expertise, acute familiarity with organizational structures and processes, deep domain level information mastery and information systems technical savvy






Data management comprises all the disciplines related to managing data as a valuable resource.

Contents

    1 Overview
    2 Corporate Data Quality Management
    3 Topics in Data Management
    4 Body of Knowledge
    5 Usage
    6 Integrated data management
    7 See also
    8 References
    9 External links

Overview

The official definition provided by DAMA International, the professional organization for those in the data management profession, is: "Data Resource Management is the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise." This definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower-level aspects of data management, such as relational database management.
The data lifecycle

Alternatively, the definition provided in the DAMA Data Management Body of Knowledge ([1]) is: "Data management is the development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets."[2]

The concept of "Data Management" arose in the 1980s as technology moved from sequential processing (first cards, then tape) to random access processing. Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that "Data Management" was more important than "Process Management" used arguments such as "a customer's home address is stored in 75 (or some other large number) places in our computer systems." During this period, random access processing was not competitively fast, so those suggesting "Process Management" was more important than "Data Management" used batch processing time as their primary argument. As applications moved into real-time, interactive applications, it became obvious to most practitioners that both management processes were important. If the data was not well defined, the data would be mis-used in applications. If the process wasn't well defined, it was impossible to meet user needs.
Corporate Data Quality Management

Corporate Data Quality Management (CDQM) is, according to the European Foundation for Quality Management and the Competence Center Corporate Data Quality (CC CDQ, University of St. Gallen), the whole set of activities intended to improve corporate data quality (both reactive and preventive). Main premise of CDQM is the business relevance of high-quality corporate data. CDQM comprises with following activity areas:.[3]

    Strategy for Corporate Data Quality: As CDQM is affected by various business drivers and requires involvement of multiple divisions in an organization; it must be considered a company-wide endeavor.
    Corporate Data Quality Controlling: Effective CDQM requires compliance with standards, policies, and procedures. Compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders.
    Corporate Data Quality Organization: CDQM requires clear roles and responsibilities for the use of corporate data. The CDQM organization defines tasks and privileges for decision making for CDQM.
    Corporate Data Quality Processes and Methods: In order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality, standard procedures and guidelines must be embedded in company’s daily processes.
    Data Architecture for Corporate Data Quality: The data architecture consists of the data object model - which comprises the unambiguous definition and the conceptual model of corporate data - and the data storage and distribution architecture.
    Applications for Corporate Data Quality: Software applications support the activities of Corporate Data Quality Management. Their use must be planned, monitored, managed and continuously improved.

Topics in Data Management

Topics in Data Management, grouped by the DAMA DMBOK Framework,[4] include:

    Data governance
        Data asset
        Data governance
        Data steward
    Data Architecture, Analysis and Design
        Data analysis
        Data architecture
        Data modeling
    Database Management
        Data maintenance
        Database administration
        Database management system
    Data Security Management
        Data access
        Data erasure
        Data privacy
        Data security
    Data Quality Management
        Data cleansing
        Data integrity
        Data enrichment
        Data quality
        Data quality assurance
    Reference and Master Data Management
        Data integration
        Master data management
        Reference data
    Data Warehousing and Business Intelligence Management
        Business intelligence
        Data mart
        Data mining
        Data movement (Extract, transform, load )
        Data warehouse
    Document, Record and Content Management
        Document management system
        Records management
    Meta Data Management
        Meta-data management
        Metadata
        Metadata discovery
        Metadata publishing
        Metadata registry
    Contact Data Management
        Business continuity planning
        Marketing operations
        Customer data integration
        Identity management
        Identity theft
        Data theft
        ERP software
        CRM software
        Address (geography)
        Postal code
        Email address
        Telephone number

Body of Knowledge

The DAMA Guide to the Data Management Body of Knowledge" (DAMA-DMBOK Guide), under the guidance of a new DAMA-DMBOK Editorial Board. This publication is available from April 5, 2009.
Usage

In modern management usage, one can easily discern a trend away from the term "data" in composite expressions to the term "information" or even "knowledge" when talking in a non-technical context. Thus there exists not only data management, but also information management and knowledge management. This is a misleading trend as it obscures that traditional data are managed or somehow processed on second looks.[citation needed] The distinction between data and derived values can be seen in the information ladder.[citation needed] While data can exist as such, "information" and "knowledge" are always in the "eye" (or rather the brain) of the beholder and can only be measured in relative units.

Several organisations have established a data management centre (DMC)[5] for their operations.
Integrated data management

Integrated data management (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its lifetime.[6][7][8][9] IDM's purpose is to:

    Produce enterprise-ready applications faster
    Improve data access, speed iterative testing
    Empower collaboration between architects, developers and DBAs
    Consistently achieve service level targets
    Automate and simplify operations
    Provide contextual intelligence across the solution stack
    Support business growth
    Accommodate new initiatives without expanding infrastructure
    Simplify application upgrades, consolidation and retirement
    Facilitate alignment, consistency and governance
    Define business policies and standards up front; share, extend, and apply throughout the lifecycle




Data Management and Curation platform
Data modelling and related technologies 
ETL, OLAP, OLTP
Data warehouses platform and related tools 
In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis, and is considered as a core component of business intelligence[1] environment. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.

The data stored in the warehouse is uploaded from the operational systems (such as marketing, sales, etc., shown in the figure to the right). The data may pass through an operational data store for additional operations before it is used in the DW for reporting.

Contents

    1 Types of systems
    2 Software tools
    3 Benefits
    4 Generic data warehouse environment
    5 History
    6 Information storage
        6.1 Facts
        6.2 Dimensional vs. normalized approach for storage of data
    7 Design methods
        7.1 Bottom-up design
        7.2 Top-down design
        7.3 Hybrid design
    8 Data warehouses versus operational systems
    9 Evolution in organization use
    10 See also
    11 References
    12 Further reading
    13 External links

Types of systems

Data mart
    A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area) hence, they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data.[2] Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.

The difference between data warehouse and data mart
Data warehouse 	data mart
enterprise-wide data 	department-wide data
multiple subject areas 	single subject area
difficult to build 	easy to build
takes more time to build 	less time to build
larger memory 	limited memory

Types of data marts

    Dependent data mart
    Independent data mart
    Hybrid data mart

Online analytical processing (OLAP)
    OLAP is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by Data Mining techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day.The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing & Dicing.[3]

Online transaction processing (OLTP)
    OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining data integrity in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually 3NF).[4] Normalization is the norm for data modeling techniques in this system.

Predictive analysis
    Predictive analysis is about finding and quantifying hidden patterns in the data using complex mathematical models that can be used to predict future outcomes. Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for CRM (customer relationship management).

Software tools

The typical extract-transform-load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.[5]

This definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, cataloged and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support.[6] However, the means to retrieve and analyze data, to extract, transform and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform and load data into the repository, and tools to manage and retrieve metadata.
Benefits

A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to :

    Congregate data from multiple sources into a single database so a single query engine can be used to present data.
    Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long running, analysis queries in transaction processing databases.
    Maintain data history, even if the source transaction systems do not.
    Integrate data from multiple source systems, enabling a central view across the enterprise. This benefit is always valuable, but particularly so when the organization has grown by merger.
    Improve data quality, by providing consistent codes and descriptions, flagging or even fixing bad data.
    Present the organization's information consistently.
    Provide a single common data model for all data of interest regardless of the data's source.
    Restructure the data so that it makes sense to the business users.
    Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the operational systems.
    Add value to operational business applications, notably customer relationship management (CRM) systems.
    Make decision–support queries easier to write.

Generic data warehouse environment

The environment for data warehouses and marts includes the following:

    Source systems that provide data to the warehouse or mart;
    Data integration technology and processes that are needed to prepare the data for use;
    Different architectures for storing data in an organization's data warehouse or data marts;
    Different tools and applications for the variety of users;
    Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.

In regards to source systems listed above, Rainer[clarification needed] states, “A common source for the data in data warehouses is the company’s operational databases, which can be relational databases”.[7]

Regarding data integration, Rainer states, “It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse”.[7]

Rainer discusses storing data in an organization’s data warehouse or data marts.[7]

Metadata are data about data. “IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures“.[7]

Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers.[7] A “data warehouse” is a repository of historical data that are organized by subject to support decision makers in the organization.[7] Once data are stored in a data mart or warehouse, they can be accessed.
History

The concept of data warehousing dates back to the late 1980s[8] when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from "data marts" that were tailored for ready access by users.

Key developments in early years of data warehousing were:

    1960s — General Mills and Dartmouth College, in a joint research project, develop the terms dimensions and facts.[9]
    1970s — ACNielsen and IRI provide dimensional data marts for retail sales.[9]
    1970s — Bill Inmon begins to define and discuss the term: Data Warehouse.[citation needed]
    1975 — Sperry Univac introduces MAPPER (MAintain, Prepare, and Produce Executive Reports) is a database management and reporting system that includes the world's first 4GL. First platform designed for building Information Centers (a forerunner of contemporary Enterprise Data Warehousing platforms)
    1983 — Teradata introduces a database management system specifically designed for decision support.
    1984 — Metaphor Computer Systems, founded by David Liddle and Don Massaro, releases Data Interpretation System (DIS). DIS was a hardware/software package and GUI for business users to create a database management and analytic system.
    1988 — Barry Devlin and Paul Murphy publish the article An architecture for a business and information system where they introduce the term "business data warehouse".[10]
    1990 — Red Brick Systems, founded by Ralph Kimball, introduces Red Brick Warehouse, a database management system specifically for data warehousing.
    1991 — Prism Solutions, founded by Bill Inmon, introduces Prism Warehouse Manager, software for developing a data warehouse.
    1992 — Bill Inmon publishes the book Building the Data Warehouse.[11]
    1995 — The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.
    1996 — Ralph Kimball publishes the book The Data Warehouse Toolkit.[12]
    2012 — Bill Inmon developed and made public technology known as "textual disambiguation". Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. Textual disambiguation is accomplished through the execution of textual ETL. Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.

Information storage
Facts

A fact is a value or measurement, which represents a fact about the managed entity or system.

Facts as reported by the reporting entity are said to be at raw level. E.g. if a BTS (business transformation service) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 facts or measurements to a management system:

    tch_req_total = 1000
    tch_req_success = 820
    tch_req_fail = 180

Facts at raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information out of it. These are called aggregates or summaries or aggregated facts.

E.g. if there are 3 BTSs in a city, then facts above can be aggregated from BTS to city level in network dimension. E.g.

    t c h _ r e q _ s u c c e s s _ c i t y = t c h _ r e q _ s u c c e s s _ b t s 1 + t c h _ r e q _ s u c c e s s _ b t s 2 + t c h _ r e q _ s u c c e s s _ b t s 3 {\displaystyle tch\_req\_success\_city=tch\_req\_success\_bts1+tch\_req\_success\_bts2+tch\_req\_success\_bts3} tch\_req\_success\_city = tch\_req\_success\_bts1 + tch\_req\_success\_bts2 + tch\_req\_success\_bts3
    a v g _ t c h _ r e q _ s u c c e s s _ c i t y = ( t c h _ r e q _ s u c c e s s _ b t s 1 + t c h _ r e q _ s u c c e s s _ b t s 2 + t c h _ r e q _ s u c c e s s _ b t s 3 ) / 3 {\displaystyle avg\_tch\_req\_success\_city=(tch\_req\_success\_bts1+tch\_req\_success\_bts2+tch\_req\_success\_bts3)/3} avg\_tch\_req\_success\_city = (tch\_req\_success\_bts1 + tch\_req\_success\_bts2 + tch\_req\_success\_bts3) / 3

Dimensional vs. normalized approach for storage of data

There are three or more leading approaches to storing data in a data warehouse — the most important approaches are the dimensional approach and the normalized approach.

The dimensional approach refers to Ralph Kimball’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. The normalized approach, also called the 3NF model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.

In a dimensional approach, transaction data are partitioned into "facts", which are generally numeric transaction data, and "dimensions", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.

A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly.[12] Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus,this type of modeling technique is very useful for end-user queries in data warehouse.[3]

The main disadvantages of the dimensional approach are the following:

    In order to maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.
    It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.

In the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. Tables are grouped together by subject areas that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008)[citation needed]. The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse.

Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as Normal Forms). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).

In Information-Driven Business,[13] Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.[14]
Design methods
	This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2015) (Learn how and when to remove this template message)
Bottom-up design

In the bottom-up approach, data marts are first created to provide reporting and analytical capabilities for specific business processes. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of "the bus", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.[15]
Top-down design

The top-down approach is designed using a normalized enterprise data model. "Atomic" data, that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.[16]
Hybrid design

Data warehouses (DW) often resemble the hub and spokes architecture. Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an operational data store, the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.

The DW database in a hybrid solution is kept on third normal form to eliminate data redundancy. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a master data management solution where operational, not static information could reside.

The Data Vault Modeling components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and star schema. The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.
Data warehouses versus operational systems

Operational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. Operational system designers generally follow the Codd rules of database normalization in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. Relational databases are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.

Data warehouses are optimized for analytic access patterns. Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases. Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.
Evolution in organization use

These terms refer to the level of sophistication of a data warehouse:

Offline operational data warehouse
    Data warehouses in this stage of evolution are updated on a regular time cycle (usually daily, weekly or monthly) from the operational systems and the data is stored in an integrated reporting-oriented data
Offline data warehouse
    Data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting.
On time data warehouse
    Online Integrated Data Warehousing represent the real time Data warehouses stage data in the warehouse is updated for every transaction performed on the source data
Integrated data warehouse
    These data warehouses assemble data from different areas of business, so users can look up the information they need across other systems.[17] 

OLTP stands for On Line Transaction Processing and is a data modeling approach typically used to facilitate and manage usual business applications. Most of applications you see and use are OLTP based. OLAP stands for On Line Analytic Processing and is an approach to answer multi-dimensional queries
ETL Extract, Transform and Load is a process in data warehousing responsible for pulling data out of the source systems and placing it into a data warehouse.
OLAP is an acronym for Online Analytical Processing. OLAP performs multidimensional analysis of business data and provides the capability for complex calculations, trend analysis, and sophisticated data modeling.
Data curation platform, metadata management ETL, Curator's Workbench, DataUp, MIXED 
Backup and storage management iRODS, XArch, Nesstar
In computing, online analytical processing, or OLAP (/ˈoʊlæp/), is an approach to answering multi-dimensional analytical (MDA) queries swiftly.[1] OLAP is part of the broader category of business intelligence, which also encompasses relational database, report writing and data mining.[2] Typical applications of OLAP include business reporting for sales, marketing, management reporting, business process management (BPM),[3] budgeting and forecasting, financial reporting and similar areas, with new applications coming up, such as agriculture.[4] The term OLAP was created as a slight modification of the traditional database term online transaction processing (OLTP).[5]

OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing.[6] Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region’s sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the OLAP cube and view (dicing) the slices from different viewpoints. These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson or by date or by customer or by product or by region, etc.)

Databases configured for OLAP use a multidimensional data model, allowing for complex analytical and ad hoc queries with a rapid execution time

In information technology, a backup, or the process of backing up, refers to the copying and archiving of computer data so it may be used to restore the original after a data loss event. The verb form is to back up in two words, whereas the noun is backup.[1]

Backups have two distinct purposes. The primary purpose is to recover data after its loss, be it by data deletion or corruption. Data loss can be a common experience of computer users; a 2008 survey found that 66% of respondents had lost files on their home PC.[2] The secondary purpose of backups is to recover data from an earlier time, according to a user-defined data retention policy, typically configured within a backup application for how long copies of data are required. Though backups represent a simple form of disaster recovery, and should be part of any disaster recovery plan, backups by themselves should not be considered a complete disaster recovery plan. One reason for this is that not all backup systems are able to reconstitute a computer system or other complex configuration such as a computer cluster, active directory server, or database server by simply restoring data from a backup.

Since a backup system contains at least one copy of all data considered worth saving, the data storage requirements can be significant. Organizing this storage space and managing the backup process can be a complicated undertaking. A data repository model may be used to provide structure to the storage. Nowadays, there are many different types of data storage devices that are useful for making backups. There are also many different ways in which these devices can be arranged to provide geographic redundancy, data security, and portability.

Before data are sent to their storage locations, they are selected, extracted, and manipulated. Many different techniques have been developed to optimize the backup procedure. These include optimizations for dealing with open files and live data sources as well as compression, encryption, and de-duplication, among others. Every backup scheme should include dry runs that validate the reliability of the data being backed up. It is important to recognize the limitations and human factors involved in any backup scheme.

Contents

    1 Storage, the base of a backup system
        1.1 Data repository models
        1.2 Storage media
        1.3 Managing the data repository
    2 Selection and extraction of data
        2.1 Files
        2.2 Filesystems
        2.3 Live data
        2.4 Metadata
    3 Manipulation of data and dataset optimization
    4 Managing the backup process
        4.1 Objectives
        4.2 Limitations
        4.3 Implementation
        4.4 Measuring the process
    5 See also
    6 References
    7 External links

Storage, the base of a backup system
Data repository models

Any backup strategy starts with a concept of a data repository. The backup data needs to be stored, and probably should be organized to a degree. The organisation could be as simple as a sheet of paper with a list of all backup media (CDs etc.) and the dates they were produced. A more sophisticated setup could include a computerized index, catalog, or relational database. Different approaches have different advantages. Part of the model is the backup rotation scheme.

Unstructured 
    An unstructured repository may simply be a stack of or CD-Rs or DVD-Rs with minimal information about what was backed up and when. This is the easiest to implement, but probably the least likely to achieve a high level of recoverability as it lacks automation.
Full only / System imaging 
    A repository of this type contains complete system images taken at one or more specific points in time. This technology is frequently used by computer technicians to record known good configurations. Imaging[3] is generally more useful for deploying a standard configuration to many systems rather than as a tool for making ongoing backups of diverse systems.
Incremental 
    An incremental style repository aims to make it more feasible to store backups from more points in time by organizing the data into increments of change between points in time. This eliminates the need to store duplicate copies of unchanged data: with full backups a lot of the data will be unchanged from what has been backed up previously. Typically, a full backup (of all files) is made on one occasion (or at infrequent intervals) and serves as the reference point for an incremental backup set. After that, a number of incremental backups are made after successive time periods. Restoring the whole system to the date of the last incremental backup would require starting from the last full backup taken before the data loss, and then applying in turn each of the incremental backups since then.[4] Additionally, some backup systems can reorganize the repository to synthesize full backups from a series of incrementals.
Differential 
    Each differential backup saves the data that has changed since the last full backup. It has the advantage that only a maximum of two data sets are needed to restore the data. One disadvantage, compared to the incremental backup method, is that as time from the last full backup (and thus the accumulated changes in data) increases, so does the time to perform the differential backup. Restoring an entire system would require starting from the most recent full backup and then applying just the last differential backup since the last full backup.

        Note: Vendors have standardized on the meaning of the terms "incremental backup" and "differential backup". However, there have been cases where conflicting definitions of these terms have been used. The most relevant characteristic of an incremental backup is which reference point it uses to check for changes. By standard definition, a differential backup copies files that have been created or changed since the last full backup, regardless of whether any other differential backups have been made since then, whereas an incremental backup copies files that have been created or changed since the most recent backup of any type (full or incremental). Other variations of incremental backup include multi-level incrementals and incremental backups that compare parts of files instead of just the whole file.

Reverse delta 
    A reverse delta type repository stores a recent "mirror" of the source data and a series of differences between the mirror in its current state and its previous states. A reverse delta backup will start with a normal full backup. After the full backup is performed, the system will periodically synchronize the full backup with the live copy, while storing the data necessary to reconstruct older versions. This can either be done using hard links, or using binary diffs. This system works particularly well for large, slowly changing, data sets. Examples of programs that use this method are rdiff-backup and Time Machine.
Continuous data protection 
    Instead of scheduling periodic backups, the system immediately logs every change on the host system. This is generally done by saving byte or block-level differences rather than file-level differences.[5] It differs from simple disk mirroring in that it enables a roll-back of the log and thus restoration of old images of data.

Storage media

Regardless of the repository model that is used, the data has to be stored on some data storage medium.

Magnetic tape 
    Magnetic tape has long been the most commonly used medium for bulk data storage, backup, archiving, and interchange. Tape has typically had an order of magnitude better capacity-to-price ratio when compared to hard disk, but recently the ratios for tape and hard disk have become a lot closer.[6] There are many formats, many of which are proprietary or specific to certain markets like mainframes or a particular brand of personal computer. Tape is a sequential access medium, so even though access times may be poor, the rate of continuously writing or reading data can actually be very fast. Some new tape drives are even faster than modern hard disks.
Hard disk
    The capacity-to-price ratio of hard disk has been rapidly improving for many years. This is making it more competitive with magnetic tape as a bulk storage medium. The main advantages of hard disk storage are low access times, availability, capacity and ease of use.[7] External disks can be connected via local interfaces like SCSI, USB, FireWire, or eSATA, or via longer distance technologies like Ethernet, iSCSI, or Fibre Channel. Some disk-based backup systems, such as Virtual Tape Libraries, support data deduplication which can dramatically reduce the amount of disk storage capacity consumed by daily and weekly backup data. The main disadvantages of hard disk backups are that they are easily damaged, especially while being transported (e.g., for off-site backups), and that their stability over periods of years is a relative unknown.
Optical storage 
    Recordable CDs, DVDs, and Blu-ray Discs are commonly used with personal computers and generally have low media unit costs. However, the capacities and speeds of these and other optical discs are typically an order of magnitude lower than hard disk or tape. Many optical disk formats are WORM type, which makes them useful for archival purposes since the data cannot be changed. The use of an auto-changer or jukebox can make optical discs a feasible option for larger-scale backup systems. Some optical storage systems allow for cataloged data backups without human contact with the discs, allowing for longer data integrity.
Solid state storage 
    Also known as flash memory, thumb drives, USB flash drives, CompactFlash, SmartMedia, Memory Stick, Secure Digital cards, etc., these devices are relatively expensive for their low capacity in comparison to hard disk drives, but are very convenient for backing up relatively low data volumes. A solid-state drive does not contain any movable parts unlike its magnetic drive counterpart, making it less susceptible to physical damage, and can have huge throughput in the order of 500Mbit/s to 6Gbit/s. The capacity offered from SSDs continues to grow and prices are gradually decreasing as they become more common.
Remote backup service 
    As broadband Internet access becomes more widespread, remote backup services are gaining in popularity. Backing up via the Internet to a remote location can protect against some worst-case scenarios such as fires, floods, or earthquakes which would destroy any backups in the immediate vicinity along with everything else. There are, however, a number of drawbacks to remote backup services. First, Internet connections are usually slower than local data storage devices. Residential broadband is especially problematic as routine backups must use an upstream link that's usually much slower than the downstream link used only occasionally to retrieve a file from backup. This tends to limit the use of such services to relatively small amounts of high value data. Secondly, users must trust a third party service provider to maintain the privacy and integrity of their data, although confidentiality can be assured by encrypting the data before transmission to the backup service with an encryption key known only to the user. Ultimately the backup service must itself use one of the above methods so this could be seen as a more complex way of doing traditional backups.
Floppy disk 
    During the 1980s and early 1990s, many personal/home computer users associated backing up mostly with copying to floppy disks. However, the data capacity of floppy disks failed to catch up with growing demands, rendering them effectively obsolete.

Managing the data repository

Regardless of the data repository model, or data storage media used for backups, a balance needs to be struck between accessibility, security and cost. These media management methods are not mutually exclusive and are frequently combined to meet the user's needs. Using on-line disks for staging data before it is sent to a near-line tape library is a common example.

On-line 
    On-line backup storage is typically the most accessible type of data storage, which can begin restore in milliseconds of time. A good example is an internal hard disk or a disk array (maybe connected to SAN). This type of storage is very convenient and speedy, but is relatively expensive. On-line storage is quite vulnerable to being deleted or overwritten, either by accident, by intentional malevolent action, or in the wake of a data-deleting virus payload.
Near-line 
    Near-line storage is typically less accessible and less expensive than on-line storage, but still useful for backup data storage. A good example would be a tape library with restore times ranging from seconds to a few minutes. A mechanical device is usually used to move media units from storage into a drive where the data can be read or written. Generally it has safety properties similar to on-line storage.
Off-line 
    Off-line storage requires some direct human action to provide access to the storage media: for example inserting a tape into a tape drive or plugging in a cable. Because the data are not accessible via any computer except during limited periods in which they are written or read back, they are largely immune to a whole class of on-line backup failure modes. Access time will vary depending on whether the media are on-site or off-site.
Off-site data protection
    To protect against a disaster or other site-specific problem, many people choose to send backup media to an off-site vault. The vault can be as simple as a system administrator's home office or as sophisticated as a disaster-hardened, temperature-controlled, high-security bunker with facilities for backup media storage. Importantly a data replica can be off-site but also on-line (e.g., an off-site RAID mirror). Such a replica has fairly limited value as a backup, and should not be confused with an off-line backup.
Backup site or disaster recovery center (DR center)
    In the event of a disaster, the data on backup media will not be sufficient to recover. Computer systems onto which the data can be restored and properly configured networks are necessary too. Some organizations have their own data recovery centers that are equipped for this scenario. Other organizations contract this out to a third-party recovery center. Because a DR site is itself a huge investment, backing up is very rarely considered the preferred method of moving data to a DR site. A more typical way would be remote disk mirroring, which keeps the DR data as up to date as possible.

Selection and extraction of data

A successful backup job starts with selecting and extracting coherent units of data. Most data on modern computer systems is stored in discrete units, known as files. These files are organized into filesystems. Files that are actively being updated can be thought of as "live" and present a challenge to back up. It is also useful to save metadata that describes the computer or the filesystem being backed up.

Deciding what to back up at any given time is a harder process than it seems. By backing up too much redundant data, the data repository will fill up too quickly. Backing up an insufficient amount of data can eventually lead to the loss of critical information.
Files

Copying files 
    With file-level approach, making copies of files is the simplest and most common way to perform a backup. A means to perform this basic function is included in all backup software and all operating systems.

Partial file copying
    Instead of copying whole files, one can limit the backup to only the blocks or bytes within a file that have changed in a given period of time. This technique can use substantially less storage space on the backup medium, but requires a high level of sophistication to reconstruct files in a restore situation. Some implementations require integration with the source file system.

Deleted files 
    To prevent the unintentional restoration of files that have been intentionally deleted, a record of the deletion must be kept.

Filesystems

Filesystem dump
    Instead of copying files within a file system, a copy of the whole filesystem itself in block-level can be made. This is also known as a raw partition backup and is related to disk imaging. The process usually involves unmounting the filesystem and running a program like dd (Unix). Because the disk is read sequentially and with large buffers, this type of backup can be much faster than reading every file normally, especially when the filesystem contains many small files, is highly fragmented, or is nearly full. But because this method also reads the free disk blocks that contain no useful data, this method can also be slower than conventional reading, especially when the filesystem is nearly empty. Some filesystems, such as XFS, provide a "dump" utility that reads the disk sequentially for high performance while skipping unused sections. The corresponding restore utility can selectively restore individual files or the entire volume at the operator's choice.

Identification of changes
    Some filesystems have an archive bit for each file that says it was recently changed. Some backup software looks at the date of the file and compares it with the last backup to determine whether the file was changed.

Versioning file system 
    A versioning filesystem keeps track of all changes to a file and makes those changes accessible to the user. Generally this gives access to any previous version, all the way back to the file's creation time. An example of this is the Wayback versioning filesystem for Linux.[8]

Live data

If a computer system is in use while it is being backed up, the possibility of files being open for reading or writing is real. If a file is open, the contents on disk may not correctly represent what the owner of the file intends. This is especially true for database files of all kinds. The term fuzzy backup can be used to describe a backup of live data that looks like it ran correctly, but does not represent the state of the data at any single point in time. This is because the data being backed up changed in the period of time between when the backup started and when it finished. For databases in particular, fuzzy backups are worthless.[citation needed]

Snapshot backup
    A snapshot is an instantaneous function of some storage systems that presents a copy of the file system as if it were frozen at a specific point in time, often by a copy-on-write mechanism. An effective way to back up live data is to temporarily quiesce them (e.g. close all files), take a snapshot, and then resume live operations. At this point the snapshot can be backed up through normal methods.[9] While a snapshot is very handy for viewing a filesystem as it was at a different point in time, it is hardly an effective backup mechanism by itself.

Open file backup
    Many backup software packages feature the ability to handle open files in backup operations. Some simply check for openness and try again later. File locking is useful for regulating access to open files.
    When attempting to understand the logistics of backing up open files, one must consider that the backup process could take several minutes to back up a large file such as a database. In order to back up a file that is in use, it is vital that the entire backup represent a single-moment snapshot of the file, rather than a simple copy of a read-through. This represents a challenge when backing up a file that is constantly changing. Either the database file must be locked to prevent changes, or a method must be implemented to ensure that the original snapshot is preserved long enough to be copied, all while changes are being preserved. Backing up a file while it is being changed, in a manner that causes the first part of the backup to represent data before changes occur to be combined with later parts of the backup after the change results in a corrupted file that is unusable, as most large files contain internal references between their various parts that must remain consistent throughout the file.

Cold database backup
    During a cold backup, the database is closed or locked and not available to users. The datafiles do not change during the backup process so the database is in a consistent state when it is returned to normal operation.[10]

Hot database backup
    Some database management systems offer a means to generate a backup image of the database while it is online and usable ("hot"). This usually includes an inconsistent image of the data files plus a log of changes made while the procedure is running. Upon a restore, the changes in the log files are reapplied to bring the copy of the database up-to-date (the point in time at which the initial hot backup ended).[11]

Metadata

Not all information stored on the computer is stored in files. Accurately recovering a complete system from scratch requires keeping track of this non-file data too. [12]

System description
    System specifications are needed to procure an exact replacement after a disaster.
Boot sector 
    The boot sector can sometimes be recreated more easily than saving it. Still, it usually isn't a normal file and the system won't boot without it.
Partition layout
    The layout of the original disk, as well as partition tables and filesystem settings, is needed to properly recreate the original system.
File metadata 
    Each file's permissions, owner, group, ACLs, and any other metadata need to be backed up for a restore to properly recreate the original environment.
System metadata
    Different operating systems have different ways of storing configuration information. Microsoft Windows keeps a registry of system information that is more difficult to restore than a typical file.

Manipulation of data and dataset optimization

It is frequently useful or required to manipulate the data being backed up to optimize the backup process. These manipulations can provide many benefits including improved backup speed, restore speed, data security, media usage and/or reduced bandwidth requirements.

Compression 
    Various schemes can be employed to shrink the size of the source data to be stored so that it uses less storage space. Compression is frequently a built-in feature of tape drive hardware.
Deduplication 
    When multiple similar systems are backed up to the same destination storage device, there exists the potential for much redundancy within the backed up data. For example, if 20 Windows workstations were backed up to the same data repository, they might share a common set of system files. The data repository only needs to store one copy of those files to be able to restore any one of those workstations. This technique can be applied at the file level or even on raw blocks of data, potentially resulting in a massive reduction in required storage space. Deduplication can occur on a server before any data moves to backup media, sometimes referred to as source/client side deduplication. This approach also reduces bandwidth required to send backup data to its target media. The process can also occur at the target storage device, sometimes referred to as inline or back-end deduplication.
Duplication 
    Sometimes backup jobs are duplicated to a second set of storage media. This can be done to rearrange the backup images to optimize restore speed or to have a second copy at a different location or on a different storage medium.
Encryption 
    High capacity removable storage media such as backup tapes present a data security risk if they are lost or stolen.[13] Encrypting the data on these media can mitigate this problem, but presents new problems. Encryption is a CPU intensive process that can slow down backup speeds, and the security of the encrypted backups is only as effective as the security of the key management policy.
Multiplexing 
    When there are many more computers to be backed up than there are destination storage devices, the ability to use a single storage device with several simultaneous backups can be useful.
Refactoring
    The process of rearranging the backup sets in a data repository is known as refactoring. For example, if a backup system uses a single tape each day to store the incremental backups for all the protected computers, restoring one of the computers could potentially require many tapes. Refactoring could be used to consolidate all the backups for a single computer onto a single tape. This is especially useful for backup systems that do incrementals forever style backups.
Staging 
    Sometimes backup jobs are copied to a staging disk before being copied to tape. This process is sometimes referred to as D2D2T, an acronym for Disk to Disk to Tape. This can be useful if there is a problem matching the speed of the final destination device with the source device as is frequently faced in network-based backup systems. It can also serve as a centralized location for applying other data manipulation techniques.

Managing the backup process
	This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2014) (Learn how and when to remove this template message)

As long as new data are being created and changes are being made, backups will need to be performed at frequent intervals. Individuals and organizations with anything from one computer to thousands of computer systems all require protection of data. The scales may be very different, but the objectives and limitations are essentially the same. Those who perform backups need to know how successful the backups are, regardless of scale.
Objectives

Recovery point objective (RPO) 
    The point in time that the restarted infrastructure will reflect. Essentially, this is the roll-back that will be experienced as a result of the recovery. The most desirable RPO would be the point just prior to the data loss event. Making a more recent recovery point achievable requires increasing the frequency of synchronization between the source data and the backup repository.[14][15]
Recovery time objective (RTO) 
    The amount of time elapsed between disaster and restoration of business functions.[16]
Data security 
    In addition to preserving access to data for its owners, data must be restricted from unauthorized access. Backups must be performed in a manner that does not compromise the original owner's undertaking. This can be achieved with data encryption and proper media handling policies.
Data retention period 
    Regulations and policy can lead to situations where backups are expected to be retained for a particular period, but not any further. Retaining backups after this period can lead to unwanted liability and sub-optimal use of storage media.

Limitations

An effective backup scheme will take into consideration the limitations of the situation.

Backup window
    The period of time when backups are permitted to run on a system is called the backup window. This is typically the time when the system sees the least usage and the backup process will have the least amount of interference with normal operations. The backup window is usually planned with users' convenience in mind. If a backup extends past the defined backup window, a decision is made whether it is more beneficial to abort the backup or to lengthen the backup window.
Performance impact
    All backup schemes have some performance impact on the system being backed up. For example, for the period of time that a computer system is being backed up, the hard drive is busy reading files for the purpose of backing up, and its full bandwidth is no longer available for other tasks. Such impacts should be analyzed.
Costs of hardware, software, labor
    All types of storage media have a finite capacity with a real cost. Matching the correct amount of storage capacity (over time) with the backup needs is an important part of the design of a backup scheme. Any backup scheme has some labor requirement, but complicated schemes have considerably higher labor requirements. The cost of commercial backup software can also be considerable.
Network bandwidth
    Distributed backup systems can be affected by limited network bandwidth.

Implementation

Meeting the defined objectives in the face of the above limitations can be a difficult task. The tools and concepts below can make that task more achievable.

Scheduling
    Using a job scheduler can greatly improve the reliability and consistency of backups by removing part of the human element. Many backup software packages include this functionality.
Authentication
    Over the course of regular operations, the user accounts and/or system agents that perform the backups need to be authenticated at some level. The power to copy all data off of or onto a system requires unrestricted access. Using an authentication mechanism is a good way to prevent the backup scheme from being used for unauthorized activity.
Chain of trust 
    Removable storage media are physical items and must only be handled by trusted individuals. Establishing a chain of trusted individuals (and vendors) is critical to defining the security of the data.

Measuring the process

To ensure that the backup scheme is working as expected, key factors should be monitored and historical data maintained.

Backup validation 
    (also known as "backup success validation") Provides information about the backup, and proves compliance to regulatory bodies outside the organization: for example, an insurance company in the USA might be required under HIPAA to demonstrate that its client data meet records retention requirements.[17] Disaster, data complexity, data value and increasing dependence upon ever-growing volumes of data all contribute to the anxiety around and dependence upon successful backups to ensure business continuity. Thus many organizations rely on third-party or "independent" solutions to test, validate, and optimize their backup operations (backup reporting).
Reporting
    In larger configurations, reports are useful for monitoring media usage, device status, errors, vault coordination and other information about the backup process.
Logging
    In addition to the history of computer generated reports, activity and change logs are useful for monitoring backup system events.
Validation
    Many backup programs use checksums or hashes to validate that the data was accurately copied. These offer several advantages. First, they allow data integrity to be verified without reference to the original file: if the file as stored on the backup medium has the same checksum as the saved value, then it is very probably correct. Second, some backup programs can use checksums to avoid making redundant copies of files, and thus improve backup speed. This is particularly useful for the de-duplication process.
Monitored backup
    Backup processes are monitored by a third party monitoring center, which alerts users to any errors that occur during automated backups. Monitored backup requires software capable of pinging[clarification needed] the monitoring center's servers in the case of errors. Some monitoring services also allow collection of historical meta-data, that can be used for Storage Resource Management purposes like projection of data growth, locating redundant primary storage capacity and reclaimable backup capacity. 

Backup software are computer programs used to perform backup; they create supplementary exact copies of files, databases or entire computers. These programs may later use the supplementary copies to restore the original contents in the event of data loss.[1]

Contents

    1 Key features
        1.1 Volumes
        1.2 Data compression
        1.3 Access to open files
        1.4 Differential and incremental backups
        1.5 Schedules
        1.6 Encryption
        1.7 Transaction mechanism
    2 See also
    3 References

Key features

There are several features of backup software that make it more effective in backing up data.
Volumes
Main article: Volume (compression)

Voluming allows the ability to compress and split backup data into separate parts for storage on smaller, removable media such as CDs. It was often used because CDs were easy to transport off-site and inexpensive compared to hard drives or servers.

However, the recent increase in hard drive capacity and decrease in drive cost has made voluming a far less popular solution. The introduction of small, portable, durable USB drives, and the increase in broadband capacity has provided easier and more secure methods of transporting backup data off-site.
Data compression
Main articles: Data compression and Data deduplication

Since hard drive space has cost, compressing the data will reduce the size allowing for less drive space to be used to save money.
Access to open files
Main article: File locking

Many backup solutions offer a plug-in for access to exclusive, in use, and locked files.
Differential and incremental backups
Main article: Backup rotation scheme

Backup solutions generally support differential backups and incremental backups in addition to full backups, so only material that is newer or changed compared to the backed up data is actually backed up. The effect of these is to increase significantly the speed of the backup process over slow networks while decreasing space requirements.
Schedules
Main article: Job scheduler

Backup schedules are usually supported to reduce maintenance of the backup tool and increase the reliability of the backups.
Encryption
Main article: Encryption

To prevent data theft, some backup software offers cryptography features to protect the backup.
Transaction mechanism
Main article: Database management system § Transaction mechanism

To prevent loss of previously backed up data during a backup, some backup software (e.g. Areca Backup, Argentum Backup) offer Transaction mechanism (with commit / rollback management) for all critical processes (such as backups or merges) to guarantee the backups' integrity.
A data management plan or DMP is a formal document that outlines how you will handle your data both during your research, and after the project is completed.[1] The goal of a data management plan is to consider the many aspects of data management, metadata generation, data preservation, and analysis before the project begins; this ensures that data are well-managed in the present, and prepared for preservation in the future.

Contents

    1 Importance
    2 Major Components
        2.1 Information about data & data format
        2.2 Metadata content and format
        2.3 Policies for access, sharing, and re-use
        2.4 Long-term storage and data management
        2.5 Budget
    3 NSF Data Management Plan
    4 ESRC Data Management Plan
    5 References
    6 Further reading
    7 External links

Importance

Preparing a data management plan before data are collected ensures that data are in the correct format, organized well, and better annotated.[2] This saves time in the long term because there is no need to re-organize, re-format, or try to remember details about data. It also increases research efficiency since both the data collector and other researchers will be able to understand and use well-annotated data in the future. One component of a good data management plan is data archiving and preservation. By deciding on an archive ahead of time, the data collector can format data during collection to make its future submission to a database easier. If data are preserved, they are more relevant since they can be re-used by other researchers. It also allows the data collector to direct requests for data to the database, rather than address requests individually. Data that are preserved have the potential to lead to new, unanticipated discoveries, and they prevent duplication of scientific studies that have already been conducted. Data archiving also provides insurance against loss by the data collector.

Funding agencies are beginning to require data management plans as part of the proposal and evaluation process.[3]
Major Components
Information about data & data format

    Include a description of data to be produced by the project.[4] This might include (but is not limited to) data that are:
        Experimental
        Observational
        Raw or derived
        Physical collections
        Models
        Simulations
        Curriculum materials
        Software
        Images
    How will the data be acquired? When and where will they be acquired?
    After collection, how will the data be processed? Include information about
        Software used
        Algorithms
        Scientific workflows
    Describe the file formats that will be used, justify those formats, and describe the naming conventions used.
    Identify the quality assurance & quality control measures that will be taken during sample collection, analysis, and processing.
    If existing data are used, what are their origins? How will the data collected be combined with existing data? What is the relationship between the data collected and existing data?
    How will the data be managed in the short-term? Consider the following:
        Version control for files
        Backing up data and data products
        Security & protection of data and data products
        Who will be responsible for management

Metadata content and format

Metadata are the contextual details, including any information important for using data. This may include descriptions of temporal and spatial details, instruments, parameters, units, files, etc. Metadata is commonly referred to as “data about data”.[5] Consider the following:

    What metadata are needed? Include any details that make data meaningful.
    How will the metadata be created and/or captured? Examples include lab notebooks, GPS hand-held units, Auto-saved files on instruments, etc.
    What format will be used for the metadata? Consider the metadata standards commonly used in the scientific discipline that contains your work. There should be justification for the format chosen.

Policies for access, sharing, and re-use

    Describe any obligations that exist for sharing data collected. These may include obligations from funding agencies, institutions, other professional organizations, and legal requirements.
    Include information about how data will be shared, including when the data will be accessible, how long the data will be available, how access can be gained, and any rights that the data collector reserves for using data.
    Address any ethical or privacy issues with data sharing
    Address intellectual property & copyright issues. Who owns the copyright? What are the institutional, publisher, and/or funding agency policies associated with intellectual property? Are there embargoes for political, commercial, or patent reasons?
    Describe the intended future uses/users for the data
    Indicate how the data should be cited by others. How will the issue of persistent citation be addressed? For example, if the data will be deposited in a public archive, will the dataset have a digital object identifier (doi) assigned to it?

Long-term storage and data management

    Researchers should identify an appropriate archive for long-term preservation of their data. By identifying the archive early in the project, the data can be formatted, transformed, and documented appropriately to meet the requirements of the archive. Researchers should consult colleagues and professional societies in their discipline to determine the most appropriate database, and include a backup archive in their data management plan in case their first choice goes out of existence.
    Early in the project, the primary researcher should identify what data will be preserved in an archive. Usually, preserving the data in its most raw form is desirable, although data derivatives and products can also be preserved.
    An individual should be identified as the primary contact person for archived data, and ensure that contact information is always kept up-to-date in case there are requests for data or information about data.

Budget

Data management and preservation costs may be considerable, depending on the nature of the project. By anticipating costs ahead of time, researchers ensure that the data will be properly managed and archived. Potential expenses that should be considered are

    Personnel time for data preparation, management, documentation, and preservation
    Hardware and/or software needed for data management, backing up, security, documentation, and preservation
    Costs associated with submitting the data to an archive

The data management plan should include how these costs will be paid.
NSF Data Management Plan

All grant proposals submitted to NSF must include a Data Management Plan that is no more than two pages.[6] This is a supplement (not part of the 15 page proposal) and should describe how the proposal will conform to the Award and Administration Guide policy (see below). It may include the following:

    The types of data
    The standards to be used for data and metadata format and content
    Policies for access and sharing
    Policies and provisions for re-use
    Plans for archiving data

Policy summarized from the NSF Award and Administration Guide, Section 4 (Dissemination and Sharing of Research Results):[7]

    Promptly publish with appropriate authorship
    Share data, samples, physical collections, and supporting materials with others, within a reasonable time frame
    Share software and inventions
    Investigators can keep their legal rights over their intellectual property, but they still have to make their results, data, and collections available to others
    Policies will be implemented via
        Proposal review
        Award negotiations and conditions
        Support/incentives

ESRC Data Management Plan

Since 1995, the UK's Economic and Social Research Council (ESRC) have had a research data policy in place. The current ESRC Research Data Policy states that research data created as a result of ESRC-funded research should be openly available to the scientific community to the maximum extent possible, through long-term preservation and high quality data management.[8]

ESRC requires a data management plan for all research award applications where new data are being created. Such plans are designed to promote a structured approach to data management throughout the data lifecycle, resulting in better quality data that is ready to archive for sharing and re-use. The UK Data Service, the ESRC's flagship data service, provides practical guidance on research data management planning suitable for social science researchers in the UK and around the world.[9][10]

ESRC has a longstanding arrangement with the UK Data Archive, based at the University of Essex, as a place of deposit for research data, with award holders required to offer data resulting from their research grants via the UK Data Service.[11] The Archive enables data re-use by preserving data and making them available to the research and teaching communities.
 DATA MANAGEMENT SAMPLE PLAN 
2 
This study will only collect non-sensitive data. No pers
onal identifiers will be recorded or retained by the 
researchers  in  any  form.  There  are  no  copyright  or
  licensing  issues  associated  with  the  data  being  
submitted. 
Access, Sharing and Re-use of Data 
The  researchers  associated  with  this  study  are  no
t  aware  of  any  reasons  which  might  prohibit  the  
sharing  and  re-use  of  the  data  being  submitted.  Th
e  researchers  are  not  required  to  make  this  data  
available  publicly  but  have  elected  to  do  so.  The  da
ta  being  submitted  will  be  made  publicly  available  
through the Odum Institute for Research in Social Sc
ience located at UNC-CH by January 1, 2013. There 
will  be  no  additional  restrictions  or  permissions  required  for  accessing  the  data.  Findings  will  be  
published by the researchers based on this data; the 
estimated date of publication is June 1, 2012. There 
is  an  agreement  regarding  the  right  of  the  original  
data  collector,  creator  or  
principal  investigator  for  
first  use  of  the  data.  The  specified  embargo  period  associated  with  the  data  being  submitted  extends  
from  the  projected  conclusion  date  for  initial  
research  (December  31,  2011)  until  six  months  after  
projected publication date for the fi
ndings (June 1, 2012). The embargo 
will be lifted by January 1, 2013. 
Data Standards and Capture 
The associated data types will be captured using Qual
trics survey software and analyzed using SPSS data 
analytics tools. The researchers are not aware of any 
issues regarding the effects or limitations of these 
formats regarding the data being submitted. 
Metadata 
General  metadata  related  to  the  survey  topic  wi
ll  be  created  for  the  data  being  submitted.  The  
associated  metadata  will  be  manually  created  in  
XML  file  format.  DDI  metadata  standards  will  be  
applied during the creation of the metadata.  
Security, Storage, Management and Back-Up of Data 
The Odum Institute’s experience with, and commitment 
to, secure data archiving is well established and 
is  in  keeping  with  established  
UNC  Information  Security  Policies
.  During  the  implementation  of  the  
survey,  associated  research  data  will  be  physicall
y  stored  on  a  password-protected  secure  server  
maintained  by  UNC-CH  using  standard  SPSS  file  form
ats.  No  data  will  reside  on  portable  or  laptop  
devices, and no other external media/format(s) will be used for data storage.  
Research  data  is  backed  up  on  a  daily  basis.  The  
researchers  are  currently  responsible  for  storage,  
maintenance  and  back-up  of  the  data.  The  specific  storage  volume  of  the  data  being  submitted  will  be  
not  more  than  1GB  maximum.  The  long-term  strategy
  for  the  maintenance,  curation  and  archiving  of  
the  data  will  be  implemented  when  the  data  and  associated  research  are  migrated  to  the  Odum  
Institute for archiving usin
g the Odum Institute DVN. 
Preservation, Review and Long-Term Management of Data 
ODUM INSTITUTE 
−
 DATA MANAGEMENT SAMPLE PLAN 
3 
Data  collected  during  this  study  will  be  archived  with
  the  Odum  Institute  at  UNC-CH.  The  data  will  be  
stored in a specific virtual archive and will be ma
de publicly available through 
the Odum Institute DVN. 
As  a  result  of  this  arrangement,  there  are  no  specif
ic  financial  considerations  of  which  the  researchers  
are  currently  aware  which  might  impact  the  long-term  management  of  the  data.  The  research  and  
archival staff of the Odum Institute will review this
 DMP upon accession of the data in order to ensure 
and demonstrate compliance. The DMP will again be re
viewed by Odum Institute research and archival 
staff prior to ingest and release into the Odum DVN
Write a data management plan

A data management plan (DMP) will help you manage your data, meet funder requirements, and help others use your data if shared.

The DMPTool is a web-based tool that helps you construct data management plans using templates that address specific funder requirements. From within this tool, you can save your plans, access MIT-specific information & resources, and request a review of your DMP by a member of our Data Management Services team. To access the MIT-customized DMPTool, choose “Massachusetts Institute of Technology” as your institution to log via Touchstone.

Alternatively, you can use the questions below and any specific data management requirements from your funding agency to write your data management plan. Additional resources for creating plans are also provided below.

     Project, experiment, and data description
        What’s the purpose of the research?
        What is the data? How and in what format will the data be collected? Is it numerical data, image data, text sequences, or modeling data?
        How much data will be generated for this research?
        How long will the data be collected and how often will it change?
        Are you using data that someone else produced? If so, where is it from?
        Who is responsible for managing the data? Who will ensure that the data management plan is carried out?
    Documentation, organization, and storage
        What documentation will you be creating in order to make the data understandable by other researchers?
        Are you using metadata that is standard to your field? How will the metadata be managed and stored?
        What file formats will be used? Do these formats conform to an open standard and/or are they proprietary?
        Are you using a file format that is standard to your field? If not, how will you document the alternative you are using?
        What directory and file naming convention will be used?
        What are your local storage and backup procedures? Will this data require secure storage?
        What tools or software are required to read or view the data?
    Access, sharing, and re-use
        Who has the right to manage this data? Is it the responsibility of the PI, student, lab, MIT, or funding agency?
        What data will be shared, when, and how?
        Does sharing the data raise privacy, ethical, or confidentiality concerns?  Do you have a plan to protect or anonymize data, if needed?
        Who holds intellectual property rights for the data and other information created by the project? Will any copyrighted or licensed material be used? Do you have permission to use/disseminate this material?
        Are there any patent- or technology-licensing-related restrictions on data sharing associated with this grant? The Technology Licensing Office (TLO) can provide this information.
        Will this research be published in a journal that requires the underlying data to accompany articles?
        Will there be any embargoes on the data?
        Will you permit re-use, redistribution, or the creation of new tools, services, data sets, or products (derivatives)? Will commercial use be allowed?
    Archiving
        How will you be archiving the data? Will you be storing it in an archive or repository for long-term access? If not, how will you preserve access to the data?
        Is a discipline-specific repository available? If not, you could consider depositing your data into DSpace@MIT. Email us at data-management@mit.edu if you’re interested in using DSpace@MIT to store your data.
        How will you prepare data for preservation or data sharing? Will the data need to be anonymized or converted to more stable file formats?
        Are software or tools needed to use the data? Will these be archived?
        How long should the data be retained? 3-5 years, 10 years, or forever?

