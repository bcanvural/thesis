Big Data Analytics platforms
Big Data tools Hadoop, Spark
Distributed computing tools a plus Spark, MapReduce, Hadoop, Hive
Real time and streaming analytics systems Flume, Kafka, Storm
Hadoop Ecosystem/platform
Spotfire
Azure Data Analytics platforms HDInsight, APS and PDW
Amazon Data Analytics platform Kinesis, EMR
Other cloud based Data Analytics platforms HortonWorks, Vertica LexisNexis HPCC System
hadoop
spark
YARN
HDFS
MapReduce
Apache Flink
Apache Storm
Apache Samza
Apache Kafka
Cloudera
software framework
big data framework
User centered approaches are well known in the visualization community (although not always implemented) [D'Amico et al. 2005, Munzner et al. 2009]. Jointly developing the visualizations themselves, however, is rather rare. As we have very good experience with co-creative techniques in design and innovation, we wanted to apply them to the domain of data visualization as well. For example, we tried to experiment with data sets during a day-long workshop with a larger group of stakeholders (a session we called the “data picnic” because everyone brought his/her data and tools).
Visualization

For this paper, we focused on a pixel oriented technique [Keim 2000] to fullfill requirements such as visualization of raw data or a chronological view of data to preserve the course of events. We stack graphical representations for various parameters of a log line (such as IP, user name, request or message) so that we get small columns for each log line. Lining up these stacks produces a dense visual representation with distinct patterns. This is why we call it the Pixel Carpet. Other subgroups of our research group took different approaches that can be found at other places in this blog.
Snapshot of the Pixel Carpet interface. Each "multi pixel" represents one log line, as it a appears at the bottom of the screen.Snapshot of the Pixel Carpet interface. Each “multi pixel” represents one log line, as it a appears at the bottom of the screen.
Data and Code

Our data sources included an ssh log (~13.000 lines, unpublished for privacy reasons) and an Apache (web server) access log (~145.000 lines, unpublished), and ~4.500 lines (raw data available, including countries from ip2geo .csv | .json ).

We implemented our ideas in a demonstrator in plain HTML/JavaScript (demo online – caution, will heavily stress your CPU). It helped us iterate quickly and evaluate the idea at various stages, also with new stakeholders. While the code achieves what we need, we are also aware that computing performance is rather bad. If you want to take a look or even improve it, you can find it on github.

To bring it closer to a productive tool, we would turn the Pixel Carpet into a plugin for state-of-the-art data processing engines such as ElasticSearch/Kibana or splunk (scriptable with d3.js since version 6).
Time Series Visualizations – An overview
by Kim Albrecht	on October 17, 2013, 1 comment

“Time-series — sets of values changing over time”
A Tour Through the Visualization Zoo
http://hci.stanford.edu/jheer/files/zoo/

This description of the word “Time-Series” is very close to the explanation in Oxfords dictionary which adds that the word comes from a statistic background and often the intervals are equal within the time-series.
http://www.oxforddictionaries.com/definition/english/time-series?q=time-series

Within our research project we are mainly interested in the visualization part within the vast field of statistics. In the book “The Visual Display of Quantitative Information” Edward Tufte defines time-series visualizations as:

“With one dimension marching along to the regular rhythm of seconds, minutes, hours, days, weeks, months, years, centuries, or millennia, the natural ordering of the time scale gives this design a strength and efficiency of interpretation found in no other graphic arrangement.”
Edward R. Tufte
The Visual Display of Quantitative Information
p. 28

Classical datasets of time series visualizations are temperature, wind, condensation (or any other kind of weather measurement), stock data, population change, electricity usage etc. the field is so vast that Tufte writes that in a study that analysed graphics between 1974 and 1980 75% of the graphics where time-series visualizations. Obviously more than 30 years later the field has changed but time-series still seams to be an important part within the area.

In my opinion most Security Network Data doesn’t provide information with changing values over time initially. For example Flow Data is structured through nodes and edges with additional information. These single incidents in time don’t hold the same characteristics as usual time-series datasets where one value changes. But on a certain level of abstraction (for example by counting incidents within set timeframes) or by combining time-series with other methods like network visualizations this kind of graphics could be very helpful for us.

This article first summarises a few classical time-series examples and than looks at recent developments in the field.

The first time-series visualization was designed in the tenth or possibly eleventh century. It shows the changing positions of the planets with the time on the x-axis.

As we will see the use of the x-axis is still the most common form of presenting time-series graphics. Nathan Yau gives an overview of the most common forms of time-series visualizations in his book “data points” which are in his opinion bar graphs, line charts, dot plots & dot-bar graphs. All of this charts are actually similar in what they do. The only difference is the graphical representation of the data. While all of them use the time dimension on the x-axis, Nathan Yau gives two examples for different representation methods. Radial plots, which are similar to line charts, just circular and calendar heat maps.

Jeffrey Heer, Michael Bostock, and Vadim Ogievetsky from Stanford University are giving a different overview of time-series visualizations in their article “A Tour Through the Visualization Zoo”. Their overview starts with index charts, which is an interactive line chart.
Index Chart

Stacked Graphs. Which are Area Charts that are stacked on top of each other. They are also called stream graphs. What makes them special is the fact that we get a visual summation of all time-series values.

The controversy around stacked graphs is very big. Alberto Cairo, graphics director at El Mundo Online wrote in a blog article that stacked graphs are “one of the worst graphics the New York Times have published – ever!” on the other hand the publisher of the first paper on stacked graphs wrote: “simplifying the user’s task of tracking individual themes through time by providing a continuous ‘flow’ from one time point to the next”. Furthermore, “we believe this metaphor is familiar and easy to understand and that it requires little cognitive effort to interpret the visualization” both points seam valid to me the cognitive effort needed in some contemporary visualizations is so high that it becomes hard to understand them without putting a lot of effort into them. Stacked Graphs are very simple to understand for the complexity they hold but the information output that can be generated from them is questionable. Andy Kirk from visualisingdata.com credits both sides very fairly in his blog article about the graphs with these comments:

“… a streamgraph is a fantastic solution to displaying large data sets to a mass audience.”

“The main problem facing static streamgraphs lies in the difficulty of reading data points formed by uncommon shapes.”

Tools: D3, Processing

Paper: ThemeRiver: Visualizing Theme Changes over Time,

Stacked Graphs – Geometry & Aesthetics

Example: The Ebb and Flow of Movies, How Different Groups Spend Their Day, Trace (this one is about visualizing wireless networks)


Stacked Graph

Small Multiples are multiple time-series graphs (what kind these graphs are is another question, in this case, area charts) arranged within a grid. Small multiples are more use full to understand different datasets on its own and not as a summary apposed to the stacked graphs.
Small Multiples

The last example from the article are horizon graphs. These are actual also area charts which are mirrored and separated by occupacity. This is especially interesting in combination with small multiples because the “data density” is much higher than which classic area charts which leads to more information in a smaller space. An important factor when we are dealing with big datasets.
Horizon Graph

There is some interesting research about the usefulness of horizon graphs that I recommend: Tool, Paper, Article



The list of graphics from the Stanford Group are much more contemporary than the examples from Nathan Yau, but still all of these examples use the same mechanism to visualize time-series data by using one axis as a dimension for time. This now more than 1.000 years old way to visualize time is helpful and very common but might not always be the best choice. As we know from scatter-plot visualizations our two space dimensions within a graphic are maybe the most powerful ones for pattern recognition and time might not be the main factor to identify these patterns. So what other ways are there to use time as a dimension within a visualization a part from space?

Animation:
At least since Hans Roslings famous TED talks the usage of animation for displaying time is common and it seams to be the most obvious way to visualize time very literal though time. But the technique needs to be used with caution.
Tamara Munzners visualization principles give a great insight on page 59 why visualizing time with animation is dangerous:

Principle: external cognition vs. internal memory

    easy to compare by moving eyes between side-by-side views –harder to compare visible item to memory of what you saw

Implications for animation

    great for choreographed storytelling
    great for transitions between two states
    poor for many states with changes everywhere

There is also a paper about the topic which gives more insights into the problem.

Small multiples:
I already mentioned small multiples above but as I raised before the idea behind small multiples is more of a frame for visualizations than an actual kind of visualization. Like this we can also use each multiple as a timeframe. A beautiful example of small multiples with time as a dimension comes from the NYTimes Graphics department.

Binning time in bubbles:
The idea here is to use bubble charts where the time dimension gets binned by minutes, days, years etc. into one bubble and compared to each other. In the Nasdaq 100 Index example each year is represented by one bubble.

Scatterplots:
Scatterplots where time is displayed as connected points against two variables. This is similar to the animation idea. But in this case the animated dots leave behind a path behind. Also here the NYTimes has a good example.
Data visualization or data visualisation is viewed by many disciplines as a modern equivalent of visual communication. It involves the creation and study of the visual representation of data, meaning "information that has been abstracted in some schematic form, including attributes or variables for the units of information".[1]

A primary goal of data visualization is to communicate information clearly and efficiently via statistical graphics, plots and information graphics. Numerical data may be encoded using dots, lines, or bars, to visually communicate a quantitative message.[2] Effective visualization helps users analyze and reason about data and evidence. It makes complex data more accessible, understandable and usable. Users may have particular analytical tasks, such as making comparisons or understanding causality, and the design principle of the graphic (i.e., showing comparisons or showing causality) follows the task. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.

Data visualization is both an art and a science. It is viewed as a branch of descriptive statistics by some, but also as a grounded theory development tool by others. The rate at which data is generated has increased. Data created by internet activity and an expanding number of sensors in the environment, such as satellites, are referred to as "Big Data". Processing, analyzing and communicating this data present a variety of ethical and analytical challenges for data visualization. The field of data science and practitioners called data scientists have emerged to help address this challenge.[3]

Contents

    1 Overview
    2 Characteristics of effective graphical displays
    3 Quantitative messages
    4 Visual perception and data visualization
        4.1 Human perception/cognition and data visualization
    5 History of Data Visualization
    6 Terminology
    7 Examples of diagrams used for data visualization
    8 Other perspectives
    9 Data presentation architecture
        9.1 Objectives
        9.2 Scope
        9.3 Related fields
    10 See also
        10.1 People (Historical)
            10.1.1 People (active today)
    11 References
    12 Further reading
    13 External links

Overview
Data visualization is one of the steps in analyzing data and presenting it to users.

Data visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e.g., points, lines or bars) contained in graphics. The goal is to communicate information clearly and efficiently to users. It is one of the steps in data analysis or data science. According to Friedman (2008) the "main goal of data visualization is to communicate information clearly and effectively through graphical means. It doesn't mean that data visualization needs to look boring to be functional or extremely sophisticated to look beautiful. To convey ideas effectively, both aesthetic form and functionality need to go hand in hand, providing insights into a rather sparse and complex data set by communicating its key-aspects in a more intuitive way. Yet designers often fail to achieve a balance between form and function, creating gorgeous data visualizations which fail to serve their main purpose — to communicate information".[4]

Indeed, Fernanda Viegas and Martin M. Wattenberg have suggested that an ideal visualization should not only communicate clearly, but stimulate viewer engagement and attention.[5]

Not limited to the communication of an information, a well-crafted data visualization is also a way to a better understanding of the data (in a data-driven research perspective),[6] as it helps uncover trends, realize insights, explore sources, and tell stories.[7]

Data visualization is closely related to information graphics, information visualization, scientific visualization, exploratory data analysis and statistical graphics. In the new millennium, data visualization has become an active area of research, teaching and development. According to Post et al. (2002), it has united scientific and information visualization.[8]
Characteristics of effective graphical displays
Charles Joseph Minard's 1869 diagram of Napoleon's March - an early example of an information graphic.

The greatest value of a picture is when it forces us to notice what we never expected to see.
John Tukey[9]

Professor Edward Tufte explained that users of information displays are executing particular analytical tasks such as making comparisons or determining causality. The design principle of the information graphic should support the analytical task, showing the comparison or causality.[10]

In his 1983 book The Visual Display of Quantitative Information, Edward Tufte defines 'graphical displays' and principles for effective graphical display in the following passage: "Excellence in statistical graphics consists of complex ideas communicated with clarity, precision and efficiency. Graphical displays should:

    show the data
    induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production or something else
    avoid distorting what the data has to say
    present many numbers in a small space
    make large data sets coherent
    encourage the eye to compare different pieces of data
    reveal the data at several levels of detail, from a broad overview to the fine structure
    serve a reasonably clear purpose: description, exploration, tabulation or decoration
    be closely integrated with the statistical and verbal descriptions of a data set.

Graphics reveal data. Indeed graphics can be more precise and revealing than conventional statistical computations."[11]

For example, the Minard diagram shows the losses suffered by Napoleon's army in the 1812–1813 period. Six variables are plotted: the size of the army, its location on a two-dimensional surface (x and y), time, direction of movement, and temperature. The line width illustrates a comparison (size of the army at points in time) while the temperature axis suggests a cause of the change in army size. This multivariate display on a two dimensional surface tells a story that can be grasped immediately while identifying the source data to build credibility. Tufte wrote in 1983 that: "It may well be the best statistical graphic ever drawn."[11]

Not applying these principles may result in misleading graphs, which distort the message or support an erroneous conclusion. According to Tufte, chartjunk refers to extraneous interior decoration of the graphic that does not enhance the message, or gratuitous three dimensional or perspective effects. Needlessly separating the explanatory key from the image itself, requiring the eye to travel back and forth from the image to the key, is a form of "administrative debris." The ratio of "data to ink" should be maximized, erasing non-data ink where feasible.[11]

The Congressional Budget Office summarized several best practices for graphical displays in a June 2014 presentation. These included: a) Knowing your audience; b) Designing graphics that can stand alone outside the context of the report; and c) Designing graphics that communicate the key messages in the report.[12]
Quantitative messages
A time series illustrated with a line chart demonstrating trends in U.S. federal spending and revenue over time.
A scatterplot illustrating negative correlation between two variables (inflation and unemployment) measured at points in time.

Author Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message:

    Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.
    Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons.
    Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.
    Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.
    Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis. A boxplot helps visualize key statistics about the distribution, such as median, quartiles, outliers, etc.
    Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.
    Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.
    Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.[2][13]

Analysts reviewing a set of data may consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of exploratory data analysis.
Visual perception and data visualization

A human can distinguish differences in line length, shape orientation, and color (hue) readily without significant processing effort; these are referred to as "pre-attentive attributes." For example, it may require significant time and effort ("attentive processing") to identify the number of times the digit "5" appears in a series of numbers; but if that digit is different in size, orientation, or color, instances of the digit can be noted quickly through pre-attentive processing.[14]

Effective graphics take advantage of pre-attentive processing and attributes and the relative strength of these attributes. For example, since humans can more easily process differences in line length than surface area, it may be more effective to use a bar chart (which takes advantage of line length to show comparison) rather than pie charts (which use surface area to show comparison).[14]
Human perception/cognition and data visualization

There is a human side to data visualization. With the "studying [of] human perception and cognition ..." we are better able to understand the target of the data which we display.[15] Cognition refers to processes in human beings like perception, attention, learning, memory, thought, concept formation, reading, and problem solving.[16] The basis of data visualization evolved because as a picture is worth a thousand words, data displayed graphically allows for an easier comprehension of the information. Proper visualization provides a different approach to show potential connections, relationships, etc. which are not as obvious in non-visualized quantitative data. Visualization becomes a means of data exploration. Human brain neurons involve multiple functions but 2/3 of the brain's neurons are dedicated to vision.[17] With a well-developed sense of sight, analysis of data can be made on data, whether that data is quantitative or qualitative. Effective visualization follows from understanding the processes of human perception and being able to apply this to intuitive visualizations is important. Understanding how humans see and organize the world is critical to effectively communicating data to the reader. This leads to more intuitive designs.
History of Data Visualization

There is a history of data visualization: beginning in the 2nd century C.E. with data arrangement into columns and rows and evolving to the initial quantitative representations in the 17th century.[15] According to the Interaction Design Foundation, French philosopher and mathematician René Descartes laid the ground work for Scotsman William Playfair. Descartes developed a two-dimensional coordinate system for displaying values, which in the late 18th century Playfair saw potential for graphical communication of quantitative data.[15] In the second half of the 20th century, Jacques Bertin used quantitative graphs to represent information "intuitively, clearly, accurately, and efficiently".[15] John Tukey and more notably Edward Tufte pushed the bounds of data visualization. Tukey with his new statistical approach: exploratory data analysis and Tufte with his book "The Visual Display of Quantitative Information", the path was paved for refining data visualization techniques for more than statisticians. With the progression of technology came the progression of data visualization; starting with hand drawn visualizations and evolving into more technical applications – including interactive designs leading to software visualization.[18] Programs like SAS, SOFA, R, Minitab, and more allow for data visualization in the field of statistics. Other data visualization applications, more focused and unique to individuals, programming languages such as D3, Python and JavaScript help to make the visualization of quantitative data a possibility.
Terminology

Data visualization involves specific terminology, some of which is derived from statistics. For example, author Stephen Few defines two types of data, which are used in combination to support a meaningful analysis or visualization:

    Categorical: Text labels describing the nature of the data, such as "Name" or "Age". This term also covers qualitative (non-numerical) data.
    Quantitative: Numerical measures, such as "25" to represent the age in years.

Two primary types of information displays are tables and graphs.

    A table contains quantitative data organized into rows and columns with categorical labels. It is primarily used to look up specific values. In the example above, the table might have categorical column labels representing the name (a qualitative variable) and age (a quantitative variable), with each row of data representing one person (the sampled experimental unit or category subdivision).
    A graph is primarily used to show relationships among data and portrays values encoded as visual objects (e.g., lines, bars, or points). Numerical values are displayed within an area delineated by one or more axes. These axes provide scales (quantitative and categorical) used to label and assign values to the visual objects. Many graphs are also referred to as charts.[19]

KPI Library has developed the "Periodic Table of Visualization Methods
," an interactive chart displaying various data visualization methods. It includes six types of data visualization methods: data, information, concept, strategy, metaphor and compound.[20]
Examples of diagrams used for data visualization
	Name 	Visual Dimensions 	Example Usages
Bar chart of tips by day of week
	Bar Chart

    length/count
    category
    (color)



    Comparison of values, such as sales performance for several persons or businesses in a single time period. For a single variable measured over time (trend) a line chart is preferable.

Histogram of housing prices
	Histogram

    bin limits
    count/length
    (color)



    Determining frequency of annual stock market percentage returns within particular ranges (bins) such as 0-10%, 11-20%, etc. The height of the bar represents the number of observations (years) with a return % in the range represented by the bin.

Basic scatterplot of two variables
	Scatter plot

    x position
    y position
    (symbol/glyph)
    (color)
    (size)



    Determining the relationship (e.g., correlation) between unemployment (x) and inflation (y) for multiple time periods.

Scatter Plot
	Scatter plot (3D)

    position x
    position y
    position z
    color


Network Analysis
	Network

    nodes size
    nodes color
    ties thickness
    ties color
    spatialization



    Finding clusters in the network (e.g. grouping Facebook friends into different clusters).
    Determining the most influential nodes in the network (e.g. A company wants to target a small group of people on Twitter for a marketing campaign).

Streamgraph
	Streamgraph

    width
    color
    time (flow)

Treemap
	Treemap

    size
    color



    disk space by location / file type

Gantt Chart
	Gantt chart

    color
    time (flow)



    schedule / progress, e.g. in project planning

Heat Map
	Heat Map

    row
    column
    cluster
    color



    Analyzing risk, with green, yellow and red representing low, medium, and high risk, respectively.

Other perspectives

There are different approaches on the scope of data visualization. One common focus is on information presentation, such as Friedman (2008) presented it. In this way Friendly (2008) presumes two main parts of data visualization: statistical graphics, and thematic cartography.[1] In this line the "Data Visualization: Modern Approaches" (2007) article gives an overview of seven subjects of data visualization:[21]

    Articles & resources
    Displaying connections
    Displaying data
    Displaying news
    Displaying websites
    Mind maps
    Tools and services

All these subjects are closely related to graphic design and information representation.

On the other hand, from a computer science perspective, Frits H. Post (2002) categorized the field into a number of sub-fields:[8]

[22]

    Information visualization
    Interaction techniques and architectures
    Modelling techniques
    Multiresolution methods
    Visualization algorithms and techniques
    Volume visualization

Data presentation architecture
A data visualization from social media

Data presentation architecture (DPA) is a skill-set that seeks to identify, locate, manipulate, format and present data in such a way as to optimally communicate meaning and proper knowledge.

Historically, the term data presentation architecture is attributed to Kelly Lautt:[23] "Data Presentation Architecture (DPA) is a rarely applied skill set critical for the success and value of Business Intelligence. Data presentation architecture weds the science of numbers, data and statistics in discovering valuable information from data and making it usable, relevant and actionable with the arts of data visualization, communications, organizational psychology and change management in order to provide business intelligence solutions with the data scope, delivery timing, format and visualizations that will most effectively support and drive operational, tactical and strategic behaviour toward understood business (or organizational) goals. DPA is neither an IT nor a business skill set but exists as a separate field of expertise. Often confused with data visualization, data presentation architecture is a much broader skill set that includes determining what data on what schedule and in what exact format is to be presented, not just the best way to present data that has already been chosen (which is data visualization). Data visualization skills are one element of DPA."
Objectives

DPA has two main objectives:

    To use data to provide knowledge in the most efficient manner possible (minimize noise, complexity, and unnecessary data or detail given each audience's needs and roles)
    To use data to provide knowledge in the most effective manner possible (provide relevant, timely and complete data to each audience member in a clear and understandable manner that conveys important meaning, is actionable and can affect understanding, behavior and decisions)

Scope

With the above objectives in mind, the actual work of data presentation architecture consists of:

    Creating effective delivery mechanisms for each audience member depending on their role, tasks, locations and access to technology
    Defining important meaning (relevant knowledge) that is needed by each audience member in each context
    Determining the required periodicity of data updates (the currency of the data)
    Determining the right timing for data presentation (when and how often the user needs to see the data)
    Finding the right data (subject area, historical reach, breadth, level of detail, etc.)
    Utilizing appropriate analysis, grouping, visualization, and other presentation formats

Related fields

DPA work shares commonalities with several other fields, including:

    Business analysis in determining business goals, collecting requirements, mapping processes.
    Business process improvement in that its goal is to improve and streamline actions and decisions in furtherance of business goals
    Data visualization in that it uses well-established theories of visualization to add or highlight meaning or importance in data presentation.
    Graphic or user design: As the term DPA is used, it falls just short of design in that it does not consider such detail as colour palates, styling, branding and other aesthetic concerns, unless these design elements are specifically required or beneficial for communication of meaning, impact, severity or other information of business value. For example:
        choosing locations for various data presentation elements on a presentation page (such as in a company portal, in a report or on a web page) in order to convey hierarchy, priority, importance or a rational progression for the user is part of the DPA skill-set.
        choosing to provide a specific colour in graphical elements that represent data of specific meaning or concern is part of the DPA skill-set
    Information architecture, but information architecture's focus is on unstructured data and therefore excludes both analysis (in the statistical/data sense) and direct transformation of the actual content (data, for DPA) into new entities and combinations.
    Solution architecture in determining the optimal detailed solution, including the scope of data to include, given the business goals
    Statistical analysis or data analysis in that it creates information and knowledge out of data

See also

    Analytics
    Balanced scorecard
    Business analysis
    Business intelligence
    Data analysis
    Data profiling
    Data warehouse
    Exploratory data analysis
    Infographic
    Information architecture
    Information design
    Information visualization
    Interaction design
    Interaction techniques
    Scientific visualization
    Software visualization
    Statistical analysis
    Statistical graphics
    Visual analytics

People (Historical)

    Charles Joseph Minard
    John Tukey
    John Snow
    Otto Neurath
    Florence Nightingale
    William Playfair

People (active today)

    Alberto Cairo
    Edward Tufte
    Ola Rosling - Rosling developed the scatter-plot graphing tool used on Gapminder.org
    .
    Hans Rosling
    Aaron Koblin
    Manuel Lima
    Max Roser - Roser is an economist at the University of Oxford and author of the online data visualisation publication Our World In Data.
    Moritz Stefaner
    Ben Shneiderman
    Fernanda Viégas
    Martin M. Wattenberg
    Mona Chalabi - Data journalist at FiveThirtyEight. Previously at the Guardian, the Bank of England, and the Economist Intelligence Unit.
    George Furnas
    Branko Milanovic
    Mike Bostock - Bostock is one of the key developers of the Javascript library d3.js.
    Adrien Segal - Oakland, CA based artist known for her sculptures based on tidal and snow data.

Bigdata Platforms and Bigdata Analytics Software focuses on providing efficient analytics for extremely large datasets. These analytics helps the organisations to gain insight, by turning data into high quality information, providing deeper insights about the business situation. This enables the business to take advantage of the digital universe. IBM Bigdata Analytics, HP Bigdata , SAP Bigdata Analytics, Microsoft Bigdata, Oracle Bigdata Analytics, Talend Open Studio, Teradata Bigdata Analytics, SAS Big data, Dell Bigdata Analytics, HPCC System Big data, Palantir Bigdata, Pivotal Bigdata, Google BigQuery, Pentaho Big Data Analytics, Amazon Web Service, Cloudera Enterprise Bigdata, Hortonworks Data Platform, FICO Bigdata Analytics, Cisco Bigdata, Splunk Bigdata Analytics, Fusion-io Bigdata, Intel Bigdata, Mu Sigma Bigdata, MicroStrategy Bigdata , Opera Solutions Bigdata, Redhat Bigdata, Informatica Bigdata, MarkLogic Bigdata, Vmware Bigdata, Syncsort Bigdata, SGI Bigdata, MongoDB , Guavus Bigdata, Alteryx Bigdata, 1010data Advanced Analytics, Actian Analytics Platform, MapR, Tableau Software bigdata, QlikView Bigdata, Attivio’s Bigdata, DataStax Bigdata, Gooddata, Google Bigdata, Datameer, CSC Big Data Platform, Flytxt, Amdocs, Cisco Bigdata, Platfora and GE Bigdata are some of the Big data Analytics Platforms and Software in no particular order.
1. IBM Bigdata Analytics

IBM Bigdata Analytics solution portfolio  InfoSphere Streams , InfoSphere BigInsights , IBM Watson Explorer , IBM PureData powered by Netezza technology , DB2 with BLU Acceleration , IBM Smart Analytics System , InfoSphere Information Server and InfoSphere Master Data Management.2. HP Bigdata

HP’s Bigdata Analytics solution  HP HAVEn and HP Vertica. HP HAVEn is a platform comprised of software, services, and hardware. Big Data of any type either structured and unstructured can be analyzed to lead to powerful strategic insights. HP Vertica Dragline let organizations store their data in a cost effective manner, and provide capabilities to explore it quickly using SQL based tools

3. SAP Bigdata Analytics

SAP Bigdata Analytics platform  In Memory Platform called, SAP HANA, and SAP IQ, which is a column oriented, grid based, massively parallel processing database. There is also SAP HANA platform and Apache Hadoop solution available together. Bigdata Analytics solutions include the Predictive Analytics and Text Analytics solutions.




4. Microsoft Bigdata

Microsoft Azure is an open and flexible cloud platform which enables to quickly build, deploy and manage applications across a global network of Microsoft-managed datacenters. The applications can be  using any language, tool or framework and can integrated with other public cloud applications in the IT environment.



5. Oracle Bigdata Analytics

Oracle Bigdata Analytics solutions include Oracle Big Data Appliance, Oracle Exadata Database Machine and Oracle Exalytics In-Memory Machine. These are engineered Systems which are pre-integrated to reduce the cost and complexity of IT infrastructures. The database include Oracle Database, Oracle NoSQL Database, MySQL and MySQL Cluster, Oracle Event Processing, Oracle NoSQL Database and Oracle Coherence, Oracle Endeca Information  and in database analytics.




6. Talend Open Studio

Talend Open Studio is a versatile set of open source products for developing, testing, deploying and administrating data management and application integration projects. Talend delivers the only unified platform that makes data management and application integration easier by providing a unified environment for managing the entire lifecycle across enterprise boundaries.
Talend’s products dramatically lower the adoption barrier for businesses wanting powerful packaged solutions to operational challenges like data cleansing, master data management, and enterprise service bus deployment. Leveraging and extending leading Apache technologies, Talend’s open source ESB and open source SOA solutions help organizations to build flexible, high-performance enterprise architectures that integrate and service-enable distributed applications.


7. Teradata Bigdata Analytics

Teradata has  a simple architecture called, the Unified Data Architecture in Bigdata Analytics. The Teradata Aster  Platform ease the  of crucial business insights from all data types. With its powerful analytic applications coupled with minimal time and effort requirements, it provides the  insights needed for sophisticated companies today.

8. SAS Bigdata Analytics

SAS Bigdata Analytics solution portfolio  Credit Scoring for SAS Enterprise Miner, SAS High-Performance Data Mining, SAS Model Manager, SAS Scoring Accelerator, SAS Text Miner and SAS Visual Statistics.

9. Dell Bigdata Analytics

Dell Bigdata Analytics  Kitenga Analytics Suite, Boomi AtomSphere and SharePlex Connector for Hadoop. Kitenga Analytics Suite provides you with integrated information modeling and visualization capabilities in a big data search and business analytics platform.

10. HPCC Systems Big data

HPCC Systems is an Open-source platform for Big Data analysis. The Data Refinery engine called Thor, clean, link, transform and analyze Big Data. Thor supports ETL (Extraction, Transformation and Loading) functions like ingesting unstructured/structured data out, data profiling, data hygiene, and data linking out of the box. The Data Delivery engine (Roxie) provides highly concurrent and low latency real time query capability. The Thor processed data can be accessed by a large number of users concurrently in real time fashion using the Roxie. The programming language, Enterprise Control Language (ECL), is used to program both the data processing jobs on Thor and the queries on Roxie.

HPCC Systems is an Open-source platform for Big Data analysis.

HPCC Systems is an Open-source platform for Big Data analysis.
11. Palantir Bigdata

Palantir Bigdata solution  Palantir Gotham to integrate, manage, secure, and analyze all of the enterprise data and Palantir Metropolis to ntegrate, enrich, model, and analyze any kind of quantitative data.

12. Pivotal Bigdata

Pivotal Big Data solutions help to discover insight from all data to build applications that serve customers in the context to store, manage, and deliver value from , massive data sets using the most disruptive set of enterprise data products such as MPP and column store databases, in-memory data processing, and Hadoop.

13. Google BigQuery

Google BigQuery is a web service that enables companies to analyze massive datasets using Google’s infrastructure . This can analyze up to billions of rows in seconds. It is scalable and easy to use with the the familiar SQL query language. BigQuery lets developers and businesses tap into powerful data analytics on demand against multi-terabyte datasets in seconds.

14. Pentaho Big Data Analytics

Pentaho Big Data Analytics s a comprehensive and unified solution that supports the entire big data lifecycle. Regardless of the data source, within a single platform the solution provides visual big data analytics tools to extract and prepare the data plus the visualizations and analytics. The Open, standards based architecture, make it easy to integrate with or extend existing infrastructure.

15. Amazon Web Service

Amazon Web Services provides cloud based analytics services to help you process and analyze any volume of data, whether your need is for managed Hadoop clusters, real-time streaming data, petabyte scale data warehousing, or orchestration.

16. Cloudera Enterprise Bigdata

Cloudera Enterprise  CDH, the open source Hadoop-based platform, as well as advanced system management and data management tools plus dedicated support and community advocacy .

17. Hortonworks Data Platform

HDP is a platform for multi-workload data processing across an array of processing methods – from batch through interactive to real-time – all supported with solutions for governance, integration, security and operations.

18. FICO Bigdata Analytics

FICO s comprehensive Big Data Analytics software solutions, Predictive Analytics and Business Intelligence tools  FICO Data Orchestrator, FICO Decision Management Platform, FICO Decision Optimizer, FICO Model Builder, FICO Model Central Solution, FICO Predictive Analytics and FICO Solution Stack.

19. Cisco Bigdata

Cisco UCS Common Platform Architecture (CPA) for big data  computing, storage, connectivity, and unified management capabilities. Unique to this architecture are transparent, simplified data and management integration with an enterprise application ecosystem.

20. Splunk Bigdata Analytics

Splunk s a portfolio of Bigdata Analytics software such as Hunk: Splunk Analytics for Hadoop, NoSQL Data Stores, Splunk Hadoop Connect, Hadoop Management and Splunk DB Connect.

21. Fusion-io Bigdata

Fusion-io solutions eliminate the random workload performance deficiencies common to MongoDB, Cassandra and NoSQL databases, such as HBASE, while reducing the operational overhead of their conventional scale out architectures. Fusion based solutions deliver predictable and consistently high performance across the entire database, resulting in a more efficient overall system that can require fewer nodes, less DRAM, and use less energy for power and cooling.

22. Intel Bigdata

Intel portfolio  technology products such as Intel Xeon processors, 10 Gigabit server adapters, SSDs, and the Intel Distribution improve performance for big data projects.

23. Mu Sigma Bigdata

Mu Sigma’s platforms for Data Sciences include muXo, muHPC and muText. muXo is an advanced decision optimization engine  to solve complex business problems. It provides a suite of constantly evolving, cutting-edge meta-heuristic algorithms. muHPC is a suite of popular statistical algorithms, integrated in the form of R packages, for Big Data analysis. Written in MapReduce, muHPCTM algorithms leverage the power of parallel computation. Mu Sigma’s text mining engine enables knowledge  from unstructured and semi-structured data .

24. MicroStrategy Bigdata

MicroStrategy Bigdata solution called PRIME, which is deployed on the Cloud, provides visualization and dashboarding engine with an innovative massively parallel in-memory data store. This architecture allows companies to rapidly build and deploy powerful information-driven apps that deliver analytics to hundreds of thousands of users in a fraction of the time and cost of other approaches.

25. Opera Solutions Bigdata

Opera Solutions Bigdata solution Vektor Big Data analytics and Signal-processing platform integrates Big Data flows from both inside and outside the enterprise; provides the technology to identify, extract, and store Signals; and supports deployment of all Signal Apps.

26. Redhat Bigdata

Majority of big data implementations run on Linux. Red Hat Enterprise Linux is a leading platform for big data deployments. Red Hat Enterprise Linux excels in distributed architectures and  features that  critical big data needs. Managing tremendous data volumes and intensive analytic processing requires an infrastructure  for high performance, reliability, fine-grained resource management, and scale-out storage.

27. Informatica Bigdata

Informatica PowerCenter Big Data Edition provides a safe, efficient way to integrate all types of data on Hadoop at any scale without having to learn Hadoop.

28. MarkLogic Bigdata

MarkLogic Bigdata solution the Enterprise NoSQL database, brings all the features into one unified system: a document-centric, schema-agnostic, structure-aware, clustered, transactional, secure, database server with -in search and a full suite of application services.

29. Vmware Bigdata

vSphere is a robust, high-performance virtualization layer that abstracts server hardware resources and makes them shareable by multiple virtual machines. Runs Hadoop workloads on vSphere to achieve higher utilization, reliability and agility.

30. Syncsort Bigdata

Syncsort Hadoop Solutions helps on the challenges of collecting, processing and integrating data in Hadoop. It remove barriers for wider Hadoop adoption: connect, develop, deploy, re-use, and accelerate. No programming or tuning are required.

31. SGI Bigdata

SGI InfiniteData Cluster s the compute platform for Hadoop Solutions with cluster installations now reaching tens of thousands of nodes.SGI UV s the solution with the industry’s most powerful shared memory platform to find hidden data relationships or perform real-time analysis.

32. MongoDB

MongoDB is the leading NoSQL database, empowering businesses to be more agile and scalable. Fortune 500 companies and startups alike are using MongoDB to create new types of applications, improve customer experience, accelerate time to market and reduce costs.

33. Guavus Bigdata

The Guavus Reflex platform is capable of creating actionable information from widely distributed, high volume data streams in near real-time. Reflex uses highly optimized computational algorithms and machine learning to distill actionable insights from very large datasets.

34. Alteryx Bigdata

Alteryx Bigdata solution access, integration, and cleaning of sources of data as varied as Hadoop ( Cloudera & MapR) or NoSQL (MongoDB) and Excel or Teradata with predictive and spatial tools, combined in a simple, workflow design environment.

35. 1010data Advanced Analytics

The 1010data analytics platform  advanced, -in analytic functions such as Statistics (distribution analysis, correlation, variance),Predictive modeling and forecasting (linear and multivariate regression, logistic regression), Machine learning (clustering analysis, Markov chains for Monte Carlo simulations, principal component analysis). These functions are integrated directly into the system, so they run incredibly quickly on large volumes of data

36. Actian Analytics Platform

Actian Analytics Platform deliver the next generation analytics in three editions- Extreme Performance Edition, Hadoop SQL Edition, Cloud Edition.Extreme Performance Edition accelerates the analytics value chain from connecting to massive amounts of raw big data all the way to delivering actionable business value from sophisticated analytics. Hadoop SQL Edition accelerates Hadoop and makes it enterprise-grade by providing high-performance data enrichment, visual design and SQL analytics on Hadoop without the need for MapReduce skills. Cloud Edition integrates cloud and on-premises applications while providing robust data quality and other data services.

37. MapR

The MapR Distribution for Apache Hadoop provides organizations with an enterprise grade distributed data platform to reliably store and process big data. MapR packages a broad set of Apache open source ecosystem projects enabling batch, interactive, or real time applications.

38. Tableau Software bigdata

Tableau Software bigdata solutions connect to any data, anytime and anywhere, regardless of its size and complexity or mix of unstructured and structured data with the technologies like Google BigQuery and a variety of Hadoop flavors.

39. QlikView Bigdata

QlikView s two approaches to handling Big Data, both deliver the same great user experience. Either with QlikView’s 100% In-Memory Architecture or QlikView Direct , which is a hybrid approach that leverages both in-memory data and data that is dynamically queried from an external source.

40. Attivio’s Bigdata

Attivio’s Active Intelligence Engine combines Big Data and Big Content,  Hadoop. Universal indexing and automatic ad hoc JOIN of all information matching a given query, without costly data modeling and with full security. There is also Advanced text analytics that adds context and signals from human-generated information sources and support for business intelligence/data visualization tools .

41. DataStax Bigdata

DataStax Enterprise (DSE), which is  on Apache Cassandra, delivers what Internet Enterprises need to compete in today. With in-memory computing capabilities, enterprise-level security,  and powerful integrated analytics and enterprise search, visual management, and expert support, DataStax Enterprise is the leading distributed database choice for online applications that require  performance with no downtime.

42. Gooddata

The GoodData Platform is a portfolio of tools, APIs and frameworks, which makes the key components of a BI solution to collect, store, combine, analyze, and visualize. These were  to exist in the cloud and be delivered as an end-to-end service.

GoodData

GoodData
43. Google Bigdata

Google Cloud Platform surfaces the same analytical engines invented and used by Google for nearly two decades to help unearth insight in your business and operational environment. Google Cloud Platform leads the industry in the ability to let you analyze data at the scale of the entire web, with the familiarity of SQL and in a fully managed, serverless architecture where backend infrastructure is fully handled on your behalf.The big data analytics products are able to scale automatically while you focus only on the business insight you want to uncover.

44. Datameer

Datameer Professional, is a SaaS big data analytics platform targeted for department specific deployments. Datameer ing features leading Hadoop cloud providers Altiscale and Bigstep. Datameer simplifies the big data analytics environment into a single application on top of the powerful Hadoop platform.

45. CSC Big Data Platform

CSC Big Data Platform as a Service (BDPaaS) helps enterprises leap past these hurdles and get value from their data much more quickly. With BDPaaS, enterprises can rapidly develop, secure and deploy next-generation big data and analytics applications with a centralized, subscription-based platform that uses leading analytics tools, infrastructure and software.

46. Flytxt Big Data Analytics platform

Flytxt’s Big Data Analytics platform is  to integrate  Data, Big Data for deriving deeper actionable insights. It follows a hybrid architecture combining scale out clusters running Hadoop with traditional RDBMS as a metadata store and an in-memory database for Real time transactional data processing.

47. Amdocs Insight Big Data Analytics Platform

Amdocs Insight Big Data Analytics Platform supports a wide variety of Amdocs analytical applications and data services to facilitate new revenues, drive business efficiency and enhance the customer experience.

48. Cisco Bigdata

Cisco provide integrated infrastructures and analytics to support our big data partner ecosystem. Cisco UCS Integrated Infrastructure for Big Data architecture provides a secure and scalable infrastructure. Cisco is bringing the computing and analytics to the data to take advantage of the valuable insight that it reveals.

49. Platfora

 on Hadoop, Spark and native cloud APIs, Platfora’s technology helps it fit in just about anywhere  your existing analytics ecosystem, hardware and BI tools.

50. GE Bigdata

The Industrial Internet co ordinate multiple industrial applications to work intelligently in order to optimize entire operational environments.

big-data-analytics

Data analysis is nothing new. Even before computers were used, information gained in the course of business or other activities was reviewed with the aim of making those processes more efficient and more profitable. These were, of course, comparatively small-scale undertakings given the limitations posed by resources and manpower; analysis had to be manual and was slow by modern standards, but it was still worthwhile. Opinion polling, for example, has been carried out since early in the 19th century, almost 200 years ago. The first national survey took place in 1916 and involved the publication Literary Digest sending out millions of postcards and counting the returns. As a result, they correctly predicted Woodrow Wilson’s election as president.

Since then, volumes of data have grown exponentially. The advent of the internet and er computing has meant that huge quantities of information can now be harvested and used to optimise business processes. The problem is that conventional methods were simply not suited to crunching through all the numbers and making sense of them. The amount of information is phenomenal, and within that information lies insights that can be extremely beneficial. Once patterns are identified, they can be used to adjust business practices, create targeted campaigns and discard ones that are not effective. However, as well as large amounts of storage, it takes specialised software to be able to make sense of all this data in a useful way.

‘Big Data’ is the emerging discipline of capturing, storing, processing, analysing and visualising these huge quantities of information. The data sets may start at a few terabytes and run to many petabytes – far more than traditional data analysis packages can handle. In 2012 Gartner defined it as, ‘high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight  and process optimization.’ This ‘3V’ classification has been  on since (particularly with the addition of veracity), such that Big Data is often described in terms of the following characteristics:

        Volume. Terabytes or petabytes of data are analysed. An estimated 2.5 quintillion bytes of data (2.5 trillion gigabytes) are created every day, an amount which will only rise in the future. However, the size of the dataset is not the only variable that characterises Big Data.
        Variety. The dataset may contain many different forms of data – not simply a large amount of the same type. The profusion of different kinds of mobile device and the variety of content consumed on them on a wide range of platforms, for example, means that companies can harvest data from an enormous array of sources, each telling them a different part of the same picture.
        Velocity. Data may change on a constant basis. For example, modern cars may have 100 or so different sensors that continually monitor different aspects of performance. Markets change on a moment-to-moment scale. Data is highly fluid, and snapshots are not always enough.
        Veracity. The data acquired may not all be accurate, or much of it may be uncertain or provisional in nature. Data quality is unreliable, especially when there is so much of it. Any system of analysis must take this into account.

In addition to the 4V characteristics, there are also two others to deal with:

    Variability. Data capture and volume may be inconsistent, not just inaccurate, so varying quantities and qualities of data will be acquired at different times.
    Together, these factors mean that managing the data can be an extremely complex process, since there are many data sources with differing types and formats of data, but these need to be correlated and made sense of if they are to be useful.

big-data-will-drive-the-next-phase-of-innovation-in-mobile-computing

Big Data companies

Due to the nature of Big Data, specialist companies have grown up around it in order to manage the volumes and complexity of information involved.

ibm-bigdata-mobile-header

IBM Big Data Analytics

Like many other big data companies, IBM builds its ings on Hadoop – so it’s , affordable and open source. It allows businesses to capture, manage and analyse structured and unstructured data with its BigInsights product. This is also available on the cloud (BigInsights on Cloud) to give the benefits of outsourcing storage and processing, providing Hadoop as a service. InfoSphere Streams is  to enable capture and analysis of data in realtime for Internet-of-Things applications. IBM’s analytics enable powerful collating and visualisation of data with excellent flexibility for storage and management. You can also find plenty of downloadable documentation and white papers on their site.

hp4750-540x334

HP Big Data

Another well-known name in IT, HP brings a wealth of experience to big data. As well as ing their own platform, they run workshops to assess organisations’ needs. Then, ‘when you’re ready to transform your infrastructure, HP can help you develop an IT architecture that provides the capacity to manage the volume, velocity, variety, voracity, and value of your data.’ The platform itself is based on Hadoop. HP look to add value beyond providing the software alone, and will consult with you to help you craft a strategy to help you make the most of the big data you collect – and how to go about it most efficiently.

bdpmicrobd

Microsoft

Microsoft’s big data solutions run on Hadoop and can be used either in the cloud or natively on Windows. Business users can use Hadoop to gain insights into their data using standard tools  Excel or Office 365. It can be integrated with core databases to analyse both structured and unstructured data and create sophisticated 3D visualisations. Polybase is incorporated so users can then easily query and combine relational and non-relational data with the same techniques required for SQL Server. Microsoft’s solution enables you to analyse Hadoop data from within Excel, adding new functionality to a familiar software package.

30680129.cms

Intel Big Data

Recognising that making the most of big data means changing your information architecture, Intel takes the approach of enabling enterprise to create a more flexible, open and distributed environment, whilst their big data platform is based on Apache’s Hadoop. They take a thorough approach that does not assume they know what your needs are, but presents a walkthrough to determine how best to help achieve your objectives. Intel’s own industry-standard hardware is at your disposal to optimise the performance of your big data project, ing speed, scalability and a cost-effective approach according to your organisation’s requirements.

amazonwebservices-100014921-orig

Amazon Web Services

Amazon is a huge name in providing web hosting and other services, and the benefits of using them are unparalleled economies of scale and uptime. Amazon tend to  a basic framework for customers to use, without providing much in the way of customer support. This means they are the ideal choice if you know exactly what you are doing and want to save money. Amazon supports products like Hadoop, Pig, Hive and Spark, enabling you to build your own solution on their platform and create your own big data stack. There are plenty of tutorials, video demos and guides to get you started as quickly and easily as possible.

dell-software-logo

Dell Big Data Analytics

Another well known and globally-established company, this time in the hardware space, Dell s its own big data package. Their solution  an automated facility to load and continuously replicate changes from an Oracle database to a Hadoop cluster to support big data analytics projects, thereby simplifying Oracle and Hadoop data integration. Data can be integrated in near real-time, from a wide range of data stores and applications, and from both on- and off-premises sources. Techniques such as natural language processing, machine learning and sentiment analysis are made accessible through straightforward search and powerful visualisation to enable users to learn relationships between different data streams and leverage these for their businesses.

Teradata-Logo-620x265

Teradata

Teradata call their big data product a ‘data warehouse system’, which stores and manages data. The different server nodes share nothing, having their own memory and processing power, and each new node increases storage capacity. The database sits over these and the workload is shared among them. The company started taking an interest in big data in 2010, adding analytics for text documents,  unstructured data and semi-structured data (e.g. word processor documents and spreadsheets). They also work with unstructured data gathered from online interactions.

bigquery_0

Google BigQuery

Google is the big daddy of internet search: the outright market leader with the vast majority of search traffic to its name. No other search engine comes close, so perhaps it’s not surprising that Google should  an analytics package to crunch through the phenomenal amount of data it produces in the course of its day-to-day work for millions of businesses around the world. It already hosts the hugely popular Google Analytics, but BigQuery is  for a different order of magnitude of data. It puts Google’s impressive infrastructure at your disposal, allowing you to analyse massive datasets in the cloud with , SQL-like queries – analysing multi-terabyte datasets in just seconds. Being Google it’s also very scalable and straightforward to use.

vmware-logo

VMware Big Data

VMware is well-known in the world of best cloud storage and IaaS. Their big data solutions use their established vSphere product to virtualise Hadoop whilst maintaining excellent performance.  and elastic scaling is possible due to an approach that separates out storage from computing, keeping data safe and persistent, enabling greater efficiency and flexibility. Essentially this is a sophisticated and safe approach to Hadoop-as-a-service, which utilises many of VMware’s strengths to deliver a big data platform reliably and in a cost-effective way.

red-hat-1-large

Redhat

As might be expected, Redhat take an open source approach to big data, believing that changing workloads and technologies require an open approach. They take a modular approach so that the building blocks of their platform work interoperably with other elements of your data centre. Building blocks include Platform-as-a-Service (PaaS), so you can develop apps er, process data in real time, and easily integrate systems; Infrastructure-as-a-Service (IaaS), to enable deployment and management of service providers, tools, and components of IT architecture across platforms and technology stacks in a consistent, unified way; Middleware, integration and automation, to streamline data sources and interaction; and Storage, of the most appropriate kind for the task in hand.

it_photo_102724

Tableau Software

Tableau s significant flexibility over how you work with data. Using Tableau’s own servers and Desktop visualisation with your existing big data storage makes it a versatile and powerful system. There are two options: connecting to your data live, or bringing it into memory for  response queries. Memory management means all laptop/PC memory is used, down to the hard disk, to maintain speed and performance, even at large scale. Tableau supports more than 30 databases and formats, and is easy to connect to and manage. Multi-million row tables can be visually analysed directly on the database itself, extremely quickly.

informatica-cloud

Informatica Big Data

Another provider that builds its platform on Hadoop, Informatica has several options that make life easy by giving you access to the functionality and allow you to integrate all types of data efficiently without having to learn Hadoop itself. Informatica Big Data Edition uses a visual development environment to save time and improve accessibility (Informatica claims this makes it approximately five times er than hand-coding a solution). This also has the advantage of not needing to hire dedicated Hadoop experts, since there are more than 100,000 Informatica experts worldwide. This makes for a fantastically versatile solution that is still simple enough to be used without intensive training.

splunk-logo

Splunk

Splunk collects and analyses machine data as it comes in. Realtime alerts are used to spot trends and identify patterns as they occur. It’s extremely easy to deploy and use, and highly scalable: ‘from a single server to multiple datacenters.’ There is also a strong emphasis on security, with role-based access controls and auditability. Splunk is  for Hadoop and NoSQL data stores to enable analysis and visualisation of unstructured data. There’s also a community forum and online support centre, should you need assistance getting set up or figuring out how things work.

datastax_logo_blue

DataStax Big Data

DataStax big data solution is  on Apache Cassandra, an open source and enterprise-ready platform that is commercially supported. It is used by a number of the world’s most innovative and best-known companies, such as Netflix and eBay. Their chief product, DataStax Enterprise, leverages Cassandra’s properties to give vast scalability, continuous availability and strong security. The combination of commercial software and open source platform means that it’s  and low-cost compared to many other options on the market. It’s also relatively easy to run. DataStax boast that their product ‘enables you to perform real-time transactions with Cassandra, analytics with Apache Hadoop and enterprise search with Apache Solr, in a single, smartly integrated big data platform that works across multiple datacenters and the cloud.

MongoDB_Logo_Full

MongoDB

‘Mongo’ comes from ‘humongous’ and takes a different approach to normal, using JSON-like documents instead of table-based relational database structures. This allows it to integrate certain types of data er and more easily. Is it free and open-source software, released under a combination of the GNU Affero General Public License and the Apache License. Mongo has been adopted by a number of well-known and very large websites, such as Craigslist, eBay and the New York Times. Mongo’s analytics are  to scale and are  into the operational database, meaning you have access to them in realtime.

gooddata_vertical_black-1

Gooddata

Gooddata is an all-in-one cloud analytics platform. They have a wide range of customers,  HP and Nestle. Operating fully in the cloud, Gooddata manage hosting, data and technology, meaning that the customer is able to focus completely on the analytics. They are recognised as industry leaders, with a number of awards to their name,  from Gartner. There’s an emphasis on usability, with interactive dashboards that facilitate collaboration by team-members as well as visual data , so that teams can move quickly on insights gained. The responsive UI is  to be easy to use on any device or platform,  mobile devices.

qlikview_logo_large

QlikView

QlikView s two big data solutions, enabling users to switch between them as the require. Their In-Memory architecture uses a patented data engine to compress data by a factor of 10, so that up to 2 TB can be stored on a 256 GB RAM server. This s exceptional performance, and other features further enhance response rates and make exploring very large data sets extremely . This is used by many of Qlik’s customers to analyse volumes of data stored in data warehouses or Hadoop clusters. This hybrid approach means big data can be made accessible to users without knowledge of programming. It also allows a highly focused and granular view of data when required.

Attivio-Small-PR-trim

Attivio

Attivio’s Active Intelligence Engine (AIE) brings together a number of separate capabilities – business intelligence, enterprise search, business analytics, data warehousing and process automation – to produce comprehensive information, presented in a user-friendly way. AIE puts together both structured and unstructured data into one index to be searched, collated and analysed; regular search queries and SQL can be used and a wide range of queries are therefore possible, from broad to highly focused. It can be integrated with a large number of data sources by giving it access with other software applications. It uses proprietary, patented technology, unlike many of its open-source-based rivals.

img15

1010data Advanced Analytics

1010data s a complete suite of products, enabling companies to engage with the data they harvest in their everyday business. Data is analysed on the same platform on which it is stored, minimising delays from moving data. This enables  responses to changing market information and an agile approach that reacts in near-realtime. There is ‘immediate, direct, unfettered access to all relevant data, even voluminous, granular, raw data’. 1010’s platform can be implemented on the cloud, so that anyone with the correct access rights can use it from anywhere in the world. The company s an ‘Analytical Platform as a Service’ (APaaS) approach that gives enterprise-grade cloud security, reliability, and interoperability, along with cost-effective, on-demand performance and storage scalability.

Actian-logo

Actian

Actian’s Vortex is  on Apache Hadoop, an open source framework written in Java for distributed storage and processing of very large data sets. This means that Actian’s big data solutions will always be open themselves, so that customers are not locked into a proprietary platform. They claim their software is , despite the large size of the datasets they deal with. Whilst Hadoop is complex, Actian’s platform is far more straightforward to use, making it enterprise ready and emphasising security and scalability. It gives full SQL support to your data. Actian is used by thousands of big-name customers worldwide,  Nikon, China Telecom and GE Transportation.

Conclusion

Big data isn’t just an emerging phenomenon. It’s already here and being used by major companies to drive their business forwards. Traditional analytics packages simply aren’t capable of dealing with the quantity, variety and changeability of data that can now be harvested from diverse sources – machine sensors, text documents, structured and unstructured data, social media and more. When these are combined and analysed as a whole, new patterns emerge. The right big data package will allow enterprises to track these trends in real time, spotting them as they occur and enabling businesses to leverage the insights provided.

However, not all big data platforms and software are alike. As ever, which you decide on will depend on a number of factors. These include not just the nature of the data you are working with, but organisational budgets, infrastructure and the skillset of your team, amongst other things. Some solutions are  to be used off-the-peg, providing powerful visualisations and connecting easily to your data stores. Others are intended to be more flexible but should only be used by those with coding expertise. You should also think to the future, and the long-term implications of being tied to your platform of choice – particularly in terms of open-source vs proprietary software.

Big data refers to massive,
heterogeneous, and often
unstructured digital content that is
difficult to process using traditional
data management tools and
techniques. The term encompasses
Published by the IEEE Computer Society
the complexity and variety of data
and data types, real-time data
collection and processing needs, and
the value that can be obtained by
smart analytics.
Advanced data mining techniques
and associated tools can help
extract information from large,
complex datasets that is useful
in making informed decisions
in many business and scientific
applications  tax payment
collection, market sales, social
studies, biosciences, and high-
energy physics. Combining big data
analytics and knowledge
techniques with scalable computing
systems will produce new insights in
a shorter time.
Although few cloud-based
analytics platforms are available
today, current research work
anticipates that they will become
common within a few years. Some
current solutions are based on open
0018-9162/13/$31.00 © 2013 IEEEsource systems such as Apache
Hadoop and SciDB, while others
are proprietary solutions provided
by companies such as Google, IBM,
EMC, BigML, Splunk Storm, Kognitio,
and InsightsOne.
As more such platforms emerge,
researchers will port increasingly
powerful data mining programming
tools and strategies to the cloud
to exploit complex and flexible
software models such as the
distributed workflow paradigm.
The growing use of service-oriented
computing could accelerate this
trend (http://tinyurl.com/d26o2j5).
DATA ANALYTICS SERVICE
MODELS
Developers and researchers can
adopt the software as a service
(SaaS), platform as a service (PaaS),
and infrastructure as a service
(IaaS) models to implement big data
analytics solutions in the cloud.
The SaaS model s complete
big data analytics applications
to end users, who can exploit
the cloud’s scalability in both
data storage and processing
power to execute analysis on
large or complex datasets.
The PaaS model provides data
analytics programming suites and
environments in which data mining
developers can design scalable
analytics services and applications.
Researchers can exploit the
IaaS model to compose a set of
virtualized hardware and software
resources for running data analysis
frameworks or applications.
Column Contributions
W
e welcome short articles (1,500 to
2,000 words) for publication in the
Cloud Cover column that  the
questions outlined in Computer’s
January 2013 issue (S. Murugesan,
As Table 1 shows, developers
can implement big data analytics
services within each of these three
models:
•	 data analytics software as a ser-
vice—provides a well-defined
data mining algorithm or ready-
to-use knowledge  tool
as an Internet service to end
users, who can access it directly
through a Web browser;
•	 data analytics platform as a ser-
vice—provides a supporting
platform that developers can
use to build their own data ana-
lytics applications or extend
existing ones without concern
about the underlying infrastruc-
ture or distributed computing
issues; and
•	 data analytics infrastructure
as a service—provides a set of
virtualized resources that devel-
opers can use as a computing
infrastructure to run data
mining applications or to imple-
ment data analytics systems
from scratch.
End users whose goal is to per-
form complex data analysis can
“Cloud Computing: The New Normal?,”
pp. 77-79). Submit your ideas for
advancing the technology or share your
experiences in harnessing the cloud at
cloudcover@computer.org.
apply the recently implemented Data
Mining Cloud Framework (http://
tinyurl.com/c4b4f5k) as a high-level
PaaS programming environment
and create a set of SaaS suites for big
data analytics. With this approach,
users need not be concerned about
cloud platform or application pro-
gramming details.
BIG DATA ANALYTICS
WORKFLOWS
Developers can use workflows,
which consist of complex graphs
of many concurrent tasks, to
 the complexity of scientific
and business applications. This
approach supports data analytics
design by providing a paradigm that
encompasses all the steps of data
analytics, from data access and
filtering to data mining and sharing
produced knowledge.
Workflow-based data mining
frameworks that run on cloud
platforms and use a service-
oriented approach  a flexible
programming model, distributed
task interoperability, and
execution scalability that reduces
data analytics completion time.
Application developers can design
Table 1. Cloud-based data analytics services.
Cloud service model
Features
Users
Data analytics software as a service A single and complete data mining application or task
( data sources) ed as a service End users, analytics managers, data
analysts
Data analytics platform as a service A data analysis suite or framework for programming or
developing high-level applications, hiding the cloud infra-
structure and data storage Data mining application developers,
data scientists
Data analytics infrastructure as a
service A set of virtualized resources provided to a programmer or
data mining researcher for developing, configuring, and
running data analysis frameworks or applications Data mining programmers, data
management developers, data
mining researchers

MAY 2013
99Clo ud C ov er
Figure 1. Data analysis workflow application  using the Data Mining Cloud Framework’s graphical programming interface.
data analysis tasks, scientific
computation methods, and complex
simulation techniques as workflows
that integrate single Web services
and execute them concurrently on
virtual machines in the cloud.
Figure 1 shows a data analysis
workflow application  using
the Data Mining Cloud Framework’s
graphical programming interface
recently developed in our laboratory
(http://tinyurl.com/crnork2). Data
sources and tools such as data
mining algorithms, filters, and data
splitters are connected through
direct edges that define specific
dependency relationships among
them.
When creating an edge between
two nodes, the system automatically
attaches a label to it that represents
the relationship between them.
To ease workflow composition
and allow users to monitor its
execution, each resource icon has
an associated tag—the checkmarks
in Figure 1—representing the status
of a corresponding resource.
The experimental results of a
set of studies using the framework
to analyze genomics, network
intrusion, and bioinformatics data
demonstrated its effectiveness,
as well as the linear scalability
achieved through concurrent
execution of the workflow tasks on a
pool of virtual servers (http://tinyurl.
com/c4b4f5k).
Current research focuses on the
workflow composition interface,
with the aim of extending supported
design patterns such as conditional
branches and iterations and
evaluating its functionality and
Cloud Computing
Special Technical Community
T
he CS Cloud Computing Special
Technical Community (CS CCSTC)
focuses on cloud activities across
the IEEE Computer Society, involving
both CS members and nonmembers.
The STC’s work is complementary
	100
computer
to the IEEE Cloud Computing
Initiative (IEEE CCI), a three-year
project to promote cloud efforts
across IEEE.
For details or to join the STC, visit
www.computer.org/cc.
performance during the design and
execution of complex data mining
workflows on large datasets in the
cloud.
RESEARCH
RECOMMENDATIONS
Cloud-based data analytics
requires high-level, easy-to-use
design tools for programming
large applications dealing with
huge, distributed data sources.
This necessitates further research
and development in several key
areas.
•	 Programming abstracts for big
data analytics. Big data analyt-
ics programming tools require
novel complex abstract struc-
tures. The MapReduce model
is often used on clusters and
clouds, but more research is
needed to develop scalable
higher-level models and tools.
•	 Data and tool interoperability
and openness. Interoperability
is a main issue in large-scale
applications that use resources
such as data and computing
nodes. Standard formats and
models are needed to support
interoperability and ease co-
operation among teams usingdifferent data formats and
tools.
•	 Integration of big data analytics
frameworks. The service-
oriented paradigm allows run-
ning large-scale distributed
workflows on heterogeneous
platforms along with software
components developed using
different programming lan-
guages or tools. The Web and
cloud services paradigms can
help manage worldwide integra-
tion of multiple data analytics
frameworks.
•	 Data provenance and annota-
tion mechanisms. Provenance
is captured as a set of dependencies between elements that
researchers can use to interpret
data and provide reproducible
analysis. Research is needed to
develop innovative techniques
for visualizing and mining provenance data.
These solutions, together with
others ing data privacy and
security concerns, will promote
cloud-based data analytics in large
companies, and eventually will
benefit users such as independent
research teams, start-ups, and
small enterprises that aren’t deeply
skilled in cloud programming and
management.
Advancing the cloud
from a computation
and data management
infrastructure to a pervasive and
scalable data analytics platform
