Diffusion of innovations is a theory that seeks to explain how, why, and at what rate new ideas and technology spread. Everett Rogers, a professor of communication studies, popularized the theory in his book Diffusion of Innovations; the book was first published in 1962, and is now in its fifth edition (2003).[1] Rogers argues that diffusion is the process by which an innovation is communicated over time among the participants in a social system. The origins of the diffusion of innovations theory are varied and span multiple disciplines.

Rogers proposes that four main elements influence the spread of a new idea: the innovation itself, communication channels, time, and a social system. This process relies heavily on human capital. The innovation must be widely adopted in order to self-sustain. Within the rate of adoption, there is a point at which an innovation reaches critical mass.

The categories of adopters are innovators, early adopters, early majority, late majority, and laggards.[2] Diffusion manifests itself in different ways and is highly subject to the type of adopters and innovation-decision process. The criterion for the adopter categorization is innovativeness, defined as the degree to which an individual adopts a new idea.

Contents

    1 History
    2 Elements
        2.1 Characteristics of innovations
        2.2 Characteristics of individual adopters
        2.3 Characteristics of organizations
    3 Process
    4 Decisions
    5 Rate of adoption
        5.1 Adoption strategies
        5.2 Diffusion vs adoption
    6 Adopter categories
    7 Failed diffusion
    8 Heterophily and communication channels
    9 The role of social systems
        9.1 Opinion leaders
        9.2 Electronic communication social networks
        9.3 Organizations
    10 Extensions of the theory
        10.1 Policy
        10.2 Technology
    11 Consequences of adoption
        11.1 Public versus private
        11.2 Benefits versus costs
    12 Mathematical treatment
        12.1 Complex Systems models
    13 Criticism
    14 See also
    15 References
        15.1 Notes
    16 External links

History

The concept of diffusion was first studied by the French sociologist Gabriel Tarde in late 19th century[3] and by German and Austrian anthropologists and geographers such as Friedrich Ratzel and Leo Frobenius.[4] The study of diffusion of innovations took off in the subfield of rural sociology in the midwestern United States in the 1920s and 1930s. Agriculture technology was advancing rapidly, and researchers started to examine how independent farmers were adopting hybrid seeds, equipment, and techniques.[5] A study of the adoption of hybrid corn seed in Iowa by Ryan and Gross (1943) solidified the prior work on diffusion into a distinct paradigm that would be cited consistently in the future.[5][6] Since its start in rural sociology, Diffusion of Innovations has been applied to numerous contexts, including medical sociology, communications, marketing, development studies, health promotion, organizational studies, knowledge management, and complexity studies,[7] with a particularly large impact on the use of medicines, medical techniques, and health communications.[8] In organizational studies, its basic epidemiological or internal-influence form was formulated by H. Earl Pemberton,[9] who provided examples of institutional diffusion[10] such as postage stamps and standardized school ethics codes.

In 1962, Everett Rogers, a professor of rural sociology, published his seminal work: Diffusion of Innovations. Rogers synthesized research from over 508 diffusion studies across the fields that initially influenced the theory: anthropology, early sociology, rural sociology, education, industrial sociology and medical sociology. Using his synthesis, Rogers produced a theory of the adoption of innovations among individuals and organizations.[11] Diffusion of Innovations and Rogers' later books are among the most often cited in diffusion research. His methodologies are closely followed in recent diffusion research, even as the field has expanded into, and been influenced by, other methodological disciplines such as social network analysis and communication.[12][13]
Elements

The key elements in diffusion research are:
Element 	Definition
Innovation 	Innovations are a broad category, relative to the current knowledge of the analyzed unit. Any idea, practice, or object that is perceived as new by an individual or other unit of adoption could be considered an innovation available for study.[14]
Adopters 	Adopters are the minimal unit of analysis. In most studies, adopters are individuals, but can also be organizations (businesses, schools, hospitals, etc.), clusters within social networks, or countries.[15]
Communication channels 	Diffusion, by definition, takes place among people or organizations. Communication channels allow the transfer of information from one unit to the other.[16] Communication patterns or capabilities must be established between parties as a minimum for diffusion to occur.[17]
Time 	The passage of time is necessary for innovations to be adopted; they are rarely adopted instantaneously. In fact, in the Ryan and Gross (1943) study on hybrid corn adoption, adoption occurred over more than ten years, and most farmers only dedicated a fraction on their fields to the new corn in the first years after adoption.[6][18]
Social system 	The social system is the combination of external influences (mass media, organizational or governmental mandates) and internal influences (strong and weak social relationships, distance from opinion leaders).[19] There are many roles in a social system, and their combination represents the total influences on a potential adopter.[20]
Characteristics of innovations

Studies have explored many characteristics of innovations. Meta-reviews have identified several characteristics that are common among most studies.[21] These are in line with the characteristics that Rogers initially cited in his reviews.[22]

Potential adopters evaluate an innovation on its relative advantage (the perceived efficiencies gained by the innovation relative to current tools or procedures), its compatibility with the pre-existing system, its complexity or difficulty to learn, its trialability or testability, its potential for reinvention (using the tool for initially unintended purposes), and its observed effects. These qualities interact and are judged as a whole. For example, an innovation might be extremely complex, reducing its likelihood to be adopted and diffused, but it might be very compatible with a large advantage relative to current tools. Even with this high learning curve, potential adopters might adopt the innovation anyway.[22]

Studies also identify other characteristics of innovations, but these are not as common as the ones that Rogers lists above.[23] The fuzziness of the boundaries of the innovation can impact its adoption. Specifically, innovations with a small core and large periphery are easier to adopt.[24] Innovations that are less risky are easier to adopt as the potential loss from failed integration is lower.[25] Innovations that are disruptive to routine tasks, even when they bring a large relative advantage, might not be adopted because of added instability. Likewise, innovations that make tasks easier are likely to be adopted.[26] Closely related to relative complexity, knowledge requirements are the ability barrier to use presented by the difficulty to use the innovation. Even when there are high knowledge requirements, support from prior adopters or other sources can increase the chances for adoption.[27]
Characteristics of individual adopters

Like innovations, adopters have been determined to have traits that affect their likelihood to adopt an innovation. A bevy of individual personality traits have been explored for their impacts on adoption, but with little agreement.[28] Ability and motivation, which vary on situation unlike personality traits, have a large impact on a potential adopter's likelihood to adopt an innovation. Unsurprisingly, potential adopters who are motivated to adopt an innovation are likely to make the adjustments needed to adopt it.[29] Motivation can be impacted by the meaning that an innovation holds; innovations can have symbolic value that encourage (or discourage) adoption.[30] First proposed by Ryan and Gross (1943), the overall connectedness of a potential adopter to the broad community represented by a city.[6] Potential adopters who frequent metropolitan areas are more likely to adopt an innovation. Finally, potential adopters who have the power or agency to create change, particularly in organizations, are more likely to adopt an innovation than someone with less power over his choices.[31]
Characteristics of organizations

Organizations face more complex adoption possibilities because organizations are both the aggregate of its individuals and its own system with a set of procedures and norms.[32] Three organizational characteristics match well with the individual characteristics above: tension for change (motivation and ability), innovation-system fit (compatibility), and assessment of implications (observability). Organizations can feel pressured by a tension for change. If the organization's situation is untenable, it will be motivated to adopt an innovation to change its fortunes. This tension often plays out among its individual members. Innovations that match the organization's pre-existing system require fewer coincidental changes and are easy to assess are more likely to be adopted.[33] The wider environment of the organization, often an industry, community, or economy, exerts pressures on the organization, too. Where an innovation is diffusing through the organization's environment for any reason, the organization is more likely to adopt it.[25] Innovations that are intentionally spread, including by political mandate or directive, are also likely to diffuse quickly.[34][35]
Process

Diffusion occurs through a five–step decision-making process. It occurs through a series of communication channels over a period of time among the members of a similar social system. Ryan and Gross first identified adoption as a process in 1943.[36] Rogers' five stages (steps): awareness, interest, evaluation, trial, and adoption are integral to this theory. An individual might reject an innovation at any time during or after the adoption process. Abrahamson examined this process critically by posing questions such as: How do technically inefficient innovations diffuse and what impedes technically efficient innovations from catching on? Abrahamson makes suggestions for how organizational scientists can more comprehensively evaluate the spread of innovations.[37] In later editions of Diffusion of Innovation, Rogers changes his terminology of the five stages to: knowledge, persuasion, decision, implementation, and confirmation. However, the descriptions of the categories have remained similar throughout the editions.
DoI Stages.jpg
Five stages of the adoption process Stage 	Definition
Knowledge 	The individual is first exposed to an innovation, but lacks information about the innovation. During this stage the individual has not yet been inspired to find out more information about the innovation.
Persuasion 	The individual is interested in the innovation and actively seeks related information/details.
Decision 	The individual takes the concept of the change and weighs the advantages/disadvantages of using the innovation and decides whether to adopt or reject the innovation. Due to the individualistic nature of this stage, Rogers notes that it is the most difficult stage on which to acquire empirical evidence.[11]
Implementation 	The individual employs the innovation to a varying degree depending on the situation. During this stage the individual also determines the usefulness of the innovation and may search for further information about it.
Confirmation 	The individual finalizes his/her decision to continue using the innovation. This stage is both intrapersonal (may cause cognitive dissonance) and interpersonal, confirmation the group has made the right decision.
Decisions

Two factors determine what type a particular decision is:

    Whether the decision is made freely and implemented voluntarily
    Who makes the decision.

Based on these considerations, three types of innovation-decisions have been identified.[citation needed]
Type 	Definition
Optional Innovation-Decision 	made by an individual who is in some way distinguished from others.
Collective Innovation-Decision 	made collectively by all participants.
Authority Innovation-Decision 	made for the entire social system by individuals in positions of influence or power.
Rate of adoption

The rate of adoption is defined as the relative speed at which participants adopt an innovation. Rate is usually measured by the length of time required for a certain percentage of the members of a social system to adopt an innovation.[38] The rates of adoption for innovations are determined by an individual’s adopter category. In general, individuals who first adopt an innovation require a shorter adoption period (adoption process) when compared to late adopters.

Within the adoption curve at some point the innovation reaches critical mass. This is when the number of individual adopters ensures that the innovation is self-sustaining.
Adoption strategies

Rogers outlines several strategies in order to help an innovation reach this stage, including when an innovation adopted by a highly respected individual within a social network and creating an instinctive desire for a specific innovation. Another strategy includes injecting an innovation into a group of individuals who would readily use said technology, as well as providing positive reactions and benefits for early adopters.
Diffusion vs adoption

Adoption is an individual process detailing the series of stages one undergoes from first hearing about a product to finally adopting it. Diffusion signifies a group phenomenon, which suggests how an innovation spreads.
Adopter categories

Rogers defines an adopter category as a classification of individuals within a social system on the basis of innovativeness. In the book Diffusion of Innovations, Rogers suggests a total of five categories of adopters in order to standardize the usage of adopter categories in diffusion research. The adoption of an innovation follows an S curve when plotted over a length of time.[39] The categories of adopters are: innovators, early adopters, early majority, late majority and laggards[2] In addition to the gatekeepers and opinion leaders who exist within a given community, change agents may come from outside the community. Change agents bring innovations to new communities– ﬁrst through the gatekeepers, then through the opinion leaders, and so on through the community.
Adopter category 	Definition
Innovators 	Innovators are willing to take risks, have the highest social status, have financial liquidity, are social and have closest contact to scientific sources and interaction with other innovators. Their risk tolerance allows them to adopt technologies that may ultimately fail. Financial resources help absorb these failures.[40]
Early adopters 	These individuals have the highest degree of opinion leadership among the adopter categories. Early adopters have a higher social status, financial liquidity, advanced education and are more socially forward than late adopters. They are more discreet in adoption choices than innovators. They use judicious choice of adoption to help them maintain a central communication position.[41]
Early Majority 	They adopt an innovation after a varying degree of time that is significantly longer than the innovators and early adopters. Early Majority have above average social status, contact with early adopters and seldom hold positions of opinion leadership in a system (Rogers 1962, p. 283)
Late Majority 	They adopt an innovation after the average participant. These individuals approach an innovation with a high degree of skepticism and after the majority of society has adopted the innovation. Late Majority are typically skeptical about an innovation, have below average social status, little financial liquidity, in contact with others in late majority and early majority and little opinion leadership.
Laggards 	They are the last to adopt an innovation. Unlike some of the previous categories, individuals in this category show little to no opinion leadership. These individuals typically have an aversion to change-agents. Laggards typically tend to be focused on "traditions", lowest social status, lowest financial liquidity, oldest among adopters, and in contact with only family and close friends.
Failed diffusion

Failed diffusion does not mean that the technology was adopted by no one. Rather, failed diffusion often refers to diffusion that does not reach or approach 100% adoption due to its own weaknesses, competition from other innovations, or simply a lack of awareness. From a social networks perspective, a failed diffusion might be widely adopted within certain clusters but fail to make an impact on more distantly related people. Networks that are over-connected might suffer from a rigidity that prevents the changes an innovation might bring, as well.[42][43] Sometimes, some innovations also fail as a result of lack of local involvement and community participation.

For example, Rogers discussed a situation in Peru involving the implementation of boiling drinking water to improve health and wellness levels in the village of Los Molinas. The residents had no knowledge of the link between sanitation and illness. The campaign worked with the villagers to try to teach them to boil water, burn their garbage, install latrines and report cases of illness to local health agencies. In Los Molinas, a stigma was linked to boiled water as something that only the "unwell" consumed, and thus, the idea of healthy residents boiling water prior to consumption was frowned upon. The two-year educational campaign was considered to be largely unsuccessful. This failure exemplified the importance of the roles of the communication channels that are involved in such a campaign for social change. An examination of diffusion in El Salvador determined that there can be more than one social network at play as innovations are communicated. One network carries information and the other carries influence. While people might hear of an innovation's uses, in Rogers' Los Molinas sanitation case, a network of influence and status prevented adoption.[44][45]
Heterophily and communication channels

Lazarsfeld and Merton first called attention to the principles of homophily and its opposite, heterophily. Using their definition, Rogers defines homophily as "the degree to which pairs of individuals who interact are similar in certain attributes, such as beliefs, education, social status, and the like".[46] When given the choice, individuals usually choose to interact with someone similar to themselves. Homophilous individuals engage in more effective communication because their similarities lead to greater knowledge gain as well as attitude or behavior change. As a result, homophilous people tend to promote diffusion among each other.[47] However, diffusion requires a certain degree of heterophily to introduce new ideas into a relationship; if two individuals are identical, no diffusion occurs because there is no new information to exchange. Therefore, an ideal situation would involve potential adopters who are homophilous in every way, except in knowledge of the innovation.[48]

Promotion of healthy behavior provides an example of the balance required of homophily and heterophily. People tend to be close to others of similar health status.[49] As a result, people with unhealthy behaviors like smoking and obesity are less likely to encounter information and behaviors that encourage good health. This presents a critical challenge for health communications, as ties between heterophilous people are relatively weaker, harder to create, and harder to maintain.[50] Developing heterophilous ties to unhealthy communities can increase the effectiveness of the diffusion of good health behaviors. Once one previously homophilous tie adopts the behavior or innovation, the other members of that group are more likely to adopt it, too.[51]
The role of social systems
Opinion leaders

Not all individuals exert an equal amount of influence over others. In this sense opinion leaders are influential in spreading either positive or negative information about an innovation. Rogers relies on the ideas of Katz & Lazarsfeld and the two-step flow theory in developing his ideas on the influence of opinion leaders.[52]

Opinion leaders have the most influence during the evaluation stage of the innovation-decision process and on late adopters.[53] In addition opinion leaders typically have greater exposure to the mass media, more cosmopolitan, greater contact with change agents, more social experience and exposure, higher socioeconomic status, and are more innovative than others.

Research was done in the early 1950s at the University of Chicago attempting to assess the cost-effectiveness of broadcast advertising on the diffusion of new products and services.[54] The findings were that opinion leadership tended to be organized into a hierarchy within a society, with each level in the hierarchy having most influence over other members in the same level, and on those in the next level below it. The lowest levels were generally larger in numbers and tended to coincide with various demographic attributes that might be targeted by mass advertising. However, it found that direct word of mouth and example were far more influential than broadcast messages, which were only effective if they reinforced the direct influences. This led to the conclusion that advertising was best targeted, if possible, on those next in line to adopt, and not on those not yet reached by the chain of influence.

Other research relating the concept to public choice theory finds that the hierarchy of influence for innovations need not, and likely does not, coincide with hierarchies of official, political, or economic status.[55] Elites are often not innovators, and innovations may have to be introduced by outsiders and propagated up a hierarchy to the top decision makers.
Electronic communication social networks

Prior to the introduction of the Internet, it was argued that social networks had a crucial role in the diffusion of innovation particularly tacit knowledge in the book The IRG Solution – hierarchical incompetence and how to overcome it.[56] The book argued that the widespread adoption of computer networks of individuals would lead to much better diffusion of innovations, with greater understanding of their possible shortcomings and the identification of needed innovations that would not have otherwise occurred. The social model proposed by Ryan and Gross[36] is expanded by Valente who uses social networks as a basis for adopter categorization instead of solely relying on the system-level analysis used by Ryan and Gross. Valente also looks at an individual's personal network, which is a different application than the organizational perspective espoused by many other scholars.[57]

Recent research by Wear shows, that particularly in regional and rural areas, significantly more innovation takes place in communities which have stronger inter-personal networks.[58]
Organizations

Innovations are often adopted by organizations through two types of innovation-decisions: collective innovation decisions and authority innovation decisions. The collective decision occurs when adoption is by consensus. The authority decision occurs by adoption among very few individuals with high positions of power within an organization.[59] Unlike the optional innovation decision process, these decision processes only occur within an organization or hierarchical group. Within an organization certain individuals are termed "champions" who stand behind an innovation and break through opposition. The champion plays a very similar role as the champion used within the efficiency business model Six Sigma. The process contains five stages that are slightly similar to the innovation-decision process that individuals undertake. These stages are: agenda-setting, matching, redefining/restructuring, clarifying and routinizing.
Extensions of the theory
Policy

Diffusion of Innovations has been applied beyond its original domains. In the case of political science and administration, policy diffusion focuses on how institutional innovations are adopted by other institutions, at the local, state, or country level. An alternative term is 'policy transfer' where the focus is more on the agents of diffusion and the diffusion of policy knowledge, such as in the work of Diane Stone.[60] Specifically, policy transfer can be defined as "knowledge about how policies administrative arrangements, institutions, and ideas in one political setting (past or present) is used in the development of policies, administrative arrangements, institutions, and ideas in another political setting".[61]

The first interests with regards to policy diffusion were focused in time variation or state lottery adoption,[62] but more recently interest has shifted towards mechanisms (emulation, learning and coercion)[63][64] or in channels of diffusion[65] where researchers find that regulatory agency creation is transmitted by country and sector channels. At the local level, examining popular city-level policies make it easy to find patterns in diffusion through measuring public awareness.[66] At the international level, economic policies have been thought to transfer among countries according to local politicians' learning of successes and failures elsewhere and outside mandates made by global financial organizations.[67] As a group of countries succeed with a set of policies, others follow, as exemplified by the deregulation and liberalization across the developing world after the successes of the Asian Tigers. The reintroduction of regulations in the early 2000s also shows this learning process, which would fit under the stages of knowledge and decision, can be seen as lessons learned by following China's successful growth.[68]
Technology

Peres, Muller and Mahajan suggested that diffusion is "the process of the market penetration of new products and services that is driven by social inﬂuences, which include all interdependencies among consumers that affect various market players with or without their explicit knowledge".[69]

Eveland evaluated diffusion from a phenomenological view, stating, "Technology is information, and exists only to the degree that people can put it into practice and use it to achieve values".[70]

Diffusion of existing technologies has been measured using "S curves". These technologies include radio, television, VCR, cable, flush toilet, clothes washer, refrigerator, home ownership, air conditioning, dishwasher, electrified households, telephone, cordless phone, cellular phone, per capita airline miles, personal computer and the Internet. These data[71] can act as a predictor for future innovations.

Diffusion curves for infrastructure[72] reveal contrasts in the diffusion process of personal technologies versus infrastructure.
Consequences of adoption

Both positive and negative outcomes are possible when an individual or organization chooses to adopt a particular innovation. Rogers states that this area needs further research because of the biased positive attitude that is associated with innovation.[73] Rogers lists three categories for consequences: desirable vs. undesirable, direct vs. indirect, and anticipated vs. unanticipated.

In contrast Wejnert details two categories: public vs. private and benefits vs. costs.[74]
Public versus private

Public consequences comprise the impact of an innovation on those other than the actor, while private consequences refer to the impact on the actor. Public consequences usually involve collective actors, such as countries, states, organizations or social movements. The results are usually concerned with issues of societal well-being. Private consequences usually involve individuals or small collective entities, such as a community. The innovations are usually concerned with the improvement of quality of life or the reform of organizational or social structures.[75]
Benefits versus costs

Benefits of an innovation obviously are the positive consequences, while the costs are the negative. Costs may be monetary or nonmonetary, direct or indirect. Direct costs are usually related to financial uncertainty and the economic state of the actor. Indirect costs are more difficult to identify. An example would be the need to buy a new kind of pesticide to use innovative seeds. Indirect costs may also be social, such as social conflict caused by innovation.[75] Marketers are particularly interested in the diffusion process as it determines the success or failure of a new product. It is quite important for a marketer to understand the diffusion process so as to ensure proper management of the spread of a new product or service.
Mathematical treatment
Main article: Logistic function

The diffusion of an innovation typically follows an S shaped curve which often resembles a logistic function. Mathematical programming models such as the S-D model apply the diffusion of innovations theory to real data problems.[76]
Complex Systems models

Complex network models can also be used to investigate the spread of innovations among individuals connected to each other by a network of peer-to-peer influences, such as in a physical community or neighborhood.[77]

Such models represent a system of individuals as nodes in a network (or graph). The interactions that link these individuals are represented by the edges of the network and can be based on the probability or strength of social connections. In the dynamics of such models, each node is assigned a current state, indicating whether or not the individual has adopted the innovation, and model equations describe the evolution of these states over time.[78]

In threshold models,[79] the uptake of technologies is determined by the balance of two factors: the (perceived) usefulness (sometimes called utility) of the innovation to the individual as well as barriers to adoption, such as cost. The multiple parameters that influence decisions to adopt, both individual and socially motivated, can be represented by such models as a series of nodes and connections that represent real relationships. Borrowing from social network analysis, each node is an innovator, an adopter, or a potential adopter. Potential adopters have a threshold, which is a fraction of his neighbors who adopt the innovation that must be reached before he will adopt. Over time, each potential adopter views his neighbors and decides whether he should adopt based on the technologies they are using. When the effect of each individual node is analyzed along with its influence over the entire network, the expected level of adoption was seen to depend on the number of initial adopters and the network's structure and properties. Two factors emerge as important to successful spread of the innovation: the number of connections of nodes with their neighbors and the presence of a high degree of common connections in the network (quantified by the clustering coefficient). These models are particularly good at showing the impact of opinion leaders relative to others.[80] Computer models are often used to investigate this balance between the social aspects of diffusion and perceived intrinsic benefit to the individuals.[81]
Criticism

Because there are more than four thousand articles across many disciplines published on Diffusion of Innovations, with a vast majority written after Rogers created a systematic theory, there have been few widely adopted changes to the theory.[7] Although each study applies the theory in slightly different ways, this lack of cohesion has left the theory stagnant and difficult to apply with consistency to new problems.[82][83]

Diffusion is difficult to quantify because humans and human networks are complex. It is extremely difficult, if not impossible, to measure what exactly causes adoption of an innovation.[84] This is important, particularly in healthcare. Those encouraging adoption of health behaviors or new medical technologies need to be aware of the many forces acting on an individual and his or her decision to adopt a new behavior or technology. Diffusion theories can never account for all variables, and therefore might miss critical predictors of adoption.[85] This variety of variables has also led to inconsistent results in research, reducing heuristic value.[86]

Rogers placed the contributions and criticisms of diffusion research into four categories: pro-innovation bias, individual-blame bias, recall problem, and issues of equality. The pro-innovation bias, in particular, implies that all innovation is positive and that all innovations should be adopted.[1] Cultural traditions and beliefs can be consumed by another culture's through diffusion, which can impose significant costs on a group of people.[86] The one-way information flow, from sender to receiver, is another weakness of this theory. The message sender has a goal to persuade the receiver, and there is little to no reverse flow. The person implementing the change controls the direction and outcome of the campaign. In some cases, this is the best approach, but other cases require a more participatory approach.[87] In complex environments where the adopter is receiving information from many sources and is returning feedback to the sender, a one-way model is insufficient and multiple communication flows need to be examined.[88]
See also

    Collaborative innovation network
    Critical mass (sociodynamics)
    Delphi technique
    Hierarchical organization
    Information Revolution
    Lateral communication
    Lateral diffusion
    Lazy User Model
    Memetics
    Opinion leadership
    Pro-innovation bias
    Public Choice Theory
    Sociological theory of diffusion
    Tacit knowledge
    Technological revolution
    The Wisdom of Crowds


Ingenuity is the quality of being clever, original, and inventive, often in the process of applying ideas to solve problems or meet challenges. Ingenuity (Ingenium) is the root Latin word for engineering. For example, the process of figuring out how to cross a mountain stream using a fallen log, building an airplane model from a sheet of paper, or starting a new company in a foreign culture all involve the exercising of ingenuity. Human ingenuity has led to various technological developments through applied science, and can also be seen in the development of new social organizations, institutions, and relationships. Ingenuity involves the most complex human thought processes, bringing together our thinking and acting both individually and collectively to take advantage of opportunities and/or overcome problems.

One example of how ingenuity is used conceptually can be found in the analysis of Thomas Homer-Dixon, building on that of Paul Romer, to refer to what is usually called instructional capital. In the case of Homer-Dixon, his use of the phrase 'ingenuity gap' denotes the space between a challenge and a solution. His particular contribution is to explore the social dimensions of ingenuity. Typically we think of ingenuity being used to build faster computers or more advanced medical treatments. Homer-Dixon argues that as the complexity of the world increases, our ability to solve the problems we face is becoming critical. Human ingenuity is also included in many school systems, with most teachers encouraging students to be educated in human ingenuity.

These challenges require more than improvements arising from physics, chemistry and biology, as one will need to consider the highly complex interactions of individuals, institutions, cultures, and networks involving all of the human family around the globe. Organizing ourselves differently, communicating and making decisions in new ways, are examples of social ingenuity. If one's ability to generate adequate solutions to these problems is inadequate, the ingenuity gap will lead to a wide range of social problems. The full exploration of these ideas in meeting social challenges is featured in The Ingenuity Gap[1] 
, one of Thomas Homer-Dixon's earliest books.

In another of Homer-Dixon's book, The Up Side of Down[2] 
, he argues that increasingly expensive oil, driven by scarcity, will lead to great social instability. Walking across an empty room requires very little ingenuity. If the room is full of snakes, hungry bears, and land mines, the ingenuity requirement will have gone up considerably.

Ingenuity is often inherent in creative individuals, and thus is considered hard to separate from individual capital. It is not clear if Dixon or Romer considered it impossible to do so, or if they were simply not familiar with the prior analysis of "applied ideas", "intellectual capital", "talent", or "innovation" where instructional and individual contributions have been carefully separated, by economic theorists.
See also
	Look up ingenuity in Wiktionary, the free dictionary.

    Ingenuity Systems
    Creativity techniques
    Ingenuity Gap
    instructional capital
    International Innovation Index
    Diffusion of innovations

The selection of the research method is crucial for what conclusions you can make about a phenomenon. It affects what you can say about the cause and factors influencing the phenomenon.

It is also important to choose a research method which is within the limits of what the researcher can do. Time, money, feasibility, ethics and availability to measure the phenomenon correctly are examples of issues constraining the research.
Choosing the Measurement

Choosing the scientific measurements are also crucial for getting the correct conclusion. Some measurements might not reflect the real world, because they do not measure the phenomenon as it should.
Results
Significance Test

To test a hypothesis, quantitative research uses significance tests to determine which hypothesis is right.

The significance test can show whether the null hypothesis is more likely correct than the research hypothesis. Research methodology in a number of areas like social sciences depends heavily on significance tests.

A significance test may even drive the research process in a whole new direction, based on the findings.

The t-test (also called the Student's T-Test) is one of many statistical significance tests, which compares two supposedly equal sets of data to see if they really are alike or not. The t-test helps the researcher conclude whether a hypothesis is supported or not.
Drawing Conclusions

Drawing a conclusion is based on several factors of the research process, not just because the researcher got the expected result. It has to be based on the validity and reliability of the measurement, how good the measurement was to reflect the real world and what more could have affected the results.

The observations are often referred to as 'empirical evidence' and the logic/thinking leads to the conclusions. Anyone should be able to check the observation and logic, to see if they also reach the same conclusions.

Errors of the observations may stem from measurement-problems, misinterpretations, unlikely random events etc.

A common error is to think that correlation implies a causal relationship. This is not necessarily true.
Generalization

Generalization is to which extent the research and the conclusions of the research apply to the real world. It is not always so that good research will reflect the real world, since we can only measure a small portion of the population at a time.

Generalization in Research


Validity and Reliability

Validity refers to what degree the research reflects the given research problem, while Reliability refers to how consistent a set of measurements are.

Validity and Reliability

Types of validity:

    External Validity
    Population Validity
    Ecological Validity
    Internal Validity
    Content Validity
    Face Validity
    Construct Validity
    Convergent and Discriminant Validity
    Test Validity
    Criterion Validity
    Concurrent Validity
    Predictive Validity

A definition of reliability may be "Yielding the same or compatible results in different clinical experiments or statistical trials" (the free dictionary). Research methodology lacking reliability cannot be trusted. Replication studies are a way to test reliability.

Types of Reliability:

    Test-Retest Reliability
    Interrater Reliability
    Internal Consistency Reliability
    Instrument Reliability
    Statistical Reliability
    Reproducibility

Both validity and reliability are important aspects of the research methodology to get better explanations of the world.
Errors in Research

Logically, there are two types of errors when drawing conclusions in research:

Type 1 error is when we accept the research hypothesis when the null hypothesis is in fact correct.

Type 2 error is when we reject the research hypothesis even if the null hypothesis is wrong.
The scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge.[2] To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning.[3] The Oxford Dictionaries Online define the scientific method as "a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses".[4]

The scientific method is an ongoing process, which usually begins with observations about the natural world. Human beings are naturally inquisitive, so they often come up with questions about things they see or hear and often develop ideas (hypotheses) about why things are the way they are. The best hypotheses lead to predictions that can be tested in various ways, including making further observations about nature. In general, the strongest tests of hypotheses come from carefully controlled and replicated experiments that gather empirical data. Depending on how well the tests match the predictions, the original hypothesis may require refinement, alteration, expansion or even rejection. If a particular hypothesis becomes very well supported a general theory may be developed.[1]

Although procedures vary from one field of inquiry to another, identifiable features are frequently shared in common between them. The overall process of the scientific method involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions.[5][6] A hypothesis is a conjecture, based on knowledge obtained while formulating the question. The hypothesis might be very specific or it might be broad. Scientists then test hypotheses by conducting experiments. Under modern interpretations, a scientific hypothesis must be falsifiable, implying that it is possible to identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.[7]

The purpose of an experiment is to determine whether observations agree with or conflict with the predictions derived from a hypothesis.[8] Experiments can take place in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars, and so on. There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, it represents rather a set of general principles.[9] Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order.[10] Some philosophers and scientists have argued that there is no scientific method. For example, Lee Smolin[11] and Paul Feyerabend (in his Against Method). Nola and Sankey remark that "For some, the whole idea of a theory of scientific method is yester-year's debate".[12]
Contents

    1 Overview
        1.1 Process
        1.2 DNA example
        1.3 Other components
    2 Scientific inquiry
        2.1 Properties of scientific inquiry
        2.2 Beliefs and biases
    3 Elements of the scientific method
        3.1 Characterizations
        3.2 Hypothesis development
        3.3 Predictions from the hypothesis
        3.4 Experiments
        3.5 Evaluation and improvement
        3.6 Confirmation
    4 Models of scientific inquiry
        4.1 Classical model
        4.2 Pragmatic model
    5 Communication and community
        5.1 Peer review evaluation
        5.2 Documentation and replication
        5.3 Dimensions of practice
    6 Philosophy and sociology of science
        6.1 Role of chance in discovery
    7 History
    8 Relationship with mathematics
    9 Relationship with statistics
    10 See also
        10.1 Problems and issues
        10.2 History, philosophy, sociology
    11 Notes
    12 References
    13 Further reading
    14 External links

Overview

    The DNA example below is a synopsis of this method

Ibn al-Haytham (Alhazen), 965–1039 Iraq. A polymath, considered by some to be the father of modern scientific methodology, due to his emphasis on experimental data and reproducibility of its results.[13][14]
Johannes Kepler (1571–1630). "Kepler shows his keen logical sense in detailing the whole process by which he finally arrived at the true orbit. This is the greatest piece of Retroductive reasoning ever performed." – C. S. Peirce, c. 1896, on Kepler's reasoning through explanatory hypotheses[15]
According to Morris Kline,[16] "Modern science owes its present flourishing state to a new scientific method which was fashioned almost entirely by Galileo Galilei" (1564−1642). Dudley Shapere[17] takes a more measured view of Galileo's contribution.

The scientific method is the process by which science is carried out.[18] As in other areas of inquiry, science (through the scientific method) can build on previous knowledge and develop a more sophisticated understanding of its topics of study over time.[19][20][21][22][23][24] This model can be seen to underlay the scientific revolution.[25] One thousand years ago, Alhazen argued the importance of forming questions and subsequently testing them,[26] an approach which was advocated by Galileo in 1638 with the publication of Two New Sciences.[27] The current method is based on a hypothetico-deductive model[28] formulated in the 20th century, although it has undergone significant revision since first proposed (for a more formal discussion, see below).
Process

The overall process involves making conjectures (hypotheses), deriving predictions from them as logical consequences, and then carrying out experiments based on those predictions to determine whether the original conjecture was correct.[5] There are difficulties in a formulaic statement of method, however. Though the scientific method is often presented as a fixed sequence of steps, they are better considered as general principles.[29] Not all steps take place in every scientific inquiry (or to the same degree), and are not always in the same order. As noted by William Whewell (1794–1866), "invention, sagacity, [and] genius"[10] are required at every step.
Formulation of a question

The question can refer to the explanation of a specific observation, as in "Why is the sky blue?", but can also be open-ended, as in "How can I design a drug to cure this particular disease?" This stage frequently involves looking up and evaluating evidence from previous experiments, personal scientific observations or assertions, and/or the work of other scientists. If the answer is already known, a different question that builds on the previous evidence can be posed. When applying the scientific method to scientific research, determining a good question can be very difficult and affects the final outcome of the investigation.[30]
Hypothesis

A hypothesis is a conjecture, based on knowledge obtained while formulating the question, that may explain the observed behavior of a part of our universe. The hypothesis might be very specific, e.g., Einstein's equivalence principle or Francis Crick's "DNA makes RNA makes protein",[31] or it might be broad, e.g., unknown species of life dwell in the unexplored depths of the oceans. A statistical hypothesis is a conjecture about some population. For example, the population might be people with a particular disease. The conjecture might be that a new drug will cure the disease in some of those people. Terms commonly associated with statistical hypotheses are null hypothesis and alternative hypothesis. A null hypothesis is the conjecture that the statistical hypothesis is false, e.g., that the new drug does nothing and that any cures are due to chance effects. Researchers normally want to show that the null hypothesis is false. The alternative hypothesis is the desired outcome, e.g., that the drug does better than chance. A final point: a scientific hypothesis must be falsifiable, meaning that one can identify a possible outcome of an experiment that conflicts with predictions deduced from the hypothesis; otherwise, it cannot be meaningfully tested.
Prediction

This step involves determining the logical consequences of the hypothesis. One or more predictions are then selected for further testing. The more unlikely that a prediction would be correct simply by coincidence, then the more convincing it would be if the prediction were fulfilled; evidence is also stronger if the answer to the prediction is not already known, due to the effects of hindsight bias (see also postdiction). Ideally, the prediction must also distinguish the hypothesis from likely alternatives; if two hypotheses make the same prediction, observing the prediction to be correct is not evidence for either one over the other. (These statements about the relative strength of evidence can be mathematically derived using Bayes' Theorem).[32]
Testing

This is an investigation of whether the real world behaves as predicted by the hypothesis. Scientists (and other people) test hypotheses by conducting experiments. The purpose of an experiment is to determine whether observations of the real world agree with or conflict with the predictions derived from a hypothesis. If they agree, confidence in the hypothesis increases; otherwise, it decreases. Agreement does not assure that the hypothesis is true; future experiments may reveal problems. Karl Popper advised scientists to try to falsify hypotheses, i.e., to search for and test those experiments that seem most doubtful. Large numbers of successful confirmations are not convincing if they arise from experiments that avoid risk.[8] Experiments should be designed to minimize possible errors, especially through the use of appropriate scientific controls. For example, tests of medical treatments are commonly run as double-blind tests. Test personnel, who might unwittingly reveal to test subjects which samples are the desired test drugs and which are placebos, are kept ignorant of which are which. Such hints can bias the responses of the test subjects. Furthermore, failure of an experiment does not necessarily mean the hypothesis is false. Experiments always depend on several hypotheses, e.g., that the test equipment is working properly, and a failure may be a failure of one of the auxiliary hypotheses. (See the Duhem–Quine thesis.) Experiments can be conducted in a college lab, on a kitchen table, at CERN's Large Hadron Collider, at the bottom of an ocean, on Mars (using one of the working rovers), and so on. Astronomers do experiments, searching for planets around distant stars. Finally, most individual experiments address highly specific topics for reasons of practicality. As a result, evidence about broader topics is usually accumulated gradually.
Analysis

This involves determining what the results of the experiment show and deciding on the next actions to take. The predictions of the hypothesis are compared to those of the null hypothesis, to determine which is better able to explain the data. In cases where an experiment is repeated many times, a statistical analysis such as a chi-squared test may be required. If the evidence has falsified the hypothesis, a new hypothesis is required; if the experiment supports the hypothesis but the evidence is not strong enough for high confidence, other predictions from the hypothesis must be tested. Once a hypothesis is strongly supported by evidence, a new question can be asked to provide further insight on the same topic. Evidence from other scientists and experience are frequently incorporated at any stage in the process. Depending on the complexity of the experiment, many iterations may be required to gather sufficient evidence to answer a question with confidence, or to build up many answers to highly specific questions in order to answer a single broader question.
DNA example
DNA icon (25x25).png 	The basic elements of the scientific method are illustrated by the following example from the discovery of the structure of DNA:

    Question: Previous investigation of DNA had determined its chemical composition (the four nucleotides), the structure of each individual nucleotide, and other properties. It had been identified as the carrier of genetic information by the Avery–MacLeod–McCarty experiment in 1944,[33] but the mechanism of how genetic information was stored in DNA was unclear.
    Hypothesis: Linus Pauling, Francis Crick and James D. Watson hypothesized that DNA had a helical structure.[34]
    Prediction: If DNA had a helical structure, its X-ray diffraction pattern would be X-shaped.[35][36] This prediction was determined using the mathematics of the helix transform, which had been derived by Cochran, Crick and Vand[37] (and independently by Stokes). This prediction was a mathematical construct, completely independent from the biological problem at hand.
    Experiment: Rosalind Franklin crystallized pure DNA and performed X-ray diffraction to produce photo 51. The results showed an X-shape.
    Analysis: When Watson saw the detailed diffraction pattern, he immediately recognized it as a helix.[38][39] He and Crick then produced their model, using this information along with the previously known information about DNA's composition and about molecular interactions such as hydrogen bonds.[40]

The discovery became the starting point for many further studies involving the genetic material, such as the field of molecular genetics, and it was awarded the Nobel Prize in 1962. Each step of the example is examined in more detail later in the article.
Other components

The scientific method also includes other components required even when all the iterations of the steps above have been completed:[41]
Replication

If an experiment cannot be repeated to produce the same results, this implies that the original results might have been in error. As a result, it is common for a single experiment to be performed multiple times, especially when there are uncontrolled variables or other indications of experimental error. For significant or surprising results, other scientists may also attempt to replicate the results for themselves, especially if those results would be important to their own work.[42]
External review

The process of peer review involves evaluation of the experiment by experts, who typically give their opinions anonymously. Some journals request that the experimenter provide lists of possible peer reviewers, especially if the field is highly specialized. Peer review does not certify correctness of the results, only that, in the opinion of the reviewer, the experiments themselves were sound (based on the description supplied by the experimenter). If the work passes peer review, which occasionally may require new experiments requested by the reviewers, it will be published in a peer-reviewed scientific journal. The specific journal that publishes the results indicates the perceived quality of the work.[43]
Data recording and sharing

Scientists typically are careful in recording their data, a requirement promoted by Ludwik Fleck (1896–1961) and others.[44] Though not typically required, they might be requested to supply this data to other scientists who wish to replicate their original results (or parts of their original results), extending to the sharing of any experimental samples that may be difficult to obtain.[45]
Scientific inquiry

Scientific inquiry generally aims to obtain knowledge in the form of testable explanations that can be used to predict the results of future experiments. This allows scientists to gain a better understanding of the topic being studied, and later be able to use that understanding to intervene in its causal mechanisms (such as to cure disease). The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it is to continue explaining a body of evidence better than its alternatives. The most successful explanations, which explain and make accurate predictions in a wide range of circumstances, are often called scientific theories.

Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding is typically the result of a gradual process of development over time, sometimes across different domains of science.[46] Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question is more powerful than its alternatives at explaining the evidence. Often the explanations are altered over time, or explanations are combined to produce new explanations.
Properties of scientific inquiry

Scientific knowledge is closely tied to empirical findings, and can remain subject to falsification if new experimental observation incompatible with it is found. That is, no theory can ever be considered final, since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory can be argued to be related to how long it has persisted without major alteration to its core principles.

Theories can also subject to subsumption by other theories. For example, thousands of years of scientific observations of the planets were explained almost perfectly by Newton's laws. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicting and explaining other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected to each other, unified by principles of increasing explanatory power.[47]

Since new theories might be more comprehensive than what preceded them, and thus be able to explain more than previous ones, successor theories might be able to meet a higher standard by explaining a larger body of observations than their predecessors.[47] For example, the theory of evolution explains the diversity of life on Earth, how species adapt to their environments, and many other patterns observed in the natural world;[48][49] its most recent major modification was unification with genetics to form the modern evolutionary synthesis. In subsequent modifications, it has also subsumed aspects of many other fields such as biochemistry and molecular biology.
Beliefs and biases
Flying gallop falsified; see image below
Muybridge's photographs of The Horse in Motion, 1878, were used to answer the question whether all four feet of a galloping horse are ever off the ground at the same time. This demonstrates a use of photography in science.

Scientific methodology often directs that hypotheses be tested in controlled conditions wherever possible. This is frequently possible in certain areas, such as in the biological sciences, and more difficult in other areas, such as in astronomy. The practice of experimental control and reproducibility can have the effect of diminishing the potentially harmful effects of circumstance, and to a degree, personal bias. For example, pre-existing beliefs can alter the interpretation of results, as in confirmation bias; this is a heuristic that leads a person with a particular belief to see things as reinforcing their belief, even if another observer might disagree (in other words, people tend to observe what they expect to observe).

A historical example is the belief that the legs of a galloping horse are splayed at the point when none of the horse's legs touches the ground, to the point of this image being included in paintings by its supporters. However, the first stop-action pictures of a horse's gallop by Eadweard Muybridge showed this to be false, and that the legs are instead gathered together.[50] Another important human bias that plays a role is a preference for new, surprising statements (see appeal to novelty), which can result in a search for evidence that the new is true.[2] In contrast to this standard in the scientific method, poorly attested beliefs can be believed and acted upon via a less rigorous heuristic,[51] sometimes taking advantage of the narrative fallacy that when narrative is constructed its elements become easier to believe.[52][53] Sometimes, these have their elements assumed a priori, or contain some other logical or methodological flaw in the process that ultimately produced them.[54]
Elements of the scientific method

There are different ways of outlining the basic method used for scientific inquiry. The scientific community and philosophers of science generally agree on the following classification of method components. These methodological elements and organization of procedures tend to be more characteristic of natural sciences than social sciences. Nonetheless, the cycle of formulating hypotheses, testing and analyzing the results, and formulating new hypotheses, will resemble the cycle described below.

    Four essential elements[55][56][57] of the scientific method[58] are iterations,[59][60] recursions,[61] interleavings, or orderings of the following:

        Characterizations (observations,[62] definitions, and measurements of the subject of inquiry)
        Hypotheses[63][64] (theoretical, hypothetical explanations of observations and measurements of the subject)[65]
        Predictions (reasoning including deductive reasoning[66] from the hypothesis or theory)
        Experiments[67] (tests of all of the above)

Each element of the scientific method is subject to peer review for possible mistakes. These activities do not describe all that scientists do (see below) but apply mostly to experimental sciences (e.g., physics, chemistry, and biology). The elements above are often taught in the educational system as "the scientific method".[68]

The scientific method is not a single recipe: it requires intelligence, imagination, and creativity.[69] In this sense, it is not a mindless set of standards and procedures to follow, but is rather an ongoing cycle, constantly developing more useful, accurate and comprehensive models and methods. For example, when Einstein developed the Special and General Theories of Relativity, he did not in any way refute or discount Newton's Principia. On the contrary, if the astronomically large, the vanishingly small, and the extremely fast are removed from Einstein's theories – all phenomena Newton could not have observed – Newton's equations are what remain. Einstein's theories are expansions and refinements of Newton's theories and, thus, increase our confidence in Newton's work.

A linearized, pragmatic scheme of the four points above is sometimes offered as a guideline for proceeding:[70]

    Define a question
    Gather information and resources (observe)
    Form an explanatory hypothesis
    Test the hypothesis by performing an experiment and collecting data in a reproducible manner
    Analyze the data
    Interpret the data and draw conclusions that serve as a starting point for new hypothesis
    Publish results
    Retest (frequently done by other scientists)

The iterative cycle inherent in this step-by-step method goes from point 3 to 6 back to 3 again.

While this schema outlines a typical hypothesis/testing method,[71] it should also be noted that a number of philosophers, historians and sociologists of science (perhaps most notably Paul Feyerabend) claim that such descriptions of scientific method have little relation to the ways that science is actually practiced.
Characterizations

The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The subjects can also be called unsolved problems or the unknowns.) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; the observations often demand careful measurements and/or counting.

The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations. The measurements often require specialized scientific instruments such as thermometers, spectroscopes, particle accelerators, or voltmeters, and the progress of a scientific field is usually intimately tied to their invention and improvement.

    I am not accustomed to saying anything with certainty after only one or two observations.
    — Andreas Vesalius, (1546)[72]

Uncertainty

Measurements in scientific work are also usually accompanied by estimates of their uncertainty. The uncertainty is often estimated by making repeated measurements of the desired quantity. Uncertainties may also be calculated by consideration of the uncertainties of the individual underlying quantities used. Counts of things, such as the number of people in a nation at a particular time, may also have an uncertainty due to data collection limitations. Or counts may represent a sample of desired quantities, with an uncertainty that depends upon the sampling method used and the number of samples taken.
Definition

Measurements demand the use of operational definitions of relevant quantities. That is, a scientific quantity is described or defined by how it is measured, as opposed to some more vague, inexact or "idealized" definition. For example, electric current, measured in amperes, may be operationally defined in terms of the mass of silver deposited in a certain time on an electrode in an electrochemical device that is described in some detail. The operational definition of a thing often relies on comparisons with standards: the operational definition of "mass" ultimately relies on the use of an artifact, such as a particular kilogram of platinum-iridium kept in a laboratory in France.

The scientific definition of a term sometimes differs substantially from its natural language usage. For example, mass and weight overlap in meaning in common discourse, but have distinct meanings in mechanics. Scientific quantities are often characterized by their units of measure which can later be described in terms of conventional physical units when communicating the work.

New theories are sometimes developed after realizing certain terms have not previously been sufficiently clearly defined. For example, Albert Einstein's first paper on relativity begins by defining simultaneity and the means for determining length. These ideas were skipped over by Isaac Newton with, "I do not define time, space, place and motion, as being well known to all." Einstein's paper then demonstrates that they (viz., absolute time and length independent of motion) were approximations. Francis Crick cautions us that when characterizing a subject, however, it can be premature to define something when it remains ill-understood.[73] In Crick's study of consciousness, he actually found it easier to study awareness in the visual system, rather than to study free will, for example. His cautionary example was the gene; the gene was much more poorly understood before Watson and Crick's pioneering discovery of the structure of DNA; it would have been counterproductive to spend much time on the definition of the gene, before them.
DNA-characterizations
DNA icon (25x25).png

The history of the discovery of the structure of DNA is a classic example of the elements of the scientific method: in 1950 it was known that genetic inheritance had a mathematical description, starting with the studies of Gregor Mendel, and that DNA contained genetic information (Oswald Avery's transforming principle).[33] But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle.[74] ..2. DNA-hypotheses
Another example: precession of Mercury
Precession of the perihelion (exaggerated)

The characterization element can require extended and extensive study, even centuries. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic and European astronomers, to fully record the motion of planet Earth. Newton was able to include those measurements into consequences of his laws of motion. But the perihelion of the planet Mercury's orbit exhibits a precession that cannot be fully explained by Newton's laws of motion (see diagram to the right), as Leverrier pointed out in 1859. The observed difference for Mercury's precession between Newtonian theory and observation was one of the things that occurred to Einstein as a possible early test of his theory of General Relativity. His relativistic calculations matched observation much more closely than did Newtonian theory. The difference is approximately 43 arc-seconds per century.
Hypothesis development
Main article: Hypothesis formation

A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena.

Normally hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.

Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Charles Sanders Peirce, borrowing a page from Aristotle (Prior Analytics, 2.25) described the incipient stages of inquiry, instigated by the "irritation of doubt" to venture a plausible guess, as abductive reasoning. The history of science is filled with stories of scientists claiming a "flash of inspiration", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology.

William Glen observes that

    the success of a hypothesis, or its service to science, lies not simply in its perceived "truth", or power to displace, subsume or reduce a predecessor idea, but perhaps more in its ability to stimulate the research that will illuminate ... bald suppositions and areas of vagueness.[75]

In general scientists tend to look for theories that are "elegant" or "beautiful". In contrast to the usual English use of these terms, they here refer to a theory in accordance with the known facts, which is nevertheless relatively simple and easy to handle. Occam's Razor serves as a rule of thumb for choosing the most desirable amongst a group of equally explanatory hypotheses.
DNA-hypotheses
DNA icon (25x25).png

Linus Pauling proposed that DNA might be a triple helix.[76] This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong[77] and that Pauling would soon admit his difficulties with that structure. So, the race was on to figure out the correct structure (except that Pauling did not realize at the time that he was in a race) ..3. DNA-predictions
Predictions from the hypothesis
Main article: Prediction in science

Any useful hypothesis will enable predictions, by reasoning including deductive reasoning. It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction can also be statistical and deal only with probabilities.

It is essential that the outcome of testing such a prediction be currently unknown. Only in this case does a successful outcome increase the probability that the hypothesis is true. If the outcome is already known, it is called a consequence and should have already been considered while formulating the hypothesis.

If the predictions are not accessible by observation or experience, the hypothesis is not yet testable and so will remain to that extent unscientific in a strict sense. A new technology or theory might make the necessary experiments feasible. Thus, much scientifically based speculation might convince one (or many) that the hypothesis that other intelligent species exist is true. But since there no experiment now known which can test this hypothesis, science itself can have little to say about the possibility. In future, some new technique might lead to an experimental test and the speculation would then become part of accepted science.
DNA-predictions
DNA icon (25x25).png

James D. Watson, Francis Crick, and others hypothesized that DNA had a helical structure. This implied that DNA's X-ray diffraction pattern would be 'x shaped'.[36][78] This prediction followed from the work of Cochran, Crick and Vand[37] (and independently by Stokes). The Cochran-Crick-Vand-Stokes theorem provided a mathematical explanation for the empirical observation that diffraction from helical structures produces x shaped patterns.

In their first paper, Watson and Crick also noted that the double helix structure they proposed provided a simple mechanism for DNA replication, writing, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material".[79] ..4. DNA-experiments
Another example: general relativity
Einstein's prediction (1907): Light bends in a gravitational field

Einstein's theory of General Relativity makes several specific predictions about the observable structure of space-time, such as that light bends in a gravitational field, and that the amount of bending depends in a precise way on the strength of that gravitational field. Arthur Eddington's observations made during a 1919 solar eclipse supported General Relativity rather than Newtonian gravitation.[80]
Experiments
Main article: Experiment

Once predictions are made, they can be sought by experiments. If the test results contradict the predictions, the hypotheses which entailed them are called into question and become less tenable. Sometimes the experiments are conducted incorrectly or are not very well designed, when compared to a crucial experiment. If the experimental results confirm the predictions, then the hypotheses are considered more likely to be correct, but might still be wrong and continue to be subject to further testing. The experimental control is a technique for dealing with observational error. This technique uses the contrast between multiple samples (or observations) under differing conditions to see what varies or what remains the same. We vary the conditions for each measurement, to help isolate what has changed. Mill's canons can then help us figure out what the important factor is.[81] Factor analysis is one technique for discovering the important factor in an effect.

Depending on the predictions, the experiments can have different shapes. It could be a classical experiment in a laboratory setting, a double-blind study or an archaeological excavation. Even taking a plane from New York to Paris is an experiment which tests the aerodynamical hypotheses used for constructing the plane.

Scientists assume an attitude of openness and accountability on the part of those conducting an experiment. Detailed record keeping is essential, to aid in recording and reporting on the experimental results, and supports the effectiveness and integrity of the procedure. They will also assist in reproducing the experimental results, likely by others. Traces of this approach can be seen in the work of Hipparchus (190–120 BCE), when determining a value for the precession of the Earth, while controlled experiments can be seen in the works of Jābir ibn Hayyān (721–815 CE), al-Battani (853–929) and Alhazen (965–1039).[82]
DNA-experiments
DNA icon (25x25).png

Watson and Crick showed an initial (and incorrect) proposal for the structure of DNA to a team from Kings College – Rosalind Franklin, Maurice Wilkins, and Raymond Gosling. Franklin immediately spotted the flaws which concerned the water content. Later Watson saw Franklin's detailed X-ray diffraction images which showed an X-shape 
and was able to confirm the structure was helical.[38][39] This rekindled Watson and Crick's model building and led to the correct structure. ..1. DNA-characterizations
Evaluation and improvement

The scientific method is iterative. At any stage it is possible to refine its accuracy and precision, so that some consideration will lead the scientist to repeat an earlier part of the process. Failure to develop an interesting hypothesis may lead a scientist to re-define the subject under consideration. Failure of a hypothesis to produce interesting and testable predictions may lead to reconsideration of the hypothesis or of the definition of the subject. Failure of an experiment to produce interesting results may lead a scientist to reconsider the experimental method, the hypothesis, or the definition of the subject.

Other scientists may start their own research and enter the process at any stage. They might adopt the characterization and formulate their own hypothesis, or they might adopt the hypothesis and deduce their own predictions. Often the experiment is not done by the person who made the prediction, and the characterization is based on experiments done by someone else. Published results of experiments can also serve as a hypothesis predicting their own reproducibility.
DNA-iterations
DNA icon (25x25).png

After considerable fruitless experimentation, being discouraged by their superior from continuing, and numerous false starts,[83][84][85] Watson and Crick were able to infer the essential structure of DNA by concrete modeling of the physical shapes of the nucleotides which comprise it.[40][86] They were guided by the bond lengths which had been deduced by Linus Pauling and by Rosalind Franklin's X-ray diffraction images. ..DNA Example
Confirmation

Science is a social enterprise, and scientific work tends to be accepted by the scientific community when it has been confirmed. Crucially, experimental and theoretical results must be reproduced by others within the scientific community. Researchers have given their lives for this vision; Georg Wilhelm Richmann was killed by ball lightning (1753) when attempting to replicate the 1752 kite-flying experiment of Benjamin Franklin.[87]

To protect against bad science and fraudulent data, government research-granting agencies such as the National Science Foundation, and science journals, including Nature and Science, have a policy that researchers must archive their data and methods so that other researchers can test the data and methods and build on the research that has gone before. Scientific data archiving can be done at a number of national archives in the U.S. or in the World Data Center.
Models of scientific inquiry
Main article: Models of scientific inquiry
Classical model

The classical model of scientific inquiry derives from Aristotle,[88] who distinguished the forms of approximate and exact reasoning, set out the threefold scheme of abductive, deductive, and inductive inference, and also treated the compound forms such as reasoning by analogy.
Pragmatic model
See also: Pragmatic theory of truth

In 1877,[19] Charles Sanders Peirce (/ˈpɜːrs/ like "purse"; 1839–1914) characterized inquiry in general not as the pursuit of truth per se but as the struggle to move from irritating, inhibitory doubts born of surprises, disagreements, and the like, and to reach a secure belief, belief being that on which one is prepared to act. He framed scientific inquiry as part of a broader spectrum and as spurred, like inquiry generally, by actual doubt, not mere verbal or hyperbolic doubt, which he held to be fruitless.[89] He outlined four methods of settling opinion, ordered from least to most successful:

    The method of tenacity (policy of sticking to initial belief) – which brings comforts and decisiveness but leads to trying to ignore contrary information and others' views as if truth were intrinsically private, not public. It goes against the social impulse and easily falters since one may well notice when another's opinion is as good as one's own initial opinion. Its successes can shine but tend to be transitory.[90]
    The method of authority – which overcomes disagreements but sometimes brutally. Its successes can be majestic and long-lived, but it cannot operate thoroughly enough to suppress doubts indefinitely, especially when people learn of other societies present and past.
    The method of the a priori – which promotes conformity less brutally but fosters opinions as something like tastes, arising in conversation and comparisons of perspectives in terms of "what is agreeable to reason." Thereby it depends on fashion in paradigms and goes in circles over time. It is more intellectual and respectable but, like the first two methods, sustains accidental and capricious beliefs, destining some minds to doubt it.
    The scientific method – the method wherein inquiry regards itself as fallible and purposely tests itself and criticizes, corrects, and improves itself.

Peirce held that slow, stumbling ratiocination can be dangerously inferior to instinct and traditional sentiment in practical matters, and that the scientific method is best suited to theoretical research,[91] which in turn should not be trammeled by the other methods and practical ends; reason's "first rule" is that, in order to learn, one must desire to learn and, as a corollary, must not block the way of inquiry.[92] The scientific method excels the others by being deliberately designed to arrive – eventually – at the most secure beliefs, upon which the most successful practices can be based. Starting from the idea that people seek not truth per se but instead to subdue irritating, inhibitory doubt, Peirce showed how, through the struggle, some can come to submit to truth for the sake of belief's integrity, seek as truth the guidance of potential practice correctly to its given goal, and wed themselves to the scientific method.[19][22]

For Peirce, rational inquiry implies presuppositions about truth and the real; to reason is to presuppose (and at least to hope), as a principle of the reasoner's self-regulation, that the real is discoverable and independent of our vagaries of opinion. In that vein he defined truth as the correspondence of a sign (in particular, a proposition) to its object and, pragmatically, not as actual consensus of some definite, finite community (such that to inquire would be to poll the experts), but instead as that final opinion which all investigators would reach sooner or later but still inevitably, if they were to push investigation far enough, even when they start from different points.[93] In tandem he defined the real as a true sign's object (be that object a possibility or quality, or an actuality or brute fact, or a necessity or norm or law), which is what it is independently of any finite community's opinion and, pragmatically, depends only on the final opinion destined in a sufficient investigation. That is a destination as far, or near, as the truth itself to you or me or the given finite community. Thus, his theory of inquiry boils down to "Do the science." Those conceptions of truth and the real involve the idea of a community both without definite limits (and thus potentially self-correcting as far as needed) and capable of definite increase of knowledge.[94] As inference, "logic is rooted in the social principle" since it depends on a standpoint that is, in a sense, unlimited.[95]

Paying special attention to the generation of explanations, Peirce outlined the scientific method as a coordination of three kinds of inference in a purposeful cycle aimed at settling doubts, as follows (in §III–IV in "A Neglected Argument"[5] except as otherwise noted):

    Abduction (or retroduction). Guessing, inference to explanatory hypotheses for selection of those best worth trying. From abduction, Peirce distinguishes induction as inferring, on the basis of tests, the proportion of truth in the hypothesis. Every inquiry, whether into ideas, brute facts, or norms and laws, arises from surprising observations in one or more of those realms (and for example at any stage of an inquiry already underway). All explanatory content of theories comes from abduction, which guesses a new or outside idea so as to account in a simple, economical way for a surprising or complicative phenomenon. Oftenest, even a well-prepared mind guesses wrong. But the modicum of success of our guesses far exceeds that of sheer luck and seems born of attunement to nature by instincts developed or inherent, especially insofar as best guesses are optimally plausible and simple in the sense, said Peirce, of the "facile and natural", as by Galileo's natural light of reason and as distinct from "logical simplicity". Abduction is the most fertile but least secure mode of inference. Its general rationale is inductive: it succeeds often enough and, without it, there is no hope of sufficiently expediting inquiry (often multi-generational) toward new truths.[96] Coordinative method leads from abducing a plausible hypothesis to judging it for its testability[97] and for how its trial would economize inquiry itself.[98] Peirce calls his pragmatism "the logic of abduction".[99] His pragmatic maxim is: "Consider what effects that might conceivably have practical bearings you conceive the objects of your conception to have. Then, your conception of those effects is the whole of your conception of the object".[93] His pragmatism is a method of reducing conceptual confusions fruitfully by equating the meaning of any conception with the conceivable practical implications of its object's conceived effects—a method of experimentational mental reflection hospitable to forming hypotheses and conducive to testing them. It favors efficiency. The hypothesis, being insecure, needs to have practical implications leading at least to mental tests and, in science, lending themselves to scientific tests. A simple but unlikely guess, if uncostly to test for falsity, may belong first in line for testing. A guess is intrinsically worth testing if it has instinctive plausibility or reasoned objective probability, while subjective likelihood, though reasoned, can be misleadingly seductive. Guesses can be chosen for trial strategically, for their caution (for which Peirce gave as example the game of Twenty Questions), breadth, and incomplexity.[100] One can hope to discover only that which time would reveal through a learner's sufficient experience anyway, so the point is to expedite it; the economy of research is what demands the leap, so to speak, of abduction and governs its art.[98]
    Deduction. Two stages:
        Explication. Unclearly premissed, but deductive, analysis of the hypothesis in order to render its parts as clear as possible.
        Demonstration: Deductive Argumentation, Euclidean in procedure. Explicit deduction of hypothesis's consequences as predictions, for induction to test, about evidence to be found. Corollarial or, if needed, Theorematic.
    Induction. The long-run validity of the rule of induction is deducible from the principle (presuppositional to reasoning in general[93]) that the real is only the object of the final opinion to which adequate investigation would lead;[101] anything to which no such process would ever lead would not be real. Induction involving ongoing tests or observations follows a method which, sufficiently persisted in, will diminish its error below any predesignate degree. Three stages:
        Classification. Unclearly premissed, but inductive, classing of objects of experience under general ideas.
        Probation: direct inductive argumentation. Crude (the enumeration of instances) or gradual (new estimate of proportion of truth in the hypothesis after each test). Gradual induction is qualitative or quantitative; if qualitative, then dependent on weightings of qualities or characters;[102] if quantitative, then dependent on measurements, or on statistics, or on countings.
        Sentential Induction. "...which, by inductive reasonings, appraises the different probations singly, then their combinations, then makes self-appraisal of these very appraisals themselves, and passes final judgment on the whole result".

Communication and community
See also: Scientific community and Scholarly communication

Frequently the scientific method is employed not only by a single person, but also by several people cooperating directly or indirectly. Such cooperation can be regarded as an important element of a scientific community. Various standards of scientific methodology are used within such an environment.
Peer review evaluation

Scientific journals use a process of peer review, in which scientists' manuscripts are submitted by editors of scientific journals to (usually one to three) fellow (usually anonymous) scientists familiar with the field for evaluation. In certain journals, the journal itself selects the referees; while in others (especially journals that are extremely specialized), the manuscript author might recommend referees. The referees may or may not recommend publication, or they might recommend publication with suggested modifications, or sometimes, publication in another journal. This standard is practiced to various degrees by different journals, and can have the effect of keeping the literature free of obvious errors and to generally improve the quality of the material, especially in the journals who use the standard most rigorously. The peer review process can have limitations when considering research outside the conventional scientific paradigm: problems of "groupthink" can interfere with open and fair deliberation of some new research.[103]
Documentation and replication
Main article: Reproducibility

Sometimes experimenters may make systematic errors during their experiments, veer from standard methods and practices (Pathological science) for various reasons, or, in rare cases, deliberately report false results. Occasionally because of this then, other scientists might attempt to repeat the experiments in order to duplicate the results.
Archiving

Researchers sometimes practice scientific data archiving, such as in compliance with the policies of government funding agencies and scientific journals. In these cases, detailed records of their experimental procedures, raw data, statistical analyses and source code can be preserved in order to provide evidence of the methodology and practice of the procedure and assist in any potential future attempts to reproduce the result. These procedural records may also assist in the conception of new experiments to test the hypothesis, and may prove useful to engineers who might examine the potential practical applications of a discovery.
Data sharing

When additional information is needed before a study can be reproduced, the author of the study might be asked to provide it. They might provide it, or if the author refuses to share data, appeals can be made to the journal editors who published the study or to the institution which funded the research.
Limitations

Since it is impossible for a scientist to record everything that took place in an experiment, facts selected for their apparent relevance are reported. This may lead, unavoidably, to problems later if some supposedly irrelevant feature is questioned. For example, Heinrich Hertz did not report the size of the room used to test Maxwell's equations, which later turned out to account for a small deviation in the results. The problem is that parts of the theory itself need to be assumed in order to select and report the experimental conditions. The observations are hence sometimes described as being 'theory-laden'.
Dimensions of practice
Further information: Rhetoric of science

The primary constraints on contemporary science are:

    Publication, i.e. Peer review
    Resources (mostly funding)

It has not always been like this: in the old days of the "gentleman scientist" funding (and to a lesser extent publication) were far weaker constraints.

Both of these constraints indirectly require scientific method – work that violates the constraints will be difficult to publish and difficult to get funded. Journals require submitted papers to conform to "good scientific practice" and to a degree this can be enforced by peer review. Originality, importance and interest are more important – see for example the author guidelines 
for Nature.
Philosophy and sociology of science
See also: Philosophy of science and Sociology of science

Philosophy of science looks at the underpinning logic of the scientific method, at what separates science from non-science, and the ethic that is implicit in science. There are basic assumptions, derived from philosophy by at least one prominent scientist, that form the base of the scientific method – namely, that reality is objective and consistent, that humans have the capacity to perceive reality accurately, and that rational explanations exist for elements of the real world.[104] These assumptions from methodological naturalism form a basis on which science may be grounded. Logical Positivist, empiricist, falsificationist, and other theories have criticized these assumptions and given alternative accounts of the logic of science, but each has also itself been criticized. More generally, the scientific method can be recognized as an idealization.[105]

Thomas Kuhn examined the history of science in his The Structure of Scientific Revolutions, and found that the actual method used by scientists differed dramatically from the then-espoused method. His observations of science practice are essentially sociological and do not speak to how science is or can be practiced in other times and other cultures.

Norwood Russell Hanson, Imre Lakatos and Thomas Kuhn have done extensive work on the "theory laden" character of observation. Hanson (1958) first coined the term for the idea that all observation is dependent on the conceptual framework of the observer, using the concept of gestalt to show how preconceptions can affect both observation and description.[106] He opens Chapter 1 with a discussion of the Golgi bodies and their initial rejection as an artefact of staining technique, and a discussion of Brahe and Kepler observing the dawn and seeing a "different" sun rise despite the same physiological phenomenon. Kuhn[107] and Feyerabend[108] acknowledge the pioneering significance of his work.

Kuhn (1961) said the scientist generally has a theory in mind before designing and undertaking experiments so as to make empirical observations, and that the "route from theory to measurement can almost never be traveled backward". This implies that the way in which theory is tested is dictated by the nature of the theory itself, which led Kuhn (1961, p. 166) to argue that "once it has been adopted by a profession ... no theory is recognized to be testable by any quantitative tests that it has not already passed".[109]

Paul Feyerabend similarly examined the history of science, and was led to deny that science is genuinely a methodological process. In his book Against Method he argues that scientific progress is not the result of applying any particular method. In essence, he says that for any specific method or norm of science, one can find a historic episode where violating it has contributed to the progress of science. Thus, if believers in scientific method wish to express a single universally valid rule, Feyerabend jokingly suggests, it should be 'anything goes'.[110] Criticisms such as his led to the strong programme, a radical approach to the sociology of science.

The postmodernist critiques of science have themselves been the subject of intense controversy. This ongoing debate, known as the science wars, is the result of conflicting values and assumptions between the postmodernist and realist camps. Whereas postmodernists assert that scientific knowledge is simply another discourse (note that this term has special meaning in this context) and not representative of any form of fundamental truth, realists in the scientific community maintain that scientific knowledge does reveal real and fundamental truths about reality. Many books have been written by scientists which take on this problem and challenge the assertions of the postmodernists while defending science as a legitimate method of deriving truth.[111]
Role of chance in discovery
Main article: Role of chance in scientific discoveries

Somewhere between 33% and 50% of all scientific discoveries are estimated to have been stumbled upon, rather than sought out. This may explain why scientists so often express that they were lucky.[112] Louis Pasteur is credited with the famous saying that "Luck favours the prepared mind", but some psychologists have begun to study what it means to be 'prepared for luck' in the scientific context. Research is showing that scientists are taught various heuristics that tend to harness chance and the unexpected.[112][113] This is what Nassim Nicholas Taleb calls "Anti-fragility"; while some systems of investigation are fragile in the face of human error, human bias, and randomness, the scientific method is more than resistant or tough – it actually benefits from such randomness in many ways (it is anti-fragile). Taleb believes that the more anti-fragile the system, the more it will flourish in the real world.[23]

Psychologist Kevin Dunbar says the process of discovery often starts with researchers finding bugs in their experiments. These unexpected results lead researchers to try to fix what they think is an error in their method. Eventually, the researcher decides the error is too persistent and systematic to be a coincidence. The highly controlled, cautious and curious aspects of the scientific method are thus what make it well suited for identifying such persistent systematic errors. At this point, the researcher will begin to think of theoretical explanations for the error, often seeking the help of colleagues across different domains of expertise.[112][113]
History
Main article: History of scientific method
See also: Timeline of the history of scientific method
Accuracy dispute
	This article appears to contradict the article History of scientific method. Please see discussion on the linked talk page. (June 2015) (Learn how and when to remove this template message)
	This section may contain an excessive amount of intricate detail that may only interest a specific audience. Please help by spinning off or relocating any relevant information, and removing excessive detail that may be against Wikipedia's inclusion policy. (June 2015) (Learn how and when to remove this template message)
Aristotle, 384 BCE – 322 BCE. "As regards his method, Aristotle is recognized as the inventor of scientific method because of his refined analysis of logical implications contained in demonstrative discourse, which goes well beyond natural logic and does not owe anything to the ones who philosophized before him." – Riccardo Pozzo[114]

The development of the scientific method emerges in the history of science itself. Ancient Egyptian documents describe empirical methods in astronomy,[115] mathematics,[116] and medicine.[117] The Greeks made contributions to the scientific method, most notably through Aristotle in his six works of logic collected as the Organon. Aristotle's inductive-deductive method used inductions from observations to infer general principles, deductions from those principles to check against further observations, and more cycles of induction and deduction to continue the advance of knowledge[118]

According to Karl Popper, Parmenides (fl. 5th century BCE) had conceived an axiomatic-deductive method.[119] According to David Lindberg, Aristotle (4th century BCE) wrote about the scientific method even if he and his followers did not actually follow what he said.[67] Lindberg also notes that Ptolemy (2nd century CE) and Ibn al-Haytham (11th century CE) are among the early examples of people who carried out scientific experiments.[120] Also, John Losee writes that "the Physics and the Metaphysics contain discussions of certain aspects of scientific method", of which, he says "Aristotle viewed scientific inquiry as a progression from observations to general principles and back to observations."[121]

Early Christian leaders such as Clement of Alexandria (150–215) and Basil of Caesarea (330–379) encouraged future generations to view the Greek wisdom as "handmaidens to theology" and science was considered a means to more accurate understanding of the Bible and of God.[122]:pp.4–5 Augustine of Hippo (354–430) who contributed great philosophical wealth to the Latin Middle Ages, advocated the study of science and was wary of philosophies that disagreed with the Bible, such as astrology and the Greek belief that the world had no beginning.[122]:p.5 This Christian accommodation with Greek science "laid a foundation for the later widespread, intensive study of natural philosophy during the Late Middle Ages."[122]:pp.8,9 However, the division of Latin-speaking Western Europe from the Greek-speaking East,[122]:p.18 followed by barbarian invasions, the Plague of Justinian, and the Islamic conquests,[123] resulted in the West largely losing access to Greek wisdom.

By the 8th century Islam had conquered the Christian lands[124] of Syria, Iraq, Iran and Egypt.[125] This swift conquest further severed Western Europe from many of the great works of Aristotle, Plato, Euclid and others, many of which were housed in the great library of Alexandria. Having come upon such a wealth of knowledge, the Arabs, who viewed non-Arab languages as inferior, even as a source of pollution,[126] employed conquered Christians and Jews to translate these works from the native Greek and Syriac into Arabic.[127]

Thus equipped, Arab philosopher Alhazen (Ibn al-Haytham) performed optical and physiological experiments, reported in his manifold works, the most famous being Book of Optics (1021).[128] He was thus a forerunner of scientific method, having understood that a controlled environment involving experimentation and measurement is required in order to draw educated conclusions. Other Arab polymaths of the same era produced copious works on mathematics, philosophy, astronomy and alchemy. Most stuck closely to Aristotle, being hesitant to admit that some of Aristotle's thinking was errant,[129] while others strongly criticized him.

During these years, occasionally a paraphrased translation from the Arabic, which itself had been translated from Greek and Syriac, might make its way to the West for scholarly study. It was not until 1204, during which the Latins conquered and took Constantinople from the Byzantines in the name of the fourth Crusade, that a renewed scholarly interest in the original Greek manuscripts began to grow. Due to the new easier access to the libraries of Constantinople by Western scholars, a certain revival in the study and analysis of the original Greek texts by Western scholars began.[130] From that point a functional scientific method that would launch modern science was on the horizon.

Grosseteste (1175–1253), an English statesman, scientist and Christian theologian, was "the principal figure" in bringing about "a more adequate method of scientific inquiry" by which "medieval scientists were able eventually to outstrip their ancient European and Muslim teachers" (Dales 1973, p. 62). ... His thinking influenced Roger Bacon, who spread Grosseteste's ideas from Oxford to the University of Paris during a visit there in the 1240s. From the prestigious universities in Oxford and Paris, the new experimental science spread rapidly throughout the medieval universities: "And so it went to Galileo, William Gilbert, Francis Bacon, William Harvey, Descartes, Robert Hooke, Newton, Leibniz, and the world of the seventeenth century" (Crombie 1953, p. 15). "So it went to us as well " (Gauch 2003, pp. 52–53).
Roger Bacon (c. 1214 – c. 1292) is sometimes credited as one of the earliest European advocates of the modern scientific method inspired by the works of Aristotle.[131]

Roger Bacon (c. 1214 – c. 1292), an English thinker and experimenter, is recognized by many to be the father of modern scientific method. His view that mathematics was essential to a correct understanding of natural philosophy was considered to be 400 years ahead of its time.[132]:2 He was viewed as "a lone genius proclaiming the truth about time," having correctly calculated the calendar[132]:3 His work in optics provided the platform on which Newton, Descartes, Huygens and others later transformed the science of light. Bacon's groundbreaking advances were due largely to his discovery that experimental science must be based on mathematics. (186–187) His works Opus Majus and De Speculis Comburentibus contain many "carefully drawn diagrams showing Bacon's meticulous investigations into the behavior of light."[132]:66 He gives detailed descriptions of systematic studies using prisms and measurements by which he shows how a rainbow functions.[132]:200

Others who advanced scientific method during this era included Albertus Magnus (c. 1193 – 1280), Theodoric of Freiberg, (c. 1250 – c. 1310), William of Ockham (c. 1285 – c. 1350), and Jean Buridan (c. 1300 – c. 1358). These were not only scientists but leaders of the church – Christian archbishops, friars and priests.

By the late 15th century, the physician-scholar Niccolò Leoniceno was finding errors in Pliny's Natural History. As a physician, Leoniceno was concerned about these botanical errors propagating to the materia medica on which medicines were based.[133] To counter this, a botanical garden was established at Orto botanico di Padova, University of Padua (in use for teaching by 1546), in order that medical students might have empirical access to the plants of a pharmacopia. The philosopher and physician Francisco Sanches was led by his medical training at Rome, 1571–73, and by the philosophical skepticism recently placed in the European mainstream by the publication of Sextus Empiricus' "Outlines of Pyrrhonism", to search for a true method of knowing (modus sciendi), as nothing clear can be known by the methods of Aristotle and his followers[134] – for example, syllogism fails upon circular reasoning. Following the physician Galen's method of medicine, Sanches lists the methods of judgement and experience, which are faulty in the wrong hands,[135] and we are left with the bleak statement That Nothing is Known (1581). This challenge was taken up by René Descartes in the next generation (1637), but at the least, Sanches warns us that we ought to refrain from the methods, summaries, and commentaries on Aristotle, if we seek scientific knowledge. In this, he is echoed by Francis Bacon, also influenced by skepticism; Sanches cites the humanist Juan Luis Vives who sought a better educational system, as well as a statement of human rights as a pathway for improvement of the lot of the poor.

The modern scientific method crystallized no later than in the 17th and 18th centuries. In his work Novum Organum (1620) – a reference to Aristotle's Organon – Francis Bacon outlined a new system of logic to improve upon the old philosophical process of syllogism.[136] Then, in 1637, René Descartes established the framework for scientific method's guiding principles in his treatise, Discourse on Method. The writings of Alhazen, Bacon and Descartes are considered critical in the historical development of the modern scientific method, as are those of John Stuart Mill.[137]

In the late 19th century, Charles Sanders Peirce proposed a schema that would turn out to have considerable influence in the development of current scientific methodology generally. Peirce accelerated the progress on several fronts. Firstly, speaking in broader context in "How to Make Our Ideas Clear" (1878) 
, Peirce outlined an objectively verifiable method to test the truth of putative knowledge on a way that goes beyond mere foundational alternatives, focusing upon both deduction and induction. He thus placed induction and deduction in a complementary rather than competitive context (the latter of which had been the primary trend at least since David Hume, who wrote in the mid-to-late 18th century). Secondly, and of more direct importance to modern method, Peirce put forth the basic schema for hypothesis/testing that continues to prevail today. Extracting the theory of inquiry from its raw materials in classical logic, he refined it in parallel with the early development of symbolic logic to address the then-current problems in scientific reasoning. Peirce examined and articulated the three fundamental modes of reasoning that, as discussed above in this article, play a role in inquiry today, the processes that are currently known as abductive, deductive, and inductive inference. Thirdly, he played a major role in the progress of symbolic logic itself – indeed this was his primary specialty.

Beginning in the 1930s, Karl Popper argued that there is no such thing as inductive reasoning.[138] All inferences ever made, including in science, are purely[139] deductive according to this view. Accordingly, he claimed that the empirical character of science has nothing to do with induction – but with the deductive property of falsifiability that scientific hypotheses have. Contrasting his views with inductivism and positivism, he even denied the existence of the scientific method: "(1) There is no method of discovering a scientific theory (2) There is no method for ascertaining the truth of a scientific hypothesis, i.e., no method of verification; (3) There is no method for ascertaining whether a hypothesis is 'probable', or probably true".[140] Instead, he held that there is only one universal method, a method not particular to science: The negative method of criticism, or colloquially termed trial and error. It covers not only all products of the human mind, including science, mathematics, philosophy, art and so on, but also the evolution of life. Following Peirce and others, Popper argued that science is fallible and has no authority.[140] In contrast to empiricist-inductivist views, he welcomed metaphysics and philosophical discussion and even gave qualified support to myths[141] and pseudosciences.[142] Popper's view has become known as critical rationalism.

Although science in a broad sense existed before the modern era, and in many historical civilizations (as described above), modern science is so distinct in its approach and successful in its results that it now defines what science is in the strictest sense of the term.[143]
Relationship with mathematics

Science is the process of gathering, comparing, and evaluating proposed models against observables. A model can be a simulation, mathematical or chemical formula, or set of proposed steps. Science is like mathematics in that researchers in both disciplines can clearly distinguish what is known from what is unknown at each stage of discovery. Models, in both science and mathematics, need to be internally consistent and also ought to be falsifiable (capable of disproof). In mathematics, a statement need not yet be proven; at such a stage, that statement would be called a conjecture. But when a statement has attained mathematical proof, that statement gains a kind of immortality which is highly prized by mathematicians, and for which some mathematicians devote their lives.[144]

Mathematical work and scientific work can inspire each other.[145] For example, the technical concept of time arose in science, and timelessness was a hallmark of a mathematical topic. But today, the Poincaré conjecture has been proven using time as a mathematical concept in which objects can flow (see Ricci flow).

Nevertheless, the connection between mathematics and reality (and so science to the extent it describes reality) remains obscure. Eugene Wigner's paper, The Unreasonable Effectiveness of Mathematics in the Natural Sciences, is a very well known account of the issue from a Nobel Prize-winning physicist. In fact, some observers (including some well known mathematicians such as Gregory Chaitin, and others such as Lakoff and Núñez) have suggested that mathematics is the result of practitioner bias and human limitation (including cultural ones), somewhat like the post-modernist view of science.

George Pólya's work on problem solving,[146] the construction of mathematical proofs, and heuristic[147][148] show that the mathematical method and the scientific method differ in detail, while nevertheless resembling each other in using iterative or recursive steps.
	Mathematical method 	Scientific method
1 	Understanding 	Characterization from experience and observation
2 	Analysis 	Hypothesis: a proposed explanation
3 	Synthesis 	Deduction: prediction from the hypothesis
4 	Review/Extend 	Test and experiment

In Pólya's view, understanding involves restating unfamiliar definitions in your own words, resorting to geometrical figures, and questioning what we know and do not know already; analysis, which Pólya takes from Pappus,[149] involves free and heuristic construction of plausible arguments, working backward from the goal, and devising a plan for constructing the proof; synthesis is the strict Euclidean exposition of step-by-step details[150] of the proof; review involves reconsidering and re-examining the result and the path taken to it.

Gauss, when asked how he came about his theorems, once replied "durch planmässiges Tattonieren" (through systematic palpable experimentation).[151]

Imre Lakatos argued that mathematicians actually use contradiction, criticism and revision as principles for improving their work.[152] In like manner to science, where truth is sought, but certainty is not found, in Proofs and refutations (1976), what Lakatos tried to establish was that no theorem of informal mathematics is final or perfect. This means that we should not think that a theorem is ultimately true, only that no counterexample has yet been found. Once a counterexample, i.e. an entity contradicting/not explained by the theorem is found, we adjust the theorem, possibly extending the domain of its validity. This is a continuous way our knowledge accumulates, through the logic and process of proofs and refutations. (If axioms are given for a branch of mathematics, however, Lakatos claimed that proofs from those axioms were tautological, i.e. logically true, by rewriting them, as did Poincaré (Proofs and Refutations, 1976).)

Lakatos proposed an account of mathematical knowledge based on Polya's idea of heuristics. In Proofs and Refutations, Lakatos gave several basic rules for finding proofs and counterexamples to conjectures. He thought that mathematical 'thought experiments' are a valid way to discover mathematical conjectures and proofs.[153]
Relationship with statistics

The scientific method has been extremely successful in bringing the world out of medieval times, especially once it was combined with industrial processes.[154] However, when the scientific method employs statistics as part of its arsenal, there are a number of both mathematical and practical issues that can have a deleterious effect on the reliability of the output of the scientific methods. This is outlined in detail in the most downloaded 2005 scientific paper "Why Most Published Research Findings Are False"[155] ever by John Ioannidis.

The particular points raised are statistical ("The smaller the studies conducted in a scientific field, the less likely the research findings are to be true" and "The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true.") and economical ("The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true" and "The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true.") Hence: "Most research findings are false for most research designs and for most fields" and "As shown, the majority of modern biomedical research is operating in areas with very low pre- and poststudy probability for true findings." However: "Nevertheless, most new discoveries will continue to stem from hypothesis-generating research with low or very low pre-study odds," which means that *new* discoveries will come from research that, when that research started, had low or very low odds (a low or very low chance) of succeeding. Hence, if the scientific method is used to expand the frontiers of knowledge, research into areas that are outside the mainstream will yield most new discoveries.
See also

    Armchair theorizing
    Confirmability
    Contingency
    Empirical limits in science
    Evidence-based medicine
    Fuzzy logic
    Inquiry
    Information theory
    Logic
    Methodology
        Historical
        Philosophical
        Phronetic
        Scholarly
    Operationalization
    Quantitative research
    Replication crisis
    Social research
    Statistical hypothesis testing
    Strong inference
    Testability

Problems and issues

    Demarcation problem
    Holistic science
    Inductive reasoning
    Junk science
    List of cognitive biases
    Normative science
    Occam's razor
    Pseudoscience
    Poverty of the stimulus
    Problem of induction
    Reference class problem
    Scientific misconduct
    Skeptical hypotheses
    Underdetermination

History, philosophy, sociology

    Epistemology
    Epistemic truth
    History of scientific method
    Mertonian norms
    Normal science
    Post-normal science
    Science studies
    Sociology of scientific knowledge
    Timeline of the history of scientific method


Engaging learners in the excitement of science, helping them discover the value of evidence-based reasoning and higher-order cognitive skills, and teaching them to become creative problem solvers have long been goals of science education reformers. But the means to achieve these goals, especially methods to promote creative thinking in scientific problem solving, have not become widely known or used. In this essay, I review the evidence that creativity is not a single hard-to-measure property. The creative process can be explained by reference to increasingly well-understood cognitive skills such as cognitive flexibility and inhibitory control that are widely distributed in the population. I explore the relationship between creativity and the higher-order cognitive skills, review assessment methods, and describe several instructional strategies for enhancing creative problem solving in the college classroom. Evidence suggests that instruction to support the development of creativity requires inquiry-based teaching that includes explicit strategies to promote cognitive flexibility. Students need to be repeatedly reminded and shown how to be creative, to integrate material across subject areas, to question their own assumptions, and to imagine other viewpoints and possibilities. Further research is required to determine whether college students' learning will be enhanced by these measures.
INTRODUCTION

Dr. Dunne paces in front of his section of first-year college students, today not as their Bio 110 teacher but in the role of facilitator in their monthly “invention session.” For this meeting, the topic is stem cell therapy in heart disease. Members of each team of four students have primed themselves on the topic by reading selected articles from accessible sources such as Science, Nature, and Scientific American, and searching the World Wide Web, triangulating for up-to-date, accurate, background information. Each team knows that their first goal is to define a set of problems or limitations to overcome within the topic and to begin to think of possible solutions. Dr. Dunne starts the conversation by reminding the group of the few ground rules: one speaker at a time, listen carefully and have respect for others' ideas, question your own and others' assumptions, focus on alternative paths or solutions, maintain an atmosphere of collaboration and mutual support. He then sparks the discussion by asking one of the teams to describe a problem in need of solution.

Science in the United States is widely credited as a major source of discovery and economic development. According to the 2005 TAP Report produced by a prominent group of corporate leaders, “To maintain our country's competitiveness in the twenty-first century, we must cultivate the skilled scientists and engineers needed to create tomorrow's innovations.” (www.tap2015.org/about/TAP_report2.pdf). A panel of scientists, engineers, educators, and policy makers convened by the National Research Council (NRC) concurred with this view, reporting that the vitality of the nation “is derived in large part from the productivity of well-trained people and the steady stream of scientific and technical innovations they produce” (NRC, 2007 blue right-pointing triangle).

For many decades, science education reformers have promoted the idea that learners should be engaged in the excitement of science; they should be helped to discover the value of evidence-based reasoning and higher-order cognitive skills, and be taught to become innovative problem solvers (for reviews, see DeHaan, 2005 blue right-pointing triangle; Hake, 2005 blue right-pointing triangle; Nelson, 2008 blue right-pointing triangle; Perkins and Wieman, 2008 blue right-pointing triangle). But the means to achieve these goals, especially methods to promote creative thinking in scientific problem solving, are not widely known or used. An invention session such as that led by the fictional Dr. Dunne, described above, may seem fanciful as a means of teaching students to think about science as something more than a body of facts and terms to memorize. In recent years, however, models for promoting creative problem solving were developed for classroom use, as detailed by Treffinger and Isaksen (2005) blue right-pointing triangle, and such techniques are often used in the real world of high technology. To promote imaginative thinking, the advertising executive Alex F. Osborn invented brainstorming (Osborn, 1948 blue right-pointing triangle, 1979 blue right-pointing triangle), a technique that has since been successful in stimulating inventiveness among engineers and scientists. Could such strategies be transferred to a class for college students? Could they serve as a supplement to a high-quality, scientific teaching curriculum that helps students learn the facts and conceptual frameworks of science and make progress along the novice–expert continuum? Could brainstorming or other instructional strategies that are specifically designed to promote creativity teach students to be more adaptive in their growing expertise, more innovative in their problem-solving abilities? To begin to answer those questions, we first need to understand what is meant by “creativity.”
OVERVIEW
What Is Creativity? Big-C versus Mini-C Creativity

How to define creativity is an age-old question. Justice Potter Stewart's famous dictum regarding obscenity “I know it when I see it” has also long been an accepted test of creativity. But this is not an adequate criterion for developing an instructional approach. A scientist colleague of mine recently noted that “Many of us [in the scientific community] rarely give the creative process a second thought, imagining one either ‘has it’ or doesn't.” We often think of inventiveness or creativity in scientific fields as the kind of gift associated with a Michelangelo or Einstein. This is what Kaufman and Beghetto (2008) blue right-pointing triangle call big-C creativity, borrowing the term that earlier workers applied to the talents of experts in various fields who were identified as particularly creative by their expert colleagues (MacKinnon, 1978 blue right-pointing triangle). In this sense, creativity is seen as the ability of individuals to generate new ideas that contribute substantially to an intellectual domain. Howard Gardner defined such a creative person as one who “regularly solves problems, fashions products, or defines new questions in a domain in a way that is initially considered novel but that ultimately comes to be accepted in a particular cultural setting” (Gardner, 1993 blue right-pointing triangle, p. 35).

But there is another level of inventiveness termed by various authors as “little-c” (Craft, 2000 blue right-pointing triangle) or “mini-c” (Kaufman and Beghetto, 2008 blue right-pointing triangle) creativity that is widespread among all populations. This would be consistent with the workplace definition of creativity offered by Amabile and her coworkers: “coming up with fresh ideas for changing products, services and processes so as to better achieve the organization's goals” (Amabile et al., 2005 blue right-pointing triangle). Mini-c creativity is based on what Craft calls “possibility thinking” (Craft, 2000 blue right-pointing triangle, pp. 3–4), as experienced when a worker suddenly has the insight to visualize a new, improved way to accomplish a task; it is represented by the “aha” moment when a student first sees two previously disparate concepts or facts in a new relationship, an example of what Arthur Koestler identified as bisociation: “perceiving a situation or event in two habitually incompatible associative contexts” (Koestler, 1964 blue right-pointing triangle, p. 95).

In this essay, I maintain that mini-c creativity is not a mysterious, innate endowment of rare individuals. Instead, I argue that creative thinking is a multicomponent process, mediated through social interactions, that can be explained by reference to increasingly well-understood mental abilities such as cognitive flexibility and cognitive control that are widely distributed in the population. Moreover, I explore some of the recent research evidence (though with no effort at a comprehensive literature review) showing that these mental abilities are teachable; like other higher-order cognitive skills (HOCS), they can be enhanced by explicit instruction.
Creativity Is a Multicomponent Process

Efforts to define creativity in psychological terms go back to J. P. Guilford (Guilford, 1950 blue right-pointing triangle) and E. P. Torrance (Torrance, 1974 blue right-pointing triangle), both of whom recognized that underlying the construct were other cognitive variables such as ideational fluency, originality of ideas, and sensitivity to missing elements. Many authors since then have extended the argument that a creative act is not a singular event but a process, an interplay among several interactive cognitive and affective elements. In this view, the creative act has two phases, a generative and an exploratory or evaluative phase (Finke et al., 1996 blue right-pointing triangle). During the generative process, the creative mind pictures a set of novel mental models as potential solutions to a problem. In the exploratory phase, we evaluate the multiple options and select the best one. Early scholars of creativity, such as J. P. Guilford, characterized the two phases as divergent thinking and convergent thinking (Guilford, 1950 blue right-pointing triangle). Guilford defined divergent thinking as the ability to produce a broad range of associations to a given stimulus or to arrive at many solutions to a problem (for overviews of the field from different perspectives, see Amabile, 1996 blue right-pointing triangle; Banaji et al., 2006 blue right-pointing triangle; Sawyer, 2006 blue right-pointing triangle). In neurocognitive terms, divergent thinking is referred to as associative richness (Gabora, 2002 blue right-pointing triangle; Simonton, 2004 blue right-pointing triangle), which is often measured experimentally by comparing the number of words that an individual generates from memory in response to stimulus words on a word association test. In contrast, convergent thinking refers to the capacity to quickly focus on the one best solution to a problem.

The idea that there are two stages to the creative process is consistent with results from cognition research indicating that there are two distinct modes of thought, associative and analytical (Neisser, 1963 blue right-pointing triangle; Sloman, 1996 blue right-pointing triangle). In the associative mode, thinking is defocused, suggestive, and intuitive, revealing remote or subtle connections between items that may be correlated, or may not, and are usually not causally related (Burton, 2008 blue right-pointing triangle). In the analytical mode, thought is focused and evaluative, more conducive to analyzing relationships of cause and effect (for a review of other cognitive aspects of creativity, see Runco, 2004 blue right-pointing triangle). Science educators associate the analytical mode with the upper levels (analysis, synthesis, and evaluation) of Bloom's taxonomy (e.g., Crowe et al., 2008 blue right-pointing triangle), or with “critical thinking,” the process that underlies the “purposeful, self-regulatory judgment that drives problem-solving and decision-making” (Quitadamo et al., 2008 blue right-pointing triangle, p. 328). These modes of thinking are under cognitive control through the executive functions of the brain. The core executive functions, which are thought to underlie all planning, problem solving, and reasoning, are defined (Blair and Razza, 2007 blue right-pointing triangle) as working memory control (mentally holding and retrieving information), cognitive flexibility (considering multiple ideas and seeing different perspectives), and inhibitory control (resisting several thoughts or actions to focus on one). Readers wishing to delve further into the neuroscience of the creative process can refer to the cerebrocerebellar theory of creativity (Vandervert et al., 2007 blue right-pointing triangle) in which these mental activities are described neurophysiologically as arising through interactions among different parts of the brain.

The main point from all of these works is that creativity is not some single hard-to-measure property or act. There is ample evidence that the creative process requires both divergent and convergent thinking and that it can be explained by reference to increasingly well-understood underlying mental abilities (Haring-Smith, 2006 blue right-pointing triangle; Kim, 2006 blue right-pointing triangle; Sawyer, 2006 blue right-pointing triangle; Kaufman and Sternberg, 2007 blue right-pointing triangle) and cognitive processes (Simonton, 2004 blue right-pointing triangle; Diamond et al., 2007 blue right-pointing triangle; Vandervert et al., 2007 blue right-pointing triangle).
Creativity Is Widely Distributed and Occurs in a Social Context

Although it is understandable to speak of an aha moment as a creative act by the person who experiences it, authorities in the field have long recognized (e.g., Simonton, 1975 blue right-pointing triangle) that creative thinking is not so much an individual trait but rather a social phenomenon involving interactions among people within their specific group or cultural settings. “Creativity isn't just a property of individuals, it is also a property of social groups” (Sawyer, 2006 blue right-pointing triangle, p. 305). Indeed, Osborn introduced his brainstorming method because he was convinced that group creativity is always superior to individual creativity. He drew evidence for this conclusion from activities that demand collaborative output, for example, the improvisations of a jazz ensemble. Although each musician is individually creative during a performance, the novelty and inventiveness of each performer's playing is clearly influenced, and often enhanced, by “social and interactional processes” among the musicians (Sawyer, 2006 blue right-pointing triangle, p. 120). Recently, Brophy (2006) blue right-pointing triangle offered evidence that for problem solving, the situation may be more nuanced. He confirmed that groups of interacting individuals were better at solving complex, multipart problems than single individuals. However, when dealing with certain kinds of single-issue problems, individual problem solvers produced a greater number of solutions than interacting groups, and those solutions were judged to be more original and useful.

Consistent with the findings of Brophy (2006) blue right-pointing triangle, many scholars acknowledge that creative discoveries in the real world such as solving the problems of cutting-edge science—which are usually complex and multipart—are influenced or even stimulated by social interaction among experts. The common image of the lone scientist in the laboratory experiencing a flash of creative inspiration is probably a myth from earlier days. As a case in point, the science historian Mara Beller analyzed the social processes that underlay some of the major discoveries of early twentieth-century quantum physics. Close examination of successive drafts of publications by members of the Copenhagen group revealed a remarkable degree of influence and collaboration among 10 or more colleagues, although many of these papers were published under the name of a single author (Beller, 1999 blue right-pointing triangle). Sociologists Bruno Latour and Steve Woolgar's study (Latour and Woolgar, 1986 blue right-pointing triangle) of a neuroendocrinology laboratory at the Salk Institute for Biological Studies make the related point that social interactions among the participating scientists determined to a remarkable degree what discoveries were made and how they were interpreted. In the laboratory, researchers studied the chemical structure of substances released by the brain. By analysis of the Salk scientists' verbalizations of concepts, theories, formulas, and results of their investigations, Latour and Woolgar showed that the structures and interpretations that were agreed upon, that is, the discoveries announced by the laboratory, were mediated by social interactions and power relationships among members of the laboratory group. By studying the discovery process in other fields of the natural sciences, sociologists and anthropologists have provided more cases that further illustrate how social and cultural dimensions affect scientific insights (for a thoughtful review, see Knorr Cetina, 1995 blue right-pointing triangle).

In sum, when an individual experiences an aha moment that feels like a singular creative act, it may rather have resulted from a multicomponent process, under the influence of group interactions and social context. The process that led up to what may be sensed as a sudden insight will probably have included at least three diverse, but testable elements: 1) divergent thinking, including ideational fluency or cognitive flexibility, which is the cognitive executive function that underlies the ability to visualize and accept many ideas related to a problem; 2) convergent thinking or the application of inhibitory control to focus and mentally evaluate ideas; and 3) analogical thinking, the ability to understand a novel idea in terms of one that is already familiar.
LITERATURE REVIEW
What Do We Know about How to Teach Creativity?

The possibility of teaching for creative problem solving gained credence in the 1960s with the studies of Jerome Bruner, who argued that children should be encouraged to “treat a task as a problem for which one invents an answer, rather than finding one out there in a book or on the blackboard” (Bruner, 1965 blue right-pointing triangle, pp. 1013–1014). Since that time, educators and psychologists have devised programs of instruction designed to promote creativity and inventiveness in virtually every student population: pre–K, elementary, high school, and college, as well as in disadvantaged students, athletes, and students in a variety of specific disciplines (for review, see Scott et al., 2004 blue right-pointing triangle). Smith (1998) blue right-pointing triangle identified 172 instructional approaches that have been applied at one time or another to develop divergent thinking skills.

Some of the most convincing evidence that elements of creativity can be enhanced by instruction comes from work with young children. Bodrova and Leong (2001) blue right-pointing triangle developed the Tools of the Mind (Tools) curriculum to improve all of the three core mental executive functions involved in creative problem solving: cognitive flexibility, working memory, and inhibitory control. In a year-long randomized study of 5-yr-olds from low-income families in 21 preschool classrooms, half of the teachers applied the districts' balanced literacy curriculum (literacy), whereas the experimenters trained the other half to teach the same academic content by using the Tools curriculum (Diamond et al., 2007 blue right-pointing triangle). At the end of the year, when the children were tested with a battery of neurocognitive tests including a test for cognitive flexibility (Durston et al., 2003 blue right-pointing triangle; Davidson et al., 2006 blue right-pointing triangle), those exposed to the Tools curriculum outperformed the literacy children by as much as 25% (Diamond et al., 2007 blue right-pointing triangle). Although the Tools curriculum and literacy program were similar in academic content and in many other ways, they differed primarily in that Tools teachers spent 80% of their time explicitly reminding the children to think of alternative ways to solve a problem and building their executive function skills.

Teaching older students to be innovative also demands instruction that explicitly promotes creativity but is rigorously content-rich as well. A large body of research on the differences between novice and expert cognition indicates that creative thinking requires at least a minimal level of expertise and fluency within a knowledge domain (Bransford et al., 2000 blue right-pointing triangle; Crawford and Brophy, 2006 blue right-pointing triangle). What distinguishes experts from novices, in addition to their deeper knowledge of the subject, is their recognition of patterns in information, their ability to see relationships among disparate facts and concepts, and their capacity for organizing content into conceptual frameworks or schemata (Bransford et al., 2000 blue right-pointing triangle; Sawyer, 2005 blue right-pointing triangle).

Such expertise is often lacking in the traditional classroom. For students attempting to grapple with new subject matter, many kinds of problems that are presented in high school or college courses or that arise in the real world can be solved merely by applying newly learned algorithms or procedural knowledge. With practice, problem solving of this kind can become routine and is often considered to represent mastery of a subject, producing what Sternberg refers to as “pseudoexperts” (Sternberg, 2003 blue right-pointing triangle). But beyond such routine use of content knowledge the instructor's goal must be to produce students who have gained the HOCS needed to apply, analyze, synthesize, and evaluate knowledge (Crowe et al., 2008 blue right-pointing triangle). The aim is to produce students who know enough about a field to grasp meaningful patterns of information, who can readily retrieve relevant knowledge from memory, and who can apply such knowledge effectively to novel problems. This condition is referred to as adaptive expertise (Hatano and Ouro, 2003 blue right-pointing triangle; Schwartz et al., 2005 blue right-pointing triangle). Instead of applying already mastered procedures, adaptive experts are able to draw on their knowledge to invent or adapt strategies for solving unique or novel problems within a knowledge domain. They are also able, ideally, to transfer conceptual frameworks and schemata from one domain to another (e.g., Schwartz et al., 2005 blue right-pointing triangle). Such flexible, innovative application of knowledge is what results in inventive or creative solutions to problems (Crawford and Brophy, 2006 blue right-pointing triangle; Crawford, 2007 blue right-pointing triangle).
Promoting Creative Problem Solving in the College Classroom

In most college courses, instructors teach science primarily through lectures and textbooks that are dominated by facts and algorithmic processing rather than by concepts, principles, and evidence-based ways of thinking. This is despite ample evidence that many students gain little new knowledge from traditional lectures (Hrepic et al., 2007 blue right-pointing triangle). Moreover, it is well documented that these methods engender passive learning rather than active engagement, boredom instead of intellectual excitement, and linear thinking rather than cognitive flexibility (e.g., Halpern and Hakel, 2003 blue right-pointing triangle; Nelson, 2008 blue right-pointing triangle; Perkins and Wieman, 2008 blue right-pointing triangle). Cognitive flexibility, as noted, is one of the three core mental executive functions involved in creative problem solving (Ausubel, 1963 blue right-pointing triangle, 2000 blue right-pointing triangle). The capacity to apply ideas creatively in new contexts, referred to as the ability to “transfer” knowledge (see Mestre, 2005 blue right-pointing triangle), requires that learners have opportunities to actively develop their own representations of information to convert it to a usable form. Especially when a knowledge domain is complex and fraught with ill-structured information, as in a typical introductory college biology course, instruction that emphasizes active-learning strategies is demonstrably more effective than traditional linear teaching in reducing failure rates and in promoting learning and transfer (e.g., Freeman et al., 2007 blue right-pointing triangle). Furthermore, there is already some evidence that inclusion of creativity training as part of a college curriculum can have positive effects. Hunsaker (2005) blue right-pointing triangle has reviewed a number of such studies. He cites work by McGregor (2001) blue right-pointing triangle, for example, showing that various creativity training programs including brainstorming and creative problem solving increase student scores on tests of creative-thinking abilities.

What explicit instructional strategies are available to promote creative problem solving? In addition to brainstorming, McFadzean (2002) blue right-pointing triangle discusses several “paradigm-stretching” techniques that can encourage creative ideas. One method, known as heuristic ideation, encourages participants to force together two unrelated concepts to discover novel relationships, a modern version of Koestler's bisociation (Koestler, 1964 blue right-pointing triangle). On the website of the Center for Development and Learning, Robert Sternberg and Wendy M. Williams offer 24 “tips” for teachers wishing to promote creativity in their students (Sternberg and Williams, 1998 blue right-pointing triangle). Among them, the following techniques might apply to a science classroom:

    Model creativity—students develop creativity when instructors model creative thinking and inventiveness.
    Repeatedly encourage idea generation—students need to be reminded to generate their own ideas and solutions in an environment free of criticism.
    Cross-fertilize ideas—where possible, avoid teaching in subject-area boxes: a math box, a social studies box, etc; students' creative ideas and insights often result from learning to integrate material across subject areas.
    Build self-efficacy—all students have the capacity to create and to experience the joy of having new ideas, but they must be helped to believe in their own capacity to be creative.
    Constantly question assumptions—make questioning a part of the daily classroom exchange; it is more important for students to learn what questions to ask and how to ask them than to learn the answers.
    Imagine other viewpoints—students broaden their perspectives by learning to reflect upon ideas and concepts from different points of view.

Although these strategies are all consistent with the knowledge about creativity that I have reviewed above, evidence from well-designed investigations to warrant the claim that they can enhance measurable indicators of creativity in college students is only recently beginning to materialize. If creativity most often occurs in “a mental state where attention is defocused, thought is associative, and a large number of mental representations are simultaneously activated” (Martindale, 1999 blue right-pointing triangle, p. 149), the question arises whether instructional strategies designed to enhance the HOCS also foster such a mental state? Do valid tests exist to show that creative problem solving can be enhanced by such instruction?
How Is Creativity Related to Critical Thinking and the Higher-Order Cognitive Skills?

It is not uncommon to associate creativity and ingenuity with scientific reasoning (Sawyer, 2005 blue right-pointing triangle; 2006 blue right-pointing triangle). When instructors apply scientific teaching strategies (Handelsman et al., 2004 blue right-pointing triangle; DeHaan, 2005 blue right-pointing triangle; Wood, 2009 blue right-pointing triangle) by using instructional methods based on learning research, according to Ebert-May and Hodder (2008 blue right-pointing triangle), “we see students actively engaged in the thinking, creativity, rigor, and experimentation we associate with the practice of science—in much the same way we see students learn in the field and in laboratories” (p. 2). Perkins and Wieman (2008) blue right-pointing triangle note that “To be successful innovators in science and engineering, students must develop a deep conceptual understanding of the underlying science ideas, an ability to apply these ideas and concepts broadly in different contexts, and a vision to see their relevance and usefulness in real-world applications … An innovator is able to perceive and realize potential connections and opportunities better than others” (pp. 181–182). The results of Scott et al. (2004) blue right-pointing triangle suggest that nontraditional courses in science that are based on constructivist principles and that use strategies of scientific teaching to promote the HOCS and enhance content mastery and dexterity in scientific thinking (Handelsman et al., 2007 blue right-pointing triangle; Nelson, 2008 blue right-pointing triangle) also should be effective in promoting creativity and cognitive flexibility if students are explicitly guided to learn these skills.

Creativity is an essential element of problem solving (Mumford et al., 1991 blue right-pointing triangle; Runco, 2004 blue right-pointing triangle) and of critical thinking (Abrami et al., 2008 blue right-pointing triangle). As such, it is common to think of applications of creativity such as inventiveness and ingenuity among the HOCS as defined in Bloom's taxonomy (Crowe et al., 2008 blue right-pointing triangle). Thus, it should come as no surprise that creativity, like other elements of the HOCS, can be taught most effectively through inquiry-based instruction, informed by constructivist theory (Ausubel, 1963 blue right-pointing triangle, 2000 blue right-pointing triangle; Duch et al., 2001 blue right-pointing triangle; Nelson, 2008 blue right-pointing triangle). In a survey of 103 instructors who taught college courses that included creativity instruction, Bull et al. (1995) blue right-pointing triangle asked respondents to rate the importance of various course characteristics for enhancing student creativity. Items ranking high on the list were: providing a social climate in which students feels safe, an open classroom environment that promotes tolerance for ambiguity and independence, the use of humor, metaphorical thinking, and problem defining. Many of the responses emphasized the same strategies as those advanced to promote creative problem solving (e.g., Mumford et al., 1991 blue right-pointing triangle; McFadzean, 2002 blue right-pointing triangle; Treffinger and Isaksen, 2005 blue right-pointing triangle) and critical thinking (Abrami et al., 2008 blue right-pointing triangle).

In a careful meta-analysis, Scott et al. (2004) blue right-pointing triangle examined 70 instructional interventions designed to enhance and measure creative performance. The results were striking. Courses that stressed techniques such as critical thinking, convergent thinking, and constraint identification produced the largest positive effect sizes. More open techniques that provided less guidance in strategic approaches had less impact on the instructional outcomes. A striking finding was the effectiveness of being explicit; approaches that clearly informed students about the nature of creativity and offered clear strategies for creative thinking were most effective. Approaches such as social modeling, cooperative learning, and case-based (project-based) techniques that required the application of newly acquired knowledge were found to be positively correlated to high effect sizes. The most clear-cut result to emerge from the Scott et al. (2004) blue right-pointing triangle study was simply to confirm that creativity instruction can be highly successful in enhancing divergent thinking, problem solving, and imaginative performance. Most importantly, of the various cognitive processes examined, those linked to the generation of new ideas such as problem finding, conceptual combination, and idea generation showed the greatest improvement. The success of creativity instruction, the authors concluded, can be attributed to “developing and providing guidance concerning the application of requisite cognitive capacities … [and] a set of heuristics or strategies for working with already available knowledge” (p. 382).

Many of the scientific teaching practices that have been shown by research to foster content mastery and HOCS, and that are coming more widely into use, also would be consistent with promoting creativity. Wood (2009) blue right-pointing triangle has recently reviewed examples of such practices and how to apply them. These include relatively small modifications of the traditional lecture to engender more active learning, such as the use of concept tests and peer instruction (Mazur, 1996 blue right-pointing triangle), Just-in-Time-Teaching techniques (Novak et al., 1999 blue right-pointing triangle), and student response systems known as “clickers” (Knight and Wood, 2005 blue right-pointing triangle; Crossgrove and Curran, 2008 blue right-pointing triangle), all designed to allow the instructor to frequently and effortlessly elicit and respond to student thinking. Other strategies can transform the lecture hall into a workshop or studio classroom (Gaffney et al., 2008 blue right-pointing triangle) where the teaching curriculum may emphasize problem-based (also known as project-based or case-based) learning strategies (Duch et al., 2001 blue right-pointing triangle; Ebert-May and Hodder, 2008 blue right-pointing triangle) or “community-based inquiry” in which students engage in research that enhances their critical-thinking skills (Quitadamo et al., 2008 blue right-pointing triangle).

Another important approach that could readily subserve explicit creativity instruction is the use of computer-based interactive simulations, or “sims” (Perkins and Wieman, 2008 blue right-pointing triangle) to facilitate inquiry learning and effective, easy self-assessment. An example in the biological sciences would be Neurons in Action (http://neuronsinaction.com/home/main). In such educational environments, students gain conceptual understanding of scientific ideas through interactive engagement with materials (real or virtual), with each other, and with instructors. Following the tenets of scientific teaching, students are encouraged to pose and answer their own questions, to make sense of the materials, and to construct their own understanding. The question I pose here is whether an additional focus—guiding students to meet these challenges in a context that explicitly promotes creativity—would enhance learning and advance students' progress toward adaptive expertise?
Assessment of Creativity

To teach creativity, there must be measurable indicators to judge how much students have gained from instruction. Educational programs intended to teach creativity became popular after the Torrance Tests of Creative Thinking (TTCT) was introduced in the 1960s (Torrance, 1974 blue right-pointing triangle). But it soon became apparent that there were major problems in devising tests for creativity, both because of the difficulty of defining the construct and because of the number and complexity of elements that underlie it. Tests of intelligence and other personality characteristics on creative individuals revealed a host of related traits such as verbal fluency, metaphorical thinking, flexible decision making, tolerance of ambiguity, willingness to take risks, autonomy, divergent thinking, self-confidence, problem finding, ideational fluency, and belief in oneself as being “creative” (Barron and Harrington, 1981 blue right-pointing triangle; Tardif and Sternberg, 1988 blue right-pointing triangle; Runco and Nemiro, 1994 blue right-pointing triangle; Snyder et al., 2004 blue right-pointing triangle). Many of these traits have been the focus of extensive research of recent decades, but, as noted above, creativity is not defined by any one trait; there is now reason to believe that it is the interplay among the cognitive and affective processes that underlie inventiveness and the ability to find novel solutions to a problem.

Although the early creativity researchers recognized that assessing divergent thinking as a measure of creativity required tests for other underlying capacities (Guilford, 1950 blue right-pointing triangle; Torrance, 1974 blue right-pointing triangle), these workers and their colleagues nonetheless believed that a high score for divergent thinking alone would correlate with real creative output. Unfortunately, no such correlation was shown (Barron and Harrington, 1981 blue right-pointing triangle). Results produced by many of the instruments initially designed to measure various aspects of creative thinking proved to be highly dependent on the test itself. A review of several hundred early studies showed that an individual's creativity score could be affected by simple test variables, for example, how the verbal pretest instructions were worded (Barron and Harrington, 1981 blue right-pointing triangle, pp. 442–443). Most scholars now agree that divergent thinking, as originally defined, was not an adequate measure of creativity. The process of creative thinking requires a complex combination of elements that include cognitive flexibility, memory control, inhibitory control, and analogical thinking, enabling the mind to free-range and analogize, as well as to focus and test.

More recently, numerous psychometric measures have been developed and empirically tested (see Plucker and Renzulli, 1999 blue right-pointing triangle) that allow more reliable and valid assessment of specific aspects of creativity. For example, the creativity quotient devised by Snyder et al. (2004) blue right-pointing triangle tests the ability of individuals to link different ideas and different categories of ideas into a novel synthesis. The Wallach–Kogan creativity test (Wallach and Kogan, 1965 blue right-pointing triangle) explores the uniqueness of ideas associated with a stimulus. For a more complete list and discussion, see the Creativity Tests website (www.indiana.edu/∼bobweb/Handout/cretv_6.html).

The most widely used measure of creativity is the TTCT, which has been modified four times since its original version in 1966 to take into account subsequent research. The TTCT-Verbal and the TTCT-Figural are two versions (Torrance, 1998 blue right-pointing triangle; see http://ststesting.com/2005giftttct.html). The TTCT-Verbal consists of five tasks; the “stimulus” for each task is a picture to which the test-taker responds briefly in writing. A sample task that can be viewed from the TTCT Demonstrator website asks, “Suppose that people could transport themselves from place to place with just a wink of the eye or a twitch of the nose. What might be some things that would happen as a result? You have 3 min.” (www.indiana.edu/∼bobweb/Handout/d3.ttct.htm).

In the TTCT-Figural, participants are asked to construct a picture from a stimulus in the form of a partial line drawing given on the test sheet (see example below; Figure 1). Specific instructions are to “Add lines to the incomplete figures below to make pictures out of them. Try to tell complete stories with your pictures. Give your pictures titles. You have 3 min.” In the introductory materials, test-takers are urged to “… think of a picture or object that no one else will think of. Try to make it tell as complete and as interesting a story as you can …” (Torrance et al., 2008 blue right-pointing triangle, p. 2).
Figure 1.
Figure 1.
Sample figural test item from the TTCT Demonstrator website (www.indiana.edu/∼bobweb/Handout/d3.ttct.htm).

How would an instructor in a biology course judge the creativity of students' responses to such an item? To assist in this task, the TTCT has scoring and norming guides (Torrance, 1998 blue right-pointing triangle; Torrance et al., 2008 blue right-pointing triangle) with numerous samples and responses representing different levels of creativity. The guides show sample evaluations based upon specific indicators such as fluency, originality, elaboration (or complexity), unusual visualization, extending or breaking boundaries, humor, and imagery. These examples are easy to use and provide a high degree of validity and generalizability to the tests. The TTCT has been more intensively researched and analyzed than any other creativity instrument, and the norming samples have longitudinal validations and high predictive validity over a wide age range. In addition to global creativity scores, the TTCT is designed to provide outcome measures in various domains and thematic areas to allow for more insightful analysis (Kaufman and Baer, 2006 blue right-pointing triangle). Kim (2006) blue right-pointing triangle has examined the characteristics of the TTCT, including norms, reliability, and validity, and concludes that the test is an accurate measure of creativity. When properly used, it has been shown to be fair in terms of gender, race, community status, and language background. According to Kim (2006) blue right-pointing triangle and other authorities in the field (McIntyre et al., 2003 blue right-pointing triangle; Scott et al., 2004 blue right-pointing triangle), Torrance's research and the development of the TTCT have provided groundwork for the idea that creative levels can be measured and then increased through instruction and practice.
SCIENTIFIC TEACHING TO PROMOTE CREATIVITY
How Could Creativity Instruction Be Integrated into Scientific Teaching?

Guidelines for designing specific course units that emphasize HOCS by using strategies of scientific teaching are now available from the current literature. As an example, Karen Cloud-Hansen and colleagues (Cloud-Hansen et al., 2008 blue right-pointing triangle) describe a course titled, “Ciprofloxacin Resistance in Neisseria gonorrhoeae.” They developed this undergraduate seminar to introduce college freshmen to important concepts in biology within a real-world context and to increase their content knowledge and critical-thinking skills. The centerpiece of the unit is a case study in which teams of students are challenged to take the role of a director of a local public health clinic. One of the county commissioners overseeing the clinic is an epidemiologist who wants to know “how you plan to address the emergence of ciprofloxacin resistance in Neisseria gonorrhoeae” (p. 304). State budget cuts limit availability of expensive antibiotics and some laboratory tests to patients. Student teams are challenged to 1) develop a plan to address the medical, economic, and political questions such a clinic director would face in dealing with ciprofloxacin-resistant N. gonorrhoeae; 2) provide scientific data to support their conclusions; and 3) describe their clinic plan in a one- to two-page referenced written report.

Throughout the 3-wk unit, in accordance with the principles of problem-based instruction (Duch et al., 2001 blue right-pointing triangle), course instructors encourage students to seek, interpret, and synthesize their own information to the extent possible. Students have access to a variety of instructional formats, and active-learning experiences are incorporated throughout the unit. These activities are interspersed among minilectures and give the students opportunities to apply new information to their existing base of knowledge. The active-learning activities emphasize the key concepts of the minilectures and directly confront common misconceptions about antibiotic resistance, gene expression, and evolution. Weekly classes include question/answer/discussion sessions to address student misconceptions and 20-min minilectures on such topics as antibiotic resistance, evolution, and the central dogma of molecular biology. Students gather information about antibiotic resistance in N. gonorrhoeae, epidemiology of gonorrhea, and treatment options for the disease, and each team is expected to formulate a plan to address ciprofloxacin resistance in N. gonorrhoeae.

In this project, the authors assessed student gains in terms of content knowledge regarding topics covered such as the role of evolution in antibiotic resistance, mechanisms of gene expression, and the role of oncogenes in human disease. They also measured HOCS as gains in problem solving, according to a rubric that assessed self-reported abilities to communicate ideas logically, solve difficult problems about microbiology, propose hypotheses, analyze data, and draw conclusions. Comparing the pre- and posttests, students reported significant learning of scientific content. Among the thinking skill categories, students demonstrated measurable gains in their ability to solve problems about microbiology but the unit seemed to have little impact on their more general perceived problem-solving skills (Cloud-Hansen et al., 2008 blue right-pointing triangle).

What would such a class look like with the addition of explicit creativity-promoting approaches? Would the gains in problem-solving abilities have been greater if during the minilectures and other activities, students had been introduced explicitly to elements of creative thinking from the Sternberg and Williams (1998) blue right-pointing triangle list described above? Would the students have reported greater gains if their instructors had encouraged idea generation with weekly brainstorming sessions; if they had reminded students to cross-fertilize ideas by integrating material across subject areas; built self-efficacy by helping students believe in their own capacity to be creative; helped students question their own assumptions; and encouraged students to imagine other viewpoints and possibilities? Of most relevance, could the authors have been more explicit in assessing the originality of the student plans? In an experiment that required college students to develop plans of a different, but comparable, type, Osborn and Mumford (2006) blue right-pointing triangle created an originality rubric (Figure 2) that could apply equally to assist instructors in judging student plans in any course. With such modifications, would student gains in problem-solving abilities or other HOCS have been greater? Would their plans have been measurably more imaginative?
Figure 2.
Figure 2.
Originality rubric (adapted from Osburn and Mumford, 2006 blue right-pointing triangle, p. 183).

Answers to these questions can only be obtained when a course like that described by Cloud-Hansen et al. (2008) blue right-pointing triangle is taught with explicit instruction in creativity of the type I described above. But, such answers could be based upon more than subjective impressions of the course instructors. For example, students could be pretested with items from the TTCT-Verbal or TTCT-Figural like those shown. If, during minilectures and at every contact with instructors, students were repeatedly reminded and shown how to be as creative as possible, to integrate material across subject areas, to question their own assumptions and imagine other viewpoints and possibilities, would their scores on TTCT posttest items improve? Would the plans they formulated to address ciprofloxacin resistance become more imaginative?

Recall that in their meta-analysis, Scott et al. (2004) blue right-pointing triangle found that explicitly informing students about the nature of creativity and offering strategies for creative thinking were the most effective components of instruction. From their careful examination of 70 experimental studies, they concluded that approaches such as social modeling, cooperative learning, and case-based (project-based) techniques that required the application of newly acquired knowledge were positively correlated with high effect sizes. The study was clear in confirming that explicit creativity instruction can be successful in enhancing divergent thinking and problem solving. Would the same strategies work for courses in ecology and environmental biology, as detailed by Ebert-May and Hodder (2008) blue right-pointing triangle, or for a unit elaborated by Knight and Wood (2005) blue right-pointing triangle that applies classroom response clickers?

Finally, I return to my opening question with the fictional Dr. Dunne. Could a weekly brainstorming “invention session” included in a course like those described here serve as the site where students are introduced to concepts and strategies of creative problem solving? As frequently applied in schools of engineering (Paulus and Nijstad, 2003 blue right-pointing triangle), brainstorming provides an opportunity for the instructor to pose a problem and to ask the students to suggest as many solutions as possible in a brief period, thus enhancing ideational fluency. Here, students can be encouraged explicitly to build on the ideas of others and to think flexibly. Would brainstorming enhance students' divergent thinking or creative abilities as measured by TTCT items or an originality rubric? Many studies have demonstrated that group interactions such as brainstorming, under the right conditions, can indeed enhance creativity (Paulus and Nijstad, 2003 blue right-pointing triangle; Scott et al., 2004 blue right-pointing triangle), but there is little information from an undergraduate science classroom setting. Intellectual Ventures, a firm founded by Nathan Myhrvold, the creator of Microsoft's Research Division, has gathered groups of engineers and scientists around a table for day-long sessions to brainstorm about a prearranged topic. Here, the method seems to work. Since it was founded in 2000, Intellectual Ventures has filed hundreds of patent applications in more than 30 technology areas, applying the “invention session” strategy (Gladwell, 2008 blue right-pointing triangle). Currently, the company ranks among the top 50 worldwide in number of patent applications filed annually. Whether such a technique could be applied successfully in a college science course will only be revealed by future research.
